public static void flume_f0_0(String[] args)
{    System.out.println("- Downloader started");    File baseDirectory = new File(args[0]);    System.out.println("- Using base directory: " + baseDirectory.getAbsolutePath());            File mavenWrapperPropertyFile = new File(baseDirectory, MAVEN_WRAPPER_PROPERTIES_PATH);    String url = DEFAULT_DOWNLOAD_URL;    if (mavenWrapperPropertyFile.exists()) {        FileInputStream mavenWrapperPropertyFileInputStream = null;        try {            mavenWrapperPropertyFileInputStream = new FileInputStream(mavenWrapperPropertyFile);            Properties mavenWrapperProperties = new Properties();            mavenWrapperProperties.load(mavenWrapperPropertyFileInputStream);            url = mavenWrapperProperties.getProperty(PROPERTY_NAME_WRAPPER_URL, url);        } catch (IOException e) {            System.out.println("- ERROR loading '" + MAVEN_WRAPPER_PROPERTIES_PATH + "'");        } finally {            try {                if (mavenWrapperPropertyFileInputStream != null) {                    mavenWrapperPropertyFileInputStream.close();                }            } catch (IOException e) {                        }        }    }    System.out.println("- Downloading from: : " + url);    File outputFile = new File(baseDirectory.getAbsolutePath(), MAVEN_WRAPPER_JAR_PATH);    if (!outputFile.getParentFile().exists()) {        if (!outputFile.getParentFile().mkdirs()) {            System.out.println("- ERROR creating output direcrory '" + outputFile.getParentFile().getAbsolutePath() + "'");        }    }    System.out.println("- Downloading to: " + outputFile.getAbsolutePath());    try {        downloadFileFromURL(url, outputFile);        System.out.println("Done");        System.exit(0);    } catch (Throwable e) {        System.out.println("- Error downloading");        e.printStackTrace();        System.exit(1);    }}
private static void flume_f1_0(String urlString, File destination) throws Exception
{    URL website = new URL(urlString);    ReadableByteChannel rbc;    rbc = Channels.newChannel(website.openStream());    FileOutputStream fos = new FileOutputStream(destination);    fos.getChannel().transferFrom(rbc, 0, Long.MAX_VALUE);    fos.close();    rbc.close();}
public static RpcClient flume_f2_0(Properties props)
{    ThriftRpcClient client = new SecureThriftRpcClient();    client.configure(props);    return client;}
protected void flume_f3_0(Properties properties) throws FlumeException
{    super.configure(properties);    serverPrincipal = properties.getProperty(SERVER_PRINCIPAL);    if (serverPrincipal == null || serverPrincipal.isEmpty()) {        throw new IllegalArgumentException("Flume in secure mode, but Flume config doesn't " + "specify a server principal to use for Kerberos auth.");    }    String clientPrincipal = properties.getProperty(CLIENT_PRINCIPAL);    String keytab = properties.getProperty(CLIENT_KEYTAB);    this.privilegedExecutor = FlumeAuthenticationUtil.getAuthenticator(clientPrincipal, keytab);    if (!privilegedExecutor.isAuthenticated()) {        throw new FlumeException("Authentication failed in Kerberos mode for " + "principal " + clientPrincipal + " keytab " + keytab);    }}
protected TTransport flume_f4_0(TSocket tsocket) throws Exception
{    Map<String, String> saslProperties = new HashMap<String, String>();    saslProperties.put(Sasl.QOP, "auth");    String[] names;    try {        names = FlumeAuthenticationUtil.splitKerberosName(serverPrincipal);    } catch (IOException e) {        throw new FlumeException("Error while trying to resolve Principal name - " + serverPrincipal, e);    }    return new UgiSaslClientTransport("GSSAPI", null, names[0], names[1], saslProperties, null, tsocket, privilegedExecutor);}
public void flume_f5_0() throws FlumeException
{    try {        this.privilegedExecutor.execute(new PrivilegedExceptionAction<Void>() {            public Void run() throws FlumeException {                                                callSuperClassOpen();                return null;            }        });    } catch (InterruptedException e) {        throw new FlumeException("Interrupted while opening underlying transport", e);    } catch (Exception e) {        throw new FlumeException("Failed to open SASL transport", e);    }}
public Void flume_f6_0() throws FlumeException
{            callSuperClassOpen();    return null;}
private void flume_f7_0() throws FlumeException
{    try {        super.open();    } catch (TTransportException e) {        throw new FlumeException("Failed to open SASL transport", e);    }}
public static synchronized FlumeAuthenticator flume_f8_0(String principal, String keytab) throws SecurityException
{    if (principal == null && keytab == null) {        return SimpleAuthenticator.getSimpleAuthenticator();    }    Preconditions.checkArgument(principal != null, "Principal can not be null when keytab is provided");    Preconditions.checkArgument(keytab != null, "Keytab can not be null when Principal is provided");    if (kerbAuthenticator == null) {        kerbAuthenticator = new KerberosAuthenticator();    }    kerbAuthenticator.authenticate(principal, keytab);    return kerbAuthenticator;}
public static CallbackHandler flume_f9_0()
{    return new SaslRpcServer.SaslGssCallbackHandler();}
public static String[] flume_f10_0(String principal) throws IOException
{    String resolvedPrinc = SecurityUtil.getServerPrincipal(principal, "");    return SaslRpcServer.splitKerberosName(resolvedPrinc);}
 static void flume_f11_0()
{    kerbAuthenticator = null;}
public T flume_f12_0(PrivilegedAction<T> action)
{    return privilegedExecutor.execute(action);}
public T flume_f13_0(PrivilegedExceptionAction<T> action) throws Exception
{    return privilegedExecutor.execute(action);}
public synchronized PrivilegedExecutor flume_f14_0(String proxyUserName)
{    if (proxyUserName == null || proxyUserName.isEmpty()) {        return this;    }    if (proxyCache.get(proxyUserName) == null) {        UserGroupInformation proxyUgi;        proxyUgi = UserGroupInformation.createProxyUser(proxyUserName, ugi);        printUGI(proxyUgi);        proxyCache.put(proxyUserName, new UGIExecutor(proxyUgi));    }    return proxyCache.get(proxyUserName);}
public boolean flume_f15_0()
{    return true;}
public synchronized void flume_f16_1(String principal, String keytab)
{        Preconditions.checkArgument(principal != null && !principal.isEmpty(), "Invalid Kerberos principal: " + String.valueOf(principal));    Preconditions.checkArgument(keytab != null && !keytab.isEmpty(), "Invalid Kerberos keytab: " + String.valueOf(keytab));    File keytabFile = new File(keytab);    Preconditions.checkArgument(keytabFile.isFile() && keytabFile.canRead(), "Keytab is not a readable file: " + String.valueOf(keytab));        String resolvedPrincipal;    try {                        resolvedPrincipal = SecurityUtil.getServerPrincipal(principal, "");    } catch (IOException e) {        throw new IllegalArgumentException("Host lookup error resolving kerberos principal (" + principal + "). Exception follows.", e);    }    Preconditions.checkNotNull(resolvedPrincipal, "Resolved Principal must not be null");                            KerberosUser newUser = new KerberosUser(resolvedPrincipal, keytab);    Preconditions.checkState(prevUser == null || prevUser.equals(newUser), "Cannot use multiple kerberos principals in the same agent. " + " Must restart agent to use new principal or keytab. " + "Previous = %s, New = %s", prevUser, newUser);        if (!UserGroupInformation.isSecurityEnabled()) {        Configuration conf = new Configuration(false);        conf.set(HADOOP_SECURITY_AUTHENTICATION, "kerberos");        UserGroupInformation.setConfiguration(conf);    }        UserGroupInformation curUser = null;    try {        curUser = UserGroupInformation.getLoginUser();        if (curUser != null && !curUser.hasKerberosCredentials()) {            curUser = null;        }    } catch (IOException e) {            }    /*     *  if ugi is not null,     *     if ugi matches currently logged in kerberos user, we are good     *     else we are logged out, so relogin our ugi     *  else if ugi is null, login and populate state     */    try {        if (ugi != null) {            if (curUser != null && curUser.getUserName().equals(ugi.getUserName())) {                            } else {                                ugi.reloginFromKeytab();            }        } else {                        UserGroupInformation.loginUserFromKeytab(resolvedPrincipal, keytab);            this.ugi = UserGroupInformation.getLoginUser();            this.prevUser = new KerberosUser(resolvedPrincipal, keytab);            this.privilegedExecutor = new UGIExecutor(this.ugi);        }    } catch (IOException e) {        throw new SecurityException("Authentication error while attempting to " + "login as kerberos principal (" + resolvedPrincipal + ") using " + "keytab (" + keytab + "). Exception follows.", e);    }    printUGI(this.ugi);}
private void flume_f17_1(UserGroupInformation ugi)
{    if (ugi != null) {                AuthenticationMethod authMethod = ugi.getAuthenticationMethod();            }}
public void flume_f18_1()
{        int CHECK_TGT_INTERVAL = 120;    ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);    scheduler.scheduleWithFixedDelay(new Runnable() {        @Override        public void run() {            try {                ugi.checkTGTAndReloginFromKeytab();            } catch (IOException e) {                            }        }    }, CHECK_TGT_INTERVAL, CHECK_TGT_INTERVAL, TimeUnit.SECONDS);}
public void flume_f19_1()
{    try {        ugi.checkTGTAndReloginFromKeytab();    } catch (IOException e) {            }}
 String flume_f20_0()
{    if (ugi != null) {        return ugi.getUserName();    } else {        return null;    }}
public String flume_f21_0()
{    return principal;}
public String flume_f22_0()
{    return keyTab;}
public boolean flume_f23_0(Object obj)
{    if (obj == null) {        return false;    }    if (getClass() != obj.getClass()) {        return false;    }    final KerberosUser other = (KerberosUser) obj;    if ((this.principal == null) ? (other.principal != null) : !this.principal.equals(other.principal)) {        return false;    }    if ((this.keyTab == null) ? (other.keyTab != null) : !this.keyTab.equals(other.keyTab)) {        return false;    }    return true;}
public int flume_f24_0()
{    int hash = 7;    hash = 41 * hash + (this.principal != null ? this.principal.hashCode() : 0);    hash = 41 * hash + (this.keyTab != null ? this.keyTab.hashCode() : 0);    return hash;}
public String flume_f25_0()
{    return "{ principal: " + principal + ", keytab: " + keyTab + " }";}
public static SimpleAuthenticator flume_f26_0()
{    return SimpleAuthenticatorHolder.authenticator;}
public T flume_f27_0(PrivilegedExceptionAction<T> action) throws Exception
{    return action.run();}
public T flume_f28_0(PrivilegedAction<T> action)
{    return action.run();}
public synchronized PrivilegedExecutor flume_f29_0(String proxyUserName)
{    if (proxyUserName == null || proxyUserName.isEmpty()) {        return this;    }    if (proxyCache.get(proxyUserName) == null) {        UserGroupInformation proxyUgi;        try {            proxyUgi = UserGroupInformation.createProxyUser(proxyUserName, UserGroupInformation.getCurrentUser());        } catch (IOException e) {            throw new SecurityException("Unable to create proxy User", e);        }        proxyCache.put(proxyUserName, new UGIExecutor(proxyUgi));    }    return proxyCache.get(proxyUserName);}
public boolean flume_f30_0()
{    return false;}
public void flume_f31_0()
{}
public T flume_f32_0(PrivilegedAction<T> action)
{    ensureValidAuth();    return ugi.doAs(action);}
public T flume_f33_0(PrivilegedExceptionAction<T> action) throws Exception
{    ensureValidAuth();    return ugi.doAs(action);}
private void flume_f34_0()
{    reloginUGI(ugi);    if (ugi.getAuthenticationMethod().equals(AuthenticationMethod.PROXY)) {        reloginUGI(ugi.getRealUser());    }}
private void flume_f35_0(UserGroupInformation ugi)
{    try {        if (ugi.hasKerberosCredentials()) {            long now = System.currentTimeMillis();            if (now - lastReloginAttempt < MIN_TIME_BEFORE_RELOGIN) {                return;            }            lastReloginAttempt = now;            ugi.checkTGTAndReloginFromKeytab();        }    } catch (IOException e) {        throw new SecurityException("Error trying to relogin from keytab for user " + ugi.getUserName(), e);    }}
 String flume_f36_0()
{    if (ugi != null) {        return ugi.getUserName();    } else {        return null;    }}
public static void flume_f37_0() throws Exception
{    workDir = new File(System.getProperty("test.dir", "target"), TestFlumeAuthenticator.class.getSimpleName());    flumeKeytab = new File(workDir, "flume.keytab");    aliceKeytab = new File(workDir, "alice.keytab");    conf = MiniKdc.createConf();    kdc = new MiniKdc(conf, workDir);    kdc.start();    kdc.createPrincipal(flumeKeytab, flumePrincipal);    flumePrincipal = flumePrincipal + "@" + kdc.getRealm();    kdc.createPrincipal(aliceKeytab, alicePrincipal);    alicePrincipal = alicePrincipal + "@" + kdc.getRealm();}
public static void flume_f38_0()
{    if (kdc != null) {        kdc.stop();    }}
public void flume_f39_0()
{        FlumeAuthenticationUtil.clearCredentials();}
public void flume_f40_0() throws IOException
{    String principal = null;    String keytab = null;    FlumeAuthenticator authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab);    assertFalse(authenticator.isAuthenticated());}
public void flume_f41_0() throws IOException
{    String principal = flumePrincipal;    String keytab = flumeKeytab.getAbsolutePath();    String expResult = principal;    FlumeAuthenticator authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab);    assertTrue(authenticator.isAuthenticated());    String result = ((KerberosAuthenticator) authenticator).getUserName();    assertEquals("Initial login failed", expResult, result);    authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab);    result = ((KerberosAuthenticator) authenticator).getUserName();    assertEquals("Re-login failed", expResult, result);    principal = alicePrincipal;    keytab = aliceKeytab.getAbsolutePath();    try {        authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab);        result = ((KerberosAuthenticator) authenticator).getUserName();        fail("Login should have failed with a new principal: " + result);    } catch (Exception ex) {        assertTrue("Login with a new principal failed, but for an unexpected " + "reason: " + ex.getMessage(), ex.getMessage().contains("Cannot use multiple kerberos principals"));    }}
public void flume_f42_0() throws Exception
{    String principal = flumePrincipal;    String keytab = flumeKeytab.getAbsolutePath();    FlumeAuthenticator authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab);    assertTrue(authenticator instanceof KerberosAuthenticator);    authenticator.execute(new PrivilegedExceptionAction<Object>() {        @Override        public Object run() throws Exception {            throw new IOException();        }    });}
public Object flume_f43_0() throws Exception
{    throw new IOException();}
public void flume_f44_0() throws Exception
{    FlumeAuthenticator authenticator = FlumeAuthenticationUtil.getAuthenticator(null, null);    assertTrue(authenticator instanceof SimpleAuthenticator);    authenticator.execute(new PrivilegedExceptionAction<Object>() {        @Override        public Object run() throws Exception {            throw new IOException();        }    });}
public Object flume_f45_0() throws Exception
{    throw new IOException();}
public void flume_f46_0() throws IOException
{    String username = "alice";    String expResult = username;    FlumeAuthenticator authenticator = FlumeAuthenticationUtil.getAuthenticator(null, null);    String result = ((UGIExecutor) (authenticator.proxyAs(username))).getUserName();    assertEquals("Proxy as didn't generate the expected username", expResult, result);    authenticator = FlumeAuthenticationUtil.getAuthenticator(flumePrincipal, flumeKeytab.getAbsolutePath());    String login = ((KerberosAuthenticator) authenticator).getUserName();    assertEquals("Login succeeded, but the principal doesn't match", flumePrincipal, login);    result = ((UGIExecutor) (authenticator.proxyAs(username))).getUserName();    assertEquals("Proxy as didn't generate the expected username", expResult, result);}
public void flume_f47_0() throws Exception
{    String principal = "flume";    File keytab = new File(workDir, "flume2.keytab");    kdc.createPrincipal(keytab, principal);    String expResult = principal + "@" + kdc.getRealm();    FlumeAuthenticator authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab.getAbsolutePath());    assertTrue(authenticator.isAuthenticated());    String result = ((KerberosAuthenticator) authenticator).getUserName();    assertEquals("Initial login failed", expResult, result);    authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab.getAbsolutePath());    result = ((KerberosAuthenticator) authenticator).getUserName();    assertEquals("Re-login failed", expResult, result);    principal = "alice";    keytab = aliceKeytab;    try {        authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab.getAbsolutePath());        result = ((KerberosAuthenticator) authenticator).getUserName();        fail("Login should have failed with a new principal: " + result);    } catch (Exception ex) {        assertTrue("Login with a new principal failed, but for an unexpected " + "reason: " + ex.getMessage(), ex.getMessage().contains("Cannot use multiple kerberos principals"));    }}
public boolean flume_f48_1() throws IOException, Exception
{        List<LogFile.SequentialReader> logReaders = Lists.newArrayList();    for (File logFile : logFiles) {        try {            logReaders.add(LogFileFactory.getSequentialReader(logFile, null, fsyncPerTransaction));        } catch (EOFException e) {                    }    }    long transactionIDSeed = 0;    long writeOrderIDSeed = 0;    try {        for (LogFile.SequentialReader log : logReaders) {            LogRecord entry;            int fileID = log.getLogFileID();            while ((entry = log.next()) != null) {                int offset = entry.getOffset();                TransactionEventRecord record = entry.getEvent();                long trans = record.getTransactionID();                long writeOrderID = record.getLogWriteOrderID();                transactionIDSeed = Math.max(trans, transactionIDSeed);                writeOrderIDSeed = Math.max(writeOrderID, writeOrderIDSeed);                if (record.getRecordType() == TransactionEventRecord.Type.PUT.get()) {                    uncommittedPuts.put(record.getTransactionID(), new ComparableFlumeEventPointer(new FlumeEventPointer(fileID, offset), record.getLogWriteOrderID()));                } else if (record.getRecordType() == TransactionEventRecord.Type.TAKE.get()) {                    Take take = (Take) record;                    uncommittedTakes.put(record.getTransactionID(), new ComparableFlumeEventPointer(new FlumeEventPointer(take.getFileID(), take.getOffset()), record.getLogWriteOrderID()));                } else if (record.getRecordType() == TransactionEventRecord.Type.COMMIT.get()) {                    Commit commit = (Commit) record;                    if (commit.getType() == TransactionEventRecord.Type.PUT.get()) {                        Set<ComparableFlumeEventPointer> puts = uncommittedPuts.get(record.getTransactionID());                        if (puts != null) {                            for (ComparableFlumeEventPointer put : puts) {                                if (!pendingTakes.remove(put)) {                                    committedPuts.add(put);                                }                            }                        }                    } else {                        Set<ComparableFlumeEventPointer> takes = uncommittedTakes.get(record.getTransactionID());                        if (takes != null) {                            for (ComparableFlumeEventPointer take : takes) {                                if (!committedPuts.remove(take)) {                                    pendingTakes.add(take);                                }                            }                        }                    }                } else if (record.getRecordType() == TransactionEventRecord.Type.ROLLBACK.get()) {                    if (uncommittedPuts.containsKey(record.getTransactionID())) {                        uncommittedPuts.removeAll(record.getTransactionID());                    } else {                        uncommittedTakes.removeAll(record.getTransactionID());                    }                }            }        }    } catch (Exception e) {                return false;    } finally {        TransactionIDOracle.setSeed(transactionIDSeed);        WriteOrderOracle.setSeed(writeOrderIDSeed);        for (LogFile.SequentialReader reader : logReaders) {            reader.close();        }    }    Set<ComparableFlumeEventPointer> sortedPuts = Sets.newTreeSet(committedPuts);    int count = 0;    for (ComparableFlumeEventPointer put : sortedPuts) {        queue.addTail(put.pointer);        count++;    }        return true;}
private void flume_f49_1() throws IOException
{    long checkpointLogOrderID = 0;    List<LogFile.MetaDataWriter> metaDataWriters = Lists.newArrayList();    for (File logFile : logFiles) {        String name = logFile.getName();        metaDataWriters.add(LogFileFactory.getMetaDataWriter(logFile, Integer.parseInt(name.substring(name.lastIndexOf('-') + 1))));    }    try {        if (queue.checkpoint(true)) {            checkpointLogOrderID = queue.getLogWriteOrderID();            for (LogFile.MetaDataWriter metaDataWriter : metaDataWriters) {                metaDataWriter.markCheckpoint(checkpointLogOrderID);            }        }    } catch (Exception e) {            } finally {        for (LogFile.MetaDataWriter metaDataWriter : metaDataWriters) {            metaDataWriter.close();        }    }}
public int flume_f50_0(ComparableFlumeEventPointer o)
{    if (orderID < o.orderID) {        return -1;    } else {                return 1;    }}
public int flume_f51_0()
{    return pointer.hashCode();}
public boolean flume_f52_0(Object o)
{    if (this == o) {        return true;    }    if (o == null) {        return false;    }    if (o.getClass() != this.getClass()) {        return false;    }    return pointer.equals(((ComparableFlumeEventPointer) o).pointer);}
public static void flume_f53_1(String[] args) throws Exception
{    Options options = new Options();    Option opt = new Option("c", true, "checkpoint directory");    opt.setRequired(true);    options.addOption(opt);    opt = new Option("l", true, "comma-separated list of log directories");    opt.setRequired(true);    options.addOption(opt);    options.addOption(opt);    opt = new Option("t", true, "capacity of the channel");    opt.setRequired(true);    options.addOption(opt);    CommandLineParser parser = new GnuParser();    CommandLine cli = parser.parse(options, args);    File checkpointDir = new File(cli.getOptionValue("c"));    String[] logDirs = cli.getOptionValue("l").split(",");    List<File> logFiles = Lists.newArrayList();    for (String logDir : logDirs) {        logFiles.addAll(LogUtils.getLogs(new File(logDir)));    }    int capacity = Integer.parseInt(cli.getOptionValue("t"));    File checkpointFile = new File(checkpointDir, "checkpoint");    if (checkpointFile.exists()) {            } else {        EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpointFile, capacity, "channel", new FileChannelCounter("Main"));        FlumeEventQueue queue = new FlumeEventQueue(backingStore, new File(checkpointDir, "inflighttakes"), new File(checkpointDir, "inflightputs"), new File(checkpointDir, Log.QUEUE_SET));        CheckpointRebuilder rebuilder = new CheckpointRebuilder(logFiles, queue, true);        if (rebuilder.rebuild()) {            rebuilder.writeCheckpoint();        } else {                    }    }}
public void flume_f54_0(DataInput in) throws IOException
{    super.readFields(in);    type = in.readShort();}
 void flume_f55_0(OutputStream out) throws IOException
{    ProtosFactory.Commit.Builder commitBuilder = ProtosFactory.Commit.newBuilder();    commitBuilder.setType(type);    commitBuilder.build().writeDelimitedTo(out);}
 void flume_f56_0(InputStream in) throws IOException
{    ProtosFactory.Commit commit = Preconditions.checkNotNull(ProtosFactory.Commit.parseDelimitedFrom(in), "Commit cannot be null");    type = (short) commit.getType();}
 short flume_f57_0()
{    return type;}
public void flume_f58_0(DataOutput out) throws IOException
{    super.write(out);    out.writeShort(type);}
 short flume_f59_0()
{    return Type.COMMIT.get();}
public String flume_f60_0()
{    StringBuilder builder = new StringBuilder();    builder.append("Commit [type=");    builder.append(type);    builder.append(", getLogWriteOrderID()=");    builder.append(getLogWriteOrderID());    builder.append(", getTransactionID()=");    builder.append(getTransactionID());    builder.append("]");    return builder.toString();}
public Encryptor.Builder<AESCTRNoPaddingEncryptor> flume_f61_0()
{    return new EncryptorBuilder();}
public Decryptor.Builder<AESCTRNoPaddingDecryptor> flume_f62_0()
{    return new DecryptorBuilder();}
public AESCTRNoPaddingEncryptor flume_f63_0()
{    ByteBuffer buffer = ByteBuffer.allocate(16);    byte[] seed = new byte[12];    SecureRandom random = new SecureRandom();    random.nextBytes(seed);    buffer.put(seed).putInt(1);    return new AESCTRNoPaddingEncryptor(key, buffer.array());}
public AESCTRNoPaddingDecryptor flume_f64_0()
{    return new AESCTRNoPaddingDecryptor(key, parameters);}
public byte[] flume_f65_0()
{    return parameters;}
public String flume_f66_0()
{    return TYPE;}
public byte[] flume_f67_0(byte[] clearText)
{    return doFinal(cipher, clearText);}
public byte[] flume_f68_0(byte[] cipherText)
{    return doFinal(cipher, cipherText);}
public String flume_f69_0()
{    return TYPE;}
private static byte[] flume_f70_1(Cipher cipher, byte[] input) throws DecryptionFailureException
{    try {        return cipher.doFinal(input);    } catch (Exception e) {        String msg = "Unable to encrypt or decrypt data " + TYPE + " input.length " + input.length;                throw new DecryptionFailureException(msg, e);    }}
private static Cipher flume_f71_1(Key key, int mode, byte[] parameters)
{    try {        Cipher cipher = Cipher.getInstance(TYPE);        cipher.init(mode, key, new IvParameterSpec(parameters));        return cipher;    } catch (Exception e) {        String msg = "Unable to load key using transformation: " + TYPE;        if (e instanceof InvalidKeyException) {            try {                int maxAllowedLen = Cipher.getMaxAllowedKeyLength(TYPE);                if (maxAllowedLen < 256) {                    msg += "; Warning: Maximum allowed key length = " + maxAllowedLen + " with the available JCE security policy files. Have you" + " installed the JCE unlimited strength jurisdiction policy" + " files?";                }            } catch (NoSuchAlgorithmException ex) {                msg += "; Unable to find specified algorithm?";            }        }                throw Throwables.propagate(e);    }}
public Builder<T> flume_f72_0(Key key)
{    this.key = Preconditions.checkNotNull(key, "key cannot be null");    return this;}
public Builder<T> flume_f73_0(Key key)
{    this.key = Preconditions.checkNotNull(key, "key cannot be null");    return this;}
public Builder<T> flume_f74_0(byte[] parameters)
{    this.parameters = parameters;    return this;}
public static CipherProvider.Encryptor flume_f75_0(String cipherProviderType, Key key)
{    if (cipherProviderType == null) {        return null;    }    CipherProvider provider = getProvider(cipherProviderType);    return provider.newEncryptorBuilder().setKey(key).build();}
public static CipherProvider.Decryptor flume_f76_0(String cipherProviderType, Key key, byte[] parameters)
{    if (cipherProviderType == null) {        return null;    }    CipherProvider provider = getProvider(cipherProviderType);    return provider.newDecryptorBuilder().setKey(key).setParameters(parameters).build();}
private static CipherProvider flume_f77_1(String cipherProviderType)
{    Preconditions.checkNotNull(cipherProviderType, "cipher provider type must not be null");        CipherProviderType type;    try {        type = CipherProviderType.valueOf(cipherProviderType.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {                type = CipherProviderType.OTHER;    }    Class<? extends CipherProvider> providerClass = type.getProviderClass();        if (providerClass == null) {        try {            Class c = Class.forName(cipherProviderType);            if (c != null && CipherProvider.class.isAssignableFrom(c)) {                providerClass = (Class<? extends CipherProvider>) c;            } else {                String errMessage = "Unable to instantiate provider from " + cipherProviderType;                                throw new FlumeException(errMessage);            }        } catch (ClassNotFoundException ex) {                        throw new FlumeException(ex);        }    }    try {        return providerClass.newInstance();    } catch (Exception ex) {        String errMessage = "Cannot instantiate provider: " + cipherProviderType;                throw new FlumeException(errMessage, ex);    }}
public Class<? extends CipherProvider> flume_f78_0()
{    return providerClass;}
public Key flume_f79_0(String alias)
{    String passwordFile = keyStorePasswordFile.getAbsolutePath();    try {        char[] keyPassword = keyStorePassword;        if (aliasPasswordFileMap.containsKey(alias)) {            File keyPasswordFile = aliasPasswordFileMap.get(alias);            keyPassword = Files.toString(keyPasswordFile, Charsets.UTF_8).trim().toCharArray();            passwordFile = keyPasswordFile.getAbsolutePath();        }        Key key = ks.getKey(alias, keyPassword);        if (key == null) {            throw new IllegalStateException("KeyStore returned null for " + alias);        }        return key;    } catch (Exception e) {        String msg = e.getClass().getName() + ": " + e.getMessage() + ". " + "Key = " + alias + ", passwordFile = " + passwordFile;        throw new RuntimeException(msg, e);    }}
public KeyProvider flume_f80_1(Context context)
{    String keyStoreFileName = context.getString(EncryptionConfiguration.JCE_FILE_KEY_STORE_FILE);    String keyStorePasswordFileName = context.getString(EncryptionConfiguration.JCE_FILE_KEY_STORE_PASSWORD_FILE);    Preconditions.checkState(!Strings.isNullOrEmpty(keyStoreFileName), "KeyStore file not specified");    Preconditions.checkState(!Strings.isNullOrEmpty(keyStorePasswordFileName), "KeyStore password  file not specified");    Map<String, File> aliasPasswordFileMap = Maps.newHashMap();    String passwordProtectedKeys = context.getString(EncryptionConfiguration.JCE_FILE_KEYS);    Preconditions.checkState(!Strings.isNullOrEmpty(passwordProtectedKeys), "Keys available to KeyStore was not specified or empty");    for (String passwordName : passwordProtectedKeys.trim().split("\\s+")) {        String propertyName = Joiner.on(".").join(EncryptionConfiguration.JCE_FILE_KEYS, passwordName, EncryptionConfiguration.JCE_FILE_KEY_PASSWORD_FILE);        String passwordFileName = context.getString(propertyName, keyStorePasswordFileName);        File passwordFile = new File(passwordFileName.trim());        if (passwordFile.isFile()) {            aliasPasswordFileMap.put(passwordName, passwordFile);        } else {                    }    }    File keyStoreFile = new File(keyStoreFileName.trim());    File keyStorePasswordFile = new File(keyStorePasswordFileName.trim());    return new JCEFileKeyProvider(keyStoreFile, keyStorePasswordFile, aliasPasswordFileMap);}
public static KeyProvider flume_f81_1(String keyProviderType, Context context)
{    Preconditions.checkNotNull(keyProviderType, "key provider type must not be null");        KeyProviderType type;    try {        type = KeyProviderType.valueOf(keyProviderType.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {                type = KeyProviderType.OTHER;    }    Class<? extends KeyProvider.Builder> providerClass = type.getBuilderClass();        if (providerClass == null) {        try {            Class c = Class.forName(keyProviderType);            if (c != null && KeyProvider.Builder.class.isAssignableFrom(c)) {                providerClass = (Class<? extends KeyProvider.Builder>) c;            } else {                String errMessage = "Unable to instantiate Builder from " + keyProviderType;                                throw new FlumeException(errMessage);            }        } catch (ClassNotFoundException ex) {                        throw new FlumeException(ex);        }    }        KeyProvider.Builder provider;    try {        provider = providerClass.newInstance();    } catch (InstantiationException ex) {        String errMessage = "Cannot instantiate builder: " + keyProviderType;                throw new FlumeException(errMessage, ex);    } catch (IllegalAccessException ex) {        String errMessage = "Cannot instantiate builder: " + keyProviderType;                throw new FlumeException(errMessage, ex);    }    return provider.build(context);}
public Class<? extends KeyProvider.Builder> flume_f82_0()
{    return keyProviderClass;}
 int flume_f83_0()
{    return queueSize;}
 void flume_f84_0(int size)
{    queueSize = size;}
 int flume_f85_0()
{    return queueHead;}
 void flume_f86_0(int head)
{    queueHead = head;}
 int flume_f87_0()
{    return capacity;}
 String flume_f88_0()
{    return name;}
protected void flume_f89_0(long logWriteOrderID)
{    this.logWriteOrderID = logWriteOrderID;}
 long flume_f90_0()
{    return logWriteOrderID;}
 static EventQueueBackingStore flume_f91_0(File checkpointFile, int capacity, String name, FileChannelCounter counter) throws Exception
{    return get(checkpointFile, capacity, name, counter, true);}
 static EventQueueBackingStore flume_f92_0(File checkpointFile, int capacity, String name, FileChannelCounter counter, boolean upgrade) throws Exception
{    return get(checkpointFile, null, capacity, name, counter, upgrade, false, false);}
 static EventQueueBackingStore flume_f93_1(File checkpointFile, File backupCheckpointDir, int capacity, String name, FileChannelCounter counter, boolean upgrade, boolean shouldBackup, boolean compressBackup) throws Exception
{    File metaDataFile = Serialization.getMetaDataFile(checkpointFile);    RandomAccessFile checkpointFileHandle = null;    try {        boolean checkpointExists = checkpointFile.exists();        boolean metaDataExists = metaDataFile.exists();        if (metaDataExists) {                        if (!checkpointExists || checkpointFile.length() == 0) {                                throw new BadCheckpointException("The last checkpoint was not completed correctly, " + "since Checkpoint file does not exist while metadata " + "file does.");            }        }                if (!checkpointExists) {            if (!checkpointFile.createNewFile()) {                throw new IOException("Cannot create " + checkpointFile);            }            return new EventQueueBackingStoreFileV3(checkpointFile, capacity, name, counter, backupCheckpointDir, shouldBackup, compressBackup);        }                if (metaDataExists) {            return new EventQueueBackingStoreFileV3(checkpointFile, capacity, name, counter, backupCheckpointDir, shouldBackup, compressBackup);        }        checkpointFileHandle = new RandomAccessFile(checkpointFile, "r");        int version = (int) checkpointFileHandle.readLong();        if (Serialization.VERSION_2 == version) {            if (upgrade) {                return upgrade(checkpointFile, capacity, name, backupCheckpointDir, shouldBackup, compressBackup, counter);            }            return new EventQueueBackingStoreFileV2(checkpointFile, capacity, name, counter);        }                throw new BadCheckpointException("Checkpoint file exists with " + Serialization.VERSION_3 + " but no metadata file found.");    } finally {        if (checkpointFileHandle != null) {            try {                checkpointFileHandle.close();            } catch (IOException e) {                            }        }    }}
private static EventQueueBackingStore flume_f94_1(File checkpointFile, int capacity, String name, File backupCheckpointDir, boolean shouldBackup, boolean compressBackup, FileChannelCounter counter) throws Exception
{        EventQueueBackingStoreFileV2 backingStoreV2 = new EventQueueBackingStoreFileV2(checkpointFile, capacity, name, counter);    String backupName = checkpointFile.getName() + "-backup-" + System.currentTimeMillis();    Files.copy(checkpointFile, new File(checkpointFile.getParentFile(), backupName));    File metaDataFile = Serialization.getMetaDataFile(checkpointFile);    EventQueueBackingStoreFileV3.upgrade(backingStoreV2, checkpointFile, metaDataFile);    return new EventQueueBackingStoreFileV3(checkpointFile, capacity, name, counter, backupCheckpointDir, shouldBackup, compressBackup);}
protected long flume_f95_0()
{    return elementsBuffer.get(INDEX_WRITE_ORDER_ID);}
protected void flume_f96_1(File backupDirectory) throws IOException
{    int availablePermits = backupCompletedSema.drainPermits();    Preconditions.checkState(availablePermits == 0, "Expected no permits to be available in the backup semaphore, " + "but " + availablePermits + " permits were available.");    if (slowdownBackup) {        try {            TimeUnit.SECONDS.sleep(10);        } catch (Exception ex) {            Throwables.propagate(ex);        }    }    File backupFile = new File(backupDirectory, BACKUP_COMPLETE_FILENAME);    if (backupExists(backupDirectory)) {        if (!backupFile.delete()) {            throw new IOException("Error while doing backup of checkpoint. Could " + "not remove" + backupFile.toString() + ".");        }    }    Serialization.deleteAllFiles(backupDirectory, Log.EXCLUDES);    File checkpointDir = checkpointFile.getParentFile();    File[] checkpointFiles = checkpointDir.listFiles();    Preconditions.checkNotNull(checkpointFiles, "Could not retrieve files " + "from the checkpoint directory. Cannot complete backup of the " + "checkpoint.");    for (File origFile : checkpointFiles) {        if (Log.EXCLUDES.contains(origFile.getName())) {            continue;        }        if (compressBackup && origFile.equals(checkpointFile)) {            Serialization.compressFile(origFile, new File(backupDirectory, origFile.getName() + COMPRESSED_FILE_EXTENSION));        } else {            Serialization.copyFile(origFile, new File(backupDirectory, origFile.getName()));        }    }    Preconditions.checkState(!backupFile.exists(), "The backup file exists " + "while it is not supposed to. Are multiple channels configured to use " + "this directory: " + backupDirectory.toString() + " as backup?");    if (!backupFile.createNewFile()) {            }}
public static boolean flume_f97_0(File checkpointDir, File backupDir) throws IOException
{    if (!backupExists(backupDir)) {        return false;    }    Serialization.deleteAllFiles(checkpointDir, Log.EXCLUDES);    File[] backupFiles = backupDir.listFiles();    if (backupFiles == null) {        return false;    } else {        for (File backupFile : backupFiles) {            String fileName = backupFile.getName();            if (!fileName.equals(BACKUP_COMPLETE_FILENAME) && !fileName.equals(Log.FILE_LOCK)) {                if (fileName.endsWith(COMPRESSED_FILE_EXTENSION)) {                    Serialization.decompressFile(backupFile, new File(checkpointDir, fileName.substring(0, fileName.lastIndexOf("."))));                } else {                    Serialization.copyFile(backupFile, new File(checkpointDir, fileName));                }            }        }        return true;    }}
 void flume_f98_1() throws IOException
{        if (shouldBackup) {        int permits = backupCompletedSema.drainPermits();        Preconditions.checkState(permits <= 1, "Expected only one or less " + "permits to checkpoint, but got " + String.valueOf(permits) + " permits");        if (permits < 1) {                        throw new IOException("Previous backup of checkpoint files is still " + "in progress. Will attempt to checkpoint only at the end of the " + "next checkpoint interval. Try increasing the checkpoint interval " + "if this error happens often.");        }    }        elementsBuffer.put(INDEX_CHECKPOINT_MARKER, CHECKPOINT_INCOMPLETE);    mappedBuffer.force();}
 void flume_f99_1() throws IOException
{    setLogWriteOrderID(WriteOrderOracle.next());        elementsBuffer.put(INDEX_WRITE_ORDER_ID, getLogWriteOrderID());    try {        writeCheckpointMetaData();    } catch (IOException e) {        throw new IOException("Error writing metadata", e);    }    Iterator<Integer> it = overwriteMap.keySet().iterator();    while (it.hasNext()) {        int index = it.next();        long value = overwriteMap.get(index);        elementsBuffer.put(index, value);        it.remove();    }    Preconditions.checkState(overwriteMap.isEmpty(), "concurrent update detected ");        elementsBuffer.put(INDEX_CHECKPOINT_MARKER, CHECKPOINT_COMPLETE);    mappedBuffer.force();    if (shouldBackup) {        startBackupThread();    }}
private void flume_f100_1()
{    Preconditions.checkNotNull(checkpointBackUpExecutor, "Expected the checkpoint backup exector to be non-null, " + "but it is null. Checkpoint will not be backed up.");        checkpointBackUpExecutor.submit(new Runnable() {        @Override        public void run() {            boolean error = false;            try {                backupCheckpoint(backupDir);            } catch (Throwable throwable) {                fileChannelCounter.incrementCheckpointBackupWriteErrorCount();                error = true;                            } finally {                backupCompletedSema.release();            }            if (!error) {                            }        }    });}
public void flume_f101_1()
{    boolean error = false;    try {        backupCheckpoint(backupDir);    } catch (Throwable throwable) {        fileChannelCounter.incrementCheckpointBackupWriteErrorCount();        error = true;            } finally {        backupCompletedSema.release();    }    if (!error) {            }}
 void flume_f102_1()
{    mappedBuffer.force();    try {        checkpointFileHandle.close();    } catch (IOException e) {            }    if (checkpointBackUpExecutor != null && !checkpointBackUpExecutor.isShutdown()) {        checkpointBackUpExecutor.shutdown();        try {                        while (!checkpointBackUpExecutor.awaitTermination(1, TimeUnit.SECONDS)) {            }        } catch (InterruptedException ex) {                    }    }}
 long flume_f103_0(int index)
{    int realIndex = getPhysicalIndex(index);    long result = EMPTY;    if (overwriteMap.containsKey(realIndex)) {        result = overwriteMap.get(realIndex);    } else {        result = elementsBuffer.get(realIndex);    }    return result;}
 ImmutableSortedSet<Integer> flume_f104_0()
{    return ImmutableSortedSet.copyOf(logFileIDReferenceCounts.keySet());}
 void flume_f105_0(int index, long value)
{    int realIndex = getPhysicalIndex(index);    overwriteMap.put(realIndex, value);}
 boolean flume_f106_0()
{    return overwriteMap.size() > 0;}
protected void flume_f107_0(int fileID)
{    AtomicInteger counter = logFileIDReferenceCounts.get(fileID);    if (counter == null) {        counter = new AtomicInteger(0);        logFileIDReferenceCounts.put(fileID, counter);    }    counter.incrementAndGet();}
protected void flume_f108_0(int fileID)
{    AtomicInteger counter = logFileIDReferenceCounts.get(fileID);    Preconditions.checkState(counter != null, "null counter ");    int count = counter.decrementAndGet();    if (count == 0) {        logFileIDReferenceCounts.remove(fileID);    }}
protected int flume_f109_0(int index)
{    return HEADER_SIZE + (getHead() + index) % getCapacity();}
protected static void flume_f110_0(File file, long totalBytes) throws IOException
{    RandomAccessFile checkpointFile = new RandomAccessFile(file, "rw");    boolean success = false;    try {        if (totalBytes <= MAX_ALLOC_BUFFER_SIZE) {            /*         * totalBytes <= MAX_ALLOC_BUFFER_SIZE, so this can be cast to int         * without a problem.         */            checkpointFile.write(new byte[(int) totalBytes]);        } else {            byte[] initBuffer = new byte[MAX_ALLOC_BUFFER_SIZE];            long remainingBytes = totalBytes;            while (remainingBytes >= MAX_ALLOC_BUFFER_SIZE) {                checkpointFile.write(initBuffer);                remainingBytes -= MAX_ALLOC_BUFFER_SIZE;            }            /*         * At this point, remainingBytes is < MAX_ALLOC_BUFFER_SIZE,         * so casting to int is fine.         */            if (remainingBytes > 0) {                checkpointFile.write(initBuffer, 0, (int) remainingBytes);            }        }        success = true;    } finally {        try {            checkpointFile.close();        } catch (IOException e) {            if (success) {                throw e;            }        }    }}
public static boolean flume_f111_0(File backupDir)
{    return new File(backupDir, BACKUP_COMPLETE_FILENAME).exists();}
public static void flume_f112_0(String[] args) throws Exception
{    File file = new File(args[0]);    File inflightTakesFile = new File(args[1]);    File inflightPutsFile = new File(args[2]);    File queueSetDir = new File(args[3]);    if (!file.exists()) {        throw new IOException("File " + file + " does not exist");    }    if (file.length() == 0) {        throw new IOException("File " + file + " is empty");    }    int capacity = (int) ((file.length() - (HEADER_SIZE * 8L)) / 8L);    EventQueueBackingStoreFile backingStore = (EventQueueBackingStoreFile) EventQueueBackingStoreFactory.get(file, capacity, "debug", new FileChannelCounter("Main"), false);    System.out.println("File Reference Counts" + backingStore.logFileIDReferenceCounts);    System.out.println("Queue Capacity " + backingStore.getCapacity());    System.out.println("Queue Size " + backingStore.getSize());    System.out.println("Queue Head " + backingStore.getHead());    for (int index = 0; index < backingStore.getCapacity(); index++) {        long value = backingStore.get(backingStore.getPhysicalIndex(index));        int fileID = (int) (value >>> 32);        int offset = (int) value;        System.out.println(index + ":" + Long.toHexString(value) + " fileID = " + fileID + ", offset = " + offset);    }    FlumeEventQueue queue = new FlumeEventQueue(backingStore, inflightTakesFile, inflightPutsFile, queueSetDir);    SetMultimap<Long, Long> putMap = queue.deserializeInflightPuts();    System.out.println("Inflight Puts:");    for (Long txnID : putMap.keySet()) {        Set<Long> puts = putMap.get(txnID);        System.out.println("Transaction ID: " + String.valueOf(txnID));        for (long value : puts) {            int fileID = (int) (value >>> 32);            int offset = (int) value;            System.out.println(Long.toHexString(value) + " fileID = " + fileID + ", offset = " + offset);        }    }    SetMultimap<Long, Long> takeMap = queue.deserializeInflightTakes();    System.out.println("Inflight takes:");    for (Long txnID : takeMap.keySet()) {        Set<Long> takes = takeMap.get(txnID);        System.out.println("Transaction ID: " + String.valueOf(txnID));        for (long value : takes) {            int fileID = (int) (value >>> 32);            int offset = (int) value;            System.out.println(Long.toHexString(value) + " fileID = " + fileID + ", offset = " + offset);        }    }}
protected int flume_f113_0()
{    return Serialization.VERSION_2;}
protected void flume_f114_0(int fileID)
{    super.incrementFileID(fileID);    Preconditions.checkState(logFileIDReferenceCounts.size() < MAX_ACTIVE_LOGS, "Too many active logs ");}
private Pair<Integer, Integer> flume_f115_0(long value)
{    int fileId = (int) (value >>> 32);    int count = (int) value;    return Pair.of(fileId, count);}
private long flume_f116_0(int fileId, int count)
{    long result = fileId;    result = (long) fileId << 32;    result += (long) count;    return result;}
protected void flume_f117_0()
{    elementsBuffer.put(INDEX_SIZE, getSize());    elementsBuffer.put(INDEX_HEAD, getHead());    List<Long> fileIdAndCountEncoded = new ArrayList<Long>();    for (Integer fileId : logFileIDReferenceCounts.keySet()) {        Integer count = logFileIDReferenceCounts.get(fileId).get();        long value = encodeActiveLogCounter(fileId, count);        fileIdAndCountEncoded.add(value);    }    int emptySlots = MAX_ACTIVE_LOGS - fileIdAndCountEncoded.size();    for (int i = 0; i < emptySlots; i++) {        fileIdAndCountEncoded.add(0L);    }    for (int i = 0; i < MAX_ACTIVE_LOGS; i++) {        elementsBuffer.put(i + INDEX_ACTIVE_LOG, fileIdAndCountEncoded.get(i));    }}
 File flume_f118_0()
{    return metaDataFile;}
protected int flume_f119_0()
{    return Serialization.VERSION_3;}
protected void flume_f120_1() throws IOException
{    ProtosFactory.Checkpoint.Builder checkpointBuilder = ProtosFactory.Checkpoint.newBuilder();    checkpointBuilder.setVersion(getVersion());    checkpointBuilder.setQueueHead(getHead());    checkpointBuilder.setQueueSize(getSize());    checkpointBuilder.setWriteOrderID(getLogWriteOrderID());    for (Integer logFileID : logFileIDReferenceCounts.keySet()) {        int count = logFileIDReferenceCounts.get(logFileID).get();        if (count != 0) {            ProtosFactory.ActiveLog.Builder activeLogBuilder = ProtosFactory.ActiveLog.newBuilder();            activeLogBuilder.setLogFileID(logFileID);            activeLogBuilder.setCount(count);            checkpointBuilder.addActiveLogs(activeLogBuilder.build());        }    }    FileOutputStream outputStream = new FileOutputStream(metaDataFile);    try {        checkpointBuilder.build().writeDelimitedTo(outputStream);        outputStream.getChannel().force(true);    } finally {        try {            outputStream.close();        } catch (IOException e) {                    }    }}
 static void flume_f121_1(EventQueueBackingStoreFileV2 backingStoreV2, File checkpointFile, File metaDataFile) throws IOException
{    int head = backingStoreV2.getHead();    int size = backingStoreV2.getSize();    long writeOrderID = backingStoreV2.getLogWriteOrderID();    Map<Integer, AtomicInteger> referenceCounts = backingStoreV2.logFileIDReferenceCounts;    ProtosFactory.Checkpoint.Builder checkpointBuilder = ProtosFactory.Checkpoint.newBuilder();    checkpointBuilder.setVersion(Serialization.VERSION_3);    checkpointBuilder.setQueueHead(head);    checkpointBuilder.setQueueSize(size);    checkpointBuilder.setWriteOrderID(writeOrderID);    for (Integer logFileID : referenceCounts.keySet()) {        int count = referenceCounts.get(logFileID).get();        if (count > 0) {            ProtosFactory.ActiveLog.Builder activeLogBuilder = ProtosFactory.ActiveLog.newBuilder();            activeLogBuilder.setLogFileID(logFileID);            activeLogBuilder.setCount(count);            checkpointBuilder.addActiveLogs(activeLogBuilder.build());        }    }    FileOutputStream outputStream = new FileOutputStream(metaDataFile);    try {        checkpointBuilder.build().writeDelimitedTo(outputStream);        outputStream.getChannel().force(true);    } finally {        try {            outputStream.close();        } catch (IOException e) {                    }    }    RandomAccessFile checkpointFileHandle = new RandomAccessFile(checkpointFile, "rw");    try {        checkpointFileHandle.seek(INDEX_VERSION * Serialization.SIZE_OF_LONG);        checkpointFileHandle.writeLong(Serialization.VERSION_3);        checkpointFileHandle.getChannel().force(true);    } finally {        try {            checkpointFileHandle.close();        } catch (IOException e) {                    }    }}
public static Event flume_f122_0(TransactionEventRecord transactionEventRecord)
{    if (transactionEventRecord instanceof Put) {        return ((Put) transactionEventRecord).getEvent();    }    return null;}
public synchronized void flume_f123_0(String name)
{    channelNameDescriptor = "[channel=" + name + "]";    super.setName(name);}
public void flume_f124_1(Context context)
{    useDualCheckpoints = context.getBoolean(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, FileChannelConfiguration.DEFAULT_USE_DUAL_CHECKPOINTS);    compressBackupCheckpoint = context.getBoolean(FileChannelConfiguration.COMPRESS_BACKUP_CHECKPOINT, FileChannelConfiguration.DEFAULT_COMPRESS_BACKUP_CHECKPOINT);    String homePath = System.getProperty("user.home").replace('\\', '/');    String strCheckpointDir = context.getString(FileChannelConfiguration.CHECKPOINT_DIR, homePath + "/.flume/file-channel/checkpoint").trim();    String strBackupCheckpointDir = context.getString(FileChannelConfiguration.BACKUP_CHECKPOINT_DIR, "").trim();    String[] strDataDirs = Iterables.toArray(Splitter.on(",").trimResults().omitEmptyStrings().split(context.getString(FileChannelConfiguration.DATA_DIRS, homePath + "/.flume/file-channel/data")), String.class);    checkpointDir = new File(strCheckpointDir);    if (useDualCheckpoints) {        Preconditions.checkState(!strBackupCheckpointDir.isEmpty(), "Dual checkpointing is enabled, but the backup directory is not set. " + "Please set " + FileChannelConfiguration.BACKUP_CHECKPOINT_DIR + " " + "to enable dual checkpointing");        backupCheckpointDir = new File(strBackupCheckpointDir);        /*       * If the backup directory is the same as the checkpoint directory,       * then throw an exception and force the config system to ignore this       * channel.       */        Preconditions.checkState(!backupCheckpointDir.equals(checkpointDir), "Could not configure " + getName() + ". The checkpoint backup " + "directory and the checkpoint directory are " + "configured to be the same.");    }    dataDirs = new File[strDataDirs.length];    for (int i = 0; i < strDataDirs.length; i++) {        dataDirs[i] = new File(strDataDirs[i]);    }    capacity = context.getInteger(FileChannelConfiguration.CAPACITY, FileChannelConfiguration.DEFAULT_CAPACITY);    if (capacity <= 0) {        capacity = FileChannelConfiguration.DEFAULT_CAPACITY;            }    keepAlive = context.getInteger(FileChannelConfiguration.KEEP_ALIVE, FileChannelConfiguration.DEFAULT_KEEP_ALIVE);    transactionCapacity = context.getInteger(FileChannelConfiguration.TRANSACTION_CAPACITY, FileChannelConfiguration.DEFAULT_TRANSACTION_CAPACITY);    if (transactionCapacity <= 0) {        transactionCapacity = FileChannelConfiguration.DEFAULT_TRANSACTION_CAPACITY;            }    Preconditions.checkState(transactionCapacity <= capacity, "File Channel transaction capacity cannot be greater than the " + "capacity of the channel.");    checkpointInterval = context.getLong(FileChannelConfiguration.CHECKPOINT_INTERVAL, FileChannelConfiguration.DEFAULT_CHECKPOINT_INTERVAL);    if (checkpointInterval <= 0) {                checkpointInterval = FileChannelConfiguration.DEFAULT_CHECKPOINT_INTERVAL;    }        maxFileSize = Math.min(context.getLong(FileChannelConfiguration.MAX_FILE_SIZE, FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE), FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE);    minimumRequiredSpace = Math.max(context.getLong(FileChannelConfiguration.MINIMUM_REQUIRED_SPACE, FileChannelConfiguration.DEFAULT_MINIMUM_REQUIRED_SPACE), FileChannelConfiguration.FLOOR_MINIMUM_REQUIRED_SPACE);    useLogReplayV1 = context.getBoolean(FileChannelConfiguration.USE_LOG_REPLAY_V1, FileChannelConfiguration.DEFAULT_USE_LOG_REPLAY_V1);    useFastReplay = context.getBoolean(FileChannelConfiguration.USE_FAST_REPLAY, FileChannelConfiguration.DEFAULT_USE_FAST_REPLAY);    Context encryptionContext = new Context(context.getSubProperties(EncryptionConfiguration.ENCRYPTION_PREFIX + "."));    String encryptionKeyProviderName = encryptionContext.getString(EncryptionConfiguration.KEY_PROVIDER);    encryptionActiveKey = encryptionContext.getString(EncryptionConfiguration.ACTIVE_KEY);    encryptionCipherProvider = encryptionContext.getString(EncryptionConfiguration.CIPHER_PROVIDER);    if (encryptionKeyProviderName != null) {        Preconditions.checkState(!Strings.isNullOrEmpty(encryptionActiveKey), "Encryption configuration problem: " + EncryptionConfiguration.ACTIVE_KEY + " is missing");        Preconditions.checkState(!Strings.isNullOrEmpty(encryptionCipherProvider), "Encryption configuration problem: " + EncryptionConfiguration.CIPHER_PROVIDER + " is missing");        Context keyProviderContext = new Context(encryptionContext.getSubProperties(EncryptionConfiguration.KEY_PROVIDER + "."));        encryptionKeyProvider = KeyProviderFactory.getInstance(encryptionKeyProviderName, keyProviderContext);    } else {        Preconditions.checkState(encryptionActiveKey == null, "Encryption configuration problem: " + EncryptionConfiguration.ACTIVE_KEY + " is present while key " + "provider name is not.");        Preconditions.checkState(encryptionCipherProvider == null, "Encryption configuration problem: " + EncryptionConfiguration.CIPHER_PROVIDER + " is present while " + "key provider name is not.");    }    fsyncPerTransaction = context.getBoolean(FileChannelConfiguration.FSYNC_PER_TXN, FileChannelConfiguration.DEFAULT_FSYNC_PRE_TXN);    fsyncInterval = context.getInteger(FileChannelConfiguration.FSYNC_INTERVAL, FileChannelConfiguration.DEFAULT_FSYNC_INTERVAL);    checkpointOnClose = context.getBoolean(FileChannelConfiguration.CHKPT_ONCLOSE, FileChannelConfiguration.DEFAULT_CHKPT_ONCLOSE);    if (queueRemaining == null) {        queueRemaining = new Semaphore(capacity, true);    }    if (log != null) {        log.setCheckpointInterval(checkpointInterval);        log.setMaxFileSize(maxFileSize);    }    if (channelCounter == null) {        channelCounter = new FileChannelCounter(getName());    }    channelCounter.setUnhealthy(0);}
public synchronized void flume_f125_1()
{        channelCounter.start();    try {        Builder builder = createLogBuilder();        log = builder.build();        log.replay();        setOpen(true);        int depth = getDepth();        Preconditions.checkState(queueRemaining.tryAcquire(depth), "Unable to acquire " + depth + " permits " + channelNameDescriptor);            } catch (Throwable t) {        setOpen(false);        channelCounter.setUnhealthy(1);        startupError = t;                if (t instanceof Error) {            throw (Error) t;        }    }    if (open) {        channelCounter.setChannelSize(getDepth());        channelCounter.setChannelCapacity(capacity);    }    super.start();}
 Builder flume_f126_0()
{    Builder builder = new Log.Builder();    builder.setCheckpointInterval(checkpointInterval);    builder.setMaxFileSize(maxFileSize);    builder.setMinimumRequiredSpace(minimumRequiredSpace);    builder.setQueueSize(capacity);    builder.setCheckpointDir(checkpointDir);    builder.setLogDirs(dataDirs);    builder.setChannelName(getName());    builder.setUseLogReplayV1(useLogReplayV1);    builder.setUseFastReplay(useFastReplay);    builder.setEncryptionKeyProvider(encryptionKeyProvider);    builder.setEncryptionKeyAlias(encryptionActiveKey);    builder.setEncryptionCipherProvider(encryptionCipherProvider);    builder.setUseDualCheckpoints(useDualCheckpoints);    builder.setCompressBackupCheckpoint(compressBackupCheckpoint);    builder.setBackupCheckpointDir(backupCheckpointDir);    builder.setFsyncPerTransaction(fsyncPerTransaction);    builder.setFsyncInterval(fsyncInterval);    builder.setCheckpointOnClose(checkpointOnClose);    builder.setChannelCounter(channelCounter);    return builder;}
public synchronized void flume_f127_1()
{        startupError = null;    int size = getDepth();    close();    if (!open) {        channelCounter.setChannelSize(size);        channelCounter.stop();    }    super.stop();}
public String flume_f128_0()
{    return "FileChannel " + getName() + " { dataDirs: " + Arrays.toString(dataDirs) + " }";}
protected BasicTransactionSemantics flume_f129_0()
{    if (!open) {        String msg = "Channel closed " + channelNameDescriptor;        if (startupError != null) {            msg += ". Due to " + startupError.getClass().getName() + ": " + startupError.getMessage();            throw new IllegalStateException(msg, startupError);        }        throw new IllegalStateException(msg);    }    FileBackedTransaction trans = transactions.get();    if (trans != null && !trans.isClosed()) {        Preconditions.checkState(false, "Thread has transaction which is still open: " + trans.getStateAsString() + channelNameDescriptor);    }    trans = new FileBackedTransaction(log, TransactionIDOracle.next(), transactionCapacity, keepAlive, queueRemaining, getName(), fsyncPerTransaction, channelCounter);    transactions.set(trans);    return trans;}
protected int flume_f130_0()
{    Preconditions.checkState(open, "Channel closed" + channelNameDescriptor);    Preconditions.checkNotNull(log, "log");    FlumeEventQueue queue = log.getFlumeEventQueue();    Preconditions.checkNotNull(queue, "queue");    return queue.getSize();}
 void flume_f131_1()
{    if (open) {        setOpen(false);        try {            log.close();        } catch (Exception e) {                        Throwables.propagate(e);        }        log = null;        queueRemaining = null;    }}
 boolean flume_f132_0()
{    return log.didFastReplay();}
 boolean flume_f133_0()
{    return log.didFullReplayDueToBadCheckpointException();}
public boolean flume_f134_0()
{    return open;}
private void flume_f135_0(boolean open)
{    this.open = open;    channelCounter.setOpen(this.open);}
 boolean flume_f136_0()
{    if (log != null) {        return log.backupRestored();    }    return false;}
 Log flume_f137_0()
{    return log;}
 FileChannelCounter flume_f138_0()
{    return channelCounter;}
public long flume_f139_0()
{    return transactionCapacity;}
private boolean flume_f140_0()
{    return State.CLOSED.equals(getState());}
private String flume_f141_0()
{    return String.valueOf(getState());}
protected void flume_f142_0(Event event) throws InterruptedException
{    channelCounter.incrementEventPutAttemptCount();    if (putList.remainingCapacity() == 0) {        throw new ChannelException("Put queue for FileBackedTransaction " + "of capacity " + putList.size() + " full, consider " + "committing more frequently, increasing capacity or " + "increasing thread count. " + channelNameDescriptor);    }        if (!queueRemaining.tryAcquire(keepAlive, TimeUnit.SECONDS)) {        throw new ChannelFullException("The channel has reached it's capacity. " + "This might be the result of a sink on the channel having too " + "low of batch size, a downstream system running slower than " + "normal, or that the channel capacity is just too low. " + channelNameDescriptor);    }    boolean success = false;    log.lockShared();    try {        FlumeEventPointer ptr = log.put(transactionID, event);        Preconditions.checkState(putList.offer(ptr), "putList offer failed " + channelNameDescriptor);        queue.addWithoutCommit(ptr, transactionID);        success = true;    } catch (IOException e) {        channelCounter.incrementEventPutErrorCount();        throw new ChannelException("Put failed due to IO error " + channelNameDescriptor, e);    } finally {        log.unlockShared();        if (!success) {                                    queueRemaining.release();        }    }}
protected Event flume_f143_1() throws InterruptedException
{    channelCounter.incrementEventTakeAttemptCount();    if (takeList.remainingCapacity() == 0) {        throw new ChannelException("Take list for FileBackedTransaction, capacity " + takeList.size() + " full, consider committing more frequently, " + "increasing capacity, or increasing thread count. " + channelNameDescriptor);    }    log.lockShared();    try {        while (true) {            FlumeEventPointer ptr = queue.removeHead(transactionID);            if (ptr == null) {                return null;            } else {                try {                                                            Preconditions.checkState(takeList.offer(ptr), "takeList offer failed " + channelNameDescriptor);                                        log.take(transactionID, ptr);                    Event event = log.get(ptr);                    return event;                } catch (IOException e) {                    channelCounter.incrementEventTakeErrorCount();                    throw new ChannelException("Take failed due to IO error " + channelNameDescriptor, e);                } catch (NoopRecordException e) {                                        takeList.remove(ptr);                } catch (CorruptEventException ex) {                    channelCounter.incrementEventTakeErrorCount();                    if (fsyncPerTransaction) {                        throw new ChannelException(ex);                    }                                        takeList.remove(ptr);                }            }        }    } finally {        log.unlockShared();    }}
protected void flume_f144_1() throws InterruptedException
{    int puts = putList.size();    int takes = takeList.size();    if (puts > 0) {        Preconditions.checkState(takes == 0, "nonzero puts and takes " + channelNameDescriptor);        log.lockShared();        try {            log.commitPut(transactionID);            channelCounter.addToEventPutSuccessCount(puts);            synchronized (queue) {                while (!putList.isEmpty()) {                    if (!queue.addTail(putList.removeFirst())) {                        StringBuilder msg = new StringBuilder();                        msg.append("Queue add failed, this shouldn't be able to ");                        msg.append("happen. A portion of the transaction has been ");                        msg.append("added to the queue but the remaining portion ");                        msg.append("cannot be added. Those messages will be consumed ");                        msg.append("despite this transaction failing. Please report.");                        msg.append(channelNameDescriptor);                                                Preconditions.checkState(false, msg.toString());                    }                }                queue.completeTransaction(transactionID);            }        } catch (IOException e) {            throw new ChannelException("Commit failed due to IO error " + channelNameDescriptor, e);        } finally {            log.unlockShared();        }    } else if (takes > 0) {        log.lockShared();        try {            log.commitTake(transactionID);            queue.completeTransaction(transactionID);            channelCounter.addToEventTakeSuccessCount(takes);        } catch (IOException e) {            throw new ChannelException("Commit failed due to IO error " + channelNameDescriptor, e);        } finally {            log.unlockShared();        }        queueRemaining.release(takes);    }    putList.clear();    takeList.clear();    channelCounter.setChannelSize(queue.getSize());}
protected void flume_f145_0() throws InterruptedException
{    int puts = putList.size();    int takes = takeList.size();    log.lockShared();    try {        if (takes > 0) {            Preconditions.checkState(puts == 0, "nonzero puts and takes " + channelNameDescriptor);            synchronized (queue) {                while (!takeList.isEmpty()) {                    Preconditions.checkState(queue.addHead(takeList.removeLast()), "Queue add failed, this shouldn't be able to happen " + channelNameDescriptor);                }            }        }        putList.clear();        takeList.clear();        queue.completeTransaction(transactionID);        channelCounter.setChannelSize(queue.getSize());        log.rollback(transactionID);    } catch (IOException e) {        throw new ChannelException("Commit failed due to IO error " + channelNameDescriptor, e);    } finally {        log.unlockShared();                        queueRemaining.release(puts);    }}
protected CharsetEncoder flume_f146_0()
{    return Charset.forName("UTF-8").newEncoder().onMalformedInput(CodingErrorAction.REPLACE).onUnmappableCharacter(CodingErrorAction.REPLACE);}
protected CharsetDecoder flume_f147_0()
{    return Charset.forName("UTF-8").newDecoder().onMalformedInput(CodingErrorAction.REPLACE).onUnmappableCharacter(CodingErrorAction.REPLACE);}
public Map<String, String> flume_f148_0()
{    return headers;}
public void flume_f149_0(Map<String, String> headers)
{    this.headers = headers;}
public byte[] flume_f150_0()
{    return body;}
public void flume_f151_0(byte[] body)
{    this.body = body;}
public void flume_f152_0(DataOutput out) throws IOException
{    out.writeByte(0);    Map<String, String> writeHeaders = getHeaders();    if (null != writeHeaders) {        out.writeInt(headers.size());        CharsetEncoder encoder = ENCODER_FACTORY.get();        for (String key : headers.keySet()) {            out.writeByte(EVENT_MAP_TEXT_WRITABLE_ID);            ByteBuffer keyBytes = encoder.encode(CharBuffer.wrap(key.toCharArray()));            int keyLength = keyBytes.limit();            WritableUtils.writeVInt(out, keyLength);            out.write(keyBytes.array(), 0, keyLength);            String value = headers.get(key);            out.write(EVENT_MAP_TEXT_WRITABLE_ID);            ByteBuffer valueBytes = encoder.encode(CharBuffer.wrap(value.toCharArray()));            int valueLength = valueBytes.limit();            WritableUtils.writeVInt(out, valueLength);            out.write(valueBytes.array(), 0, valueLength);        }    } else {        out.writeInt(0);    }    byte[] body = getBody();    if (body == null) {        out.writeInt(-1);    } else {        out.writeInt(body.length);        out.write(body);    }}
public void flume_f153_0(DataInput in) throws IOException
{        byte newClasses = in.readByte();        for (byte i = 0; i < newClasses; i++) {        in.readByte();        in.readUTF();    }    Map<String, String> newHeaders = new HashMap<String, String>();    int numEntries = in.readInt();    CharsetDecoder decoder = DECODER_FACTORY.get();    for (int i = 0; i < numEntries; i++) {        byte keyClassId = in.readByte();        assert (keyClassId == EVENT_MAP_TEXT_WRITABLE_ID);        int keyLength = WritableUtils.readVInt(in);        byte[] keyBytes = new byte[keyLength];        in.readFully(keyBytes, 0, keyLength);        String key = decoder.decode(ByteBuffer.wrap(keyBytes)).toString();        byte valueClassId = in.readByte();        assert (valueClassId == EVENT_MAP_TEXT_WRITABLE_ID);        int valueLength = WritableUtils.readVInt(in);        byte[] valueBytes = new byte[valueLength];        in.readFully(valueBytes, 0, valueLength);        String value = decoder.decode(ByteBuffer.wrap(valueBytes)).toString();        newHeaders.put(key, value);    }    setHeaders(newHeaders);    byte[] body = null;    int bodyLength = in.readInt();    if (bodyLength != -1) {        body = new byte[bodyLength];        in.readFully(body);    }    setBody(body);}
 static FlumeEvent flume_f154_0(DataInput in) throws IOException
{    FlumeEvent event = new FlumeEvent();    event.readFields(in);    return event;}
 int flume_f155_0()
{    return fileID;}
 int flume_f156_0()
{    return offset;}
public long flume_f157_0()
{    long result = fileID;    result = (long) fileID << 32;    result += (long) offset;    return result;}
public int flume_f158_0()
{    final int prime = 31;    int result = 1;    result = prime * result + fileID;    result = prime * result + offset;    return result;}
public boolean flume_f159_0(Object obj)
{    if (this == obj) {        return true;    }    if (obj == null) {        return false;    }    if (getClass() != obj.getClass()) {        return false;    }    FlumeEventPointer other = (FlumeEventPointer) obj;    if (fileID != other.fileID) {        return false;    }    if (offset != other.offset) {        return false;    }    return true;}
public String flume_f160_0()
{    return "FlumeEventPointer [fileID=" + fileID + ", offset=" + offset + "]";}
public static FlumeEventPointer flume_f161_0(long value)
{    int fileID = (int) (value >>> 32);    int offset = (int) value;    return new FlumeEventPointer(fileID, offset);}
 SetMultimap<Long, Long> flume_f162_0() throws IOException, BadCheckpointException
{    return inflightPuts.deserialize();}
 SetMultimap<Long, Long> flume_f163_0() throws IOException, BadCheckpointException
{    return inflightTakes.deserialize();}
 synchronized long flume_f164_0()
{    return backingStore.getLogWriteOrderID();}
 synchronized boolean flume_f165_1(boolean force) throws Exception
{    if (!backingStore.syncRequired() && !inflightTakes.syncRequired() && !force) {                                return false;    }    backingStore.beginCheckpoint();    inflightPuts.serializeAndWrite();    inflightTakes.serializeAndWrite();    backingStore.checkpoint();    return true;}
 synchronized FlumeEventPointer flume_f166_0(long transactionID)
{    if (backingStore.getSize() == 0) {        return null;    }    long value = remove(0, transactionID);    Preconditions.checkState(value != EMPTY, "Empty value " + channelNameDescriptor);    FlumeEventPointer ptr = FlumeEventPointer.fromLong(value);    backingStore.decrementFileID(ptr.getFileID());    return ptr;}
 synchronized boolean flume_f167_1(FlumeEventPointer e)
{        if (backingStore.getSize() == backingStore.getCapacity()) {                return false;    }    long value = e.toLong();    Preconditions.checkArgument(value != EMPTY);    backingStore.incrementFileID(e.getFileID());    add(0, value);    return true;}
 synchronized boolean flume_f168_0(FlumeEventPointer e)
{    if (getSize() == backingStore.getCapacity()) {        return false;    }    long value = e.toLong();    Preconditions.checkArgument(value != EMPTY);    backingStore.incrementFileID(e.getFileID());    add(backingStore.getSize(), value);    return true;}
 synchronized void flume_f169_0(FlumeEventPointer e, long transactionID)
{    inflightPuts.addEvent(transactionID, e.toLong());}
 synchronized boolean flume_f170_0(FlumeEventPointer e)
{    long value = e.toLong();    Preconditions.checkArgument(value != EMPTY);    if (queueSet == null) {        throw new IllegalStateException("QueueSet is null, thus replayComplete" + " has been called which is illegal");    }    if (!queueSet.contains(value)) {        return false;    }    searchCount++;    long start = System.currentTimeMillis();    for (int i = 0; i < backingStore.getSize(); i++) {        if (get(i) == value) {            remove(i, 0);            FlumeEventPointer ptr = FlumeEventPointer.fromLong(value);            backingStore.decrementFileID(ptr.getFileID());            searchTime += System.currentTimeMillis() - start;            return true;        }    }    searchTime += System.currentTimeMillis() - start;    return false;}
 synchronized SortedSet<Integer> flume_f171_0()
{                SortedSet<Integer> fileIDs = new TreeSet<Integer>(backingStore.getReferenceCounts());    fileIDs.addAll(inflightPuts.getFileIDs());    fileIDs.addAll(inflightTakes.getFileIDs());    return fileIDs;}
protected long flume_f172_0(int index)
{    if (index < 0 || index > backingStore.getSize() - 1) {        throw new IndexOutOfBoundsException(String.valueOf(index) + channelNameDescriptor);    }    return backingStore.get(index);}
private void flume_f173_0(int index, long value)
{    if (index < 0 || index > backingStore.getSize() - 1) {        throw new IndexOutOfBoundsException(String.valueOf(index) + channelNameDescriptor);    }    backingStore.put(index, value);}
protected boolean flume_f174_0(int index, long value)
{    if (index < 0 || index > backingStore.getSize()) {        throw new IndexOutOfBoundsException(String.valueOf(index) + channelNameDescriptor);    }    if (backingStore.getSize() == backingStore.getCapacity()) {        return false;    }    backingStore.setSize(backingStore.getSize() + 1);    if (index <= backingStore.getSize() / 2) {                backingStore.setHead(backingStore.getHead() - 1);        if (backingStore.getHead() < 0) {            backingStore.setHead(backingStore.getCapacity() - 1);        }        for (int i = 0; i < index; i++) {            set(i, get(i + 1));        }    } else {                for (int i = backingStore.getSize() - 1; i > index; i--) {            set(i, get(i - 1));        }    }    set(index, value);    if (queueSet != null) {        queueSet.add(value);    }    return true;}
 synchronized void flume_f175_0(long transactionID)
{    if (!inflightPuts.completeTransaction(transactionID)) {        inflightTakes.completeTransaction(transactionID);    }}
protected synchronized long flume_f176_0(int index, long transactionID)
{    if (index < 0 || index > backingStore.getSize() - 1) {        throw new IndexOutOfBoundsException("index = " + index + ", queueSize " + backingStore.getSize() + " " + channelNameDescriptor);    }    copyCount++;    long start = System.currentTimeMillis();    long value = get(index);    if (queueSet != null) {        queueSet.remove(value);    }        if (transactionID != 0) {        inflightTakes.addEvent(transactionID, value);    }    if (index > backingStore.getSize() / 2) {                for (int i = index; i < backingStore.getSize() - 1; i++) {            long rightValue = get(i + 1);            set(i, rightValue);        }        set(backingStore.getSize() - 1, EMPTY);    } else {                for (int i = index - 1; i >= 0; i--) {            long leftValue = get(i);            set(i + 1, leftValue);        }        set(0, EMPTY);        backingStore.setHead(backingStore.getHead() + 1);        if (backingStore.getHead() == backingStore.getCapacity()) {            backingStore.setHead(0);        }    }    backingStore.setSize(backingStore.getSize() - 1);    copyTime += System.currentTimeMillis() - start;    return value;}
protected synchronized int flume_f177_0()
{    return backingStore.getSize() + inflightTakes.getSize();}
public int flume_f178_0()
{    return backingStore.getCapacity();}
 synchronized void flume_f179_1() throws IOException
{    try {        if (db != null) {            db.close();        }    } catch (Exception ex) {            }    try {        backingStore.close();        inflightPuts.close();        inflightTakes.close();    } catch (IOException e) {            }}
 synchronized void flume_f180_1()
{    String msg = "Search Count = " + searchCount + ", Search Time = " + searchTime + ", Copy Count = " + copyCount + ", Copy Time = " + copyTime;        if (db != null) {        db.close();    }    queueSet = null;    db = null;}
 long flume_f181_0()
{    return searchCount;}
 long flume_f182_0()
{    return copyCount;}
public boolean flume_f183_0(Long transactionID)
{    if (!inflightEvents.containsKey(transactionID)) {        return false;    }    inflightEvents.removeAll(transactionID);    inflightFileIDs.removeAll(transactionID);    syncRequired = true;    return true;}
public void flume_f184_0(Long transactionID, Long pointer)
{    inflightEvents.put(transactionID, pointer);    inflightFileIDs.put(transactionID, FlumeEventPointer.fromLong(pointer).getFileID());    syncRequired = true;}
public void flume_f185_1() throws Exception
{    Collection<Long> values = inflightEvents.values();    if (!fileChannel.isOpen()) {        file = new RandomAccessFile(inflightEventsFile, "rw");        fileChannel = file.getChannel();    }    if (values.isEmpty()) {        file.setLength(0L);    }    try {        int expectedFileSize = ((        (inflightEvents.keySet().size() * 2) +         values.size()) *         8) +         16;                        file.setLength(expectedFileSize);        Preconditions.checkState(file.length() == expectedFileSize, "Expected File size of inflight events file does not match the " + "current file size. Checkpoint is incomplete.");        file.seek(0);        final ByteBuffer buffer = ByteBuffer.allocate(expectedFileSize);        LongBuffer longBuffer = buffer.asLongBuffer();        for (Long txnID : inflightEvents.keySet()) {            Set<Long> pointers = inflightEvents.get(txnID);            longBuffer.put(txnID);            longBuffer.put((long) pointers.size());                        long[] written = ArrayUtils.toPrimitive(pointers.toArray(new Long[0]));            longBuffer.put(written);        }        byte[] checksum = digest.digest(buffer.array());        file.write(checksum);        buffer.position(0);        fileChannel.write(buffer);        fileChannel.force(true);        syncRequired = false;    } catch (IOException ex) {                throw ex;    }}
public SetMultimap<Long, Long> flume_f186_1() throws IOException, BadCheckpointException
{    SetMultimap<Long, Long> inflights = HashMultimap.create();    if (!fileChannel.isOpen()) {        file = new RandomAccessFile(inflightEventsFile, "rw");        fileChannel = file.getChannel();    }    if (file.length() == 0) {        return inflights;    }    file.seek(0);    byte[] checksum = new byte[16];    file.read(checksum);    ByteBuffer buffer = ByteBuffer.allocate((int) (file.length() - file.getFilePointer()));    fileChannel.read(buffer);    byte[] fileChecksum = digest.digest(buffer.array());    if (!Arrays.equals(checksum, fileChecksum)) {        throw new BadCheckpointException("Checksum of inflights file differs" + " from the checksum expected.");    }    buffer.position(0);    LongBuffer longBuffer = buffer.asLongBuffer();    try {        while (true) {            long txnID = longBuffer.get();            int numEvents = (int) (longBuffer.get());            for (int i = 0; i < numEvents; i++) {                long val = longBuffer.get();                inflights.put(txnID, val);            }        }    } catch (BufferUnderflowException ex) {            }    return inflights;}
public int flume_f187_0()
{    return inflightEvents.size();}
public boolean flume_f188_0()
{    return syncRequired;}
public Collection<Integer> flume_f189_0()
{    return inflightFileIDs.values();}
public Collection<Long> flume_f190_0()
{    return inflightEvents.values();}
public void flume_f191_0() throws IOException
{    file.close();}
public boolean flume_f192_0()
{    return open;}
public void flume_f193_0(boolean open)
{    this.open = open;}
public int flume_f194_0()
{    return open ? 0 : 1;}
public int flume_f195_0()
{    return unhealthy;}
public void flume_f196_0(int unhealthy)
{    this.unhealthy = unhealthy;}
public long flume_f197_0()
{    return get(EVENT_PUT_ERROR_COUNT);}
public void flume_f198_0()
{    increment(EVENT_PUT_ERROR_COUNT);}
public long flume_f199_0()
{    return get(EVENT_TAKE_ERROR_COUNT);}
public void flume_f200_0()
{    increment(EVENT_TAKE_ERROR_COUNT);}
public long flume_f201_0()
{    return get(CHECKPOINT_WRITE_ERROR_COUNT);}
public void flume_f202_0()
{    increment(CHECKPOINT_WRITE_ERROR_COUNT);}
public long flume_f203_0()
{    return get(CHECKPOINT_BACKUP_WRITE_ERROR_COUNT);}
public void flume_f204_0()
{    increment(CHECKPOINT_BACKUP_WRITE_ERROR_COUNT);}
 boolean flume_f205_0()
{    return fsyncPerTransaction;}
 void flume_f206_0(boolean fsyncPerTransaction)
{    this.fsyncPerTransaction = fsyncPerTransaction;}
 int flume_f207_0()
{    return fsyncInterval;}
 void flume_f208_0(int fsyncInterval)
{    this.fsyncInterval = fsyncInterval;}
 Builder flume_f209_0(long usableSpaceRefreshInterval)
{    bUsableSpaceRefreshInterval = usableSpaceRefreshInterval;    return this;}
 Builder flume_f210_0(long interval)
{    bCheckpointInterval = interval;    return this;}
 Builder flume_f211_0(long maxSize)
{    bMaxFileSize = maxSize;    return this;}
 Builder flume_f212_0(int capacity)
{    bQueueCapacity = capacity;    return this;}
 Builder flume_f213_0(File cpDir)
{    bCheckpointDir = cpDir;    return this;}
 Builder flume_f214_0(File[] dirs)
{    bLogDirs = dirs;    return this;}
 Builder flume_f215_0(String name)
{    bName = name;    return this;}
 Builder flume_f216_0(long minimumRequiredSpace)
{    bMinimumRequiredSpace = minimumRequiredSpace;    return this;}
 Builder flume_f217_0(boolean useLogReplayV1)
{    this.useLogReplayV1 = useLogReplayV1;    return this;}
 Builder flume_f218_0(boolean useFastReplay)
{    this.useFastReplay = useFastReplay;    return this;}
 Builder flume_f219_0(KeyProvider encryptionKeyProvider)
{    bEncryptionKeyProvider = encryptionKeyProvider;    return this;}
 Builder flume_f220_0(String encryptionKeyAlias)
{    bEncryptionKeyAlias = encryptionKeyAlias;    return this;}
 Builder flume_f221_0(String encryptionCipherProvider)
{    bEncryptionCipherProvider = encryptionCipherProvider;    return this;}
 Builder flume_f222_0(boolean UseDualCheckpoints)
{    this.bUseDualCheckpoints = UseDualCheckpoints;    return this;}
 Builder flume_f223_0(boolean compressBackupCheckpoint)
{    this.bCompressBackupCheckpoint = compressBackupCheckpoint;    return this;}
 Builder flume_f224_0(File backupCheckpointDir)
{    this.bBackupCheckpointDir = backupCheckpointDir;    return this;}
 Builder flume_f225_0(boolean enableCheckpointOnClose)
{    this.checkpointOnClose = enableCheckpointOnClose;    return this;}
 Builder flume_f226_0(FileChannelCounter channelCounter)
{    this.channelCounter = channelCounter;    return this;}
 Log flume_f227_0() throws IOException
{    return new Log(bCheckpointInterval, bMaxFileSize, bQueueCapacity, bUseDualCheckpoints, bCompressBackupCheckpoint, bCheckpointDir, bBackupCheckpointDir, bName, useLogReplayV1, useFastReplay, bMinimumRequiredSpace, bEncryptionKeyProvider, bEncryptionKeyAlias, bEncryptionCipherProvider, bUsableSpaceRefreshInterval, fsyncPerTransaction, fsyncInterval, checkpointOnClose, channelCounter, bLogDirs);}
 void flume_f228_1() throws IOException
{    Preconditions.checkState(!open, "Cannot replay after Log has been opened");    lockExclusive();    try {        /*       * First we are going to look through the data directories       * and find all log files. We will store the highest file id       * (at the end of the filename) we find and use that when we       * create additional log files.       *       * Also store up the list of files so we can replay them later.       */                nextFileID.set(0);        List<File> dataFiles = Lists.newArrayList();        for (File logDir : logDirs) {            for (File file : LogUtils.getLogs(logDir)) {                int id = LogUtils.getIDForFile(file);                dataFiles.add(file);                nextFileID.set(Math.max(nextFileID.get(), id));                idLogFileMap.put(id, LogFileFactory.getRandomReader(new File(logDir, PREFIX + id), encryptionKeyProvider, fsyncPerTransaction));            }        }                /*       * sort the data files by file id so we can replay them by file id       * which should approximately give us sequential events       */        LogUtils.sort(dataFiles);        boolean shouldFastReplay = this.useFastReplay;        /*       * Read the checkpoint (in memory queue) from one of two alternating       * locations. We will read the last one written to disk.       */        File checkpointFile = new File(checkpointDir, "checkpoint");        if (shouldFastReplay) {            if (checkpointFile.exists()) {                                shouldFastReplay = false;            } else {                            }        }        File inflightTakesFile = new File(checkpointDir, "inflighttakes");        File inflightPutsFile = new File(checkpointDir, "inflightputs");        File queueSetDir = new File(checkpointDir, QUEUE_SET);        EventQueueBackingStore backingStore = null;        try {            backingStore = EventQueueBackingStoreFactory.get(checkpointFile, backupCheckpointDir, queueCapacity, channelNameDescriptor, channelCounter, true, this.useDualCheckpoints, this.compressBackupCheckpoint);            queue = new FlumeEventQueue(backingStore, inflightTakesFile, inflightPutsFile, queueSetDir);                        /*         * We now have everything we need to actually replay the log files         * the queue, the timestamp the queue was written to disk, and         * the list of data files.         *         * This will throw if and only if checkpoint file was fine,         * but the inflights were not. If the checkpoint was bad, the backing         * store factory would have thrown.         */            doReplay(queue, dataFiles, encryptionKeyProvider, shouldFastReplay);        } catch (BadCheckpointException ex) {            backupRestored = false;            if (useDualCheckpoints) {                                if (EventQueueBackingStoreFile.backupExists(backupCheckpointDir)) {                    backupRestored = EventQueueBackingStoreFile.restoreBackup(checkpointDir, backupCheckpointDir);                }            }            if (!backupRestored) {                                if (!Serialization.deleteAllFiles(checkpointDir, EXCLUDES)) {                    throw new IOException("Could not delete files in checkpoint " + "directory to recover from a corrupt or incomplete checkpoint");                }            }            backingStore = EventQueueBackingStoreFactory.get(checkpointFile, backupCheckpointDir, queueCapacity, channelNameDescriptor, channelCounter, true, useDualCheckpoints, compressBackupCheckpoint);            queue = new FlumeEventQueue(backingStore, inflightTakesFile, inflightPutsFile, queueSetDir);                                    shouldFastReplay = this.useFastReplay;            doReplay(queue, dataFiles, encryptionKeyProvider, shouldFastReplay);            if (!shouldFastReplay) {                didFullReplayDueToBadCheckpointException = true;            }        }        for (int index = 0; index < logDirs.length; index++) {                        roll(index);        }        /*       * Now that we have replayed, write the current queue to disk       */        writeCheckpoint(true);        open = true;    } catch (Exception ex) {                if (ex instanceof IOException) {            throw (IOException) ex;        }        Throwables.propagate(ex);    } finally {        unlockExclusive();    }}
private void flume_f229_1(FlumeEventQueue queue, List<File> dataFiles, KeyProvider encryptionKeyProvider, boolean useFastReplay) throws Exception
{    CheckpointRebuilder rebuilder = new CheckpointRebuilder(dataFiles, queue, fsyncPerTransaction);    if (useFastReplay && rebuilder.rebuild()) {        didFastReplay = true;            } else {        ReplayHandler replayHandler = new ReplayHandler(queue, encryptionKeyProvider, fsyncPerTransaction);        if (useLogReplayV1) {                        replayHandler.replayLogv1(dataFiles);        } else {                        replayHandler.replayLog(dataFiles);        }        readCount = replayHandler.getReadCount();        putCount = replayHandler.getPutCount();        takeCount = replayHandler.getTakeCount();        rollbackCount = replayHandler.getRollbackCount();        committedCount = replayHandler.getCommitCount();    }}
 boolean flume_f230_0()
{    return didFastReplay;}
public int flume_f231_0()
{    return readCount;}
public int flume_f232_0()
{    return putCount;}
public int flume_f233_0()
{    return takeCount;}
public int flume_f234_0()
{    return committedCount;}
public int flume_f235_0()
{    return rollbackCount;}
 boolean flume_f236_0()
{    return backupRestored;}
 boolean flume_f237_0()
{    return didFullReplayDueToBadCheckpointException;}
 int flume_f238_0()
{    Preconditions.checkState(open, "Log is closed");    return nextFileID.get();}
 FlumeEventQueue flume_f239_0()
{    Preconditions.checkState(open, "Log is closed");    return queue;}
 FlumeEvent flume_f240_0(FlumeEventPointer pointer) throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    Preconditions.checkState(open, "Log is closed");    int id = pointer.getFileID();    LogFile.RandomReader logFile = idLogFileMap.get(id);    Preconditions.checkNotNull(logFile, "LogFile is null for id " + id);    try {        return logFile.get(pointer.getOffset());    } catch (CorruptEventException ex) {        if (fsyncPerTransaction) {            open = false;            throw new IOException("Corrupt event found. Please run File Channel " + "Integrity tool.", ex);        }        throw ex;    }}
 FlumeEventPointer flume_f241_0(long transactionID, Event event) throws IOException
{    Preconditions.checkState(open, "Log is closed");    FlumeEvent flumeEvent = new FlumeEvent(event.getHeaders(), event.getBody());    Put put = new Put(transactionID, WriteOrderOracle.next(), flumeEvent);    ByteBuffer buffer = TransactionEventRecord.toByteBuffer(put);    int logFileIndex = nextLogWriter(transactionID);    long usableSpace = logFiles.get(logFileIndex).getUsableSpace();    long requiredSpace = minimumRequiredSpace + buffer.limit();    if (usableSpace <= requiredSpace) {        throw new IOException("Usable space exhausted, only " + usableSpace + " bytes remaining, required " + requiredSpace + " bytes");    }    boolean error = true;    try {        try {            FlumeEventPointer ptr = logFiles.get(logFileIndex).put(buffer);            error = false;            return ptr;        } catch (LogFileRetryableIOException e) {            if (!open) {                throw e;            }            roll(logFileIndex, buffer);            FlumeEventPointer ptr = logFiles.get(logFileIndex).put(buffer);            error = false;            return ptr;        }    } finally {        if (error && open) {            roll(logFileIndex);        }    }}
 void flume_f242_0(long transactionID, FlumeEventPointer pointer) throws IOException
{    Preconditions.checkState(open, "Log is closed");    Take take = new Take(transactionID, WriteOrderOracle.next(), pointer.getOffset(), pointer.getFileID());    ByteBuffer buffer = TransactionEventRecord.toByteBuffer(take);    int logFileIndex = nextLogWriter(transactionID);    long usableSpace = logFiles.get(logFileIndex).getUsableSpace();    long requiredSpace = minimumRequiredSpace + buffer.limit();    if (usableSpace <= requiredSpace) {        throw new IOException("Usable space exhausted, only " + usableSpace + " bytes remaining, required " + requiredSpace + " bytes");    }    boolean error = true;    try {        try {            logFiles.get(logFileIndex).take(buffer);            error = false;        } catch (LogFileRetryableIOException e) {            if (!open) {                throw e;            }            roll(logFileIndex, buffer);            logFiles.get(logFileIndex).take(buffer);            error = false;        }    } finally {        if (error && open) {            roll(logFileIndex);        }    }}
 void flume_f243_1(long transactionID) throws IOException
{    Preconditions.checkState(open, "Log is closed");    if (LOGGER.isDebugEnabled()) {            }    Rollback rollback = new Rollback(transactionID, WriteOrderOracle.next());    ByteBuffer buffer = TransactionEventRecord.toByteBuffer(rollback);    int logFileIndex = nextLogWriter(transactionID);    long usableSpace = logFiles.get(logFileIndex).getUsableSpace();    long requiredSpace = minimumRequiredSpace + buffer.limit();    if (usableSpace <= requiredSpace) {        throw new IOException("Usable space exhausted, only " + usableSpace + " bytes remaining, required " + requiredSpace + " bytes");    }    boolean error = true;    try {        try {            logFiles.get(logFileIndex).rollback(buffer);            error = false;        } catch (LogFileRetryableIOException e) {            if (!open) {                throw e;            }            roll(logFileIndex, buffer);            logFiles.get(logFileIndex).rollback(buffer);            error = false;        }    } finally {        if (error && open) {            roll(logFileIndex);        }    }}
 void flume_f244_0(long transactionID) throws IOException, InterruptedException
{    Preconditions.checkState(open, "Log is closed");    commit(transactionID, TransactionEventRecord.Type.PUT.get());}
 void flume_f245_0(long transactionID) throws IOException, InterruptedException
{    Preconditions.checkState(open, "Log is closed");    commit(transactionID, TransactionEventRecord.Type.TAKE.get());}
private void flume_f246_0()
{    checkpointWriterLock.unlock();}
 void flume_f247_0()
{    checkpointReadLock.lock();}
 void flume_f248_0()
{    checkpointReadLock.unlock();}
private void flume_f249_0()
{    checkpointWriterLock.lock();}
 void flume_f250_1() throws IOException
{    lockExclusive();    try {        open = false;        try {            if (checkpointOnClose) {                                writeCheckpoint(true);            }        } catch (Exception err) {                    }        shutdownWorker();        if (logFiles != null) {            for (int index = 0; index < logFiles.length(); index++) {                LogFile.Writer writer = logFiles.get(index);                if (writer != null) {                    writer.close();                }            }        }        synchronized (idLogFileMap) {            for (Integer logId : idLogFileMap.keySet()) {                LogFile.RandomReader reader = idLogFileMap.get(logId);                if (reader != null) {                    reader.close();                }            }        }        queue.close();        try {            unlock(checkpointDir);        } catch (IOException ex) {                    }        if (useDualCheckpoints) {            try {                unlock(backupCheckpointDir);            } catch (IOException ex) {                            }        }        for (File logDir : logDirs) {            try {                unlock(logDir);            } catch (IOException ex) {                            }        }    } finally {        unlockExclusive();    }}
 void flume_f251_1()
{    String msg = "Attempting to shutdown background worker.";    System.out.println(msg);        workerExecutor.shutdown();    try {        workerExecutor.awaitTermination(10, TimeUnit.SECONDS);    } catch (InterruptedException e) {            }}
 void flume_f252_0(long checkpointInterval)
{    this.checkpointInterval = checkpointInterval;}
 void flume_f253_0(long maxFileSize)
{    this.maxFileSize = maxFileSize;}
private void flume_f254_0(long transactionID, short type) throws IOException
{    Preconditions.checkState(open, "Log is closed");    Commit commit = new Commit(transactionID, WriteOrderOracle.next(), type);    ByteBuffer buffer = TransactionEventRecord.toByteBuffer(commit);    int logFileIndex = nextLogWriter(transactionID);    long usableSpace = logFiles.get(logFileIndex).getUsableSpace();    long requiredSpace = minimumRequiredSpace + buffer.limit();    if (usableSpace <= requiredSpace) {        throw new IOException("Usable space exhausted, only " + usableSpace + " bytes remaining, required " + requiredSpace + " bytes");    }    boolean error = true;    try {        try {            LogFile.Writer logFileWriter = logFiles.get(logFileIndex);                                                logFileWriter.commit(buffer);            logFileWriter.sync();            error = false;        } catch (LogFileRetryableIOException e) {            if (!open) {                throw e;            }            roll(logFileIndex, buffer);            LogFile.Writer logFileWriter = logFiles.get(logFileIndex);            logFileWriter.commit(buffer);            logFileWriter.sync();            error = false;        }    } finally {        if (error && open) {            roll(logFileIndex);        }    }}
private int flume_f255_0(long transactionID)
{    return (int) Math.abs(transactionID % (long) logFiles.length());}
private void flume_f256_0(int index) throws IOException
{    roll(index, null);}
private synchronized void flume_f257_1(int index, ByteBuffer buffer) throws IOException
{    lockShared();    try {        LogFile.Writer oldLogFile = logFiles.get(index);                if (oldLogFile == null || buffer == null || oldLogFile.isRollRequired(buffer)) {            try {                                int fileID = nextFileID.incrementAndGet();                File file = new File(logDirs[index], PREFIX + fileID);                LogFile.Writer writer = LogFileFactory.getWriter(file, fileID, maxFileSize, encryptionKey, encryptionKeyAlias, encryptionCipherProvider, usableSpaceRefreshInterval, fsyncPerTransaction, fsyncInterval);                idLogFileMap.put(fileID, LogFileFactory.getRandomReader(file, encryptionKeyProvider, fsyncPerTransaction));                                logFiles.set(index, writer);                                if (oldLogFile != null) {                    oldLogFile.close();                }            } finally {                            }        }    } finally {        unlockShared();    }}
private boolean flume_f258_0() throws Exception
{    return writeCheckpoint(false);}
private Boolean flume_f259_1(Boolean force) throws Exception
{    boolean checkpointCompleted = false;    long usableSpace = checkpointDir.getUsableSpace();    if (usableSpace <= minimumRequiredSpace) {        throw new IOException("Usable space exhausted, only " + usableSpace + " bytes remaining, required " + minimumRequiredSpace + " bytes");    }    lockExclusive();    SortedSet<Integer> logFileRefCountsAll = null;    SortedSet<Integer> logFileRefCountsActive = null;    try {        if (queue.checkpoint(force)) {            long logWriteOrderID = queue.getLogWriteOrderID();                                                                                    logFileRefCountsAll = queue.getFileIDs();            logFileRefCountsActive = new TreeSet<Integer>(logFileRefCountsAll);            int numFiles = logFiles.length();            for (int i = 0; i < numFiles; i++) {                LogFile.Writer logWriter = logFiles.get(i);                int logFileID = logWriter.getLogFileID();                File logFile = logWriter.getFile();                LogFile.MetaDataWriter writer = LogFileFactory.getMetaDataWriter(logFile, logFileID);                try {                    writer.markCheckpoint(logWriter.position(), logWriteOrderID);                } finally {                    writer.close();                }                logFileRefCountsAll.remove(logFileID);                            }                        Iterator<Integer> idIterator = logFileRefCountsAll.iterator();            while (idIterator.hasNext()) {                int id = idIterator.next();                LogFile.RandomReader reader = idLogFileMap.remove(id);                File file = reader.getFile();                reader.close();                LogFile.MetaDataWriter writer = LogFileFactory.getMetaDataWriter(file, id);                try {                    writer.markCheckpoint(logWriteOrderID);                } finally {                    reader = LogFileFactory.getRandomReader(file, encryptionKeyProvider, fsyncPerTransaction);                    idLogFileMap.put(id, reader);                    writer.close();                }                                idIterator.remove();            }            Preconditions.checkState(logFileRefCountsAll.size() == 0, "Could not update all data file timestamps: " + logFileRefCountsAll);                        for (int index = 0; index < logDirs.length; index++) {                logFileRefCountsActive.add(logFiles.get(index).getLogFileID());            }            checkpointCompleted = true;        }    } finally {        unlockExclusive();    }        if (open && checkpointCompleted) {        removeOldLogs(logFileRefCountsActive);    }        return true;}
private void flume_f260_1(SortedSet<Integer> fileIDs)
{    Preconditions.checkState(open, "Log is closed");        for (File fileToDelete : pendingDeletes) {                FileUtils.deleteQuietly(fileToDelete);    }    pendingDeletes.clear();            int minFileID = fileIDs.first();        for (File logDir : logDirs) {        List<File> logs = LogUtils.getLogs(logDir);                LogUtils.sort(logs);                int size = logs.size() - MIN_NUM_LOGS;        for (int index = 0; index < size; index++) {            File logFile = logs.get(index);            int logFileID = LogUtils.getIDForFile(logFile);            if (logFileID < minFileID) {                LogFile.RandomReader reader = idLogFileMap.remove(logFileID);                if (reader != null) {                    reader.close();                }                File metaDataFile = Serialization.getMetaDataFile(logFile);                pendingDeletes.add(logFile);                pendingDeletes.add(metaDataFile);            }        }    }}
private void flume_f261_1(File dir) throws IOException
{    FileLock lock = tryLock(dir);    if (lock == null) {        String msg = "Cannot lock " + dir + ". The directory is already locked. " + channelNameDescriptor;                throw new IOException(msg);    }    FileLock secondLock = tryLock(dir);    if (secondLock != null) {                secondLock.release();        secondLock.channel().close();    }    locks.put(dir.getAbsolutePath(), lock);}
private FileLock flume_f262_1(File dir) throws IOException
{    File lockF = new File(dir, FILE_LOCK);    lockF.deleteOnExit();    RandomAccessFile file = new RandomAccessFile(lockF, "rws");    FileLock res = null;    try {        res = file.getChannel().tryLock();    } catch (OverlappingFileLockException oe) {        file.close();        return null;    } catch (IOException e) {                file.close();        throw e;    }    return res;}
private void flume_f263_0(File dir) throws IOException
{    FileLock lock = locks.remove(dir.getAbsolutePath());    if (lock == null) {        return;    }    lock.release();    lock.channel().close();    lock = null;}
public void flume_f264_1()
{    try {        if (log.open) {            log.writeCheckpoint();        }    } catch (IOException e) {        log.channelCounter.incrementCheckpointWriteErrorCount();            } catch (Throwable e) {        log.channelCounter.incrementCheckpointWriteErrorCount();            }}
protected static void flume_f265_0(RandomAccessFile fileHandle, int offset) throws IOException
{    fileHandle.seek(offset);    int length = fileHandle.readInt();    fileHandle.skipBytes(length);}
protected RandomAccessFile flume_f266_0()
{    return writeFileHandle;}
protected void flume_f267_0(long lastCheckpointOffset)
{    this.lastCheckpointOffset = lastCheckpointOffset;}
protected void flume_f268_0(long lastCheckpointWriteOrderID)
{    this.lastCheckpointWriteOrderID = lastCheckpointWriteOrderID;}
protected long flume_f269_0()
{    return lastCheckpointOffset;}
protected long flume_f270_0()
{    return lastCheckpointWriteOrderID;}
protected File flume_f271_0()
{    return file;}
protected int flume_f272_0()
{    return logFileID;}
 void flume_f273_0(long logWriteOrderID) throws IOException
{    markCheckpoint(lastCheckpointOffset, logWriteOrderID);}
 void flume_f274_1()
{    try {        writeFileHandle.close();    } catch (IOException e) {            }}
 void flume_f275_0(long numBytes)
{    Preconditions.checkArgument(numBytes >= 0, "numBytes less than zero");    value.addAndGet(-numBytes);}
 long flume_f276_0()
{    long now = System.currentTimeMillis();    if (now - interval > lastRefresh.get()) {        value.set(fs.getUsableSpace());        lastRefresh.set(now);    }    return Math.max(value.get(), 0L);}
public void flume_f277_1()
{    try {        sync();    } catch (Throwable ex) {            }}
protected CipherProvider.Encryptor flume_f278_0()
{    return encryptor;}
 int flume_f279_0()
{    return logFileID;}
 File flume_f280_0()
{    return file;}
 String flume_f281_0()
{    return file.getParent();}
 long flume_f282_0()
{    return usableSpace.getUsableSpace();}
 long flume_f283_0()
{    return maxFileSize;}
 long flume_f284_0()
{    return lastCommitPosition;}
 long flume_f285_0()
{    return lastSyncPosition;}
 long flume_f286_0()
{    return syncCount;}
 synchronized long flume_f287_0() throws IOException
{    return getFileChannel().position();}
 synchronized FlumeEventPointer flume_f288_0(ByteBuffer buffer) throws IOException
{    if (encryptor != null) {        buffer = ByteBuffer.wrap(encryptor.encrypt(buffer.array()));    }    Pair<Integer, Integer> pair = write(buffer);    return new FlumeEventPointer(pair.getLeft(), pair.getRight());}
 synchronized void flume_f289_0(ByteBuffer buffer) throws IOException
{    if (encryptor != null) {        buffer = ByteBuffer.wrap(encryptor.encrypt(buffer.array()));    }    write(buffer);}
 synchronized void flume_f290_0(ByteBuffer buffer) throws IOException
{    if (encryptor != null) {        buffer = ByteBuffer.wrap(encryptor.encrypt(buffer.array()));    }    write(buffer);}
 synchronized void flume_f291_0(ByteBuffer buffer) throws IOException
{    if (encryptor != null) {        buffer = ByteBuffer.wrap(encryptor.encrypt(buffer.array()));    }    write(buffer);    dirty = true;    lastCommitPosition = position();}
private Pair<Integer, Integer> flume_f292_0(ByteBuffer buffer) throws IOException
{    if (!isOpen()) {        throw new LogFileRetryableIOException("File closed " + file);    }    long length = position();    long expectedLength = length + (long) buffer.limit();    if (expectedLength > maxFileSize) {        throw new LogFileRetryableIOException(expectedLength + " > " + maxFileSize);    }    int offset = (int) length;    Preconditions.checkState(offset >= 0, String.valueOf(offset));        int recordLength = 1 + (int) Serialization.SIZE_OF_INT + buffer.limit();    usableSpace.decrement(recordLength);    preallocate(recordLength);    ByteBuffer toWrite = ByteBuffer.allocate(recordLength);    toWrite.put(OP_RECORD);    writeDelimitedBuffer(toWrite, buffer);    toWrite.position(0);    int wrote = getFileChannel().write(toWrite);    Preconditions.checkState(wrote == toWrite.limit());    return Pair.of(getLogFileID(), offset);}
 synchronized boolean flume_f293_0(ByteBuffer buffer) throws IOException
{    return isOpen() && position() + (long) buffer.limit() > getMaxSize();}
 synchronized void flume_f294_1() throws IOException
{    if (!fsyncPerTransaction && !dirty) {        if (LOG.isDebugEnabled()) {                    }        return;    }    if (!isOpen()) {        throw new LogFileRetryableIOException("File closed " + file);    }    if (lastSyncPosition < lastCommitPosition) {        getFileChannel().force(false);        lastSyncPosition = position();        syncCount++;        dirty = false;    }}
protected boolean flume_f295_0()
{    return open;}
protected RandomAccessFile flume_f296_0()
{    return writeFileHandle;}
protected FileChannel flume_f297_0()
{    return writeFileChannel;}
 synchronized void flume_f298_1()
{    if (open) {        open = false;        if (!fsyncPerTransaction) {                        if (syncExecutor != null) {                                syncExecutor.shutdown();            }        }        if (writeFileChannel.isOpen()) {                        try {                writeFileChannel.force(true);            } catch (IOException e) {                            }            try {                writeFileHandle.close();            } catch (IOException e) {                            }        }    }}
protected void flume_f299_1(int size) throws IOException
{    long position = position();    if (position + size > getFileChannel().size()) {                synchronized (FILL) {            FILL.position(0);            getFileChannel().write(FILL, position);        }    }}
public void flume_f300_1(long offset) throws IOException
{                    fileHandle.seek(offset);    byte byteRead = fileHandle.readByte();    Preconditions.checkState(byteRead == OP_RECORD || byteRead == OP_NOOP, "Expected to read a record but the byte read indicates EOF");    fileHandle.seek(offset);        fileHandle.writeByte(OP_NOOP);}
public void flume_f301_1()
{    try {        fileHandle.getFD().sync();        fileHandle.close();    } catch (IOException e) {            }}
 File flume_f302_0()
{    return file;}
protected KeyProvider flume_f303_0()
{    return encryptionKeyProvider;}
 FlumeEvent flume_f304_0(int offset) throws IOException, InterruptedException, CorruptEventException, NoopRecordException
{    Preconditions.checkState(open, "File closed");    RandomAccessFile fileHandle = checkOut();    boolean error = true;    try {        fileHandle.seek(offset);        byte operation = fileHandle.readByte();        if (operation == OP_NOOP) {            throw new NoopRecordException("No op record found. Corrupt record " + "may have been repaired by File Channel Integrity tool");        }        if (operation != OP_RECORD) {            throw new CorruptEventException("Operation code is invalid. File " + "is corrupt. Please run File Channel Integrity tool.");        }        TransactionEventRecord record = doGet(fileHandle);        if (!(record instanceof Put)) {            Preconditions.checkState(false, "Record is " + record.getClass().getSimpleName());        }        error = false;        return ((Put) record).getEvent();    } finally {        if (error) {            close(fileHandle, file);        } else {            checkIn(fileHandle);        }    }}
 synchronized void flume_f305_1()
{    if (open) {        open = false;                List<RandomAccessFile> fileHandles = Lists.newArrayList();        while (readFileHandles.drainTo(fileHandles) > 0) {            for (RandomAccessFile fileHandle : fileHandles) {                synchronized (fileHandle) {                    try {                        fileHandle.close();                    } catch (IOException e) {                                            }                }            }            fileHandles.clear();            try {                Thread.sleep(5L);            } catch (InterruptedException e) {                        }        }    }}
private RandomAccessFile flume_f306_0() throws IOException
{    return new RandomAccessFile(file, "r");}
private void flume_f307_0(RandomAccessFile fileHandle)
{    if (!readFileHandles.offer(fileHandle)) {        close(fileHandle, file);    }}
private RandomAccessFile flume_f308_1() throws IOException, InterruptedException
{    RandomAccessFile fileHandle = readFileHandles.poll();    if (fileHandle != null) {        return fileHandle;    }    int remaining = readFileHandles.remainingCapacity();    if (remaining > 0) {                return open();    }    return readFileHandles.take();}
private static void flume_f309_1(RandomAccessFile fileHandle, File file)
{    if (fileHandle != null) {        try {            fileHandle.close();        } catch (IOException e) {                    }    }}
protected void flume_f310_0(long lastCheckpointPosition)
{    this.lastCheckpointPosition = lastCheckpointPosition;}
protected void flume_f311_0(long lastCheckpointWriteOrderID)
{    this.lastCheckpointWriteOrderID = lastCheckpointWriteOrderID;}
protected void flume_f312_0(long backupCheckpointPosition)
{    this.backupCheckpointPosition = backupCheckpointPosition;}
protected void flume_f313_0(long backupCheckpointWriteOrderID)
{    this.backupCheckpointWriteOrderID = backupCheckpointWriteOrderID;}
protected void flume_f314_0(int logFileID)
{    this.logFileID = logFileID;    Preconditions.checkArgument(logFileID >= 0, "LogFileID is not positive: " + Integer.toHexString(logFileID));}
protected KeyProvider flume_f315_0()
{    return encryptionKeyProvider;}
protected RandomAccessFile flume_f316_0()
{    return fileHandle;}
 int flume_f317_0()
{    return logFileID;}
 void flume_f318_1(long checkpointWriteOrderID) throws IOException
{    if (lastCheckpointPosition > 0L) {        long position = 0;        if (lastCheckpointWriteOrderID <= checkpointWriteOrderID) {            position = lastCheckpointPosition;        } else if (backupCheckpointWriteOrderID <= checkpointWriteOrderID && backupCheckpointPosition > 0) {            position = backupCheckpointPosition;        }        fileChannel.position(position);            } else {            }}
public LogRecord flume_f319_1() throws IOException, CorruptEventException
{    int offset = -1;    try {        long position = fileChannel.position();        if (position > FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE) {                    }        offset = (int) position;        Preconditions.checkState(offset >= 0);        while (offset < fileHandle.length()) {            byte operation = fileHandle.readByte();            if (operation == OP_RECORD) {                break;            } else if (operation == OP_EOF) {                                return null;            } else if (operation == OP_NOOP) {                                skipRecord(fileHandle, offset + 1);                offset = (int) fileHandle.getFilePointer();                continue;            } else {                                return null;            }        }        if (offset >= fileHandle.length()) {            return null;        }        return doNext(offset);    } catch (EOFException e) {        return null;    } catch (IOException e) {        throw new IOException("Unable to read next Transaction from log file " + file.getCanonicalPath() + " at offset " + offset, e);    }}
public long flume_f320_0() throws IOException
{    return fileChannel.position();}
public void flume_f321_0()
{    if (fileHandle != null) {        try {            fileHandle.close();        } catch (IOException e) {        }    }}
protected static void flume_f322_0(ByteBuffer output, ByteBuffer buffer) throws IOException
{    output.putInt(buffer.limit());    output.put(buffer);}
protected static byte[] flume_f323_0(RandomAccessFile fileHandle) throws IOException, CorruptEventException
{    int length = fileHandle.readInt();    if (length < 0) {        throw new CorruptEventException("Length of event is: " + String.valueOf(length) + ". Event must have length >= 0. Possible corruption of data or partial fsync.");    }    byte[] buffer = new byte[length];    try {        fileHandle.readFully(buffer);    } catch (EOFException ex) {        throw new CorruptEventException("Remaining data in file less than " + "expected size of event.", ex);    }    return buffer;}
public static void flume_f324_0(String[] args) throws EOFException, IOException, CorruptEventException
{    File file = new File(args[0]);    LogFile.SequentialReader reader = null;    try {        reader = LogFileFactory.getSequentialReader(file, null, false);        LogRecord entry;        FlumeEventPointer ptr;                        int fileId = reader.getLogFileID();        int count = 0;        int readCount = 0;        int putCount = 0;        int takeCount = 0;        int rollbackCount = 0;        int commitCount = 0;        while ((entry = reader.next()) != null) {            int offset = entry.getOffset();            TransactionEventRecord record = entry.getEvent();            short type = record.getRecordType();            long trans = record.getTransactionID();            long ts = record.getLogWriteOrderID();            readCount++;            ptr = null;            if (type == TransactionEventRecord.Type.PUT.get()) {                putCount++;                ptr = new FlumeEventPointer(fileId, offset);            } else if (type == TransactionEventRecord.Type.TAKE.get()) {                takeCount++;                Take take = (Take) record;                ptr = new FlumeEventPointer(take.getFileID(), take.getOffset());            } else if (type == TransactionEventRecord.Type.ROLLBACK.get()) {                rollbackCount++;            } else if (type == TransactionEventRecord.Type.COMMIT.get()) {                commitCount++;            } else {                Preconditions.checkArgument(false, "Unknown record type: " + Integer.toHexString(type));            }            System.out.println(Joiner.on(", ").skipNulls().join(trans, ts, fileId, offset, TransactionEventRecord.getName(type), ptr));        }        System.out.println("Replayed " + count + " from " + file + " read: " + readCount + ", put: " + putCount + ", take: " + takeCount + ", rollback: " + rollbackCount + ", commit: " + commitCount);    } catch (EOFException e) {        System.out.println("Hit EOF on " + file);    } finally {        if (reader != null) {            reader.close();        }    }}
 static LogFile.MetaDataWriter flume_f325_1(File file, int logFileID) throws IOException
{    RandomAccessFile logFile = null;    try {        File metaDataFile = Serialization.getMetaDataFile(file);        if (metaDataFile.exists()) {            return new LogFileV3.MetaDataWriter(file, logFileID);        }        logFile = new RandomAccessFile(file, "r");        int version = logFile.readInt();        if (Serialization.VERSION_2 == version) {            return new LogFileV2.MetaDataWriter(file, logFileID);        }        throw new IOException("File " + file + " has bad version " + Integer.toHexString(version));    } finally {        if (logFile != null) {            try {                logFile.close();            } catch (IOException e) {                            }        }    }}
 static LogFile.Writer flume_f326_0(File file, int logFileID, long maxFileSize, @Nullable Key encryptionKey, @Nullable String encryptionKeyAlias, @Nullable String encryptionCipherProvider, long usableSpaceRefreshInterval, boolean fsyncPerTransaction, int fsyncInterval) throws IOException
{    Preconditions.checkState(!file.exists(), "File already exists " + file.getAbsolutePath());    Preconditions.checkState(file.createNewFile(), "File could not be created " + file.getAbsolutePath());    return new LogFileV3.Writer(file, logFileID, maxFileSize, encryptionKey, encryptionKeyAlias, encryptionCipherProvider, usableSpaceRefreshInterval, fsyncPerTransaction, fsyncInterval);}
 static LogFile.RandomReader flume_f327_1(File file, @Nullable KeyProvider encryptionKeyProvider, boolean fsyncPerTransaction) throws IOException
{    RandomAccessFile logFile = new RandomAccessFile(file, "r");    try {        File metaDataFile = Serialization.getMetaDataFile(file);                if (logFile.length() == 0L || metaDataFile.exists()) {            return new LogFileV3.RandomReader(file, encryptionKeyProvider, fsyncPerTransaction);        }        int version = logFile.readInt();        if (Serialization.VERSION_2 == version) {            return new LogFileV2.RandomReader(file);        }        throw new IOException("File " + file + " has bad version " + Integer.toHexString(version));    } finally {        if (logFile != null) {            try {                logFile.close();            } catch (IOException e) {                            }        }    }}
 static LogFile.SequentialReader flume_f328_1(File file, @Nullable KeyProvider encryptionKeyProvider, boolean fsyncPerTransaction) throws IOException
{    RandomAccessFile logFile = null;    try {        File metaDataFile = Serialization.getMetaDataFile(file);        File oldMetadataFile = Serialization.getOldMetaDataFile(file);        File tempMetadataFile = Serialization.getMetaDataTempFile(file);        boolean hasMeta = false;                if (metaDataFile.exists()) {            hasMeta = true;        } else if (tempMetadataFile.exists()) {            if (tempMetadataFile.renameTo(metaDataFile)) {                hasMeta = true;            } else {                throw new IOException("Renaming of " + tempMetadataFile.getName() + " to " + metaDataFile.getName() + " failed");            }        } else if (oldMetadataFile.exists()) {            if (oldMetadataFile.renameTo(metaDataFile)) {                hasMeta = true;            } else {                throw new IOException("Renaming of " + oldMetadataFile.getName() + " to " + metaDataFile.getName() + " failed");            }        }        if (hasMeta) {                        if (oldMetadataFile.exists()) {                oldMetadataFile.delete();            }            if (tempMetadataFile.exists()) {                tempMetadataFile.delete();            }            if (metaDataFile.length() == 0L) {                if (file.length() != 0L) {                    String msg = String.format("MetaData file %s is empty, but log %s" + " is of size %d", metaDataFile, file, file.length());                    throw new IllegalStateException(msg);                }                throw new EOFException(String.format("MetaData file %s is empty", metaDataFile));            }            return new LogFileV3.SequentialReader(file, encryptionKeyProvider, fsyncPerTransaction);        }        logFile = new RandomAccessFile(file, "r");        int version = logFile.readInt();        if (Serialization.VERSION_2 == version) {            return new LogFileV2.SequentialReader(file);        }        throw new IOException("File " + file + " has bad version " + Integer.toHexString(version));    } finally {        if (logFile != null) {            try {                logFile.close();            } catch (IOException e) {                            }        }    }}
 int flume_f329_0()
{    return Serialization.VERSION_2;}
 void flume_f330_1(long currentPosition, long logWriteOrderID) throws IOException
{    RandomAccessFile writeFileHandle = getFileHandle();    writeFileHandle.seek(OFFSET_CHECKPOINT);    writeFileHandle.writeLong(currentPosition);    writeFileHandle.writeLong(logWriteOrderID);    writeFileHandle.getChannel().force(true);    }
 int flume_f331_0()
{    return Serialization.VERSION_2;}
 int flume_f332_0()
{    return Serialization.VERSION_2;}
protected TransactionEventRecord flume_f333_0(RandomAccessFile fileHandle) throws IOException
{    return TransactionEventRecord.fromDataInputV2(fileHandle);}
public int flume_f334_0()
{    return Serialization.VERSION_2;}
 LogRecord flume_f335_0(int offset) throws IOException
{    TransactionEventRecord event = TransactionEventRecord.fromDataInputV2(getFileHandle());    return new LogRecord(getLogFileID(), offset, event);}
 int flume_f336_0()
{    return Serialization.VERSION_3;}
 void flume_f337_0(long currentPosition, long logWriteOrderID) throws IOException
{    ProtosFactory.LogFileMetaData.Builder metaDataBuilder = ProtosFactory.LogFileMetaData.newBuilder(logFileMetaData);    metaDataBuilder.setCheckpointPosition(currentPosition);    metaDataBuilder.setCheckpointWriteOrderID(logWriteOrderID);    /*       * Set the previous checkpoint position and write order id so that it       * would be possible to recover from a backup.       */    metaDataBuilder.setBackupCheckpointPosition(logFileMetaData.getCheckpointPosition());    metaDataBuilder.setBackupCheckpointWriteOrderID(logFileMetaData.getCheckpointWriteOrderID());    logFileMetaData = metaDataBuilder.build();    writeDelimitedTo(logFileMetaData, metaDataFile);}
 ProtosFactory.LogFileMetaData flume_f338_1() throws IOException
{    FileInputStream inputStream = new FileInputStream(metaDataFile);    try {        ProtosFactory.LogFileMetaData metaData = Preconditions.checkNotNull(ProtosFactory.LogFileMetaData.parseDelimitedFrom(inputStream), "Metadata cannot be null");        if (metaData.getLogFileID() != logFileID) {            throw new IOException("The file id of log file: " + logFile + " is different from expected " + " id: expected = " + logFileID + ", found = " + metaData.getLogFileID());        }        return metaData;    } finally {        try {            inputStream.close();        } catch (IOException e) {                    }    }}
public static void flume_f339_1(GeneratedMessage msg, File file) throws IOException
{    File tmp = Serialization.getMetaDataTempFile(file);    FileOutputStream outputStream = new FileOutputStream(tmp);    boolean closed = false;    try {        msg.writeDelimitedTo(outputStream);        outputStream.getChannel().force(true);        outputStream.close();        closed = true;        if (!tmp.renameTo(file)) {                                                                        File oldFile = Serialization.getOldMetaDataFile(file);            if (!file.renameTo(oldFile)) {                throw new IOException("Unable to rename " + file + " to " + oldFile);            }            if (!tmp.renameTo(file)) {                throw new IOException("Unable to rename " + tmp + " over " + file);            }            oldFile.delete();        }    } finally {        if (!closed) {            try {                outputStream.close();            } catch (IOException e) {                            }        }    }}
 int flume_f340_0()
{    return Serialization.VERSION_3;}
private void flume_f341_1() throws IOException
{    File metaDataFile = Serialization.getMetaDataFile(getFile());    FileInputStream inputStream = new FileInputStream(metaDataFile);    try {        ProtosFactory.LogFileMetaData metaData = Preconditions.checkNotNull(ProtosFactory.LogFileMetaData.parseDelimitedFrom(inputStream), "MetaData cannot be null");        int version = metaData.getVersion();        if (version != getVersion()) {            throw new IOException("Version is " + Integer.toHexString(version) + " expected " + Integer.toHexString(getVersion()) + " file: " + getFile().getCanonicalPath());        }        encryptionEnabled = false;        if (metaData.hasEncryption()) {            if (getKeyProvider() == null) {                throw new IllegalStateException("Data file is encrypted but no " + " provider was specified");            }            ProtosFactory.LogFileEncryption encryption = metaData.getEncryption();            key = getKeyProvider().getKey(encryption.getKeyAlias());            cipherProvider = encryption.getCipherProvider();            parameters = encryption.getParameters().toByteArray();            encryptionEnabled = true;        }    } finally {        try {            inputStream.close();        } catch (IOException e) {                    }    }}
private CipherProvider.Decryptor flume_f342_0()
{    CipherProvider.Decryptor decryptor = decryptors.poll();    if (decryptor == null) {        decryptor = CipherProviderFactory.getDecrypter(cipherProvider, key, parameters);    }    return decryptor;}
 int flume_f343_0()
{    return Serialization.VERSION_3;}
protected TransactionEventRecord flume_f344_0(RandomAccessFile fileHandle) throws IOException, CorruptEventException
{        synchronized (this) {        if (!initialized) {            initialized = true;            initialize();        }    }    boolean success = false;    CipherProvider.Decryptor decryptor = null;    try {        byte[] buffer = readDelimitedBuffer(fileHandle);        if (encryptionEnabled) {            decryptor = getDecryptor();            buffer = decryptor.decrypt(buffer);        }        TransactionEventRecord event = TransactionEventRecord.fromByteArray(buffer);        success = true;        return event;    } catch (DecryptionFailureException ex) {        throw new CorruptEventException("Error decrypting event", ex);    } finally {        if (success && encryptionEnabled && decryptor != null) {            decryptors.offer(decryptor);        }    }}
public int flume_f345_0()
{    return Serialization.VERSION_3;}
 LogRecord flume_f346_1(int offset) throws IOException, CorruptEventException, DecryptionFailureException
{    byte[] buffer = null;    TransactionEventRecord event = null;    try {        buffer = readDelimitedBuffer(getFileHandle());        if (decryptor != null) {            buffer = decryptor.decrypt(buffer);        }        event = TransactionEventRecord.fromByteArray(buffer);    } catch (CorruptEventException ex) {                        if (!fsyncPerTransaction) {            return null;        }        throw ex;    } catch (DecryptionFailureException ex) {        if (!fsyncPerTransaction) {                        return null;        }        throw ex;    }    return new LogRecord(getLogFileID(), offset, event);}
public int flume_f347_0()
{    return fileID;}
public int flume_f348_0()
{    return offset;}
public TransactionEventRecord flume_f349_0()
{    return event;}
public int flume_f350_0(LogRecord o)
{    int result = new Long(event.getLogWriteOrderID()).compareTo(o.getEvent().getLogWriteOrderID());    if (result == 0) {                        result = new Long(event.getTransactionID()).compareTo(o.getEvent().getTransactionID());        if (result == 0) {                                    Integer thisIndex = Arrays.binarySearch(replaySortOrder, event.getRecordType());            Integer thatIndex = Arrays.binarySearch(replaySortOrder, o.getEvent().getRecordType());            return thisIndex.compareTo(thatIndex);        }    }    return result;}
 static void flume_f351_0(List<File> logs)
{    Collections.sort(logs, new Comparator<File>() {        @Override        public int compare(File file1, File file2) {            int id1 = getIDForFile(file1);            int id2 = getIDForFile(file2);            if (id1 > id2) {                return 1;            } else if (id1 == id2) {                return 0;            }            return -1;        }    });}
public int flume_f352_0(File file1, File file2)
{    int id1 = getIDForFile(file1);    int id2 = getIDForFile(file2);    if (id1 > id2) {        return 1;    } else if (id1 == id2) {        return 0;    }    return -1;}
 static int flume_f353_0(File file)
{    return Integer.parseInt(file.getName().substring(Log.PREFIX.length()));}
 static List<File> flume_f354_0(File logDir)
{    List<File> result = Lists.newArrayList();    File[] files = logDir.listFiles();    if (files == null) {        String msg = logDir + ".listFiles() returned null: ";        msg += "File = " + logDir.isFile() + ", ";        msg += "Exists = " + logDir.exists() + ", ";        msg += "Writable = " + logDir.canWrite();        throw new IllegalStateException(msg);    }    for (File file : files) {        String name = file.getName();        if (pattern.matcher(name).matches()) {            result.add(file);        }    }    return result;}
 L flume_f355_0()
{    return left;}
 R flume_f356_0()
{    return right;}
 static Pair<L, R> flume_f357_0(L left, R right)
{    return new Pair<L, R>(left, right);}
public static void flume_f358_0(com.google.protobuf.ExtensionRegistry registry)
{}
public static Checkpoint flume_f359_0()
{    return defaultInstance;}
public Checkpoint flume_f360_0()
{    return defaultInstance;}
public final com.google.protobuf.UnknownFieldSet flume_f361_0()
{    return this.unknownFields;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f362_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Checkpoint_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f363_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Checkpoint_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint.class, org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint.Builder.class);}
public Checkpoint flume_f364_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new Checkpoint(input, extensionRegistry);}
public com.google.protobuf.Parser<Checkpoint> flume_f365_0()
{    return PARSER;}
public boolean flume_f366_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public int flume_f367_0()
{    return version_;}
public boolean flume_f368_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public long flume_f369_0()
{    return writeOrderID_;}
public boolean flume_f370_0()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
public int flume_f371_0()
{    return queueSize_;}
public boolean flume_f372_0()
{    return ((bitField0_ & 0x00000008) == 0x00000008);}
public int flume_f373_0()
{    return queueHead_;}
public java.util.List<org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog> flume_f374_0()
{    return activeLogs_;}
public java.util.List<? extends org.apache.flume.channel.file.proto.ProtosFactory.ActiveLogOrBuilder> flume_f375_0()
{    return activeLogs_;}
public int flume_f376_0()
{    return activeLogs_.size();}
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f377_0(int index)
{    return activeLogs_.get(index);}
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLogOrBuilder flume_f378_0(int index)
{    return activeLogs_.get(index);}
private void flume_f379_0()
{    version_ = 0;    writeOrderID_ = 0L;    queueSize_ = 0;    queueHead_ = 0;    activeLogs_ = java.util.Collections.emptyList();}
public final boolean flume_f380_0()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasVersion()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasWriteOrderID()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasQueueSize()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasQueueHead()) {        memoizedIsInitialized = 0;        return false;    }    for (int i = 0; i < getActiveLogsCount(); i++) {        if (!getActiveLogs(i).isInitialized()) {            memoizedIsInitialized = 0;            return false;        }    }    memoizedIsInitialized = 1;    return true;}
public void flume_f381_0(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeSFixed32(1, version_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeSFixed64(2, writeOrderID_);    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        output.writeSFixed32(3, queueSize_);    }    if (((bitField0_ & 0x00000008) == 0x00000008)) {        output.writeSFixed32(4, queueHead_);    }    for (int i = 0; i < activeLogs_.size(); i++) {        output.writeMessage(5, activeLogs_.get(i));    }    getUnknownFields().writeTo(output);}
public int flume_f382_0()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(1, version_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(2, writeOrderID_);    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(3, queueSize_);    }    if (((bitField0_ & 0x00000008) == 0x00000008)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(4, queueHead_);    }    for (int i = 0; i < activeLogs_.size(); i++) {        size += com.google.protobuf.CodedOutputStream.computeMessageSize(5, activeLogs_.get(i));    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
protected java.lang.Object flume_f383_0() throws java.io.ObjectStreamException
{    return super.writeReplace();}
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint flume_f384_0(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint flume_f385_0(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint flume_f386_0(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint flume_f387_0(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint flume_f388_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint flume_f389_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint flume_f390_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint flume_f391_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint flume_f392_0(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint flume_f393_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static Builder flume_f394_0()
{    return Builder.create();}
public Builder flume_f395_0()
{    return newBuilder();}
public static Builder flume_f396_0(org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint prototype)
{    return newBuilder().mergeFrom(prototype);}
public Builder flume_f397_0()
{    return newBuilder(this);}
protected Builder flume_f398_0(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f399_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Checkpoint_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f400_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Checkpoint_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint.class, org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint.Builder.class);}
private void flume_f401_0()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {        getActiveLogsFieldBuilder();    }}
private static Builder flume_f402_0()
{    return new Builder();}
public Builder flume_f403_0()
{    super.clear();    version_ = 0;    bitField0_ = (bitField0_ & ~0x00000001);    writeOrderID_ = 0L;    bitField0_ = (bitField0_ & ~0x00000002);    queueSize_ = 0;    bitField0_ = (bitField0_ & ~0x00000004);    queueHead_ = 0;    bitField0_ = (bitField0_ & ~0x00000008);    if (activeLogsBuilder_ == null) {        activeLogs_ = java.util.Collections.emptyList();        bitField0_ = (bitField0_ & ~0x00000010);    } else {        activeLogsBuilder_.clear();    }    return this;}
public Builder flume_f404_0()
{    return create().mergeFrom(buildPartial());}
public com.google.protobuf.Descriptors.Descriptor flume_f405_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Checkpoint_descriptor;}
public org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint flume_f406_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint.getDefaultInstance();}
public org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint flume_f407_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
public org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint flume_f408_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint result = new org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.version_ = version_;    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.writeOrderID_ = writeOrderID_;    if (((from_bitField0_ & 0x00000004) == 0x00000004)) {        to_bitField0_ |= 0x00000004;    }    result.queueSize_ = queueSize_;    if (((from_bitField0_ & 0x00000008) == 0x00000008)) {        to_bitField0_ |= 0x00000008;    }    result.queueHead_ = queueHead_;    if (activeLogsBuilder_ == null) {        if (((bitField0_ & 0x00000010) == 0x00000010)) {            activeLogs_ = java.util.Collections.unmodifiableList(activeLogs_);            bitField0_ = (bitField0_ & ~0x00000010);        }        result.activeLogs_ = activeLogs_;    } else {        result.activeLogs_ = activeLogsBuilder_.build();    }    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
public Builder flume_f409_0(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint) other);    } else {        super.mergeFrom(other);        return this;    }}
public Builder flume_f410_0(org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint.getDefaultInstance())        return this;    if (other.hasVersion()) {        setVersion(other.getVersion());    }    if (other.hasWriteOrderID()) {        setWriteOrderID(other.getWriteOrderID());    }    if (other.hasQueueSize()) {        setQueueSize(other.getQueueSize());    }    if (other.hasQueueHead()) {        setQueueHead(other.getQueueHead());    }    if (activeLogsBuilder_ == null) {        if (!other.activeLogs_.isEmpty()) {            if (activeLogs_.isEmpty()) {                activeLogs_ = other.activeLogs_;                bitField0_ = (bitField0_ & ~0x00000010);            } else {                ensureActiveLogsIsMutable();                activeLogs_.addAll(other.activeLogs_);            }            onChanged();        }    } else {        if (!other.activeLogs_.isEmpty()) {            if (activeLogsBuilder_.isEmpty()) {                activeLogsBuilder_.dispose();                activeLogsBuilder_ = null;                activeLogs_ = other.activeLogs_;                bitField0_ = (bitField0_ & ~0x00000010);                activeLogsBuilder_ = com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ? getActiveLogsFieldBuilder() : null;            } else {                activeLogsBuilder_.addAllMessages(other.activeLogs_);            }        }    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
public final boolean flume_f411_0()
{    if (!hasVersion()) {        return false;    }    if (!hasWriteOrderID()) {        return false;    }    if (!hasQueueSize()) {        return false;    }    if (!hasQueueHead()) {        return false;    }    for (int i = 0; i < getActiveLogsCount(); i++) {        if (!getActiveLogs(i).isInitialized()) {            return false;        }    }    return true;}
public Builder flume_f412_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
public boolean flume_f413_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public int flume_f414_0()
{    return version_;}
public Builder flume_f415_0(int value)
{    bitField0_ |= 0x00000001;    version_ = value;    onChanged();    return this;}
public Builder flume_f416_0()
{    bitField0_ = (bitField0_ & ~0x00000001);    version_ = 0;    onChanged();    return this;}
public boolean flume_f417_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public long flume_f418_0()
{    return writeOrderID_;}
public Builder flume_f419_0(long value)
{    bitField0_ |= 0x00000002;    writeOrderID_ = value;    onChanged();    return this;}
public Builder flume_f420_0()
{    bitField0_ = (bitField0_ & ~0x00000002);    writeOrderID_ = 0L;    onChanged();    return this;}
public boolean flume_f421_0()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
public int flume_f422_0()
{    return queueSize_;}
public Builder flume_f423_0(int value)
{    bitField0_ |= 0x00000004;    queueSize_ = value;    onChanged();    return this;}
public Builder flume_f424_0()
{    bitField0_ = (bitField0_ & ~0x00000004);    queueSize_ = 0;    onChanged();    return this;}
public boolean flume_f425_0()
{    return ((bitField0_ & 0x00000008) == 0x00000008);}
public int flume_f426_0()
{    return queueHead_;}
public Builder flume_f427_0(int value)
{    bitField0_ |= 0x00000008;    queueHead_ = value;    onChanged();    return this;}
public Builder flume_f428_0()
{    bitField0_ = (bitField0_ & ~0x00000008);    queueHead_ = 0;    onChanged();    return this;}
private void flume_f429_0()
{    if (!((bitField0_ & 0x00000010) == 0x00000010)) {        activeLogs_ = new java.util.ArrayList<org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog>(activeLogs_);        bitField0_ |= 0x00000010;    }}
public java.util.List<org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog> flume_f430_0()
{    if (activeLogsBuilder_ == null) {        return java.util.Collections.unmodifiableList(activeLogs_);    } else {        return activeLogsBuilder_.getMessageList();    }}
public int flume_f431_0()
{    if (activeLogsBuilder_ == null) {        return activeLogs_.size();    } else {        return activeLogsBuilder_.getCount();    }}
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f432_0(int index)
{    if (activeLogsBuilder_ == null) {        return activeLogs_.get(index);    } else {        return activeLogsBuilder_.getMessage(index);    }}
public Builder flume_f433_0(int index, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog value)
{    if (activeLogsBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        ensureActiveLogsIsMutable();        activeLogs_.set(index, value);        onChanged();    } else {        activeLogsBuilder_.setMessage(index, value);    }    return this;}
public Builder flume_f434_0(int index, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder builderForValue)
{    if (activeLogsBuilder_ == null) {        ensureActiveLogsIsMutable();        activeLogs_.set(index, builderForValue.build());        onChanged();    } else {        activeLogsBuilder_.setMessage(index, builderForValue.build());    }    return this;}
public Builder flume_f435_0(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog value)
{    if (activeLogsBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        ensureActiveLogsIsMutable();        activeLogs_.add(value);        onChanged();    } else {        activeLogsBuilder_.addMessage(value);    }    return this;}
public Builder flume_f436_0(int index, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog value)
{    if (activeLogsBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        ensureActiveLogsIsMutable();        activeLogs_.add(index, value);        onChanged();    } else {        activeLogsBuilder_.addMessage(index, value);    }    return this;}
public Builder flume_f437_0(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder builderForValue)
{    if (activeLogsBuilder_ == null) {        ensureActiveLogsIsMutable();        activeLogs_.add(builderForValue.build());        onChanged();    } else {        activeLogsBuilder_.addMessage(builderForValue.build());    }    return this;}
public Builder flume_f438_0(int index, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder builderForValue)
{    if (activeLogsBuilder_ == null) {        ensureActiveLogsIsMutable();        activeLogs_.add(index, builderForValue.build());        onChanged();    } else {        activeLogsBuilder_.addMessage(index, builderForValue.build());    }    return this;}
public Builder flume_f439_0(java.lang.Iterable<? extends org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog> values)
{    if (activeLogsBuilder_ == null) {        ensureActiveLogsIsMutable();        super.addAll(values, activeLogs_);        onChanged();    } else {        activeLogsBuilder_.addAllMessages(values);    }    return this;}
public Builder flume_f440_0()
{    if (activeLogsBuilder_ == null) {        activeLogs_ = java.util.Collections.emptyList();        bitField0_ = (bitField0_ & ~0x00000010);        onChanged();    } else {        activeLogsBuilder_.clear();    }    return this;}
public Builder flume_f441_0(int index)
{    if (activeLogsBuilder_ == null) {        ensureActiveLogsIsMutable();        activeLogs_.remove(index);        onChanged();    } else {        activeLogsBuilder_.remove(index);    }    return this;}
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder flume_f442_0(int index)
{    return getActiveLogsFieldBuilder().getBuilder(index);}
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLogOrBuilder flume_f443_0(int index)
{    if (activeLogsBuilder_ == null) {        return activeLogs_.get(index);    } else {        return activeLogsBuilder_.getMessageOrBuilder(index);    }}
public java.util.List<? extends org.apache.flume.channel.file.proto.ProtosFactory.ActiveLogOrBuilder> flume_f444_0()
{    if (activeLogsBuilder_ != null) {        return activeLogsBuilder_.getMessageOrBuilderList();    } else {        return java.util.Collections.unmodifiableList(activeLogs_);    }}
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder flume_f445_0()
{    return getActiveLogsFieldBuilder().addBuilder(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.getDefaultInstance());}
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder flume_f446_0(int index)
{    return getActiveLogsFieldBuilder().addBuilder(index, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.getDefaultInstance());}
public java.util.List<org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder> flume_f447_0()
{    return getActiveLogsFieldBuilder().getBuilderList();}
private com.google.protobuf.RepeatedFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLogOrBuilder> flume_f448_0()
{    if (activeLogsBuilder_ == null) {        activeLogsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLogOrBuilder>(activeLogs_, ((bitField0_ & 0x00000010) == 0x00000010), getParentForChildren(), isClean());        activeLogs_ = null;    }    return activeLogsBuilder_;}
public static ActiveLog flume_f449_0()
{    return defaultInstance;}
public ActiveLog flume_f450_0()
{    return defaultInstance;}
public final com.google.protobuf.UnknownFieldSet flume_f451_0()
{    return this.unknownFields;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f452_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_ActiveLog_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f453_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_ActiveLog_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.class, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder.class);}
public ActiveLog flume_f454_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new ActiveLog(input, extensionRegistry);}
public com.google.protobuf.Parser<ActiveLog> flume_f455_0()
{    return PARSER;}
public boolean flume_f456_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public int flume_f457_0()
{    return logFileID_;}
public boolean flume_f458_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public int flume_f459_0()
{    return count_;}
private void flume_f460_0()
{    logFileID_ = 0;    count_ = 0;}
public final boolean flume_f461_0()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasLogFileID()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasCount()) {        memoizedIsInitialized = 0;        return false;    }    memoizedIsInitialized = 1;    return true;}
public void flume_f462_0(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeSFixed32(1, logFileID_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeSFixed32(2, count_);    }    getUnknownFields().writeTo(output);}
public int flume_f463_0()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(1, logFileID_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(2, count_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
protected java.lang.Object flume_f464_0() throws java.io.ObjectStreamException
{    return super.writeReplace();}
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f465_0(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f466_0(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f467_0(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f468_0(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f469_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f470_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f471_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f472_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f473_0(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f474_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static Builder flume_f475_0()
{    return Builder.create();}
public Builder flume_f476_0()
{    return newBuilder();}
public static Builder flume_f477_0(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog prototype)
{    return newBuilder().mergeFrom(prototype);}
public Builder flume_f478_0()
{    return newBuilder(this);}
protected Builder flume_f479_0(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f480_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_ActiveLog_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f481_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_ActiveLog_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.class, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder.class);}
private void flume_f482_0()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
private static Builder flume_f483_0()
{    return new Builder();}
public Builder flume_f484_0()
{    super.clear();    logFileID_ = 0;    bitField0_ = (bitField0_ & ~0x00000001);    count_ = 0;    bitField0_ = (bitField0_ & ~0x00000002);    return this;}
public Builder flume_f485_0()
{    return create().mergeFrom(buildPartial());}
public com.google.protobuf.Descriptors.Descriptor flume_f486_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_ActiveLog_descriptor;}
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f487_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.getDefaultInstance();}
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f488_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog flume_f489_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog result = new org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.logFileID_ = logFileID_;    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.count_ = count_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
public Builder flume_f490_0(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog) other);    } else {        super.mergeFrom(other);        return this;    }}
public Builder flume_f491_0(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.getDefaultInstance())        return this;    if (other.hasLogFileID()) {        setLogFileID(other.getLogFileID());    }    if (other.hasCount()) {        setCount(other.getCount());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
public final boolean flume_f492_0()
{    if (!hasLogFileID()) {        return false;    }    if (!hasCount()) {        return false;    }    return true;}
public Builder flume_f493_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
public boolean flume_f494_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public int flume_f495_0()
{    return logFileID_;}
public Builder flume_f496_0(int value)
{    bitField0_ |= 0x00000001;    logFileID_ = value;    onChanged();    return this;}
public Builder flume_f497_0()
{    bitField0_ = (bitField0_ & ~0x00000001);    logFileID_ = 0;    onChanged();    return this;}
public boolean flume_f498_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public int flume_f499_0()
{    return count_;}
public Builder flume_f500_0(int value)
{    bitField0_ |= 0x00000002;    count_ = value;    onChanged();    return this;}
public Builder flume_f501_0()
{    bitField0_ = (bitField0_ & ~0x00000002);    count_ = 0;    onChanged();    return this;}
public static LogFileMetaData flume_f502_0()
{    return defaultInstance;}
public LogFileMetaData flume_f503_0()
{    return defaultInstance;}
public final com.google.protobuf.UnknownFieldSet flume_f504_0()
{    return this.unknownFields;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f505_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileMetaData_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f506_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileMetaData_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData.class, org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData.Builder.class);}
public LogFileMetaData flume_f507_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new LogFileMetaData(input, extensionRegistry);}
public com.google.protobuf.Parser<LogFileMetaData> flume_f508_0()
{    return PARSER;}
public boolean flume_f509_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public int flume_f510_0()
{    return version_;}
public boolean flume_f511_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public int flume_f512_0()
{    return logFileID_;}
public boolean flume_f513_0()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
public long flume_f514_0()
{    return checkpointPosition_;}
public boolean flume_f515_0()
{    return ((bitField0_ & 0x00000008) == 0x00000008);}
public long flume_f516_0()
{    return checkpointWriteOrderID_;}
public boolean flume_f517_0()
{    return ((bitField0_ & 0x00000010) == 0x00000010);}
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f518_0()
{    return encryption_;}
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryptionOrBuilder flume_f519_0()
{    return encryption_;}
public boolean flume_f520_0()
{    return ((bitField0_ & 0x00000020) == 0x00000020);}
public long flume_f521_0()
{    return backupCheckpointPosition_;}
public boolean flume_f522_0()
{    return ((bitField0_ & 0x00000040) == 0x00000040);}
public long flume_f523_0()
{    return backupCheckpointWriteOrderID_;}
private void flume_f524_0()
{    version_ = 0;    logFileID_ = 0;    checkpointPosition_ = 0L;    checkpointWriteOrderID_ = 0L;    encryption_ = org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.getDefaultInstance();    backupCheckpointPosition_ = 0L;    backupCheckpointWriteOrderID_ = 0L;}
public final boolean flume_f525_0()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasVersion()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasLogFileID()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasCheckpointPosition()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasCheckpointWriteOrderID()) {        memoizedIsInitialized = 0;        return false;    }    if (hasEncryption()) {        if (!getEncryption().isInitialized()) {            memoizedIsInitialized = 0;            return false;        }    }    memoizedIsInitialized = 1;    return true;}
public void flume_f526_0(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeSFixed32(1, version_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeSFixed32(2, logFileID_);    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        output.writeSFixed64(3, checkpointPosition_);    }    if (((bitField0_ & 0x00000008) == 0x00000008)) {        output.writeSFixed64(4, checkpointWriteOrderID_);    }    if (((bitField0_ & 0x00000010) == 0x00000010)) {        output.writeMessage(5, encryption_);    }    if (((bitField0_ & 0x00000020) == 0x00000020)) {        output.writeSFixed64(6, backupCheckpointPosition_);    }    if (((bitField0_ & 0x00000040) == 0x00000040)) {        output.writeSFixed64(7, backupCheckpointWriteOrderID_);    }    getUnknownFields().writeTo(output);}
public int flume_f527_0()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(1, version_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(2, logFileID_);    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(3, checkpointPosition_);    }    if (((bitField0_ & 0x00000008) == 0x00000008)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(4, checkpointWriteOrderID_);    }    if (((bitField0_ & 0x00000010) == 0x00000010)) {        size += com.google.protobuf.CodedOutputStream.computeMessageSize(5, encryption_);    }    if (((bitField0_ & 0x00000020) == 0x00000020)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(6, backupCheckpointPosition_);    }    if (((bitField0_ & 0x00000040) == 0x00000040)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(7, backupCheckpointWriteOrderID_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
protected java.lang.Object flume_f528_0() throws java.io.ObjectStreamException
{    return super.writeReplace();}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData flume_f529_0(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData flume_f530_0(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData flume_f531_0(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData flume_f532_0(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData flume_f533_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData flume_f534_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData flume_f535_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData flume_f536_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData flume_f537_0(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData flume_f538_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static Builder flume_f539_0()
{    return Builder.create();}
public Builder flume_f540_0()
{    return newBuilder();}
public static Builder flume_f541_0(org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData prototype)
{    return newBuilder().mergeFrom(prototype);}
public Builder flume_f542_0()
{    return newBuilder(this);}
protected Builder flume_f543_0(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f544_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileMetaData_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f545_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileMetaData_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData.class, org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData.Builder.class);}
private void flume_f546_0()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {        getEncryptionFieldBuilder();    }}
private static Builder flume_f547_0()
{    return new Builder();}
public Builder flume_f548_0()
{    super.clear();    version_ = 0;    bitField0_ = (bitField0_ & ~0x00000001);    logFileID_ = 0;    bitField0_ = (bitField0_ & ~0x00000002);    checkpointPosition_ = 0L;    bitField0_ = (bitField0_ & ~0x00000004);    checkpointWriteOrderID_ = 0L;    bitField0_ = (bitField0_ & ~0x00000008);    if (encryptionBuilder_ == null) {        encryption_ = org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.getDefaultInstance();    } else {        encryptionBuilder_.clear();    }    bitField0_ = (bitField0_ & ~0x00000010);    backupCheckpointPosition_ = 0L;    bitField0_ = (bitField0_ & ~0x00000020);    backupCheckpointWriteOrderID_ = 0L;    bitField0_ = (bitField0_ & ~0x00000040);    return this;}
public Builder flume_f549_0()
{    return create().mergeFrom(buildPartial());}
public com.google.protobuf.Descriptors.Descriptor flume_f550_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileMetaData_descriptor;}
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData flume_f551_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData.getDefaultInstance();}
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData flume_f552_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData flume_f553_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData result = new org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.version_ = version_;    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.logFileID_ = logFileID_;    if (((from_bitField0_ & 0x00000004) == 0x00000004)) {        to_bitField0_ |= 0x00000004;    }    result.checkpointPosition_ = checkpointPosition_;    if (((from_bitField0_ & 0x00000008) == 0x00000008)) {        to_bitField0_ |= 0x00000008;    }    result.checkpointWriteOrderID_ = checkpointWriteOrderID_;    if (((from_bitField0_ & 0x00000010) == 0x00000010)) {        to_bitField0_ |= 0x00000010;    }    if (encryptionBuilder_ == null) {        result.encryption_ = encryption_;    } else {        result.encryption_ = encryptionBuilder_.build();    }    if (((from_bitField0_ & 0x00000020) == 0x00000020)) {        to_bitField0_ |= 0x00000020;    }    result.backupCheckpointPosition_ = backupCheckpointPosition_;    if (((from_bitField0_ & 0x00000040) == 0x00000040)) {        to_bitField0_ |= 0x00000040;    }    result.backupCheckpointWriteOrderID_ = backupCheckpointWriteOrderID_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
public Builder flume_f554_0(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData) other);    } else {        super.mergeFrom(other);        return this;    }}
public Builder flume_f555_0(org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData.getDefaultInstance())        return this;    if (other.hasVersion()) {        setVersion(other.getVersion());    }    if (other.hasLogFileID()) {        setLogFileID(other.getLogFileID());    }    if (other.hasCheckpointPosition()) {        setCheckpointPosition(other.getCheckpointPosition());    }    if (other.hasCheckpointWriteOrderID()) {        setCheckpointWriteOrderID(other.getCheckpointWriteOrderID());    }    if (other.hasEncryption()) {        mergeEncryption(other.getEncryption());    }    if (other.hasBackupCheckpointPosition()) {        setBackupCheckpointPosition(other.getBackupCheckpointPosition());    }    if (other.hasBackupCheckpointWriteOrderID()) {        setBackupCheckpointWriteOrderID(other.getBackupCheckpointWriteOrderID());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
public final boolean flume_f556_0()
{    if (!hasVersion()) {        return false;    }    if (!hasLogFileID()) {        return false;    }    if (!hasCheckpointPosition()) {        return false;    }    if (!hasCheckpointWriteOrderID()) {        return false;    }    if (hasEncryption()) {        if (!getEncryption().isInitialized()) {            return false;        }    }    return true;}
public Builder flume_f557_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
public boolean flume_f558_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public int flume_f559_0()
{    return version_;}
public Builder flume_f560_0(int value)
{    bitField0_ |= 0x00000001;    version_ = value;    onChanged();    return this;}
public Builder flume_f561_0()
{    bitField0_ = (bitField0_ & ~0x00000001);    version_ = 0;    onChanged();    return this;}
public boolean flume_f562_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public int flume_f563_0()
{    return logFileID_;}
public Builder flume_f564_0(int value)
{    bitField0_ |= 0x00000002;    logFileID_ = value;    onChanged();    return this;}
public Builder flume_f565_0()
{    bitField0_ = (bitField0_ & ~0x00000002);    logFileID_ = 0;    onChanged();    return this;}
public boolean flume_f566_0()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
public long flume_f567_0()
{    return checkpointPosition_;}
public Builder flume_f568_0(long value)
{    bitField0_ |= 0x00000004;    checkpointPosition_ = value;    onChanged();    return this;}
public Builder flume_f569_0()
{    bitField0_ = (bitField0_ & ~0x00000004);    checkpointPosition_ = 0L;    onChanged();    return this;}
public boolean flume_f570_0()
{    return ((bitField0_ & 0x00000008) == 0x00000008);}
public long flume_f571_0()
{    return checkpointWriteOrderID_;}
public Builder flume_f572_0(long value)
{    bitField0_ |= 0x00000008;    checkpointWriteOrderID_ = value;    onChanged();    return this;}
public Builder flume_f573_0()
{    bitField0_ = (bitField0_ & ~0x00000008);    checkpointWriteOrderID_ = 0L;    onChanged();    return this;}
public boolean flume_f574_0()
{    return ((bitField0_ & 0x00000010) == 0x00000010);}
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f575_0()
{    if (encryptionBuilder_ == null) {        return encryption_;    } else {        return encryptionBuilder_.getMessage();    }}
public Builder flume_f576_0(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption value)
{    if (encryptionBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        encryption_ = value;        onChanged();    } else {        encryptionBuilder_.setMessage(value);    }    bitField0_ |= 0x00000010;    return this;}
public Builder flume_f577_0(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.Builder builderForValue)
{    if (encryptionBuilder_ == null) {        encryption_ = builderForValue.build();        onChanged();    } else {        encryptionBuilder_.setMessage(builderForValue.build());    }    bitField0_ |= 0x00000010;    return this;}
public Builder flume_f578_0(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption value)
{    if (encryptionBuilder_ == null) {        if (((bitField0_ & 0x00000010) == 0x00000010) && encryption_ != org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.getDefaultInstance()) {            encryption_ = org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.newBuilder(encryption_).mergeFrom(value).buildPartial();        } else {            encryption_ = value;        }        onChanged();    } else {        encryptionBuilder_.mergeFrom(value);    }    bitField0_ |= 0x00000010;    return this;}
public Builder flume_f579_0()
{    if (encryptionBuilder_ == null) {        encryption_ = org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.getDefaultInstance();        onChanged();    } else {        encryptionBuilder_.clear();    }    bitField0_ = (bitField0_ & ~0x00000010);    return this;}
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.Builder flume_f580_0()
{    bitField0_ |= 0x00000010;    onChanged();    return getEncryptionFieldBuilder().getBuilder();}
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryptionOrBuilder flume_f581_0()
{    if (encryptionBuilder_ != null) {        return encryptionBuilder_.getMessageOrBuilder();    } else {        return encryption_;    }}
private com.google.protobuf.SingleFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption, org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.Builder, org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryptionOrBuilder> flume_f582_0()
{    if (encryptionBuilder_ == null) {        encryptionBuilder_ = new com.google.protobuf.SingleFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption, org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.Builder, org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryptionOrBuilder>(encryption_, getParentForChildren(), isClean());        encryption_ = null;    }    return encryptionBuilder_;}
public boolean flume_f583_0()
{    return ((bitField0_ & 0x00000020) == 0x00000020);}
public long flume_f584_0()
{    return backupCheckpointPosition_;}
public Builder flume_f585_0(long value)
{    bitField0_ |= 0x00000020;    backupCheckpointPosition_ = value;    onChanged();    return this;}
public Builder flume_f586_0()
{    bitField0_ = (bitField0_ & ~0x00000020);    backupCheckpointPosition_ = 0L;    onChanged();    return this;}
public boolean flume_f587_0()
{    return ((bitField0_ & 0x00000040) == 0x00000040);}
public long flume_f588_0()
{    return backupCheckpointWriteOrderID_;}
public Builder flume_f589_0(long value)
{    bitField0_ |= 0x00000040;    backupCheckpointWriteOrderID_ = value;    onChanged();    return this;}
public Builder flume_f590_0()
{    bitField0_ = (bitField0_ & ~0x00000040);    backupCheckpointWriteOrderID_ = 0L;    onChanged();    return this;}
public static LogFileEncryption flume_f591_0()
{    return defaultInstance;}
public LogFileEncryption flume_f592_0()
{    return defaultInstance;}
public final com.google.protobuf.UnknownFieldSet flume_f593_0()
{    return this.unknownFields;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f594_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileEncryption_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f595_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileEncryption_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.class, org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.Builder.class);}
public LogFileEncryption flume_f596_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new LogFileEncryption(input, extensionRegistry);}
public com.google.protobuf.Parser<LogFileEncryption> flume_f597_0()
{    return PARSER;}
public boolean flume_f598_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public java.lang.String flume_f599_0()
{    java.lang.Object ref = cipherProvider_;    if (ref instanceof java.lang.String) {        return (java.lang.String) ref;    } else {        com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;        java.lang.String s = bs.toStringUtf8();        if (bs.isValidUtf8()) {            cipherProvider_ = s;        }        return s;    }}
public com.google.protobuf.ByteString flume_f600_0()
{    java.lang.Object ref = cipherProvider_;    if (ref instanceof java.lang.String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        cipherProvider_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
public boolean flume_f601_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public java.lang.String flume_f602_0()
{    java.lang.Object ref = keyAlias_;    if (ref instanceof java.lang.String) {        return (java.lang.String) ref;    } else {        com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;        java.lang.String s = bs.toStringUtf8();        if (bs.isValidUtf8()) {            keyAlias_ = s;        }        return s;    }}
public com.google.protobuf.ByteString flume_f603_0()
{    java.lang.Object ref = keyAlias_;    if (ref instanceof java.lang.String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        keyAlias_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
public boolean flume_f604_0()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
public com.google.protobuf.ByteString flume_f605_0()
{    return parameters_;}
private void flume_f606_0()
{    cipherProvider_ = "";    keyAlias_ = "";    parameters_ = com.google.protobuf.ByteString.EMPTY;}
public final boolean flume_f607_0()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasCipherProvider()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasKeyAlias()) {        memoizedIsInitialized = 0;        return false;    }    memoizedIsInitialized = 1;    return true;}
public void flume_f608_0(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeBytes(1, getCipherProviderBytes());    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeBytes(2, getKeyAliasBytes());    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        output.writeBytes(3, parameters_);    }    getUnknownFields().writeTo(output);}
public int flume_f609_0()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeBytesSize(1, getCipherProviderBytes());    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeBytesSize(2, getKeyAliasBytes());    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        size += com.google.protobuf.CodedOutputStream.computeBytesSize(3, parameters_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
protected java.lang.Object flume_f610_0() throws java.io.ObjectStreamException
{    return super.writeReplace();}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f611_0(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f612_0(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f613_0(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f614_0(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f615_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f616_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f617_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f618_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f619_0(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f620_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static Builder flume_f621_0()
{    return Builder.create();}
public Builder flume_f622_0()
{    return newBuilder();}
public static Builder flume_f623_0(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption prototype)
{    return newBuilder().mergeFrom(prototype);}
public Builder flume_f624_0()
{    return newBuilder(this);}
protected Builder flume_f625_0(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f626_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileEncryption_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f627_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileEncryption_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.class, org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.Builder.class);}
private void flume_f628_0()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
private static Builder flume_f629_0()
{    return new Builder();}
public Builder flume_f630_0()
{    super.clear();    cipherProvider_ = "";    bitField0_ = (bitField0_ & ~0x00000001);    keyAlias_ = "";    bitField0_ = (bitField0_ & ~0x00000002);    parameters_ = com.google.protobuf.ByteString.EMPTY;    bitField0_ = (bitField0_ & ~0x00000004);    return this;}
public Builder flume_f631_0()
{    return create().mergeFrom(buildPartial());}
public com.google.protobuf.Descriptors.Descriptor flume_f632_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileEncryption_descriptor;}
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f633_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.getDefaultInstance();}
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f634_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption flume_f635_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption result = new org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.cipherProvider_ = cipherProvider_;    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.keyAlias_ = keyAlias_;    if (((from_bitField0_ & 0x00000004) == 0x00000004)) {        to_bitField0_ |= 0x00000004;    }    result.parameters_ = parameters_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
public Builder flume_f636_0(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption) other);    } else {        super.mergeFrom(other);        return this;    }}
public Builder flume_f637_0(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.getDefaultInstance())        return this;    if (other.hasCipherProvider()) {        bitField0_ |= 0x00000001;        cipherProvider_ = other.cipherProvider_;        onChanged();    }    if (other.hasKeyAlias()) {        bitField0_ |= 0x00000002;        keyAlias_ = other.keyAlias_;        onChanged();    }    if (other.hasParameters()) {        setParameters(other.getParameters());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
public final boolean flume_f638_0()
{    if (!hasCipherProvider()) {        return false;    }    if (!hasKeyAlias()) {        return false;    }    return true;}
public Builder flume_f639_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
public boolean flume_f640_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public java.lang.String flume_f641_0()
{    java.lang.Object ref = cipherProvider_;    if (!(ref instanceof java.lang.String)) {        java.lang.String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();        cipherProvider_ = s;        return s;    } else {        return (java.lang.String) ref;    }}
public com.google.protobuf.ByteString flume_f642_0()
{    java.lang.Object ref = cipherProvider_;    if (ref instanceof String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        cipherProvider_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
public Builder flume_f643_0(java.lang.String value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000001;    cipherProvider_ = value;    onChanged();    return this;}
public Builder flume_f644_0()
{    bitField0_ = (bitField0_ & ~0x00000001);    cipherProvider_ = getDefaultInstance().getCipherProvider();    onChanged();    return this;}
public Builder flume_f645_0(com.google.protobuf.ByteString value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000001;    cipherProvider_ = value;    onChanged();    return this;}
public boolean flume_f646_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public java.lang.String flume_f647_0()
{    java.lang.Object ref = keyAlias_;    if (!(ref instanceof java.lang.String)) {        java.lang.String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();        keyAlias_ = s;        return s;    } else {        return (java.lang.String) ref;    }}
public com.google.protobuf.ByteString flume_f648_0()
{    java.lang.Object ref = keyAlias_;    if (ref instanceof String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        keyAlias_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
public Builder flume_f649_0(java.lang.String value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000002;    keyAlias_ = value;    onChanged();    return this;}
public Builder flume_f650_0()
{    bitField0_ = (bitField0_ & ~0x00000002);    keyAlias_ = getDefaultInstance().getKeyAlias();    onChanged();    return this;}
public Builder flume_f651_0(com.google.protobuf.ByteString value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000002;    keyAlias_ = value;    onChanged();    return this;}
public boolean flume_f652_0()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
public com.google.protobuf.ByteString flume_f653_0()
{    return parameters_;}
public Builder flume_f654_0(com.google.protobuf.ByteString value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000004;    parameters_ = value;    onChanged();    return this;}
public Builder flume_f655_0()
{    bitField0_ = (bitField0_ & ~0x00000004);    parameters_ = getDefaultInstance().getParameters();    onChanged();    return this;}
public static TransactionEventHeader flume_f656_0()
{    return defaultInstance;}
public TransactionEventHeader flume_f657_0()
{    return defaultInstance;}
public final com.google.protobuf.UnknownFieldSet flume_f658_0()
{    return this.unknownFields;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f659_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventHeader_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f660_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventHeader_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader.class, org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader.Builder.class);}
public TransactionEventHeader flume_f661_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new TransactionEventHeader(input, extensionRegistry);}
public com.google.protobuf.Parser<TransactionEventHeader> flume_f662_0()
{    return PARSER;}
public boolean flume_f663_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public int flume_f664_0()
{    return type_;}
public boolean flume_f665_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public long flume_f666_0()
{    return transactionID_;}
public boolean flume_f667_0()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
public long flume_f668_0()
{    return writeOrderID_;}
private void flume_f669_0()
{    type_ = 0;    transactionID_ = 0L;    writeOrderID_ = 0L;}
public final boolean flume_f670_0()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasType()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasTransactionID()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasWriteOrderID()) {        memoizedIsInitialized = 0;        return false;    }    memoizedIsInitialized = 1;    return true;}
public void flume_f671_0(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeSFixed32(1, type_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeSFixed64(2, transactionID_);    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        output.writeSFixed64(3, writeOrderID_);    }    getUnknownFields().writeTo(output);}
public int flume_f672_0()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(1, type_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(2, transactionID_);    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(3, writeOrderID_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
protected java.lang.Object flume_f673_0() throws java.io.ObjectStreamException
{    return super.writeReplace();}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader flume_f674_0(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader flume_f675_0(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader flume_f676_0(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader flume_f677_0(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader flume_f678_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader flume_f679_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader flume_f680_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader flume_f681_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader flume_f682_0(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader flume_f683_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static Builder flume_f684_0()
{    return Builder.create();}
public Builder flume_f685_0()
{    return newBuilder();}
public static Builder flume_f686_0(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader prototype)
{    return newBuilder().mergeFrom(prototype);}
public Builder flume_f687_0()
{    return newBuilder(this);}
protected Builder flume_f688_0(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f689_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventHeader_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f690_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventHeader_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader.class, org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader.Builder.class);}
private void flume_f691_0()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
private static Builder flume_f692_0()
{    return new Builder();}
public Builder flume_f693_0()
{    super.clear();    type_ = 0;    bitField0_ = (bitField0_ & ~0x00000001);    transactionID_ = 0L;    bitField0_ = (bitField0_ & ~0x00000002);    writeOrderID_ = 0L;    bitField0_ = (bitField0_ & ~0x00000004);    return this;}
public Builder flume_f694_0()
{    return create().mergeFrom(buildPartial());}
public com.google.protobuf.Descriptors.Descriptor flume_f695_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventHeader_descriptor;}
public org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader flume_f696_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader.getDefaultInstance();}
public org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader flume_f697_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
public org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader flume_f698_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader result = new org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.type_ = type_;    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.transactionID_ = transactionID_;    if (((from_bitField0_ & 0x00000004) == 0x00000004)) {        to_bitField0_ |= 0x00000004;    }    result.writeOrderID_ = writeOrderID_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
public Builder flume_f699_0(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader) other);    } else {        super.mergeFrom(other);        return this;    }}
public Builder flume_f700_0(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader.getDefaultInstance())        return this;    if (other.hasType()) {        setType(other.getType());    }    if (other.hasTransactionID()) {        setTransactionID(other.getTransactionID());    }    if (other.hasWriteOrderID()) {        setWriteOrderID(other.getWriteOrderID());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
public final boolean flume_f701_0()
{    if (!hasType()) {        return false;    }    if (!hasTransactionID()) {        return false;    }    if (!hasWriteOrderID()) {        return false;    }    return true;}
public Builder flume_f702_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
public boolean flume_f703_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public int flume_f704_0()
{    return type_;}
public Builder flume_f705_0(int value)
{    bitField0_ |= 0x00000001;    type_ = value;    onChanged();    return this;}
public Builder flume_f706_0()
{    bitField0_ = (bitField0_ & ~0x00000001);    type_ = 0;    onChanged();    return this;}
public boolean flume_f707_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public long flume_f708_0()
{    return transactionID_;}
public Builder flume_f709_0(long value)
{    bitField0_ |= 0x00000002;    transactionID_ = value;    onChanged();    return this;}
public Builder flume_f710_0()
{    bitField0_ = (bitField0_ & ~0x00000002);    transactionID_ = 0L;    onChanged();    return this;}
public boolean flume_f711_0()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
public long flume_f712_0()
{    return writeOrderID_;}
public Builder flume_f713_0(long value)
{    bitField0_ |= 0x00000004;    writeOrderID_ = value;    onChanged();    return this;}
public Builder flume_f714_0()
{    bitField0_ = (bitField0_ & ~0x00000004);    writeOrderID_ = 0L;    onChanged();    return this;}
public static Put flume_f715_0()
{    return defaultInstance;}
public Put flume_f716_0()
{    return defaultInstance;}
public final com.google.protobuf.UnknownFieldSet flume_f717_0()
{    return this.unknownFields;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f718_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Put_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f719_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Put_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Put.class, org.apache.flume.channel.file.proto.ProtosFactory.Put.Builder.class);}
public Put flume_f720_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new Put(input, extensionRegistry);}
public com.google.protobuf.Parser<Put> flume_f721_0()
{    return PARSER;}
public boolean flume_f722_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f723_0()
{    return event_;}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventOrBuilder flume_f724_0()
{    return event_;}
public boolean flume_f725_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public long flume_f726_0()
{    return checksum_;}
private void flume_f727_0()
{    event_ = org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.getDefaultInstance();    checksum_ = 0L;}
public final boolean flume_f728_0()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasEvent()) {        memoizedIsInitialized = 0;        return false;    }    if (!getEvent().isInitialized()) {        memoizedIsInitialized = 0;        return false;    }    memoizedIsInitialized = 1;    return true;}
public void flume_f729_0(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeMessage(1, event_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeSFixed64(2, checksum_);    }    getUnknownFields().writeTo(output);}
public int flume_f730_0()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeMessageSize(1, event_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(2, checksum_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
protected java.lang.Object flume_f731_0() throws java.io.ObjectStreamException
{    return super.writeReplace();}
public static org.apache.flume.channel.file.proto.ProtosFactory.Put flume_f732_0(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Put flume_f733_0(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Put flume_f734_0(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Put flume_f735_0(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Put flume_f736_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Put flume_f737_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Put flume_f738_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Put flume_f739_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Put flume_f740_0(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Put flume_f741_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static Builder flume_f742_0()
{    return Builder.create();}
public Builder flume_f743_0()
{    return newBuilder();}
public static Builder flume_f744_0(org.apache.flume.channel.file.proto.ProtosFactory.Put prototype)
{    return newBuilder().mergeFrom(prototype);}
public Builder flume_f745_0()
{    return newBuilder(this);}
protected Builder flume_f746_0(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f747_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Put_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f748_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Put_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Put.class, org.apache.flume.channel.file.proto.ProtosFactory.Put.Builder.class);}
private void flume_f749_0()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {        getEventFieldBuilder();    }}
private static Builder flume_f750_0()
{    return new Builder();}
public Builder flume_f751_0()
{    super.clear();    if (eventBuilder_ == null) {        event_ = org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.getDefaultInstance();    } else {        eventBuilder_.clear();    }    bitField0_ = (bitField0_ & ~0x00000001);    checksum_ = 0L;    bitField0_ = (bitField0_ & ~0x00000002);    return this;}
public Builder flume_f752_0()
{    return create().mergeFrom(buildPartial());}
public com.google.protobuf.Descriptors.Descriptor flume_f753_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Put_descriptor;}
public org.apache.flume.channel.file.proto.ProtosFactory.Put flume_f754_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.Put.getDefaultInstance();}
public org.apache.flume.channel.file.proto.ProtosFactory.Put flume_f755_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.Put result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
public org.apache.flume.channel.file.proto.ProtosFactory.Put flume_f756_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.Put result = new org.apache.flume.channel.file.proto.ProtosFactory.Put(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    if (eventBuilder_ == null) {        result.event_ = event_;    } else {        result.event_ = eventBuilder_.build();    }    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.checksum_ = checksum_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
public Builder flume_f757_0(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.Put) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.Put) other);    } else {        super.mergeFrom(other);        return this;    }}
public Builder flume_f758_0(org.apache.flume.channel.file.proto.ProtosFactory.Put other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.Put.getDefaultInstance())        return this;    if (other.hasEvent()) {        mergeEvent(other.getEvent());    }    if (other.hasChecksum()) {        setChecksum(other.getChecksum());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
public final boolean flume_f759_0()
{    if (!hasEvent()) {        return false;    }    if (!getEvent().isInitialized()) {        return false;    }    return true;}
public Builder flume_f760_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.Put parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.Put) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
public boolean flume_f761_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f762_0()
{    if (eventBuilder_ == null) {        return event_;    } else {        return eventBuilder_.getMessage();    }}
public Builder flume_f763_0(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent value)
{    if (eventBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        event_ = value;        onChanged();    } else {        eventBuilder_.setMessage(value);    }    bitField0_ |= 0x00000001;    return this;}
public Builder flume_f764_0(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.Builder builderForValue)
{    if (eventBuilder_ == null) {        event_ = builderForValue.build();        onChanged();    } else {        eventBuilder_.setMessage(builderForValue.build());    }    bitField0_ |= 0x00000001;    return this;}
public Builder flume_f765_0(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent value)
{    if (eventBuilder_ == null) {        if (((bitField0_ & 0x00000001) == 0x00000001) && event_ != org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.getDefaultInstance()) {            event_ = org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.newBuilder(event_).mergeFrom(value).buildPartial();        } else {            event_ = value;        }        onChanged();    } else {        eventBuilder_.mergeFrom(value);    }    bitField0_ |= 0x00000001;    return this;}
public Builder flume_f766_0()
{    if (eventBuilder_ == null) {        event_ = org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.getDefaultInstance();        onChanged();    } else {        eventBuilder_.clear();    }    bitField0_ = (bitField0_ & ~0x00000001);    return this;}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.Builder flume_f767_0()
{    bitField0_ |= 0x00000001;    onChanged();    return getEventFieldBuilder().getBuilder();}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventOrBuilder flume_f768_0()
{    if (eventBuilder_ != null) {        return eventBuilder_.getMessageOrBuilder();    } else {        return event_;    }}
private com.google.protobuf.SingleFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.Builder, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventOrBuilder> flume_f769_0()
{    if (eventBuilder_ == null) {        eventBuilder_ = new com.google.protobuf.SingleFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.Builder, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventOrBuilder>(event_, getParentForChildren(), isClean());        event_ = null;    }    return eventBuilder_;}
public boolean flume_f770_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public long flume_f771_0()
{    return checksum_;}
public Builder flume_f772_0(long value)
{    bitField0_ |= 0x00000002;    checksum_ = value;    onChanged();    return this;}
public Builder flume_f773_0()
{    bitField0_ = (bitField0_ & ~0x00000002);    checksum_ = 0L;    onChanged();    return this;}
public static Take flume_f774_0()
{    return defaultInstance;}
public Take flume_f775_0()
{    return defaultInstance;}
public final com.google.protobuf.UnknownFieldSet flume_f776_0()
{    return this.unknownFields;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f777_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Take_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f778_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Take_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Take.class, org.apache.flume.channel.file.proto.ProtosFactory.Take.Builder.class);}
public Take flume_f779_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new Take(input, extensionRegistry);}
public com.google.protobuf.Parser<Take> flume_f780_0()
{    return PARSER;}
public boolean flume_f781_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public int flume_f782_0()
{    return fileID_;}
public boolean flume_f783_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public int flume_f784_0()
{    return offset_;}
private void flume_f785_0()
{    fileID_ = 0;    offset_ = 0;}
public final boolean flume_f786_0()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasFileID()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasOffset()) {        memoizedIsInitialized = 0;        return false;    }    memoizedIsInitialized = 1;    return true;}
public void flume_f787_0(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeSFixed32(1, fileID_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeSFixed32(2, offset_);    }    getUnknownFields().writeTo(output);}
public int flume_f788_0()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(1, fileID_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(2, offset_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
protected java.lang.Object flume_f789_0() throws java.io.ObjectStreamException
{    return super.writeReplace();}
public static org.apache.flume.channel.file.proto.ProtosFactory.Take flume_f790_0(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Take flume_f791_0(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Take flume_f792_0(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Take flume_f793_0(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Take flume_f794_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Take flume_f795_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Take flume_f796_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Take flume_f797_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Take flume_f798_0(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Take flume_f799_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static Builder flume_f800_0()
{    return Builder.create();}
public Builder flume_f801_0()
{    return newBuilder();}
public static Builder flume_f802_0(org.apache.flume.channel.file.proto.ProtosFactory.Take prototype)
{    return newBuilder().mergeFrom(prototype);}
public Builder flume_f803_0()
{    return newBuilder(this);}
protected Builder flume_f804_0(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f805_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Take_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f806_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Take_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Take.class, org.apache.flume.channel.file.proto.ProtosFactory.Take.Builder.class);}
private void flume_f807_0()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
private static Builder flume_f808_0()
{    return new Builder();}
public Builder flume_f809_0()
{    super.clear();    fileID_ = 0;    bitField0_ = (bitField0_ & ~0x00000001);    offset_ = 0;    bitField0_ = (bitField0_ & ~0x00000002);    return this;}
public Builder flume_f810_0()
{    return create().mergeFrom(buildPartial());}
public com.google.protobuf.Descriptors.Descriptor flume_f811_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Take_descriptor;}
public org.apache.flume.channel.file.proto.ProtosFactory.Take flume_f812_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.Take.getDefaultInstance();}
public org.apache.flume.channel.file.proto.ProtosFactory.Take flume_f813_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.Take result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
public org.apache.flume.channel.file.proto.ProtosFactory.Take flume_f814_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.Take result = new org.apache.flume.channel.file.proto.ProtosFactory.Take(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.fileID_ = fileID_;    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.offset_ = offset_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
public Builder flume_f815_0(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.Take) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.Take) other);    } else {        super.mergeFrom(other);        return this;    }}
public Builder flume_f816_0(org.apache.flume.channel.file.proto.ProtosFactory.Take other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.Take.getDefaultInstance())        return this;    if (other.hasFileID()) {        setFileID(other.getFileID());    }    if (other.hasOffset()) {        setOffset(other.getOffset());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
public final boolean flume_f817_0()
{    if (!hasFileID()) {        return false;    }    if (!hasOffset()) {        return false;    }    return true;}
public Builder flume_f818_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.Take parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.Take) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
public boolean flume_f819_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public int flume_f820_0()
{    return fileID_;}
public Builder flume_f821_0(int value)
{    bitField0_ |= 0x00000001;    fileID_ = value;    onChanged();    return this;}
public Builder flume_f822_0()
{    bitField0_ = (bitField0_ & ~0x00000001);    fileID_ = 0;    onChanged();    return this;}
public boolean flume_f823_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public int flume_f824_0()
{    return offset_;}
public Builder flume_f825_0(int value)
{    bitField0_ |= 0x00000002;    offset_ = value;    onChanged();    return this;}
public Builder flume_f826_0()
{    bitField0_ = (bitField0_ & ~0x00000002);    offset_ = 0;    onChanged();    return this;}
public static Rollback flume_f827_0()
{    return defaultInstance;}
public Rollback flume_f828_0()
{    return defaultInstance;}
public final com.google.protobuf.UnknownFieldSet flume_f829_0()
{    return this.unknownFields;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f830_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Rollback_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f831_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Rollback_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Rollback.class, org.apache.flume.channel.file.proto.ProtosFactory.Rollback.Builder.class);}
public Rollback flume_f832_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new Rollback(input, extensionRegistry);}
public com.google.protobuf.Parser<Rollback> flume_f833_0()
{    return PARSER;}
private void flume_f834_0()
{}
public final boolean flume_f835_0()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    memoizedIsInitialized = 1;    return true;}
public void flume_f836_0(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    getUnknownFields().writeTo(output);}
public int flume_f837_0()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
protected java.lang.Object flume_f838_0() throws java.io.ObjectStreamException
{    return super.writeReplace();}
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback flume_f839_0(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback flume_f840_0(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback flume_f841_0(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback flume_f842_0(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback flume_f843_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback flume_f844_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback flume_f845_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback flume_f846_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback flume_f847_0(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback flume_f848_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static Builder flume_f849_0()
{    return Builder.create();}
public Builder flume_f850_0()
{    return newBuilder();}
public static Builder flume_f851_0(org.apache.flume.channel.file.proto.ProtosFactory.Rollback prototype)
{    return newBuilder().mergeFrom(prototype);}
public Builder flume_f852_0()
{    return newBuilder(this);}
protected Builder flume_f853_0(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f854_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Rollback_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f855_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Rollback_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Rollback.class, org.apache.flume.channel.file.proto.ProtosFactory.Rollback.Builder.class);}
private void flume_f856_0()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
private static Builder flume_f857_0()
{    return new Builder();}
public Builder flume_f858_0()
{    super.clear();    return this;}
public Builder flume_f859_0()
{    return create().mergeFrom(buildPartial());}
public com.google.protobuf.Descriptors.Descriptor flume_f860_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Rollback_descriptor;}
public org.apache.flume.channel.file.proto.ProtosFactory.Rollback flume_f861_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.Rollback.getDefaultInstance();}
public org.apache.flume.channel.file.proto.ProtosFactory.Rollback flume_f862_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.Rollback result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
public org.apache.flume.channel.file.proto.ProtosFactory.Rollback flume_f863_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.Rollback result = new org.apache.flume.channel.file.proto.ProtosFactory.Rollback(this);    onBuilt();    return result;}
public Builder flume_f864_0(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.Rollback) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.Rollback) other);    } else {        super.mergeFrom(other);        return this;    }}
public Builder flume_f865_0(org.apache.flume.channel.file.proto.ProtosFactory.Rollback other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.Rollback.getDefaultInstance())        return this;    this.mergeUnknownFields(other.getUnknownFields());    return this;}
public final boolean flume_f866_0()
{    return true;}
public Builder flume_f867_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.Rollback parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.Rollback) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
public static Commit flume_f868_0()
{    return defaultInstance;}
public Commit flume_f869_0()
{    return defaultInstance;}
public final com.google.protobuf.UnknownFieldSet flume_f870_0()
{    return this.unknownFields;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f871_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Commit_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f872_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Commit_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Commit.class, org.apache.flume.channel.file.proto.ProtosFactory.Commit.Builder.class);}
public Commit flume_f873_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new Commit(input, extensionRegistry);}
public com.google.protobuf.Parser<Commit> flume_f874_0()
{    return PARSER;}
public boolean flume_f875_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public int flume_f876_0()
{    return type_;}
private void flume_f877_0()
{    type_ = 0;}
public final boolean flume_f878_0()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasType()) {        memoizedIsInitialized = 0;        return false;    }    memoizedIsInitialized = 1;    return true;}
public void flume_f879_0(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeSFixed32(1, type_);    }    getUnknownFields().writeTo(output);}
public int flume_f880_0()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(1, type_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
protected java.lang.Object flume_f881_0() throws java.io.ObjectStreamException
{    return super.writeReplace();}
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit flume_f882_0(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit flume_f883_0(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit flume_f884_0(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit flume_f885_0(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit flume_f886_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit flume_f887_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit flume_f888_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit flume_f889_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit flume_f890_0(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit flume_f891_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static Builder flume_f892_0()
{    return Builder.create();}
public Builder flume_f893_0()
{    return newBuilder();}
public static Builder flume_f894_0(org.apache.flume.channel.file.proto.ProtosFactory.Commit prototype)
{    return newBuilder().mergeFrom(prototype);}
public Builder flume_f895_0()
{    return newBuilder(this);}
protected Builder flume_f896_0(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f897_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Commit_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f898_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Commit_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Commit.class, org.apache.flume.channel.file.proto.ProtosFactory.Commit.Builder.class);}
private void flume_f899_0()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
private static Builder flume_f900_0()
{    return new Builder();}
public Builder flume_f901_0()
{    super.clear();    type_ = 0;    bitField0_ = (bitField0_ & ~0x00000001);    return this;}
public Builder flume_f902_0()
{    return create().mergeFrom(buildPartial());}
public com.google.protobuf.Descriptors.Descriptor flume_f903_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Commit_descriptor;}
public org.apache.flume.channel.file.proto.ProtosFactory.Commit flume_f904_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.Commit.getDefaultInstance();}
public org.apache.flume.channel.file.proto.ProtosFactory.Commit flume_f905_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.Commit result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
public org.apache.flume.channel.file.proto.ProtosFactory.Commit flume_f906_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.Commit result = new org.apache.flume.channel.file.proto.ProtosFactory.Commit(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.type_ = type_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
public Builder flume_f907_0(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.Commit) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.Commit) other);    } else {        super.mergeFrom(other);        return this;    }}
public Builder flume_f908_0(org.apache.flume.channel.file.proto.ProtosFactory.Commit other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.Commit.getDefaultInstance())        return this;    if (other.hasType()) {        setType(other.getType());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
public final boolean flume_f909_0()
{    if (!hasType()) {        return false;    }    return true;}
public Builder flume_f910_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.Commit parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.Commit) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
public boolean flume_f911_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public int flume_f912_0()
{    return type_;}
public Builder flume_f913_0(int value)
{    bitField0_ |= 0x00000001;    type_ = value;    onChanged();    return this;}
public Builder flume_f914_0()
{    bitField0_ = (bitField0_ & ~0x00000001);    type_ = 0;    onChanged();    return this;}
public static TransactionEventFooter flume_f915_0()
{    return defaultInstance;}
public TransactionEventFooter flume_f916_0()
{    return defaultInstance;}
public final com.google.protobuf.UnknownFieldSet flume_f917_0()
{    return this.unknownFields;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f918_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventFooter_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f919_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventFooter_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter.class, org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter.Builder.class);}
public TransactionEventFooter flume_f920_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new TransactionEventFooter(input, extensionRegistry);}
public com.google.protobuf.Parser<TransactionEventFooter> flume_f921_0()
{    return PARSER;}
private void flume_f922_0()
{}
public final boolean flume_f923_0()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    memoizedIsInitialized = 1;    return true;}
public void flume_f924_0(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    getUnknownFields().writeTo(output);}
public int flume_f925_0()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
protected java.lang.Object flume_f926_0() throws java.io.ObjectStreamException
{    return super.writeReplace();}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter flume_f927_0(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter flume_f928_0(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter flume_f929_0(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter flume_f930_0(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter flume_f931_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter flume_f932_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter flume_f933_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter flume_f934_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter flume_f935_0(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter flume_f936_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static Builder flume_f937_0()
{    return Builder.create();}
public Builder flume_f938_0()
{    return newBuilder();}
public static Builder flume_f939_0(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter prototype)
{    return newBuilder().mergeFrom(prototype);}
public Builder flume_f940_0()
{    return newBuilder(this);}
protected Builder flume_f941_0(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f942_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventFooter_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f943_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventFooter_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter.class, org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter.Builder.class);}
private void flume_f944_0()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
private static Builder flume_f945_0()
{    return new Builder();}
public Builder flume_f946_0()
{    super.clear();    return this;}
public Builder flume_f947_0()
{    return create().mergeFrom(buildPartial());}
public com.google.protobuf.Descriptors.Descriptor flume_f948_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventFooter_descriptor;}
public org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter flume_f949_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter.getDefaultInstance();}
public org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter flume_f950_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
public org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter flume_f951_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter result = new org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter(this);    onBuilt();    return result;}
public Builder flume_f952_0(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter) other);    } else {        super.mergeFrom(other);        return this;    }}
public Builder flume_f953_0(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter.getDefaultInstance())        return this;    this.mergeUnknownFields(other.getUnknownFields());    return this;}
public final boolean flume_f954_0()
{    return true;}
public Builder flume_f955_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
public static FlumeEvent flume_f956_0()
{    return defaultInstance;}
public FlumeEvent flume_f957_0()
{    return defaultInstance;}
public final com.google.protobuf.UnknownFieldSet flume_f958_0()
{    return this.unknownFields;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f959_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEvent_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f960_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEvent_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.class, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.Builder.class);}
public FlumeEvent flume_f961_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new FlumeEvent(input, extensionRegistry);}
public com.google.protobuf.Parser<FlumeEvent> flume_f962_0()
{    return PARSER;}
public java.util.List<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader> flume_f963_0()
{    return headers_;}
public java.util.List<? extends org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeaderOrBuilder> flume_f964_0()
{    return headers_;}
public int flume_f965_0()
{    return headers_.size();}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f966_0(int index)
{    return headers_.get(index);}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeaderOrBuilder flume_f967_0(int index)
{    return headers_.get(index);}
public boolean flume_f968_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public com.google.protobuf.ByteString flume_f969_0()
{    return body_;}
private void flume_f970_0()
{    headers_ = java.util.Collections.emptyList();    body_ = com.google.protobuf.ByteString.EMPTY;}
public final boolean flume_f971_0()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasBody()) {        memoizedIsInitialized = 0;        return false;    }    for (int i = 0; i < getHeadersCount(); i++) {        if (!getHeaders(i).isInitialized()) {            memoizedIsInitialized = 0;            return false;        }    }    memoizedIsInitialized = 1;    return true;}
public void flume_f972_0(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    for (int i = 0; i < headers_.size(); i++) {        output.writeMessage(1, headers_.get(i));    }    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeBytes(2, body_);    }    getUnknownFields().writeTo(output);}
public int flume_f973_0()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    for (int i = 0; i < headers_.size(); i++) {        size += com.google.protobuf.CodedOutputStream.computeMessageSize(1, headers_.get(i));    }    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeBytesSize(2, body_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
protected java.lang.Object flume_f974_0() throws java.io.ObjectStreamException
{    return super.writeReplace();}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f975_0(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f976_0(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f977_0(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f978_0(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f979_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f980_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f981_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f982_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f983_0(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f984_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static Builder flume_f985_0()
{    return Builder.create();}
public Builder flume_f986_0()
{    return newBuilder();}
public static Builder flume_f987_0(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent prototype)
{    return newBuilder().mergeFrom(prototype);}
public Builder flume_f988_0()
{    return newBuilder(this);}
protected Builder flume_f989_0(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f990_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEvent_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f991_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEvent_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.class, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.Builder.class);}
private void flume_f992_0()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {        getHeadersFieldBuilder();    }}
private static Builder flume_f993_0()
{    return new Builder();}
public Builder flume_f994_0()
{    super.clear();    if (headersBuilder_ == null) {        headers_ = java.util.Collections.emptyList();        bitField0_ = (bitField0_ & ~0x00000001);    } else {        headersBuilder_.clear();    }    body_ = com.google.protobuf.ByteString.EMPTY;    bitField0_ = (bitField0_ & ~0x00000002);    return this;}
public Builder flume_f995_0()
{    return create().mergeFrom(buildPartial());}
public com.google.protobuf.Descriptors.Descriptor flume_f996_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEvent_descriptor;}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f997_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.getDefaultInstance();}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f998_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent flume_f999_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent result = new org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (headersBuilder_ == null) {        if (((bitField0_ & 0x00000001) == 0x00000001)) {            headers_ = java.util.Collections.unmodifiableList(headers_);            bitField0_ = (bitField0_ & ~0x00000001);        }        result.headers_ = headers_;    } else {        result.headers_ = headersBuilder_.build();    }    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000001;    }    result.body_ = body_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
public Builder flume_f1000_0(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent) other);    } else {        super.mergeFrom(other);        return this;    }}
public Builder flume_f1001_0(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.getDefaultInstance())        return this;    if (headersBuilder_ == null) {        if (!other.headers_.isEmpty()) {            if (headers_.isEmpty()) {                headers_ = other.headers_;                bitField0_ = (bitField0_ & ~0x00000001);            } else {                ensureHeadersIsMutable();                headers_.addAll(other.headers_);            }            onChanged();        }    } else {        if (!other.headers_.isEmpty()) {            if (headersBuilder_.isEmpty()) {                headersBuilder_.dispose();                headersBuilder_ = null;                headers_ = other.headers_;                bitField0_ = (bitField0_ & ~0x00000001);                headersBuilder_ = com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ? getHeadersFieldBuilder() : null;            } else {                headersBuilder_.addAllMessages(other.headers_);            }        }    }    if (other.hasBody()) {        setBody(other.getBody());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
public final boolean flume_f1002_0()
{    if (!hasBody()) {        return false;    }    for (int i = 0; i < getHeadersCount(); i++) {        if (!getHeaders(i).isInitialized()) {            return false;        }    }    return true;}
public Builder flume_f1003_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
private void flume_f1004_0()
{    if (!((bitField0_ & 0x00000001) == 0x00000001)) {        headers_ = new java.util.ArrayList<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader>(headers_);        bitField0_ |= 0x00000001;    }}
public java.util.List<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader> flume_f1005_0()
{    if (headersBuilder_ == null) {        return java.util.Collections.unmodifiableList(headers_);    } else {        return headersBuilder_.getMessageList();    }}
public int flume_f1006_0()
{    if (headersBuilder_ == null) {        return headers_.size();    } else {        return headersBuilder_.getCount();    }}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f1007_0(int index)
{    if (headersBuilder_ == null) {        return headers_.get(index);    } else {        return headersBuilder_.getMessage(index);    }}
public Builder flume_f1008_0(int index, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader value)
{    if (headersBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        ensureHeadersIsMutable();        headers_.set(index, value);        onChanged();    } else {        headersBuilder_.setMessage(index, value);    }    return this;}
public Builder flume_f1009_0(int index, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder builderForValue)
{    if (headersBuilder_ == null) {        ensureHeadersIsMutable();        headers_.set(index, builderForValue.build());        onChanged();    } else {        headersBuilder_.setMessage(index, builderForValue.build());    }    return this;}
public Builder flume_f1010_0(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader value)
{    if (headersBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        ensureHeadersIsMutable();        headers_.add(value);        onChanged();    } else {        headersBuilder_.addMessage(value);    }    return this;}
public Builder flume_f1011_0(int index, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader value)
{    if (headersBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        ensureHeadersIsMutable();        headers_.add(index, value);        onChanged();    } else {        headersBuilder_.addMessage(index, value);    }    return this;}
public Builder flume_f1012_0(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder builderForValue)
{    if (headersBuilder_ == null) {        ensureHeadersIsMutable();        headers_.add(builderForValue.build());        onChanged();    } else {        headersBuilder_.addMessage(builderForValue.build());    }    return this;}
public Builder flume_f1013_0(int index, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder builderForValue)
{    if (headersBuilder_ == null) {        ensureHeadersIsMutable();        headers_.add(index, builderForValue.build());        onChanged();    } else {        headersBuilder_.addMessage(index, builderForValue.build());    }    return this;}
public Builder flume_f1014_0(java.lang.Iterable<? extends org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader> values)
{    if (headersBuilder_ == null) {        ensureHeadersIsMutable();        super.addAll(values, headers_);        onChanged();    } else {        headersBuilder_.addAllMessages(values);    }    return this;}
public Builder flume_f1015_0()
{    if (headersBuilder_ == null) {        headers_ = java.util.Collections.emptyList();        bitField0_ = (bitField0_ & ~0x00000001);        onChanged();    } else {        headersBuilder_.clear();    }    return this;}
public Builder flume_f1016_0(int index)
{    if (headersBuilder_ == null) {        ensureHeadersIsMutable();        headers_.remove(index);        onChanged();    } else {        headersBuilder_.remove(index);    }    return this;}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder flume_f1017_0(int index)
{    return getHeadersFieldBuilder().getBuilder(index);}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeaderOrBuilder flume_f1018_0(int index)
{    if (headersBuilder_ == null) {        return headers_.get(index);    } else {        return headersBuilder_.getMessageOrBuilder(index);    }}
public java.util.List<? extends org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeaderOrBuilder> flume_f1019_0()
{    if (headersBuilder_ != null) {        return headersBuilder_.getMessageOrBuilderList();    } else {        return java.util.Collections.unmodifiableList(headers_);    }}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder flume_f1020_0()
{    return getHeadersFieldBuilder().addBuilder(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.getDefaultInstance());}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder flume_f1021_0(int index)
{    return getHeadersFieldBuilder().addBuilder(index, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.getDefaultInstance());}
public java.util.List<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder> flume_f1022_0()
{    return getHeadersFieldBuilder().getBuilderList();}
private com.google.protobuf.RepeatedFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeaderOrBuilder> flume_f1023_0()
{    if (headersBuilder_ == null) {        headersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeaderOrBuilder>(headers_, ((bitField0_ & 0x00000001) == 0x00000001), getParentForChildren(), isClean());        headers_ = null;    }    return headersBuilder_;}
public boolean flume_f1024_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public com.google.protobuf.ByteString flume_f1025_0()
{    return body_;}
public Builder flume_f1026_0(com.google.protobuf.ByteString value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000002;    body_ = value;    onChanged();    return this;}
public Builder flume_f1027_0()
{    bitField0_ = (bitField0_ & ~0x00000002);    body_ = getDefaultInstance().getBody();    onChanged();    return this;}
public static FlumeEventHeader flume_f1028_0()
{    return defaultInstance;}
public FlumeEventHeader flume_f1029_0()
{    return defaultInstance;}
public final com.google.protobuf.UnknownFieldSet flume_f1030_0()
{    return this.unknownFields;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f1031_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEventHeader_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f1032_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEventHeader_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.class, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder.class);}
public FlumeEventHeader flume_f1033_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new FlumeEventHeader(input, extensionRegistry);}
public com.google.protobuf.Parser<FlumeEventHeader> flume_f1034_0()
{    return PARSER;}
public boolean flume_f1035_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public java.lang.String flume_f1036_0()
{    java.lang.Object ref = key_;    if (ref instanceof java.lang.String) {        return (java.lang.String) ref;    } else {        com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;        java.lang.String s = bs.toStringUtf8();        if (bs.isValidUtf8()) {            key_ = s;        }        return s;    }}
public com.google.protobuf.ByteString flume_f1037_0()
{    java.lang.Object ref = key_;    if (ref instanceof java.lang.String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        key_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
public boolean flume_f1038_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public java.lang.String flume_f1039_0()
{    java.lang.Object ref = value_;    if (ref instanceof java.lang.String) {        return (java.lang.String) ref;    } else {        com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;        java.lang.String s = bs.toStringUtf8();        if (bs.isValidUtf8()) {            value_ = s;        }        return s;    }}
public com.google.protobuf.ByteString flume_f1040_0()
{    java.lang.Object ref = value_;    if (ref instanceof java.lang.String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        value_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
private void flume_f1041_0()
{    key_ = "";    value_ = "";}
public final boolean flume_f1042_0()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasKey()) {        memoizedIsInitialized = 0;        return false;    }    memoizedIsInitialized = 1;    return true;}
public void flume_f1043_0(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeBytes(1, getKeyBytes());    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeBytes(2, getValueBytes());    }    getUnknownFields().writeTo(output);}
public int flume_f1044_0()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeBytesSize(1, getKeyBytes());    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeBytesSize(2, getValueBytes());    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
protected java.lang.Object flume_f1045_0() throws java.io.ObjectStreamException
{    return super.writeReplace();}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f1046_0(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f1047_0(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f1048_0(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f1049_0(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f1050_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f1051_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f1052_0(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f1053_0(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f1054_0(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f1055_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
public static Builder flume_f1056_0()
{    return Builder.create();}
public Builder flume_f1057_0()
{    return newBuilder();}
public static Builder flume_f1058_0(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader prototype)
{    return newBuilder().mergeFrom(prototype);}
public Builder flume_f1059_0()
{    return newBuilder(this);}
protected Builder flume_f1060_0(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
public static final com.google.protobuf.Descriptors.Descriptor flume_f1061_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEventHeader_descriptor;}
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable flume_f1062_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEventHeader_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.class, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder.class);}
private void flume_f1063_0()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
private static Builder flume_f1064_0()
{    return new Builder();}
public Builder flume_f1065_0()
{    super.clear();    key_ = "";    bitField0_ = (bitField0_ & ~0x00000001);    value_ = "";    bitField0_ = (bitField0_ & ~0x00000002);    return this;}
public Builder flume_f1066_0()
{    return create().mergeFrom(buildPartial());}
public com.google.protobuf.Descriptors.Descriptor flume_f1067_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEventHeader_descriptor;}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f1068_0()
{    return org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.getDefaultInstance();}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f1069_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader flume_f1070_0()
{    org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader result = new org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.key_ = key_;    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.value_ = value_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
public Builder flume_f1071_0(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader) other);    } else {        super.mergeFrom(other);        return this;    }}
public Builder flume_f1072_0(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.getDefaultInstance())        return this;    if (other.hasKey()) {        bitField0_ |= 0x00000001;        key_ = other.key_;        onChanged();    }    if (other.hasValue()) {        bitField0_ |= 0x00000002;        value_ = other.value_;        onChanged();    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
public final boolean flume_f1073_0()
{    if (!hasKey()) {        return false;    }    return true;}
public Builder flume_f1074_0(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
public boolean flume_f1075_0()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
public java.lang.String flume_f1076_0()
{    java.lang.Object ref = key_;    if (!(ref instanceof java.lang.String)) {        java.lang.String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();        key_ = s;        return s;    } else {        return (java.lang.String) ref;    }}
public com.google.protobuf.ByteString flume_f1077_0()
{    java.lang.Object ref = key_;    if (ref instanceof String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        key_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
public Builder flume_f1078_0(java.lang.String value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000001;    key_ = value;    onChanged();    return this;}
public Builder flume_f1079_0()
{    bitField0_ = (bitField0_ & ~0x00000001);    key_ = getDefaultInstance().getKey();    onChanged();    return this;}
public Builder flume_f1080_0(com.google.protobuf.ByteString value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000001;    key_ = value;    onChanged();    return this;}
public boolean flume_f1081_0()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
public java.lang.String flume_f1082_0()
{    java.lang.Object ref = value_;    if (!(ref instanceof java.lang.String)) {        java.lang.String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();        value_ = s;        return s;    } else {        return (java.lang.String) ref;    }}
public com.google.protobuf.ByteString flume_f1083_0()
{    java.lang.Object ref = value_;    if (ref instanceof String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        value_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
public Builder flume_f1084_0(java.lang.String value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000002;    value_ = value;    onChanged();    return this;}
public Builder flume_f1085_0()
{    bitField0_ = (bitField0_ & ~0x00000002);    value_ = getDefaultInstance().getValue();    onChanged();    return this;}
public Builder flume_f1086_0(com.google.protobuf.ByteString value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000002;    value_ = value;    onChanged();    return this;}
public static com.google.protobuf.Descriptors.FileDescriptor flume_f1087_0()
{    return descriptor;}
public com.google.protobuf.ExtensionRegistry flume_f1088_0(com.google.protobuf.Descriptors.FileDescriptor root)
{    descriptor = root;    internal_static_Checkpoint_descriptor = getDescriptor().getMessageTypes().get(0);    internal_static_Checkpoint_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_Checkpoint_descriptor, new java.lang.String[] { "Version", "WriteOrderID", "QueueSize", "QueueHead", "ActiveLogs" });    internal_static_ActiveLog_descriptor = getDescriptor().getMessageTypes().get(1);    internal_static_ActiveLog_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_ActiveLog_descriptor, new java.lang.String[] { "LogFileID", "Count" });    internal_static_LogFileMetaData_descriptor = getDescriptor().getMessageTypes().get(2);    internal_static_LogFileMetaData_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_LogFileMetaData_descriptor, new java.lang.String[] { "Version", "LogFileID", "CheckpointPosition", "CheckpointWriteOrderID", "Encryption", "BackupCheckpointPosition", "BackupCheckpointWriteOrderID" });    internal_static_LogFileEncryption_descriptor = getDescriptor().getMessageTypes().get(3);    internal_static_LogFileEncryption_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_LogFileEncryption_descriptor, new java.lang.String[] { "CipherProvider", "KeyAlias", "Parameters" });    internal_static_TransactionEventHeader_descriptor = getDescriptor().getMessageTypes().get(4);    internal_static_TransactionEventHeader_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_TransactionEventHeader_descriptor, new java.lang.String[] { "Type", "TransactionID", "WriteOrderID" });    internal_static_Put_descriptor = getDescriptor().getMessageTypes().get(5);    internal_static_Put_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_Put_descriptor, new java.lang.String[] { "Event", "Checksum" });    internal_static_Take_descriptor = getDescriptor().getMessageTypes().get(6);    internal_static_Take_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_Take_descriptor, new java.lang.String[] { "FileID", "Offset" });    internal_static_Rollback_descriptor = getDescriptor().getMessageTypes().get(7);    internal_static_Rollback_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_Rollback_descriptor, new java.lang.String[] {});    internal_static_Commit_descriptor = getDescriptor().getMessageTypes().get(8);    internal_static_Commit_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_Commit_descriptor, new java.lang.String[] { "Type" });    internal_static_TransactionEventFooter_descriptor = getDescriptor().getMessageTypes().get(9);    internal_static_TransactionEventFooter_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_TransactionEventFooter_descriptor, new java.lang.String[] {});    internal_static_FlumeEvent_descriptor = getDescriptor().getMessageTypes().get(10);    internal_static_FlumeEvent_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_FlumeEvent_descriptor, new java.lang.String[] { "Headers", "Body" });    internal_static_FlumeEventHeader_descriptor = getDescriptor().getMessageTypes().get(11);    internal_static_FlumeEventHeader_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_FlumeEventHeader_descriptor, new java.lang.String[] { "Key", "Value" });    return null;}
 FlumeEvent flume_f1089_0()
{    return event;}
public void flume_f1090_0(DataInput in) throws IOException
{    super.readFields(in);    event = FlumeEvent.from(in);}
public void flume_f1091_0(DataOutput out) throws IOException
{    super.write(out);    event.write(out);}
 void flume_f1092_0(OutputStream out) throws IOException
{    ProtosFactory.Put.Builder putBuilder = ProtosFactory.Put.newBuilder();    ProtosFactory.FlumeEvent.Builder eventBuilder = ProtosFactory.FlumeEvent.newBuilder();    Map<String, String> headers = event.getHeaders();    ProtosFactory.FlumeEventHeader.Builder headerBuilder = ProtosFactory.FlumeEventHeader.newBuilder();    if (headers != null) {        for (String key : headers.keySet()) {            String value = headers.get(key);            headerBuilder.clear();            headerBuilder.setKey(key);            if (value != null) {                headerBuilder.setValue(value);            }            eventBuilder.addHeaders(headerBuilder.build());        }    }    eventBuilder.setBody(ByteString.copyFrom(event.getBody()));    ProtosFactory.FlumeEvent protoEvent = eventBuilder.build();    putBuilder.setEvent(protoEvent);    putBuilder.setChecksum(calculateChecksum(event.getBody()));    putBuilder.build().writeDelimitedTo(out);}
 void flume_f1093_0(InputStream in) throws IOException, CorruptEventException
{    ProtosFactory.Put put = Preconditions.checkNotNull(ProtosFactory.Put.parseDelimitedFrom(in), "Put cannot be null");    Map<String, String> headers = Maps.newHashMap();    ProtosFactory.FlumeEvent protosEvent = put.getEvent();    for (ProtosFactory.FlumeEventHeader header : protosEvent.getHeadersList()) {        headers.put(header.getKey(), header.getValue());    }    byte[] eventBody = protosEvent.getBody().toByteArray();    if (put.hasChecksum()) {        long eventBodyChecksum = calculateChecksum(eventBody);        if (eventBodyChecksum != put.getChecksum()) {            throw new CorruptEventException("Expected checksum for event was " + eventBodyChecksum + " but the checksum of the event is " + put.getChecksum());        }    }        event = new FlumeEvent(headers, eventBody);}
protected long flume_f1094_0(byte[] body)
{    checksum.reset();    checksum.update(body, 0, body.length);    return checksum.getValue();}
public short flume_f1095_0()
{    return Type.PUT.get();}
public String flume_f1096_0()
{    StringBuilder builder = new StringBuilder();    builder.append("Put [event=");    builder.append(event);    builder.append(", getLogWriteOrderID()=");    builder.append(getLogWriteOrderID());    builder.append(", getTransactionID()=");    builder.append(getTransactionID());    builder.append("]");    return builder.toString();}
public int flume_f1097_0()
{    return readCount;}
public int flume_f1098_0()
{    return putCount;}
public int flume_f1099_0()
{    return takeCount;}
public int flume_f1100_0()
{    return commitCount;}
public int flume_f1101_0()
{    return rollbackCount;}
 void flume_f1102_1(List<File> logs) throws Exception
{    int total = 0;    int count = 0;    MultiMap transactionMap = new MultiValueMap();        SetMultimap<Long, Long> inflightPuts = queue.deserializeInflightPuts();    for (Long txnID : inflightPuts.keySet()) {        Set<Long> eventPointers = inflightPuts.get(txnID);        for (Long eventPointer : eventPointers) {            transactionMap.put(txnID, FlumeEventPointer.fromLong(eventPointer));        }    }    SetMultimap<Long, Long> inflightTakes = queue.deserializeInflightTakes();        for (File log : logs) {                LogFile.SequentialReader reader = null;        try {            reader = LogFileFactory.getSequentialReader(log, encryptionKeyProvider, fsyncPerTransaction);            reader.skipToLastCheckpointPosition(queue.getLogWriteOrderID());            LogRecord entry;            FlumeEventPointer ptr;                                    int fileId = reader.getLogFileID();            while ((entry = reader.next()) != null) {                int offset = entry.getOffset();                TransactionEventRecord record = entry.getEvent();                short type = record.getRecordType();                long trans = record.getTransactionID();                readCount++;                if (record.getLogWriteOrderID() > lastCheckpoint) {                    if (type == TransactionEventRecord.Type.PUT.get()) {                        putCount++;                        ptr = new FlumeEventPointer(fileId, offset);                        transactionMap.put(trans, ptr);                    } else if (type == TransactionEventRecord.Type.TAKE.get()) {                        takeCount++;                        Take take = (Take) record;                        ptr = new FlumeEventPointer(take.getFileID(), take.getOffset());                        transactionMap.put(trans, ptr);                    } else if (type == TransactionEventRecord.Type.ROLLBACK.get()) {                        rollbackCount++;                        transactionMap.remove(trans);                    } else if (type == TransactionEventRecord.Type.COMMIT.get()) {                        commitCount++;                        @SuppressWarnings("unchecked")                        Collection<FlumeEventPointer> pointers = (Collection<FlumeEventPointer>) transactionMap.remove(trans);                        if (((Commit) record).getType() == TransactionEventRecord.Type.TAKE.get()) {                            if (inflightTakes.containsKey(trans)) {                                if (pointers == null) {                                    pointers = Sets.newHashSet();                                }                                Set<Long> takes = inflightTakes.removeAll(trans);                                Iterator<Long> it = takes.iterator();                                while (it.hasNext()) {                                    Long take = it.next();                                    pointers.add(FlumeEventPointer.fromLong(take));                                }                            }                        }                        if (pointers != null && pointers.size() > 0) {                            processCommit(((Commit) record).getType(), pointers);                            count += pointers.size();                        }                    } else {                        Preconditions.checkArgument(false, "Unknown record type: " + Integer.toHexString(type));                    }                } else {                    skipCount++;                }            }                        if (LOG.isDebugEnabled()) {                            }        } catch (EOFException e) {                    } finally {            total += count;            count = 0;            if (reader != null) {                reader.close();            }        }    }            int uncommittedTakes = 0;    for (Long inflightTxnId : inflightTakes.keySet()) {        Set<Long> inflightUncommittedTakes = inflightTakes.get(inflightTxnId);        for (Long inflightUncommittedTake : inflightUncommittedTakes) {            queue.addHead(FlumeEventPointer.fromLong(inflightUncommittedTake));            uncommittedTakes++;        }    }    inflightTakes.clear();    count += uncommittedTakes;    int pendingTakesSize = pendingTakes.size();    if (pendingTakesSize > 0) {        String msg = "Pending takes " + pendingTakesSize + " exist after the end of replay";        if (LOG.isDebugEnabled()) {            for (Long pointer : pendingTakes) {                            }        } else {                    }    }    }
 void flume_f1103_1(List<File> logs) throws Exception
{    int count = 0;    MultiMap transactionMap = new MultiValueMap();        long transactionIDSeed = lastCheckpoint, writeOrderIDSeed = lastCheckpoint;                SetMultimap<Long, Long> inflightPuts = queue.deserializeInflightPuts();    for (Long txnID : inflightPuts.keySet()) {        Set<Long> eventPointers = inflightPuts.get(txnID);        for (Long eventPointer : eventPointers) {            transactionMap.put(txnID, FlumeEventPointer.fromLong(eventPointer));        }    }    SetMultimap<Long, Long> inflightTakes = queue.deserializeInflightTakes();    try {        for (File log : logs) {                        try {                LogFile.SequentialReader reader = LogFileFactory.getSequentialReader(log, encryptionKeyProvider, fsyncPerTransaction);                reader.skipToLastCheckpointPosition(queue.getLogWriteOrderID());                Preconditions.checkState(!readers.containsKey(reader.getLogFileID()), "Readers " + readers + " already contains " + reader.getLogFileID());                readers.put(reader.getLogFileID(), reader);                LogRecord logRecord = reader.next();                if (logRecord == null) {                    readers.remove(reader.getLogFileID());                    reader.close();                } else {                    logRecordBuffer.add(logRecord);                }            } catch (EOFException e) {                            }        }        LogRecord entry = null;        FlumeEventPointer ptr = null;        while ((entry = next()) != null) {                                    int fileId = entry.getFileID();            int offset = entry.getOffset();            TransactionEventRecord record = entry.getEvent();            short type = record.getRecordType();            long trans = record.getTransactionID();            transactionIDSeed = Math.max(transactionIDSeed, trans);            writeOrderIDSeed = Math.max(writeOrderIDSeed, record.getLogWriteOrderID());            readCount++;            if (readCount % 10000 == 0 && readCount > 0) {                            }            if (record.getLogWriteOrderID() > lastCheckpoint) {                if (type == TransactionEventRecord.Type.PUT.get()) {                    putCount++;                    ptr = new FlumeEventPointer(fileId, offset);                    transactionMap.put(trans, ptr);                } else if (type == TransactionEventRecord.Type.TAKE.get()) {                    takeCount++;                    Take take = (Take) record;                    ptr = new FlumeEventPointer(take.getFileID(), take.getOffset());                    transactionMap.put(trans, ptr);                } else if (type == TransactionEventRecord.Type.ROLLBACK.get()) {                    rollbackCount++;                    transactionMap.remove(trans);                } else if (type == TransactionEventRecord.Type.COMMIT.get()) {                    commitCount++;                    @SuppressWarnings("unchecked")                    Collection<FlumeEventPointer> pointers = (Collection<FlumeEventPointer>) transactionMap.remove(trans);                    if (((Commit) record).getType() == TransactionEventRecord.Type.TAKE.get()) {                        if (inflightTakes.containsKey(trans)) {                            if (pointers == null) {                                pointers = Sets.newHashSet();                            }                            Set<Long> takes = inflightTakes.removeAll(trans);                            Iterator<Long> it = takes.iterator();                            while (it.hasNext()) {                                Long take = it.next();                                pointers.add(FlumeEventPointer.fromLong(take));                            }                        }                    }                    if (pointers != null && pointers.size() > 0) {                        processCommit(((Commit) record).getType(), pointers);                        count += pointers.size();                    }                } else {                    Preconditions.checkArgument(false, "Unknown record type: " + Integer.toHexString(type));                }            } else {                skipCount++;            }        }                queue.replayComplete();    } finally {        TransactionIDOracle.setSeed(transactionIDSeed);        WriteOrderOracle.setSeed(writeOrderIDSeed);        for (LogFile.SequentialReader reader : readers.values()) {            if (reader != null) {                reader.close();            }        }    }            int uncommittedTakes = 0;    for (Long inflightTxnId : inflightTakes.keySet()) {        Set<Long> inflightUncommittedTakes = inflightTakes.get(inflightTxnId);        for (Long inflightUncommittedTake : inflightUncommittedTakes) {            queue.addHead(FlumeEventPointer.fromLong(inflightUncommittedTake));            uncommittedTakes++;        }    }    inflightTakes.clear();    count += uncommittedTakes;    int pendingTakesSize = pendingTakes.size();    if (pendingTakesSize > 0) {            }}
private LogRecord flume_f1104_0() throws IOException, CorruptEventException
{    LogRecord resultLogRecord = logRecordBuffer.poll();    if (resultLogRecord != null) {                LogFile.SequentialReader reader = readers.get(resultLogRecord.getFileID());        LogRecord nextLogRecord;        if ((nextLogRecord = reader.next()) != null) {            logRecordBuffer.add(nextLogRecord);        }    }    return resultLogRecord;}
private void flume_f1105_0(short type, Collection<FlumeEventPointer> pointers)
{    if (type == TransactionEventRecord.Type.PUT.get()) {        for (FlumeEventPointer pointer : pointers) {            if (!queue.addTail(pointer)) {                throw new IllegalStateException("Unable to add " + pointer + ". Queue depth = " + queue.getSize() + ", Capacity = " + queue.getCapacity());            }            if (pendingTakes.remove(pointer.toLong())) {                Preconditions.checkState(queue.remove(pointer), "Take was pending and pointer was successfully added to the" + " queue but could not be removed: " + pointer);            }        }    } else if (type == TransactionEventRecord.Type.TAKE.get()) {        for (FlumeEventPointer pointer : pointers) {            boolean removed = queue.remove(pointer);            if (!removed) {                pendingTakes.add(pointer.toLong());            }        }    } else {        Preconditions.checkArgument(false, "Unknown record type: " + Integer.toHexString(type));    }}
public void flume_f1106_0(DataInput in) throws IOException
{    super.readFields(in);}
public void flume_f1107_0(DataOutput out) throws IOException
{    super.write(out);}
 void flume_f1108_0(OutputStream out) throws IOException
{    ProtosFactory.Rollback.Builder rollbackBuilder = ProtosFactory.Rollback.newBuilder();    rollbackBuilder.build().writeDelimitedTo(out);}
 void flume_f1109_0(InputStream in) throws IOException
{    @SuppressWarnings("unused")    ProtosFactory.Rollback rollback = Preconditions.checkNotNull(ProtosFactory.Rollback.parseDelimitedFrom(in), "Rollback cannot be null");}
 short flume_f1110_0()
{    return Type.ROLLBACK.get();}
public String flume_f1111_0()
{    StringBuilder builder = new StringBuilder();    builder.append("Rollback [getLogWriteOrderID()=");    builder.append(getLogWriteOrderID());    builder.append(", getTransactionID()=");    builder.append(getTransactionID());    builder.append("]");    return builder.toString();}
 static File flume_f1112_0(File metaDataFile)
{    String metaDataFileName = metaDataFile.getName() + METADATA_TMP_FILENAME;    return new File(metaDataFile.getParentFile(), metaDataFileName);}
 static File flume_f1113_0(File file)
{    String metaDataFileName = file.getName() + METADATA_FILENAME;    return new File(file.getParentFile(), metaDataFileName);}
 static File flume_f1114_0(File file)
{    String oldMetaDataFileName = file.getName() + OLD_METADATA_FILENAME;    return new File(file.getParentFile(), oldMetaDataFileName);}
 static boolean flume_f1115_1(File checkpointDir, @Nullable Set<String> excludes)
{    if (!checkpointDir.isDirectory()) {        return false;    }    File[] files = checkpointDir.listFiles();    if (files == null) {        return false;    }    StringBuilder builder;    if (files.length == 0) {        return true;    } else {        builder = new StringBuilder("Deleted the following files: ");    }    if (excludes == null) {        excludes = Collections.emptySet();    }    for (File file : files) {        if (excludes.contains(file.getName())) {                        continue;        }        if (!FileUtils.deleteQuietly(file)) {                                    return false;        }        builder.append(", ").append(file.getName());    }    builder.append(".");        return true;}
public static boolean flume_f1116_1(File from, File to) throws IOException
{    Preconditions.checkNotNull(from, "Source file is null, file copy failed.");    Preconditions.checkNotNull(to, "Destination file is null, " + "file copy failed.");    Preconditions.checkState(from.exists(), "Source file: " + from.toString() + " does not exist.");    Preconditions.checkState(!to.exists(), "Destination file: " + to.toString() + " unexpectedly exists.");    BufferedInputStream in = null;        RandomAccessFile out = null;    try {        in = new BufferedInputStream(new FileInputStream(from));        out = new RandomAccessFile(to, "rw");        byte[] buf = new byte[FILE_BUFFER_SIZE];        int total = 0;        while (true) {            int read = in.read(buf);            if (read == -1) {                break;            }            out.write(buf, 0, read);            total += read;        }        out.getFD().sync();        Preconditions.checkState(total == from.length(), "The size of the origin file and destination file are not equal.");        return true;    } catch (Exception ex) {                Throwables.propagate(ex);    } finally {        Throwable th = null;        try {            if (in != null) {                in.close();            }        } catch (Throwable ex) {                        th = ex;        }        try {            if (out != null) {                out.close();            }        } catch (IOException ex) {                        Throwables.propagate(ex);        }        if (th != null) {            Throwables.propagate(th);        }    }        throw new IOException("Copying file: " + from.toString() + " to: " + to.toString() + " may have failed.");}
public static boolean flume_f1117_1(File uncompressed, File compressed) throws IOException
{    Preconditions.checkNotNull(uncompressed, "Source file is null, compression failed.");    Preconditions.checkNotNull(compressed, "Destination file is null, compression failed.");    Preconditions.checkState(uncompressed.exists(), "Source file: " + uncompressed.toString() + " does not exist.");    Preconditions.checkState(!compressed.exists(), "Compressed file: " + compressed.toString() + " unexpectedly " + "exists.");    BufferedInputStream in = null;    FileOutputStream out = null;    SnappyOutputStream snappyOut = null;    try {        in = new BufferedInputStream(new FileInputStream(uncompressed));        out = new FileOutputStream(compressed);        snappyOut = new SnappyOutputStream(out);        byte[] buf = new byte[FILE_BUFFER_SIZE];        while (true) {            int read = in.read(buf);            if (read == -1) {                break;            }            snappyOut.write(buf, 0, read);        }        out.getFD().sync();        return true;    } catch (Exception ex) {                Throwables.propagate(ex);    } finally {        Throwable th = null;        try {            if (in != null) {                in.close();            }        } catch (Throwable ex) {                        th = ex;        }        try {            if (snappyOut != null) {                snappyOut.close();            }        } catch (IOException ex) {                        Throwables.propagate(ex);        }        if (th != null) {            Throwables.propagate(th);        }    }        throw new IOException("Copying file: " + uncompressed.toString() + " to: " + compressed.toString() + " may have failed.");}
public static boolean flume_f1118_1(File compressed, File decompressed) throws IOException
{    Preconditions.checkNotNull(compressed, "Source file is null, decompression failed.");    Preconditions.checkNotNull(decompressed, "Destination file is " + "null, decompression failed.");    Preconditions.checkState(compressed.exists(), "Source file: " + compressed.toString() + " does not exist.");    Preconditions.checkState(!decompressed.exists(), "Decompressed file: " + decompressed.toString() + " unexpectedly exists.");    BufferedInputStream in = null;    SnappyInputStream snappyIn = null;    FileOutputStream out = null;    try {        in = new BufferedInputStream(new FileInputStream(compressed));        snappyIn = new SnappyInputStream(in);        out = new FileOutputStream(decompressed);        byte[] buf = new byte[FILE_BUFFER_SIZE];        while (true) {            int read = snappyIn.read(buf);            if (read == -1) {                break;            }            out.write(buf, 0, read);        }        out.getFD().sync();        return true;    } catch (Exception ex) {                Throwables.propagate(ex);    } finally {        Throwable th = null;        try {            if (in != null) {                in.close();            }        } catch (Throwable ex) {                        th = ex;        }        try {            if (snappyIn != null) {                snappyIn.close();            }        } catch (IOException ex) {                        Throwables.propagate(ex);        }        if (th != null) {            Throwables.propagate(th);        }    }        throw new IOException("Decompressing file: " + compressed.toString() + " to: " + decompressed.toString() + " may have failed.");}
 int flume_f1119_0()
{    return offset;}
 int flume_f1120_0()
{    return fileID;}
public void flume_f1121_0(DataInput in) throws IOException
{    super.readFields(in);    offset = in.readInt();    fileID = in.readInt();}
public void flume_f1122_0(DataOutput out) throws IOException
{    super.write(out);    out.writeInt(offset);    out.writeInt(fileID);}
 void flume_f1123_0(OutputStream out) throws IOException
{    ProtosFactory.Take.Builder takeBuilder = ProtosFactory.Take.newBuilder();    takeBuilder.setFileID(fileID);    takeBuilder.setOffset(offset);    takeBuilder.build().writeDelimitedTo(out);}
 void flume_f1124_0(InputStream in) throws IOException
{    ProtosFactory.Take take = Preconditions.checkNotNull(ProtosFactory.Take.parseDelimitedFrom(in), "Take cannot be null");    fileID = take.getFileID();    offset = take.getOffset();}
 short flume_f1125_0()
{    return Type.TAKE.get();}
public String flume_f1126_0()
{    StringBuilder builder = new StringBuilder();    builder.append("Take [offset=");    builder.append(offset);    builder.append(", fileID=");    builder.append(fileID);    builder.append(", getLogWriteOrderID()=");    builder.append(getLogWriteOrderID());    builder.append(", getTransactionID()=");    builder.append(getTransactionID());    builder.append("]");    return builder.toString();}
public void flume_f1127_0(DataInput in) throws IOException
{}
public void flume_f1128_0(DataOutput out) throws IOException
{}
 long flume_f1129_0()
{    return logWriteOrderID;}
 long flume_f1130_0()
{    return transactionID;}
public short flume_f1131_0()
{    return id;}
 static ByteBuffer flume_f1132_1(TransactionEventRecord record)
{    ByteArrayOutputStream byteOutput = new ByteArrayOutputStream(512);    DataOutputStream dataOutput = new DataOutputStream(byteOutput);    try {        dataOutput.writeInt(MAGIC_HEADER);        dataOutput.writeShort(record.getRecordType());        dataOutput.writeLong(record.getTransactionID());        dataOutput.writeLong(record.getLogWriteOrderID());        record.write(dataOutput);        dataOutput.flush();                return ByteBuffer.wrap(byteOutput.toByteArray());    } catch (IOException e) {                throw Throwables.propagate(e);    } finally {        if (dataOutput != null) {            try {                dataOutput.close();            } catch (IOException e) {                            }        }    }}
 static TransactionEventRecord flume_f1133_0(DataInput in) throws IOException
{    int header = in.readInt();    if (header != MAGIC_HEADER) {        throw new IOException("Header " + Integer.toHexString(header) + " is not the required value: " + Integer.toHexString(MAGIC_HEADER));    }    short type = in.readShort();    long transactionID = in.readLong();    long writeOrderID = in.readLong();    TransactionEventRecord entry = newRecordForType(type, transactionID, writeOrderID);    entry.readFields(in);    return entry;}
 static ByteBuffer flume_f1134_1(TransactionEventRecord record)
{    ByteArrayOutputStream byteOutput = new ByteArrayOutputStream(512);    try {        ProtosFactory.TransactionEventHeader.Builder headerBuilder = ProtosFactory.TransactionEventHeader.newBuilder();        headerBuilder.setType(record.getRecordType());        headerBuilder.setTransactionID(record.getTransactionID());        headerBuilder.setWriteOrderID(record.getLogWriteOrderID());        headerBuilder.build().writeDelimitedTo(byteOutput);        record.writeProtos(byteOutput);        ProtosFactory.TransactionEventFooter footer = ProtosFactory.TransactionEventFooter.newBuilder().build();        footer.writeDelimitedTo(byteOutput);        return ByteBuffer.wrap(byteOutput.toByteArray());    } catch (IOException e) {        throw Throwables.propagate(e);    } finally {        if (byteOutput != null) {            try {                byteOutput.close();            } catch (IOException e) {                            }        }    }}
 static TransactionEventRecord flume_f1135_1(byte[] buffer) throws IOException, CorruptEventException
{    ByteArrayInputStream in = new ByteArrayInputStream(buffer);    try {        ProtosFactory.TransactionEventHeader header = Preconditions.checkNotNull(ProtosFactory.TransactionEventHeader.parseDelimitedFrom(in), "Header cannot be null");        short type = (short) header.getType();        long transactionID = header.getTransactionID();        long writeOrderID = header.getWriteOrderID();        TransactionEventRecord transactionEvent = newRecordForType(type, transactionID, writeOrderID);        transactionEvent.readProtos(in);        @SuppressWarnings("unused")        ProtosFactory.TransactionEventFooter footer = Preconditions.checkNotNull(ProtosFactory.TransactionEventFooter.parseDelimitedFrom(in), "Footer cannot be null");        return transactionEvent;    } catch (InvalidProtocolBufferException ex) {        throw new CorruptEventException("Could not parse event from data file.", ex);    } finally {        try {            in.close();        } catch (IOException e) {                    }    }}
 static String flume_f1136_0(short type)
{    Constructor<? extends TransactionEventRecord> constructor = TYPES.get(type);    Preconditions.checkNotNull(constructor, "Unknown action " + Integer.toHexString(type));    return constructor.getDeclaringClass().getSimpleName();}
private static TransactionEventRecord flume_f1137_0(short type, long transactionID, long writeOrderID)
{    Constructor<? extends TransactionEventRecord> constructor = TYPES.get(type);    Preconditions.checkNotNull(constructor, "Unknown action " + Integer.toHexString(type));    try {        return constructor.newInstance(transactionID, writeOrderID);    } catch (Exception e) {        throw Throwables.propagate(e);    }}
public static void flume_f1138_0(long highest)
{    long previous;    while (highest > (previous = TRANSACTION_ID.get())) {        TRANSACTION_ID.compareAndSet(previous, highest);    }}
public static long flume_f1139_0()
{    return TRANSACTION_ID.incrementAndGet();}
public static void flume_f1140_0(DataOutput stream, int i) throws IOException
{    writeVLong(stream, i);}
public static void flume_f1141_0(DataOutput stream, long i) throws IOException
{    if (i >= -112 && i <= 127) {        stream.writeByte((byte) i);        return;    }    int len = -112;    if (i < 0) {                i ^= -1L;        len = -120;    }    long tmp = i;    while (tmp != 0) {        tmp = tmp >> 8;        len--;    }    stream.writeByte((byte) len);    len = (len < -120) ? -(len + 120) : -(len + 112);    for (int idx = len; idx != 0; idx--) {        int shiftbits = (idx - 1) * 8;        long mask = 0xFFL << shiftbits;        stream.writeByte((byte) ((i & mask) >> shiftbits));    }}
public static long flume_f1142_0(DataInput stream) throws IOException
{    byte firstByte = stream.readByte();    int len = decodeVIntSize(firstByte);    if (len == 1) {        return firstByte;    }    long i = 0;    for (int idx = 0; idx < len - 1; idx++) {        byte b = stream.readByte();        i = i << 8;        i = i | (b & 0xFF);    }    return (isNegativeVInt(firstByte) ? (i ^ -1L) : i);}
public static int flume_f1143_0(DataInput stream) throws IOException
{    long n = readVLong(stream);    if ((n > Integer.MAX_VALUE) || (n < Integer.MIN_VALUE)) {        throw new IOException("value too long to fit in integer");    }    return (int) n;}
public static boolean flume_f1144_0(byte value)
{    return value < -120 || (value >= -112 && value < 0);}
public static int flume_f1145_0(byte value)
{    if (value >= -112) {        return 1;    } else if (value < -120) {        return -119 - value;    }    return -111 - value;}
public static void flume_f1146_0(long highest)
{    long previous;    while (highest > (previous = WRITER_ORDERER.get())) {        WRITER_ORDERER.compareAndSet(previous, highest);    }}
public static long flume_f1147_0()
{    return WRITER_ORDERER.incrementAndGet();}
public void flume_f1148_0()
{    run = true;    while (run && count < until) {        boolean error = true;        try {            if (Sink.Status.READY.equals(sink.process())) {                count++;                error = false;            }        } catch (Exception ex) {            errors.add(ex);        }        if (error) {            try {                Thread.sleep(1000L);            } catch (InterruptedException e) {            }        }    }}
public void flume_f1149_0()
{    run = false;}
public int flume_f1150_0()
{    return count;}
public List<Exception> flume_f1151_0()
{    return errors;}
public void flume_f1152_0()
{    run = true;    while (run && count < until) {        boolean error = true;        try {            if (PollableSource.Status.READY.equals(source.process())) {                count++;                error = false;            }        } catch (Exception ex) {            errors.add(ex);        }        if (error) {            try {                Thread.sleep(1000L);            } catch (InterruptedException e) {            }        }    }}
public void flume_f1153_0()
{    run = false;}
public int flume_f1154_0()
{    return count;}
public List<Exception> flume_f1155_0()
{    return errors;}
public void flume_f1156_0() throws Exception
{    testBasic();    testEmpty();    testNullPlainText();    testNullCipherText();}
public void flume_f1157_0() throws Exception
{    String expected = "mn state fair is the place to be";    byte[] cipherText = encryptor.encrypt(expected.getBytes(Charsets.UTF_8));    byte[] clearText = decryptor.decrypt(cipherText);    Assert.assertEquals(expected, new String(clearText, Charsets.UTF_8));}
public void flume_f1158_0() throws Exception
{    String expected = "";    byte[] cipherText = encryptor.encrypt(new byte[] {});    byte[] clearText = decryptor.decrypt(cipherText);    Assert.assertEquals(expected, new String(clearText));}
public void flume_f1159_0() throws Exception
{    try {        encryptor.encrypt(null);        Assert.fail();    } catch (NullPointerException e) {        }}
public void flume_f1160_0() throws Exception
{    try {        decryptor.decrypt(null);        Assert.fail();    } catch (NullPointerException e) {        }}
private static Key flume_f1161_0()
{    KeyGenerator keyGen;    try {        keyGen = KeyGenerator.getInstance("AES");        Key key = keyGen.generateKey();        return key;    } catch (Exception e) {        throw Throwables.propagate(e);    }}
public static void flume_f1162_0(File keyStoreFile, File keyStorePasswordFile, Map<String, File> keyAliasPassword) throws Exception
{    KeyStore ks = KeyStore.getInstance("jceks");    ks.load(null);    List<String> keysWithSeperatePasswords = Lists.newArrayList();    for (String alias : keyAliasPassword.keySet()) {        Key key = newKey();        char[] password = null;        File passwordFile = keyAliasPassword.get(alias);        if (passwordFile == null) {            password = Files.toString(keyStorePasswordFile, Charsets.UTF_8).toCharArray();        } else {            keysWithSeperatePasswords.add(alias);            password = Files.toString(passwordFile, Charsets.UTF_8).toCharArray();        }        ks.setKeyEntry(alias, key, password, null);    }    char[] keyStorePassword = Files.toString(keyStorePasswordFile, Charsets.UTF_8).toCharArray();    FileOutputStream outputStream = new FileOutputStream(keyStoreFile);    ks.store(outputStream, keyStorePassword);    outputStream.close();}
public static Map<String, File> flume_f1163_0(File baseDir, File keyStoreFile) throws IOException
{    Map<String, File> result = Maps.newHashMap();    if (System.getProperty("java.vendor").contains("IBM")) {        Resources.copy(Resources.getResource("ibm-test.keystore"), new FileOutputStream(keyStoreFile));    } else {        Resources.copy(Resources.getResource("sun-test.keystore"), new FileOutputStream(keyStoreFile));    }    /* Commands below:     * keytool -genseckey -alias key-0 -keypass keyPassword -keyalg AES \     *   -keysize 128 -validity 9000 -keystore src/test/resources/test.keystore \     *   -storetype jceks -storepass keyStorePassword     * keytool -genseckey -alias key-1 -keyalg AES -keysize 128 -validity 9000 \     *   -keystore src/test/resources/test.keystore -storetype jceks \     *   -storepass keyStorePassword     */        result.put("key-0", TestUtils.writeStringToFile(baseDir, "key-0", "keyPassword"));    result.put("key-1", null);    return result;}
public static Map<String, String> flume_f1164_0(File keyStoreFile, File keyStorePasswordFile, Map<String, File> keyAliasPassword) throws Exception
{    Map<String, String> context = Maps.newHashMap();    List<String> keys = Lists.newArrayList();    Joiner joiner = Joiner.on(".");    for (String alias : keyAliasPassword.keySet()) {        File passwordFile = keyAliasPassword.get(alias);        if (passwordFile == null) {            keys.add(alias);        } else {            String propertyName = joiner.join(EncryptionConfiguration.KEY_PROVIDER, EncryptionConfiguration.JCE_FILE_KEYS, alias, EncryptionConfiguration.JCE_FILE_KEY_PASSWORD_FILE);            keys.add(alias);            context.put(propertyName, passwordFile.getAbsolutePath());        }    }    context.put(joiner.join(EncryptionConfiguration.KEY_PROVIDER, EncryptionConfiguration.JCE_FILE_KEY_STORE_FILE), keyStoreFile.getAbsolutePath());    if (keyStorePasswordFile != null) {        context.put(joiner.join(EncryptionConfiguration.KEY_PROVIDER, EncryptionConfiguration.JCE_FILE_KEY_STORE_PASSWORD_FILE), keyStorePasswordFile.getAbsolutePath());    }    context.put(joiner.join(EncryptionConfiguration.KEY_PROVIDER, EncryptionConfiguration.JCE_FILE_KEYS), Joiner.on(" ").join(keys));    return context;}
public void flume_f1165_0() throws Exception
{    KeyGenerator keyGen = KeyGenerator.getInstance("AES");    key = keyGen.generateKey();    encryptor = CipherProviderFactory.getEncrypter(CipherProviderType.AESCTRNOPADDING.name(), key);    decryptor = CipherProviderFactory.getDecrypter(CipherProviderType.AESCTRNOPADDING.name(), key, encryptor.getParameters());    cipherProviderTestSuite = new CipherProviderTestSuite(encryptor, decryptor);}
public void flume_f1166_0() throws Exception
{    cipherProviderTestSuite.test();}
public void flume_f1167_0() throws Exception
{    super.setup();    keyStorePasswordFile = new File(baseDir, "keyStorePasswordFile");    Files.write("keyStorePassword", keyStorePasswordFile, Charsets.UTF_8);    keyStoreFile = new File(baseDir, "keyStoreFile");    Assert.assertTrue(keyStoreFile.createNewFile());    keyAliasPassword = Maps.newHashMap();    keyAliasPassword.putAll(EncryptionTestUtils.configureTestKeyStore(baseDir, keyStoreFile));}
public void flume_f1168_0()
{    super.teardown();}
private Map<String, String> flume_f1169_0() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(100));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(100));    return overrides;}
private Map<String, String> flume_f1170_0() throws Exception
{    Map<String, String> overrides = getOverrides();    Map<String, String> encryptionProps = EncryptionTestUtils.configureForKeyStore(keyStoreFile, keyStorePasswordFile, keyAliasPassword);    encryptionProps.put(EncryptionConfiguration.KEY_PROVIDER, KeyProviderType.JCEKSFILE.name());    encryptionProps.put(EncryptionConfiguration.CIPHER_PROVIDER, CipherProviderType.AESCTRNOPADDING.name());    encryptionProps.put(EncryptionConfiguration.ACTIVE_KEY, "key-1");    for (String key : encryptionProps.keySet()) {        overrides.put(EncryptionConfiguration.ENCRYPTION_PREFIX + "." + key, encryptionProps.get(key));    }    return overrides;}
public void flume_f1171_1() throws Exception
{    int numThreads = 20;    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(10000));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(100));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Executor executor = Executors.newFixedThreadPool(numThreads);    Set<String> in = fillChannel(channel, "threaded-consume");    final AtomicBoolean error = new AtomicBoolean(false);    final CountDownLatch startLatch = new CountDownLatch(numThreads);    final CountDownLatch stopLatch = new CountDownLatch(numThreads);    final Set<String> out = Collections.synchronizedSet(new HashSet<String>());    for (int i = 0; i < numThreads; i++) {        executor.execute(new Runnable() {            @Override            public void run() {                try {                    startLatch.countDown();                    startLatch.await();                    out.addAll(takeEvents(channel, 10));                } catch (Throwable t) {                    error.set(true);                                    } finally {                    stopLatch.countDown();                }            }        });    }    stopLatch.await();    Assert.assertFalse(error.get());    compareInputAndOut(in, out);}
public void flume_f1172_1()
{    try {        startLatch.countDown();        startLatch.await();        out.addAll(takeEvents(channel, 10));    } catch (Throwable t) {        error.set(true);            } finally {        stopLatch.countDown();    }}
public void flume_f1173_1() throws Exception
{    int numThreads = 20;    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(10000));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(100));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Executor executor = Executors.newFixedThreadPool(numThreads);    final AtomicBoolean error = new AtomicBoolean(false);    final CountDownLatch startLatch = new CountDownLatch(numThreads);    final CountDownLatch stopLatch = new CountDownLatch(numThreads);    final Set<String> in = Collections.synchronizedSet(new HashSet<String>());    for (int i = 0; i < numThreads; i++) {        executor.execute(new Runnable() {            @Override            public void run() {                try {                    startLatch.countDown();                    startLatch.await();                    in.addAll(putEvents(channel, "thread-produce", 10, 10000, true));                } catch (Throwable t) {                    error.set(true);                                    } finally {                    stopLatch.countDown();                }            }        });    }    stopLatch.await();    Set<String> out = consumeChannel(channel);    Assert.assertFalse(error.get());    compareInputAndOut(in, out);}
public void flume_f1174_1()
{    try {        startLatch.countDown();        startLatch.await();        in.addAll(putEvents(channel, "thread-produce", 10, 10000, true));    } catch (Throwable t) {        error.set(true);            } finally {        stopLatch.countDown();    }}
public void flume_f1175_0() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put("encryption.activeKey", "key-1");    overrides.put("encryption.cipherProvider", "AESCTRNOPADDING");    overrides.put("encryption.keyProvider", "JCEKSFILE");    overrides.put("encryption.keyProvider.keyStoreFile", keyStoreFile.getAbsolutePath());    overrides.put("encryption.keyProvider.keyStorePasswordFile", keyStorePasswordFile.getAbsolutePath());    overrides.put("encryption.keyProvider.keys", "key-0 key-1");    overrides.put("encryption.keyProvider.keys.key-0.passwordFile", keyAliasPassword.get("key-0").getAbsolutePath());    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "restart");    channel.stop();    channel = TestUtils.createFileChannel(checkpointDir.getAbsolutePath(), dataDir, overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
public void flume_f1176_0() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "restart");    channel.stop();    channel = TestUtils.createFileChannel(checkpointDir.getAbsolutePath(), dataDir, overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
public void flume_f1177_0() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    fillChannel(channel, "will-not-restart");    channel.stop();    Map<String, String> noEncryptionOverrides = getOverrides();    channel = createFileChannel(noEncryptionOverrides);    channel.start();    if (channel.isOpen()) {        try {            takeEvents(channel, 1, 1);            Assert.fail("Channel was opened and take did not throw exception");        } catch (ChannelException ex) {                }    }}
public void flume_f1178_0() throws Exception
{    Map<String, String> noEncryptionOverrides = getOverrides();    channel = createFileChannel(noEncryptionOverrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "unencrypted-and-encrypted");    int numEventsToRemove = in.size() / 2;    for (int i = 0; i < numEventsToRemove; i++) {        Assert.assertTrue(in.removeAll(takeEvents(channel, 1, 1)));    }        channel.stop();    Map<String, String> overrides = getOverridesForEncryption();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    in.addAll(fillChannel(channel, "unencrypted-and-encrypted"));    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
public void flume_f1179_0() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(Joiner.on(".").join(EncryptionConfiguration.ENCRYPTION_PREFIX, EncryptionConfiguration.KEY_PROVIDER), "invalid");    try {        channel = createFileChannel(overrides);        Assert.fail();    } catch (FlumeException ex) {        Assert.assertEquals("java.lang.ClassNotFoundException: invalid", ex.getMessage());    }}
public void flume_f1180_0() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(Joiner.on(".").join(EncryptionConfiguration.ENCRYPTION_PREFIX, EncryptionConfiguration.KEY_PROVIDER), String.class.getName());    try {        channel = createFileChannel(overrides);        Assert.fail();    } catch (FlumeException ex) {        Assert.assertEquals("Unable to instantiate Builder from java.lang.String", ex.getMessage());    }}
public void flume_f1181_0() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(Joiner.on(".").join(EncryptionConfiguration.ENCRYPTION_PREFIX, EncryptionConfiguration.CIPHER_PROVIDER), "invalid");    channel = createFileChannel(overrides);    channel.start();    Assert.assertFalse(channel.isOpen());}
public void flume_f1182_0() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(Joiner.on(".").join(EncryptionConfiguration.ENCRYPTION_PREFIX, EncryptionConfiguration.CIPHER_PROVIDER), String.class.getName());    channel = createFileChannel(overrides);    channel.start();    Assert.assertFalse(channel.isOpen());}
public void flume_f1183_0() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(Joiner.on(".").join(EncryptionConfiguration.ENCRYPTION_PREFIX, EncryptionConfiguration.KEY_PROVIDER, EncryptionConfiguration.JCE_FILE_KEY_STORE_FILE), "/path/does/not/exist");    try {        channel = createFileChannel(overrides);        Assert.fail();    } catch (RuntimeException ex) {        Assert.assertTrue("Exception message is incorrect: " + ex.getMessage(), ex.getMessage().startsWith("java.io.FileNotFoundException: /path/does/not/exist "));    }}
public void flume_f1184_0() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(Joiner.on(".").join(EncryptionConfiguration.ENCRYPTION_PREFIX, EncryptionConfiguration.KEY_PROVIDER, EncryptionConfiguration.JCE_FILE_KEY_STORE_PASSWORD_FILE), "/path/does/not/exist");    try {        channel = createFileChannel(overrides);        Assert.fail();    } catch (RuntimeException ex) {        Assert.assertTrue("Exception message is incorrect: " + ex.getMessage(), ex.getMessage().startsWith("java.io.FileNotFoundException: /path/does/not/exist "));    }}
public void flume_f1185_0() throws Exception
{    Files.write("invalid", keyStorePasswordFile, Charsets.UTF_8);    Map<String, String> overrides = getOverridesForEncryption();    try {        channel = TestUtils.createFileChannel(checkpointDir.getAbsolutePath(), dataDir, overrides);        Assert.fail();    } catch (RuntimeException ex) {        Assert.assertEquals("java.io.IOException: Keystore was tampered with, or " + "password was incorrect", ex.getMessage());    }}
public void flume_f1186_0() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(EncryptionConfiguration.ENCRYPTION_PREFIX + "." + EncryptionConfiguration.ACTIVE_KEY, "invalid");    channel = TestUtils.createFileChannel(checkpointDir.getAbsolutePath(), dataDir, overrides);    channel.start();    Assert.assertFalse(channel.isOpen());}
public void flume_f1187_0() throws Exception
{    baseDir = Files.createTempDir();    keyStorePasswordFile = new File(baseDir, "keyStorePasswordFile");    Files.write("keyStorePassword", keyStorePasswordFile, Charsets.UTF_8);    keyAliasPassword = Maps.newHashMap();    keyStoreFile = new File(baseDir, "keyStoreFile");    Assert.assertTrue(keyStoreFile.createNewFile());}
public void flume_f1188_0()
{    FileUtils.deleteQuietly(baseDir);}
private void flume_f1189_0(Key key)
{    encryptor = new AESCTRNoPaddingProvider.EncryptorBuilder().setKey(key).build();    decryptor = new AESCTRNoPaddingProvider.DecryptorBuilder().setKey(key).setParameters(encryptor.getParameters()).build();}
public void flume_f1190_0() throws Exception
{    createNewKeyStore();    EncryptionTestUtils.createKeyStore(keyStoreFile, keyStorePasswordFile, keyAliasPassword);    Context context = new Context(EncryptionTestUtils.configureForKeyStore(keyStoreFile, keyStorePasswordFile, keyAliasPassword));    Context keyProviderContext = new Context(context.getSubProperties(EncryptionConfiguration.KEY_PROVIDER + "."));    KeyProvider keyProvider = KeyProviderFactory.getInstance(KeyProviderType.JCEKSFILE.name(), keyProviderContext);    testKeyProvider(keyProvider);}
public void flume_f1191_0() throws Exception
{    keyAliasPassword.putAll(EncryptionTestUtils.configureTestKeyStore(baseDir, keyStoreFile));    Context context = new Context(EncryptionTestUtils.configureForKeyStore(keyStoreFile, keyStorePasswordFile, keyAliasPassword));    Context keyProviderContext = new Context(context.getSubProperties(EncryptionConfiguration.KEY_PROVIDER + "."));    KeyProvider keyProvider = KeyProviderFactory.getInstance(KeyProviderType.JCEKSFILE.name(), keyProviderContext);    testKeyProvider(keyProvider);}
private void flume_f1192_0() throws Exception
{    for (int i = 0; i < 10; i++) {                if (i % 2 == 0) {            String alias = "test-" + i;            String password = String.valueOf(i);            keyAliasPassword.put(alias, TestUtils.writeStringToFile(baseDir, alias, password));        }    }}
private void flume_f1193_0(KeyProvider keyProvider)
{    for (String alias : keyAliasPassword.keySet()) {        Key key = keyProvider.getKey(alias);        initializeForKey(key);        String expected = "some text here " + alias;        byte[] cipherText = encryptor.encrypt(expected.getBytes(Charsets.UTF_8));        byte[] clearText = decryptor.decrypt(cipherText);        Assert.assertEquals(expected, new String(clearText, Charsets.UTF_8));    }}
public void flume_f1194_0() throws IOException
{    file = File.createTempFile("Checkpoint", "");    inflightPuts = File.createTempFile("inflightPuts", "");    inflightTakes = File.createTempFile("inflightTakes", "");    queueSet = File.createTempFile("queueset", "");    Assert.assertTrue(file.isFile());    Assert.assertTrue(file.canWrite());}
public void flume_f1195_0()
{    file.delete();}
public void flume_f1196_0() throws Exception
{    EventQueueBackingStore backingStore = new EventQueueBackingStoreFileV2(file, 1, "test", new FileChannelCounter("test"));    FlumeEventPointer ptrIn = new FlumeEventPointer(10, 20);    FlumeEventQueue queueIn = new FlumeEventQueue(backingStore, inflightTakes, inflightPuts, queueSet);    queueIn.addHead(ptrIn);    FlumeEventQueue queueOut = new FlumeEventQueue(backingStore, inflightTakes, inflightPuts, queueSet);    Assert.assertEquals(0, queueOut.getLogWriteOrderID());    queueIn.checkpoint(false);    FlumeEventQueue queueOut2 = new FlumeEventQueue(backingStore, inflightTakes, inflightPuts, queueSet);    FlumeEventPointer ptrOut = queueOut2.removeHead(0L);    Assert.assertEquals(ptrIn, ptrOut);    Assert.assertTrue(queueOut2.getLogWriteOrderID() > 0);}
public void flume_f1197_0() throws Exception
{    super.setup();}
public void flume_f1198_0()
{    super.teardown();}
public void flume_f1199_0() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(50));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(50));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "checkpointBulder");    channel.stop();    File checkpointFile = new File(checkpointDir, "checkpoint");    File metaDataFile = Serialization.getMetaDataFile(checkpointFile);    File inflightTakesFile = new File(checkpointDir, "inflighttakes");    File inflightPutsFile = new File(checkpointDir, "inflightputs");    File queueSetDir = new File(checkpointDir, "queueset");    Assert.assertTrue(checkpointFile.delete());    Assert.assertTrue(metaDataFile.delete());    Assert.assertTrue(inflightTakesFile.delete());    Assert.assertTrue(inflightPutsFile.delete());    EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpointFile, 50, "test", new FileChannelCounter("test"));    FlumeEventQueue queue = new FlumeEventQueue(backingStore, inflightTakesFile, inflightPutsFile, queueSetDir);    CheckpointRebuilder checkpointRebuilder = new CheckpointRebuilder(getAllLogs(dataDirs), queue, true);    Assert.assertTrue(checkpointRebuilder.rebuild());    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
public void flume_f1200_0() throws IOException
{    baseDir = Files.createTempDir();    checkpoint = new File(baseDir, "checkpoint");    inflightTakes = new File(baseDir, "takes");    inflightPuts = new File(baseDir, "puts");    queueSetDir = new File(baseDir, "queueset");    TestUtils.copyDecompressed("fileformat-v2-checkpoint.gz", checkpoint);}
public void flume_f1201_0()
{    FileUtils.deleteQuietly(baseDir);}
public void flume_f1202_0() throws Exception
{    verify(EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test")), Serialization.VERSION_3, pointersInTestCheckpoint);}
public void flume_f1203_0() throws Exception
{    verify(EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"), true), Serialization.VERSION_3, pointersInTestCheckpoint);}
public void flume_f1204_0() throws Exception
{    verify(EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"), false), Serialization.VERSION_2, pointersInTestCheckpoint);}
public void flume_f1205_0() throws Exception
{    Assert.assertTrue(checkpoint.delete());    EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    backingStore.close();    EventQueueBackingStoreFactory.get(checkpoint, 9, "test", new FileChannelCounter("test"));    Assert.fail();}
public void flume_f1206_0() throws Exception
{    Assert.assertTrue(checkpoint.delete());    EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    backingStore.close();    EventQueueBackingStoreFactory.get(checkpoint, 11, "test", new FileChannelCounter("test"));    Assert.fail();}
public void flume_f1207_0() throws Exception
{    Assert.assertTrue(checkpoint.delete());    verify(EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"), false), Serialization.VERSION_3, Collections.<Long>emptyList());}
public void flume_f1208_0() throws Exception
{    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    try {        EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));        backingStore.close();        writer.seek(EventQueueBackingStoreFile.INDEX_VERSION * Serialization.SIZE_OF_LONG);        writer.writeLong(94L);        writer.getFD().sync();        backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    } finally {        writer.close();    }}
public void flume_f1209_0() throws Exception
{    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    try {        EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));        backingStore.close();        writer.seek(EventQueueBackingStoreFile.INDEX_CHECKPOINT_MARKER * Serialization.SIZE_OF_LONG);        writer.writeLong(EventQueueBackingStoreFile.CHECKPOINT_INCOMPLETE);        writer.getFD().sync();        backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    } finally {        writer.close();    }}
public void flume_f1210_0() throws Exception
{    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    try {        EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));        backingStore.close();        writer.seek(EventQueueBackingStoreFile.INDEX_VERSION * Serialization.SIZE_OF_LONG);        writer.writeLong(2L);        writer.getFD().sync();        backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    } finally {        writer.close();    }}
public void flume_f1211_0() throws Exception
{    FileOutputStream os = null;    try {        EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));        backingStore.close();        Assert.assertTrue(checkpoint.exists());        Assert.assertTrue(Serialization.getMetaDataFile(checkpoint).length() != 0);        FileInputStream is = new FileInputStream(Serialization.getMetaDataFile(checkpoint));        ProtosFactory.Checkpoint meta = ProtosFactory.Checkpoint.parseDelimitedFrom(is);        Assert.assertNotNull(meta);        is.close();        os = new FileOutputStream(Serialization.getMetaDataFile(checkpoint));        meta.toBuilder().setVersion(2).build().writeDelimitedTo(os);        os.flush();        backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    } finally {        os.close();    }}
public void flume_f1212_0() throws Exception
{    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    try {        EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));        backingStore.close();        writer.seek(EventQueueBackingStoreFile.INDEX_WRITE_ORDER_ID * Serialization.SIZE_OF_LONG);        writer.writeLong(2L);        writer.getFD().sync();        backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    } finally {        writer.close();    }}
public void flume_f1213_0() throws Exception
{    FileOutputStream os = null;    try {        EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));        backingStore.close();        Assert.assertTrue(checkpoint.exists());        Assert.assertTrue(Serialization.getMetaDataFile(checkpoint).length() != 0);        FileInputStream is = new FileInputStream(Serialization.getMetaDataFile(checkpoint));        ProtosFactory.Checkpoint meta = ProtosFactory.Checkpoint.parseDelimitedFrom(is);        Assert.assertNotNull(meta);        is.close();        os = new FileOutputStream(Serialization.getMetaDataFile(checkpoint));        meta.toBuilder().setWriteOrderID(1).build().writeDelimitedTo(os);        os.flush();        backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    } finally {        os.close();    }}
public void flume_f1214_0() throws Exception
{    EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    backingStore.close();    Assert.assertTrue(checkpoint.exists());    File metaFile = Serialization.getMetaDataFile(checkpoint);    Assert.assertTrue(metaFile.length() != 0);    RandomAccessFile writer = new RandomAccessFile(metaFile, "rw");    writer.setLength(0);    writer.getFD().sync();    writer.close();    backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));}
public void flume_f1215_0() throws Throwable
{    EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    backingStore.close();    Assert.assertTrue(checkpoint.exists());    File metaFile = Serialization.getMetaDataFile(checkpoint);    Assert.assertTrue(metaFile.length() != 0);    RandomAccessFile writer = new RandomAccessFile(metaFile, "rw");    writer.seek(10);    writer.writeLong(new Random().nextLong());    writer.getFD().sync();    writer.close();    try {        backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    } catch (BadCheckpointException ex) {        throw ex.getCause();    }}
private void flume_f1216_0(EventQueueBackingStore backingStore, long expectedVersion, List<Long> expectedPointers) throws Exception
{    FlumeEventQueue queue = new FlumeEventQueue(backingStore, inflightTakes, inflightPuts, queueSetDir);    List<Long> actualPointers = Lists.newArrayList();    FlumeEventPointer ptr;    while ((ptr = queue.removeHead(0L)) != null) {        actualPointers.add(ptr.toLong());    }    Assert.assertEquals(expectedPointers, actualPointers);    Assert.assertEquals(10, backingStore.getCapacity());    DataInputStream in = new DataInputStream(new FileInputStream(checkpoint));    long actualVersion = in.readLong();    Assert.assertEquals(expectedVersion, actualVersion);    in.close();}
public void flume_f1217_0()
{    FlumeEvent event = new FlumeEvent(null, new byte[5]);    Put put = new Put(1L, 1L, event);    Event returnEvent = EventUtils.getEventFromTransactionEvent(put);    Assert.assertNotNull(returnEvent);    Assert.assertEquals(5, returnEvent.getBody().length);}
public void flume_f1218_0()
{    Take take = new Take(1L, 1L);    Event returnEvent = EventUtils.getEventFromTransactionEvent(take);    Assert.assertNull(returnEvent);}
public void flume_f1219_0() throws Exception
{    super.setup();}
public void flume_f1220_0()
{    super.teardown();}
public void flume_f1221_0()
{    Map<String, String> parms = Maps.newHashMap();    parms.put(FileChannelConfiguration.CAPACITY, "-3");    parms.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "-1");    parms.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "-2");    FileChannel channel = createFileChannel(parms);    Assert.assertTrue(field("capacity").ofType(Integer.class).in(channel).get() > 0);    Assert.assertTrue(field("transactionCapacity").ofType(Integer.class).in(channel).get() > 0);    Assert.assertTrue(field("checkpointInterval").ofType(Long.class).in(channel).get() > 0);}
public void flume_f1222_0() throws Throwable
{    final FileChannel channel = createFileChannel();    channel.start();    final Set<String> eventSet = putEvents(channel, "testTakeFailBeforeCommit", 5, 5);    Transaction tx = channel.getTransaction();    takeWithoutCommit(channel, tx, 2);            Executors.newSingleThreadExecutor().submit(new Runnable() {        @Override        public void run() {            Transaction tx = channel.getTransaction();            takeWithoutCommit(channel, tx, 3);        }    }).get();    forceCheckpoint(channel);    channel.stop();        try {        Executors.newSingleThreadExecutor().submit(new Runnable() {            @Override            public void run() {                FileChannel channel = createFileChannel();                channel.start();                Set<String> output = null;                try {                    output = takeEvents(channel, 5);                } catch (Exception e) {                    Throwables.propagate(e);                }                compareInputAndOut(eventSet, output);                channel.stop();            }        }).get();    } catch (ExecutionException e) {        throw e.getCause();    }}
public void flume_f1223_0()
{    Transaction tx = channel.getTransaction();    takeWithoutCommit(channel, tx, 3);}
public void flume_f1224_0()
{    FileChannel channel = createFileChannel();    channel.start();    Set<String> output = null;    try {        output = takeEvents(channel, 5);    } catch (Exception e) {        Throwables.propagate(e);    }    compareInputAndOut(eventSet, output);    channel.stop();}
public void flume_f1225_0() throws Throwable
{    final Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "10000");    final FileChannel channel = createFileChannel(overrides);    channel.start();    Transaction tx = channel.getTransaction();    final Set<String> input = putWithoutCommit(channel, tx, "failAfterPut", 3);            final CountDownLatch latch = new CountDownLatch(1);    Executors.newSingleThreadExecutor().submit(new Runnable() {        @Override        public void run() {            Transaction tx = channel.getTransaction();            input.addAll(putWithoutCommit(channel, tx, "failAfterPut", 3));            try {                latch.await();                tx.commit();            } catch (InterruptedException e) {                tx.rollback();                Throwables.propagate(e);            } finally {                tx.close();            }        }    });    forceCheckpoint(channel);    tx.commit();    tx.close();    latch.countDown();    Thread.sleep(2000);    channel.stop();    final Set<String> out = Sets.newHashSet();        try {        Executors.newSingleThreadExecutor().submit(new Runnable() {            @Override            public void run() {                try {                    FileChannel channel = createFileChannel();                    channel.start();                    out.addAll(takeEvents(channel, 6));                    channel.stop();                } catch (Exception ex) {                    Throwables.propagate(ex);                }            }        }).get();    } catch (ExecutionException e) {        throw e.getCause();    }}
public void flume_f1226_0()
{    Transaction tx = channel.getTransaction();    input.addAll(putWithoutCommit(channel, tx, "failAfterPut", 3));    try {        latch.await();        tx.commit();    } catch (InterruptedException e) {        tx.rollback();        Throwables.propagate(e);    } finally {        tx.close();    }}
public void flume_f1227_0()
{    try {        FileChannel channel = createFileChannel();        channel.start();        out.addAll(takeEvents(channel, 6));        channel.stop();    } catch (Exception ex) {        Throwables.propagate(ex);    }}
public void flume_f1228_0() throws Exception
{    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = Sets.newHashSet();    try {        while (true) {            in.addAll(putEvents(channel, "reconfig", 1, 1));        }    } catch (ChannelException e) {        Assert.assertEquals("The channel has reached it's capacity. " + "This might be the result of a sink on the channel having too " + "low of batch size, a downstream system running slower than " + "normal, or that the channel capacity is just too low. [channel=" + channel.getName() + "]", e.getMessage());    }    Configurables.configure(channel, createContext());    Set<String> out = takeEvents(channel, 1, Integer.MAX_VALUE);    compareInputAndOut(in, out);}
public void flume_f1229_0() throws Exception
{    channel.start();    Assert.assertTrue(channel.isOpen());        int found = takeEvents(channel, 1, 5).size();    Assert.assertEquals(0, found);    Set<String> expected = Sets.newHashSet();    expected.addAll(putEvents(channel, "unbatched", 1, 5));    expected.addAll(putEvents(channel, "batched", 5, 5));    Set<String> actual = takeEvents(channel, 1);    compareInputAndOut(expected, actual);}
public void flume_f1230_0() throws Exception
{    channel.start();    Event event = EventBuilder.withBody("test body".getBytes(Charsets.UTF_8), Collections.<String, String>singletonMap(TEST_KEY, null));    Transaction txPut = channel.getTransaction();    txPut.begin();    channel.put(event);    txPut.commit();    txPut.close();    Transaction txTake = channel.getTransaction();    txTake.begin();    Event eventTaken = channel.take();    Assert.assertArrayEquals(event.getBody(), eventTaken.getBody());    Assert.assertEquals("", eventTaken.getHeaders().get(TEST_KEY));    txTake.commit();    txTake.close();}
public void flume_f1231_0() throws Exception
{    channel.start();    Assert.assertTrue(channel.isOpen());    Transaction transaction;    transaction = channel.getTransaction();    transaction.begin();    transaction.commit();    transaction.close();        channel.stop();    channel = createFileChannel();    channel.start();    Assert.assertTrue(channel.isOpen());    transaction = channel.getTransaction();    transaction.begin();    Assert.assertNull(channel.take());    transaction.commit();    transaction.close();}
public void flume_f1232_0() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(5));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(5));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    fillChannel(channel, "fillup");            Transaction transaction;    transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    transaction.rollback();    transaction.close();        Assert.assertEquals(0, fillChannel(channel, "capacity").size());        Assert.assertEquals(5, takeEvents(channel, 1, 5).size());}
public void flume_f1233_1() throws Exception
{        Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.KEEP_ALIVE, String.valueOf(10L));    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(10L));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(10L));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    fillChannel(channel, "fillup");        Future<String> put = Executors.newSingleThreadExecutor().submit(new Callable<String>() {        @Override        public String call() throws Exception {            Set<String> result = putEvents(channel, "blocked-put", 1, 1);            Assert.assertTrue(result.toString(), result.size() == 1);            Iterator<String> iter = result.iterator();            return iter.next();        }    });        Thread.sleep(1000L);        Set<String> result = takeEvents(channel, 1, 1);    Assert.assertTrue(result.toString(), result.size() == 1);    String putmsg = put.get();    Assert.assertNotNull(putmsg);    String takemsg = result.iterator().next();    Assert.assertNotNull(takemsg);        channel.stop();    channel = createFileChannel(overrides);            channel.start();    Assert.assertTrue(channel.isOpen());}
public String flume_f1234_0() throws Exception
{    Set<String> result = putEvents(channel, "blocked-put", 1, 1);    Assert.assertTrue(result.toString(), result.size() == 1);    Iterator<String> iter = result.iterator();    return iter.next();}
public void flume_f1235_1() throws IOException, InterruptedException
{    channel.start();    Assert.assertTrue(channel.isOpen());    int numThreads = 10;    final CountDownLatch producerStopLatch = new CountDownLatch(numThreads);    final CountDownLatch consumerStopLatch = new CountDownLatch(numThreads);    final List<Exception> errors = Collections.synchronizedList(new ArrayList<Exception>());    final Set<String> expected = Collections.synchronizedSet(new HashSet<String>());    final Set<String> actual = Collections.synchronizedSet(new HashSet<String>());    for (int i = 0; i < numThreads; i++) {        final int id = i;        Thread t = new Thread() {            @Override            public void run() {                try {                    if (id % 2 == 0) {                        expected.addAll(putEvents(channel, Integer.toString(id), 1, 5));                    } else {                        expected.addAll(putEvents(channel, Integer.toString(id), 5, 5));                    }                                    } catch (Exception e) {                                        errors.add(e);                } finally {                    producerStopLatch.countDown();                }            }        };        t.setDaemon(true);        t.start();    }    for (int i = 0; i < numThreads; i++) {        final int id = i;        Thread t = new Thread() {            @Override            public void run() {                try {                    while (!producerStopLatch.await(1, TimeUnit.SECONDS) || expected.size() > actual.size()) {                        if (id % 2 == 0) {                            actual.addAll(takeEvents(channel, 1, Integer.MAX_VALUE));                        } else {                            actual.addAll(takeEvents(channel, 5, Integer.MAX_VALUE));                        }                    }                    if (actual.isEmpty()) {                                            } else {                                            }                } catch (Exception e) {                                        errors.add(e);                } finally {                    consumerStopLatch.countDown();                }            }        };        t.setDaemon(true);        t.start();    }    Assert.assertTrue("Timed out waiting for producers", producerStopLatch.await(30, TimeUnit.SECONDS));    Assert.assertTrue("Timed out waiting for consumer", consumerStopLatch.await(30, TimeUnit.SECONDS));    Assert.assertEquals(Collections.EMPTY_LIST, errors);    compareInputAndOut(expected, actual);}
public void flume_f1236_1()
{    try {        if (id % 2 == 0) {            expected.addAll(putEvents(channel, Integer.toString(id), 1, 5));        } else {            expected.addAll(putEvents(channel, Integer.toString(id), 5, 5));        }            } catch (Exception e) {                errors.add(e);    } finally {        producerStopLatch.countDown();    }}
public void flume_f1237_1()
{    try {        while (!producerStopLatch.await(1, TimeUnit.SECONDS) || expected.size() > actual.size()) {            if (id % 2 == 0) {                actual.addAll(takeEvents(channel, 1, Integer.MAX_VALUE));            } else {                actual.addAll(takeEvents(channel, 5, Integer.MAX_VALUE));            }        }        if (actual.isEmpty()) {                    } else {                    }    } catch (Exception e) {                errors.add(e);    } finally {        consumerStopLatch.countDown();    }}
public void flume_f1238_0() throws IOException
{    channel.start();    Assert.assertTrue(channel.isOpen());    FileChannel fileChannel = createFileChannel();    fileChannel.start();    Assert.assertTrue(!fileChannel.isOpen());}
public void flume_f1239_0() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "10000");    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "restart");    Set<String> out = Sets.newHashSet();        Transaction tx = channel.getTransaction();    out.addAll(takeWithoutCommit(channel, tx, 1));            forceCheckpoint(channel);    tx.commit();    tx.close();    channel.stop();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());        Set<String> out2 = takeEvents(channel, 1, Integer.MAX_VALUE);    channel.stop();    in.removeAll(out);    compareInputAndOut(in, out2);}
public void flume_f1240_0() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(2));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(2));    overrides.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "10000");    FileChannel channel = createFileChannel(overrides);    channel.start();        Transaction tx = channel.getTransaction();    Set<String> in = putWithoutCommit(channel, tx, "putWithoutCommit", 1);    forceCheckpoint(channel);    tx.commit();    tx.close();    channel.stop();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = takeEvents(channel, 1);    compareInputAndOut(in, out);    channel.stop();}
public void flume_f1241_0() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(2));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(2));    overrides.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "10000");    FileChannel channel = createFileChannel(overrides);    channel.start();        Transaction tx = channel.getTransaction();    Set<String> in = putWithoutCommit(channel, tx, "doubleCheckpoint", 1);    forceCheckpoint(channel);    tx.commit();    tx.close();    forceCheckpoint(channel);    channel.stop();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = takeEvents(channel, 5);    compareInputAndOut(in, out);    channel.stop();}
public void flume_f1242_0() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "10000");    overrides.put(FileChannelConfiguration.MAX_FILE_SIZE, "150");    final FileChannel channel = createFileChannel(overrides);    channel.start();    putEvents(channel, "testing-reference-counting", 1, 15);    Transaction tx = channel.getTransaction();    takeWithoutCommit(channel, tx, 10);    forceCheckpoint(channel);    tx.rollback();            final Set<String> takenEvents = Sets.newHashSet();    Executors.newSingleThreadExecutor().submit(new Runnable() {        @Override        public void run() {            try {                takenEvents.addAll(takeEvents(channel, 15));            } catch (Exception ex) {                Throwables.propagate(ex);            }        }    }).get();    Assert.assertEquals(15, takenEvents.size());}
public void flume_f1243_0()
{    try {        takenEvents.addAll(takeEvents(channel, 15));    } catch (Exception ex) {        Throwables.propagate(ex);    }}
public void flume_f1244_0() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, String.valueOf(Integer.MAX_VALUE));    final FileChannel channel = createFileChannel(overrides);    channel.start();    FileBackedTransaction tx = (FileBackedTransaction) channel.getTransaction();    InflightEventWrapper inflightPuts = field("inflightPuts").ofType(InflightEventWrapper.class).in(field("queue").ofType(FlumeEventQueue.class).in(tx).get()).get();    tx.begin();    for (int i = 0; i < 100; i++) {        channel.put(EventBuilder.withBody("TestEvent".getBytes()));    }    Assert.assertFalse(inflightPuts.getFileIDs().isEmpty());    Assert.assertFalse(inflightPuts.getInFlightPointers().isEmpty());    tx.rollback();    tx.close();    Assert.assertTrue(inflightPuts.getFileIDs().isEmpty());    Assert.assertTrue(inflightPuts.getInFlightPointers().isEmpty());    Assert.assertTrue(channel.getDepth() == 0);    Set<String> in = putEvents(channel, "testing-rollbacks", 100, 100);    tx = (FileBackedTransaction) channel.getTransaction();    InflightEventWrapper inflightTakes = field("inflightTakes").ofType(InflightEventWrapper.class).in(field("queue").ofType(FlumeEventQueue.class).in(tx).get()).get();    tx.begin();    for (int i = 0; i < 100; i++) {        channel.take();    }    Assert.assertFalse(inflightTakes.getFileIDs().isEmpty());    Assert.assertFalse(inflightTakes.getInFlightPointers().isEmpty());    tx.rollback();    tx.close();    Assert.assertTrue(inflightTakes.getFileIDs().isEmpty());    Assert.assertTrue(inflightTakes.getInFlightPointers().isEmpty());    Assert.assertTrue(channel.getDepth() == in.size());}
public void flume_f1245_0() throws Exception
{    testChannelDiesOnCorruptEvent(true);}
public void flume_f1246_0() throws Exception
{    testChannelDiesOnCorruptEvent(false);}
private void flume_f1247_0(boolean fsyncPerTxn) throws Exception
{    Map<String, String> overrides = new HashMap<String, String>();    overrides.put(FileChannelConfiguration.FSYNC_PER_TXN, String.valueOf(fsyncPerTxn));    final FileChannel channel = createFileChannel(overrides);    channel.start();    putEvents(channel, "test-corrupt-event", 100, 100);    for (File dataDir : dataDirs) {        File[] files = dataDir.listFiles(new FilenameFilter() {            @Override            public boolean accept(File dir, String name) {                if (!name.endsWith("meta") && !name.contains("lock")) {                    return true;                }                return false;            }        });        if (files != null && files.length > 0) {            for (int j = 0; j < files.length; j++) {                RandomAccessFile fileToCorrupt = new RandomAccessFile(files[0], "rw");                fileToCorrupt.seek(50);                fileToCorrupt.writeByte(234);                fileToCorrupt.close();            }        }    }    Set<String> events;    try {        events = consumeChannel(channel, true);    } catch (IllegalStateException ex) {                                Assert.assertTrue(ex.getMessage().contains("Log is closed"));        throw ex;    }    if (fsyncPerTxn) {        Assert.fail();    } else {                        Assert.assertEquals(99, events.size());    }}
public boolean flume_f1248_0(File dir, String name)
{    if (!name.endsWith("meta") && !name.contains("lock")) {        return true;    }    return false;}
public void flume_f1249_0()
{    FileChannel channel = createFileChannel();    FileChannelCounter counter = channel.getChannelCounter();    Assert.assertEquals(counter.isOpen(), false);    channel.start();    Assert.assertEquals(counter.isOpen(), true);    channel.stop();    Assert.assertEquals(counter.isOpen(), false);}
public void flume_f1250_0() throws Exception
{    baseDir = Files.createTempDir();    checkpointDir = new File(baseDir, "chkpt");    backupDir = new File(baseDir, "backup");    uncompressedBackupCheckpoint = new File(backupDir, "checkpoint");    compressedBackupCheckpoint = new File(backupDir, "checkpoint.snappy");    Assert.assertTrue(checkpointDir.mkdirs() || checkpointDir.isDirectory());    Assert.assertTrue(backupDir.mkdirs() || backupDir.isDirectory());    dataDirs = new File[dataDirCount];    dataDir = "";    for (int i = 0; i < dataDirs.length; i++) {        dataDirs[i] = new File(baseDir, "data" + (i + 1));        Assert.assertTrue(dataDirs[i].mkdirs() || dataDirs[i].isDirectory());        dataDir += dataDirs[i].getAbsolutePath() + ",";    }    dataDir = dataDir.substring(0, dataDir.length() - 1);    channel = createFileChannel();}
public void flume_f1251_0()
{    if (channel != null && channel.isOpen()) {        channel.stop();    }    FileUtils.deleteQuietly(baseDir);}
protected Context flume_f1252_0()
{    return createContext(new HashMap<String, String>());}
protected Context flume_f1253_0(Map<String, String> overrides)
{    return TestUtils.createFileChannelContext(checkpointDir.getAbsolutePath(), dataDir, backupDir.getAbsolutePath(), overrides);}
protected FileChannel flume_f1254_0()
{    return createFileChannel(new HashMap<String, String>());}
protected FileChannel flume_f1255_0(Map<String, String> overrides)
{    return TestUtils.createFileChannel(checkpointDir.getAbsolutePath(), dataDir, backupDir.getAbsolutePath(), overrides);}
public void flume_f1256_0() throws Exception
{    final long usableSpaceRefreshInterval = 1;    FileChannel channel = Mockito.spy(createFileChannel());    Mockito.when(channel.createLogBuilder()).then(new Answer<Log.Builder>() {        @Override        public Log.Builder answer(InvocationOnMock invocation) throws Throwable {            Log.Builder ret = (Log.Builder) invocation.callRealMethod();            ret.setUsableSpaceRefreshInterval(usableSpaceRefreshInterval);            return ret;        }    });    channel.start();    FileChannelCounter channelCounter = channel.getChannelCounter();    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test1".getBytes()));    channel.put(EventBuilder.withBody("test2".getBytes()));    tx.commit();    tx.close();    assertEquals(2, channelCounter.getEventPutAttemptCount());    assertEquals(2, channelCounter.getEventPutSuccessCount());    assertEquals(0, channelCounter.getEventPutErrorCount());    tx = channel.getTransaction();    tx.begin();    channel.take();    tx.commit();    tx.close();    assertEquals(1, channelCounter.getEventTakeAttemptCount());    assertEquals(1, channelCounter.getEventTakeSuccessCount());    assertEquals(0, channelCounter.getEventTakeErrorCount());    FileUtils.deleteDirectory(baseDir);    Thread.sleep(2 * usableSpaceRefreshInterval);    tx = channel.getTransaction();    tx.begin();    ChannelException putException = null;    try {        channel.put(EventBuilder.withBody("test".getBytes()));    } catch (ChannelException ex) {        putException = ex;    }    assertNotNull(putException);    assertTrue(putException.getCause() instanceof IOException);    assertEquals(3, channelCounter.getEventPutAttemptCount());    assertEquals(2, channelCounter.getEventPutSuccessCount());    assertEquals(1, channelCounter.getEventPutErrorCount());    ChannelException takeException = null;    try {                channel.take();    } catch (ChannelException ex) {        takeException = ex;    }    assertNotNull(takeException);    assertTrue(takeException.getCause() instanceof IOException);    assertEquals(2, channelCounter.getEventTakeAttemptCount());    assertEquals(1, channelCounter.getEventTakeSuccessCount());    assertEquals(1, channelCounter.getEventTakeErrorCount());}
public Log.Builder flume_f1257_0(InvocationOnMock invocation) throws Throwable
{    Log.Builder ret = (Log.Builder) invocation.callRealMethod();    ret.setUsableSpaceRefreshInterval(usableSpaceRefreshInterval);    return ret;}
public void flume_f1258_0() throws Exception
{    FileChannel channel = createFileChannel(Collections.singletonMap(FileChannelConfiguration.FSYNC_PER_TXN, "false"));    channel.start();    FileChannelCounter channelCounter = channel.getChannelCounter();    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    tx.commit();    tx.close();    byte[] data = FileUtils.readFileToByteArray(new File(dataDirs[0], "log-1"));        data[0] = LogFile.OP_EOF;    FileUtils.writeByteArrayToFile(new File(dataDirs[0], "log-1"), data);    tx = channel.getTransaction();    tx.begin();    try {        channel.take();    } catch (Throwable t) {                                        Assert.fail("No exception should be thrown as fsyncPerTransaction is false");    }    assertEquals(1, channelCounter.getEventTakeAttemptCount());    assertEquals(0, channelCounter.getEventTakeSuccessCount());    assertEquals(1, channelCounter.getEventTakeErrorCount());}
public void flume_f1259_0() throws Exception
{    int checkpointInterval = 1500;    final FileChannel channel = createFileChannel(Collections.singletonMap(FileChannelConfiguration.CHECKPOINT_INTERVAL, String.valueOf(checkpointInterval)));    channel.start();    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    tx.commit();    tx.close();    final long beforeCheckpointWrite = System.currentTimeMillis();        assertEventuallyTrue("checkpoint should have been written", new BooleanPredicate() {        @Override        public boolean get() {            return new File(checkpointDir, "checkpoint").lastModified() > beforeCheckpointWrite;        }    }, checkpointInterval * 3);    assertEquals(0, channel.getChannelCounter().getCheckpointWriteErrorCount());    FileUtils.deleteDirectory(baseDir);        assertEventuallyTrue("checkpointWriterErrorCount should be 1", new BooleanPredicate() {        @Override        public boolean get() {            return channel.getChannelCounter().getCheckpointWriteErrorCount() == 1;        }    }, checkpointInterval * 3);}
public boolean flume_f1260_0()
{    return new File(checkpointDir, "checkpoint").lastModified() > beforeCheckpointWrite;}
public boolean flume_f1261_0()
{    return channel.getChannelCounter().getCheckpointWriteErrorCount() == 1;}
public void flume_f1262_0() throws Exception
{    FileChannel channel = createFileChannel();    assertEquals(0, channel.getChannelCounter().getUnhealthy());    assertEquals(1, channel.getChannelCounter().getClosed());    assertFalse(channel.getChannelCounter().isOpen());    channel.start();    assertEquals(0, channel.getChannelCounter().getUnhealthy());    assertEquals(0, channel.getChannelCounter().getClosed());    assertTrue(channel.getChannelCounter().isOpen());}
public void flume_f1263_0() throws Exception
{    FileChannel channel = createFileChannel();    assertEquals(0, channel.getChannelCounter().getUnhealthy());    assertEquals(1, channel.getChannelCounter().getClosed());    assertFalse(channel.getChannelCounter().isOpen());    FileUtils.write(new File(dataDirs[0], "log-1"), "invalid data file content");    channel.start();    assertEquals(1, channel.getChannelCounter().getUnhealthy());    assertEquals(1, channel.getChannelCounter().getClosed());    assertFalse(channel.getChannelCounter().isOpen());}
public void flume_f1264_0() throws IOException, InterruptedException
{    FileChannelCounter fileChannelCounter = new FileChannelCounter("test");    File checkpointFile = File.createTempFile("checkpoint", ".tmp");    File backupDir = Files.createTempDirectory("checkpoint").toFile();    backupDir.deleteOnExit();    checkpointFile.deleteOnExit();    EventQueueBackingStoreFileV3 backingStoreFileV3 = new EventQueueBackingStoreFileV3(checkpointFile, 1, "test", fileChannelCounter, backupDir, true, false);        backingStoreFileV3.checkpoint();        assertEventuallyTrue("checkpoint backup write failure should increase counter to 1", new BooleanPredicate() {        @Override        public boolean get() {            return fileChannelCounter.getCheckpointBackupWriteErrorCount() == 1;        }    }, 100);}
public boolean flume_f1265_0()
{    return fileChannelCounter.getCheckpointBackupWriteErrorCount() == 1;}
public void flume_f1266_0() throws Exception
{    int checkpointInterval = 1500;    Map config = new HashMap();    config.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, String.valueOf(checkpointInterval));    config.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, "true");    final FileChannel channel = createFileChannel(Collections.unmodifiableMap(config));    channel.start();    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    tx.commit();    tx.close();    final long beforeCheckpointWrite = System.currentTimeMillis();        assertEventuallyTrue("checkpoint backup should have been written", new BooleanPredicate() {        @Override        public boolean get() {            return new File(backupDir, "checkpoint").lastModified() > beforeCheckpointWrite;        }    }, checkpointInterval * 3);    assertEquals(0, channel.getChannelCounter().getCheckpointBackupWriteErrorCount());    FileUtils.deleteDirectory(backupDir);    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test2".getBytes()));    tx.commit();    tx.close();        assertEventuallyTrue("checkpointBackupWriteErrorCount should be 1", new BooleanPredicate() {        @Override        public boolean get() {            return channel.getChannelCounter().getCheckpointBackupWriteErrorCount() >= 1;        }    }, checkpointInterval * 3);}
public boolean flume_f1267_0()
{    return new File(backupDir, "checkpoint").lastModified() > beforeCheckpointWrite;}
public boolean flume_f1268_0()
{    return channel.getChannelCounter().getCheckpointBackupWriteErrorCount() >= 1;}
private static void flume_f1269_0(String description, BooleanPredicate expression, long timeoutMillis) throws InterruptedException
{    long start = System.currentTimeMillis();    while (System.currentTimeMillis() < start + timeoutMillis) {        if (expression.get())            break;        Thread.sleep(timeoutMillis / 10);    }    assertTrue(description, expression.get());}
public void flume_f1270_0() throws Exception
{    super.setup();}
public void flume_f1271_0()
{    super.teardown();}
public void flume_f1272_0() throws Exception
{    TestUtils.copyDecompressed("fileformat-v2-checkpoint.gz", new File(checkpointDir, "checkpoint"));    for (int i = 0; i < dataDirs.length; i++) {        int fileIndex = i + 1;        TestUtils.copyDecompressed("fileformat-v2-log-" + fileIndex + ".gz", new File(dataDirs[i], "log-" + fileIndex));    }    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(10));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(10));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> events = takeEvents(channel, 1);    Set<String> expected = new HashSet<String>();    expected.addAll(Arrays.asList((new String[] { "2684", "2685", "2686", "2687", "2688", "2689", "2690", "2691" })));    compareInputAndOut(expected, events);}
public void flume_f1273_0() throws Exception
{    doTestFileFormatV2PreFLUME1432(true);}
public void flume_f1274_0() throws Exception
{    doTestFileFormatV2PreFLUME1432(false);}
public void flume_f1275_0(boolean useLogReplayV1) throws Exception
{    TestUtils.copyDecompressed("fileformat-v2-pre-FLUME-1432-checkpoint.gz", new File(checkpointDir, "checkpoint"));    for (int i = 0; i < dataDirs.length; i++) {        int fileIndex = i + 1;        TestUtils.copyDecompressed("fileformat-v2-pre-FLUME-1432-log-" + fileIndex + ".gz", new File(dataDirs[i], "log-" + fileIndex));    }    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(10000));    overrides.put(FileChannelConfiguration.USE_LOG_REPLAY_V1, String.valueOf(useLogReplayV1));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> events = takeEvents(channel, 1);    Assert.assertEquals(50, events.size());}
public void flume_f1276_0() throws Exception
{    super.setup();}
public void flume_f1277_0()
{    super.teardown();}
protected FileChannel flume_f1278_0(Map<String, String> overrides)
{        overrides.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "6000000");    return TestUtils.createFileChannel(checkpointDir.getAbsolutePath(), dataDir, backupDir.getAbsolutePath(), overrides);}
public void flume_f1279_0() throws Exception
{    doTestRestart(true, false, false, false);}
public void flume_f1280_0() throws Exception
{    doTestRestart(false, false, false, false);}
public void flume_f1281_0() throws Exception
{    doTestRestart(true, true, true, true);}
public void flume_f1282_0() throws Exception
{    doTestRestart(false, true, true, true);}
public void flume_f1283_0() throws Exception
{    doTestRestart(true, true, false, true);}
public void flume_f1284_0() throws Exception
{    doTestRestart(false, true, false, true);}
public void flume_f1285_0() throws Exception
{    doTestRestart(true, true, true, false);}
public void flume_f1286_0() throws Exception
{    doTestRestart(false, true, true, false);}
public void flume_f1287_0(boolean useLogReplayV1, boolean forceCheckpoint, boolean deleteCheckpoint, boolean useFastReplay) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_LOG_REPLAY_V1, String.valueOf(useLogReplayV1));    overrides.put(FileChannelConfiguration.USE_FAST_REPLAY, String.valueOf(useFastReplay));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "restart");    if (forceCheckpoint) {        forceCheckpoint(channel);    }    channel.stop();    if (deleteCheckpoint) {        File checkpoint = new File(checkpointDir, "checkpoint");        Assert.assertTrue(checkpoint.delete());        File checkpointMetaData = Serialization.getMetaDataFile(checkpoint);        Assert.assertTrue(checkpointMetaData.delete());    }    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
public void flume_f1288_0() throws Exception
{    doTestRestartWhenMetaDataExistsButCheckpointDoesNot(false);}
public void flume_f1289_0() throws Exception
{    doTestRestartWhenMetaDataExistsButCheckpointDoesNot(true);}
private void flume_f1290_0(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    Assert.assertTrue(checkpoint.delete());    File checkpointMetaData = Serialization.getMetaDataFile(checkpoint);    Assert.assertTrue(checkpointMetaData.exists());    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(checkpoint.exists());    Assert.assertTrue(checkpointMetaData.exists());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
public void flume_f1291_0() throws Exception
{    doTestRestartWhenCheckpointExistsButMetaDoesNot(false);}
public void flume_f1292_0() throws Exception
{    doTestRestartWhenCheckpointExistsButMetaDoesNot(true);}
private void flume_f1293_0(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    File checkpointMetaData = Serialization.getMetaDataFile(checkpoint);    Assert.assertTrue(checkpointMetaData.delete());    Assert.assertTrue(checkpoint.exists());    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(checkpoint.exists());    Assert.assertTrue(checkpointMetaData.exists());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
public void flume_f1294_0() throws Exception
{    doTestRestartWhenNoCheckpointExists(false);}
public void flume_f1295_0() throws Exception
{    doTestRestartWhenNoCheckpointExists(true);}
private void flume_f1296_0(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    File checkpointMetaData = Serialization.getMetaDataFile(checkpoint);    Assert.assertTrue(checkpointMetaData.delete());    Assert.assertTrue(checkpoint.delete());    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(checkpoint.exists());    Assert.assertTrue(checkpointMetaData.exists());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
public void flume_f1297_0() throws Exception
{    doTestBadCheckpointVersion(false);}
public void flume_f1298_0() throws Exception
{    doTestBadCheckpointVersion(true);}
private void flume_f1299_0(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    writer.seek(EventQueueBackingStoreFile.INDEX_VERSION * Serialization.SIZE_OF_LONG);    writer.writeLong(2L);    writer.getFD().sync();    writer.close();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
public void flume_f1300_0() throws Exception
{    doTestBadCheckpointMetaVersion(false);}
public void flume_f1301_0() throws Exception
{    doTestBadCheckpointMetaVersion(true);}
private void flume_f1302_0(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    FileInputStream is = new FileInputStream(Serialization.getMetaDataFile(checkpoint));    ProtosFactory.Checkpoint meta = ProtosFactory.Checkpoint.parseDelimitedFrom(is);    Assert.assertNotNull(meta);    is.close();    FileOutputStream os = new FileOutputStream(Serialization.getMetaDataFile(checkpoint));    meta.toBuilder().setVersion(2).build().writeDelimitedTo(os);    os.flush();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
public void flume_f1303_0() throws Exception
{    doTestDifferingOrderIDCheckpointAndMetaVersion(false);}
public void flume_f1304_0() throws Exception
{    doTestDifferingOrderIDCheckpointAndMetaVersion(true);}
private void flume_f1305_0(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    FileInputStream is = new FileInputStream(Serialization.getMetaDataFile(checkpoint));    ProtosFactory.Checkpoint meta = ProtosFactory.Checkpoint.parseDelimitedFrom(is);    Assert.assertNotNull(meta);    is.close();    FileOutputStream os = new FileOutputStream(Serialization.getMetaDataFile(checkpoint));    meta.toBuilder().setWriteOrderID(12).build().writeDelimitedTo(os);    os.flush();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
public void flume_f1306_0() throws Exception
{    doTestIncompleteCheckpoint(false);}
public void flume_f1307_0() throws Exception
{    doTestIncompleteCheckpoint(true);}
private void flume_f1308_0(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    writer.seek(EventQueueBackingStoreFile.INDEX_CHECKPOINT_MARKER * Serialization.SIZE_OF_LONG);    writer.writeLong(EventQueueBackingStoreFile.CHECKPOINT_INCOMPLETE);    writer.getFD().sync();    writer.close();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
public void flume_f1309_0() throws Exception
{    doTestCorruptInflights("inflightputs", false);}
public void flume_f1310_0() throws Exception
{    doTestCorruptInflights("inflightputs", true);}
public void flume_f1311_0() throws Exception
{    doTestCorruptInflights("inflighttakes", false);}
public void flume_f1312_0() throws Exception
{    doTestCorruptInflights("inflighttakes", true);}
public void flume_f1313_0() throws Exception
{    testFastReplay(false, true);}
public void flume_f1314_0() throws Exception
{    testFastReplay(true, true);}
public void flume_f1315_0() throws Exception
{    testFastReplay(false, false);}
public void flume_f1316_0() throws Exception
{    testFastReplay(true, false);}
private void flume_f1317_0(boolean shouldCorruptCheckpoint, boolean useFastReplay) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_FAST_REPLAY, String.valueOf(useFastReplay));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    channel.stop();    if (shouldCorruptCheckpoint) {        File checkpoint = new File(checkpointDir, "checkpoint");        RandomAccessFile writer = new RandomAccessFile(Serialization.getMetaDataFile(checkpoint), "rw");        writer.seek(10);        writer.writeLong(new Random().nextLong());        writer.getFD().sync();        writer.close();    }    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    if (useFastReplay && shouldCorruptCheckpoint) {        Assert.assertTrue(channel.didFastReplay());    } else {        Assert.assertFalse(channel.didFastReplay());    }    compareInputAndOut(in, out);}
private void flume_f1318_0(String name, boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    final Set<String> in1 = putEvents(channel, "restart-", 10, 100);    Assert.assertEquals(100, in1.size());    Executors.newSingleThreadScheduledExecutor().submit(new Runnable() {        @Override        public void run() {            Transaction tx = channel.getTransaction();            Set<String> out1 = takeWithoutCommit(channel, tx, 100);            Assert.assertEquals(100, out1.size());        }    });    Transaction tx = channel.getTransaction();    Set<String> in2 = putWithoutCommit(channel, tx, "restart", 100);    Assert.assertEquals(100, in2.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    tx.commit();    tx.close();    channel.stop();    File inflight = new File(checkpointDir, name);    RandomAccessFile writer = new RandomAccessFile(inflight, "rw");    writer.write(new Random().nextInt());    writer.close();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    in1.addAll(in2);    compareInputAndOut(in1, out);}
public void flume_f1319_0()
{    Transaction tx = channel.getTransaction();    Set<String> out1 = takeWithoutCommit(channel, tx, 100);    Assert.assertEquals(100, out1.size());}
public void flume_f1320_0() throws Exception
{    doTestTruncatedCheckpointMeta(false);}
public void flume_f1321_0() throws Exception
{    doTestTruncatedCheckpointMeta(true);}
private void flume_f1322_0(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    RandomAccessFile writer = new RandomAccessFile(Serialization.getMetaDataFile(checkpoint), "rw");    writer.setLength(0);    writer.getFD().sync();    writer.close();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
public void flume_f1323_0() throws Exception
{    doTestCorruptCheckpointMeta(false);}
public void flume_f1324_0() throws Exception
{    doTestCorruptCheckpointMeta(true);}
private void flume_f1325_0(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    RandomAccessFile writer = new RandomAccessFile(Serialization.getMetaDataFile(checkpoint), "rw");    writer.seek(10);    writer.writeLong(new Random().nextLong());    writer.getFD().sync();    writer.close();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
private void flume_f1326_0(boolean backup)
{    boolean backupRestored = channel.checkpointBackupRestored();    if (backup) {        Assert.assertTrue(backupRestored);    } else {        Assert.assertFalse(backupRestored);    }}
public void flume_f1327_0() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    writer.seek(EventQueueBackingStoreFile.INDEX_VERSION * Serialization.SIZE_OF_LONG);    writer.write(new byte[] { (byte) 1, (byte) 5 });    writer.getFD().sync();    writer.close();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    Assert.assertTrue(channel.didFullReplayDueToBadCheckpointException());    compareInputAndOut(in, out);}
public void flume_f1328_0() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    writer.seek(EventQueueBackingStoreFile.INDEX_CHECKPOINT_MARKER * Serialization.SIZE_OF_LONG);    writer.write(new byte[] { (byte) 1, (byte) 5 });    writer.getFD().sync();    writer.close();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    Assert.assertTrue(channel.didFullReplayDueToBadCheckpointException());    compareInputAndOut(in, out);}
public void flume_f1329_0() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, "10");    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "10");    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "extralogs");    for (int i = 0; i < dataDirs.length; i++) {        File file = new File(dataDirs[i], Log.PREFIX + (1000 + i));        Assert.assertTrue(file.createNewFile());        Assert.assertTrue(file.length() == 0);        File metaDataFile = Serialization.getMetaDataFile(file);        File metaDataTempFile = Serialization.getMetaDataTempFile(metaDataFile);        Assert.assertTrue(metaDataTempFile.createNewFile());    }    channel.stop();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
public void flume_f1330_0() throws Exception
{    testBackupUsedEnsureNoFullReplay(false);}
public void flume_f1331_0() throws Exception
{    testBackupUsedEnsureNoFullReplay(true);}
private void flume_f1332_0(boolean compressedBackup) throws Exception
{    File dataDir = Files.createTempDir();    File tempBackup = Files.createTempDir();    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.DATA_DIRS, dataDir.getAbsolutePath());    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, "true");    overrides.put(FileChannelConfiguration.COMPRESS_BACKUP_CHECKPOINT, String.valueOf(compressedBackup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    Thread.sleep(5000);    forceCheckpoint(channel);    Thread.sleep(5000);    in = putEvents(channel, "restart", 10, 100);    takeEvents(channel, 10, 100);    Assert.assertEquals(100, in.size());    for (File file : backupDir.listFiles()) {        if (file.getName().equals(Log.FILE_LOCK)) {            continue;        }        Files.copy(file, new File(tempBackup, file.getName()));    }    forceCheckpoint(channel);    channel.stop();    Serialization.deleteAllFiles(checkpointDir, Log.EXCLUDES);                    Serialization.deleteAllFiles(backupDir, Log.EXCLUDES);    for (File file : tempBackup.listFiles()) {        if (file.getName().equals(Log.FILE_LOCK)) {            continue;        }        Files.copy(file, new File(backupDir, file.getName()));    }    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    checkIfBackupUsed(true);    Assert.assertEquals(100, channel.getLog().getPutCount());    Assert.assertEquals(20, channel.getLog().getCommittedCount());    Assert.assertEquals(100, channel.getLog().getTakeCount());    Assert.assertEquals(0, channel.getLog().getRollbackCount());        Assert.assertEquals(220, channel.getLog().getReadCount());    consumeChannel(channel);    FileUtils.deleteQuietly(dataDir);    FileUtils.deleteQuietly(tempBackup);}
public void flume_f1333_0() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, "true");    overrides.put(FileChannelConfiguration.MAX_FILE_SIZE, "1000");    channel = createFileChannel(overrides);    channel.start();    String prefix = "abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz";    Assert.assertTrue(channel.isOpen());    putEvents(channel, prefix, 10, 100);    Set<String> origFiles = Sets.newHashSet();    for (File dir : dataDirs) {        origFiles.addAll(Lists.newArrayList(dir.list()));    }    forceCheckpoint(channel);    takeEvents(channel, 10, 50);    long beforeSecondCheckpoint = System.currentTimeMillis();    forceCheckpoint(channel);    Set<String> newFiles = Sets.newHashSet();    int olderThanCheckpoint = 0;    int totalMetaFiles = 0;    for (File dir : dataDirs) {        File[] metadataFiles = dir.listFiles(new FilenameFilter() {            @Override            public boolean accept(File dir, String name) {                if (name.endsWith(".meta")) {                    return true;                }                return false;            }        });        totalMetaFiles = metadataFiles.length;        for (File metadataFile : metadataFiles) {            if (metadataFile.lastModified() < beforeSecondCheckpoint) {                olderThanCheckpoint++;            }        }        newFiles.addAll(Lists.newArrayList(dir.list()));    }    /*     * Files which are not required by the new checkpoint should not have been     * modified by the checkpoint.     */    Assert.assertTrue(olderThanCheckpoint > 0);    Assert.assertTrue(totalMetaFiles != olderThanCheckpoint);    /*     * All files needed by original checkpoint should still be there.     */    Assert.assertTrue(newFiles.containsAll(origFiles));    takeEvents(channel, 10, 50);    forceCheckpoint(channel);    newFiles = Sets.newHashSet();    for (File dir : dataDirs) {        newFiles.addAll(Lists.newArrayList(dir.list()));    }    Assert.assertTrue(!newFiles.containsAll(origFiles));}
public boolean flume_f1334_0(File dir, String name)
{    if (name.endsWith(".meta")) {        return true;    }    return false;}
public void flume_f1335_0() throws Throwable
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, "true");    overrides.put(FileChannelConfiguration.MAX_FILE_SIZE, "1000");    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    slowdownBackup(channel);    forceCheckpoint(channel);    in = putEvents(channel, "restart", 10, 100);    takeEvents(channel, 10, 100);    Assert.assertEquals(100, in.size());    try {        forceCheckpoint(channel);    } catch (ReflectionError ex) {        throw ex.getCause();    } finally {        channel.stop();    }}
public void flume_f1336_0() throws Throwable
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, "true");    overrides.put(FileChannelConfiguration.MAX_FILE_SIZE, "1000");    overrides.put(FileChannelConfiguration.COMPRESS_BACKUP_CHECKPOINT, "true");    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    putEvents(channel, "restart", 10, 100);    forceCheckpoint(channel);        Thread.sleep(2000);    Assert.assertTrue(compressedBackupCheckpoint.exists());    Serialization.decompressFile(compressedBackupCheckpoint, uncompressedBackupCheckpoint);    File checkpoint = new File(checkpointDir, "checkpoint");    Assert.assertTrue(FileUtils.contentEquals(checkpoint, uncompressedBackupCheckpoint));    channel.stop();}
public void flume_f1337_0() throws Exception
{    restartToggleCompression(true);}
public void flume_f1338_0() throws Exception
{    restartToggleCompression(false);}
public void flume_f1339_0(boolean originalCheckpointCompressed) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, "true");    overrides.put(FileChannelConfiguration.MAX_FILE_SIZE, "1000");    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "1000");    overrides.put(FileChannelConfiguration.CAPACITY, "1000");    overrides.put(FileChannelConfiguration.COMPRESS_BACKUP_CHECKPOINT, String.valueOf(originalCheckpointCompressed));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "restart");    forceCheckpoint(channel);    Thread.sleep(2000);    Assert.assertEquals(compressedBackupCheckpoint.exists(), originalCheckpointCompressed);    Assert.assertEquals(uncompressedBackupCheckpoint.exists(), !originalCheckpointCompressed);    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    Assert.assertTrue(checkpoint.delete());    File checkpointMetaData = Serialization.getMetaDataFile(checkpoint);    Assert.assertTrue(checkpointMetaData.delete());    overrides.put(FileChannelConfiguration.COMPRESS_BACKUP_CHECKPOINT, String.valueOf(!originalCheckpointCompressed));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);    forceCheckpoint(channel);    Thread.sleep(2000);    Assert.assertEquals(compressedBackupCheckpoint.exists(), !originalCheckpointCompressed);    Assert.assertEquals(uncompressedBackupCheckpoint.exists(), originalCheckpointCompressed);}
private static void flume_f1340_0(FileChannel channel)
{    Log log = field("log").ofType(Log.class).in(channel).get();    FlumeEventQueue queue = field("queue").ofType(FlumeEventQueue.class).in(log).get();    EventQueueBackingStore backingStore = field("backingStore").ofType(EventQueueBackingStore.class).in(queue).get();    field("slowdownBackup").ofType(Boolean.class).in(backingStore).set(true);}
public void flume_f1341_0() throws Exception
{    super.setup();}
public void flume_f1342_0()
{    super.teardown();}
public void flume_f1343_0() throws Exception
{    channel.start();    Assert.assertTrue(channel.isOpen());    Transaction transaction;    transaction = channel.getTransaction();    transaction.begin();    transaction.rollback();    transaction.close();        channel.stop();    channel = createFileChannel();    channel.start();    Assert.assertTrue(channel.isOpen());    transaction = channel.getTransaction();    transaction.begin();    Assert.assertNull(channel.take());    transaction.commit();    transaction.close();}
public void flume_f1344_0() throws Exception
{    channel.start();    Assert.assertTrue(channel.isOpen());    int numEvents = 50;    Set<String> in = putEvents(channel, "rollback", 1, numEvents);    Transaction transaction;        transaction = channel.getTransaction();    transaction.begin();    channel.put(EventBuilder.withBody("rolled back".getBytes(Charsets.UTF_8)));    transaction.rollback();    transaction.close();        channel.stop();    channel = createFileChannel();    channel.start();    Assert.assertTrue(channel.isOpen());        Set<String> out = takeEvents(channel, 1, numEvents);    compareInputAndOut(in, out);}
public void flume_f1345_1() throws Exception
{    channel.start();    Assert.assertTrue(channel.isOpen());    int numEvents = 100;    LoggerSink sink = new LoggerSink();    sink.setChannel(channel);        CountingSinkRunner runner = new CountingSinkRunner(sink, numEvents - 1);    runner.start();    putEvents(channel, "rollback", 10, numEvents);    Transaction transaction;        transaction = channel.getTransaction();    transaction.begin();    byte[] bytes = "rolled back".getBytes(Charsets.UTF_8);    channel.put(EventBuilder.withBody(bytes));    transaction.rollback();    transaction.close();    while (runner.isAlive()) {        Thread.sleep(10L);    }    Assert.assertEquals(numEvents - 1, runner.getCount());    for (Exception ex : runner.getErrors()) {            }    Assert.assertEquals(Collections.EMPTY_LIST, runner.getErrors());        channel.stop();    channel = createFileChannel();    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = takeEvents(channel, 1, 1);    Assert.assertEquals(1, out.size());    String s = out.iterator().next();    Assert.assertTrue(s, s.startsWith("rollback-90-9"));}
public void flume_f1346_0()
{    Map<String, String> headers = Maps.newHashMap();    headers.put("key", "value");    byte[] body = "flume".getBytes(Charsets.UTF_8);    FlumeEvent event = new FlumeEvent(headers, body);    Assert.assertEquals(headers, event.getHeaders());    Assert.assertTrue(Arrays.equals(body, event.getBody()));}
public void flume_f1347_0() throws IOException
{    Map<String, String> headers = Maps.newHashMap();    headers.put("key", "value");    byte[] body = "flume".getBytes(Charsets.UTF_8);    FlumeEvent in = new FlumeEvent(headers, body);    FlumeEvent out = FlumeEvent.from(TestUtils.toDataInput(in));    Assert.assertEquals(headers, out.getHeaders());    Assert.assertTrue(Arrays.equals(body, out.getBody()));    in.setHeaders(null);    in.setBody(null);    out = FlumeEvent.from(TestUtils.toDataInput(in));    Assert.assertEquals(Maps.newHashMap(), out.getHeaders());    Assert.assertNull(out.getBody());}
public void flume_f1348_0()
{    FlumeEventPointer pointer = new FlumeEventPointer(1, 1);    Assert.assertEquals(1, pointer.getFileID());    Assert.assertEquals(1, pointer.getOffset());}
public void flume_f1349_0()
{    FlumeEventPointer pointerA = new FlumeEventPointer(1, 1);    FlumeEventPointer pointerB = new FlumeEventPointer(1, 1);    Assert.assertEquals(pointerA, pointerB);    Assert.assertEquals(pointerB, pointerA);    pointerA = new FlumeEventPointer(1, 1);    pointerB = new FlumeEventPointer(2, 2);    Assert.assertFalse(pointerA.equals(pointerB));    Assert.assertFalse(pointerB.equals(pointerA));}
public void flume_f1350_0()
{    FlumeEventPointer pointerA = new FlumeEventPointer(1, 1);    FlumeEventPointer pointerB = new FlumeEventPointer(1, 1);    Assert.assertEquals(pointerA.hashCode(), pointerB.hashCode());    pointerA = new FlumeEventPointer(1, 1);    pointerB = new FlumeEventPointer(2, 2);    Assert.assertFalse(pointerA.hashCode() == pointerB.hashCode());}
public void flume_f1351_0()
{    FlumeEventPointer pointerA = new FlumeEventPointer(1, 1);    FlumeEventPointer pointerB = new FlumeEventPointer(1, 2);    Assert.assertEquals(4294967297L, pointerA.toLong());    Assert.assertEquals(4294967298L, pointerB.toLong());    Assert.assertEquals(pointerA, FlumeEventPointer.fromLong(pointerA.toLong()));    Assert.assertEquals(pointerB, FlumeEventPointer.fromLong(pointerB.toLong()));}
 File flume_f1352_0()
{    return checkpoint;}
 File flume_f1353_0()
{    return inflightPuts;}
 File flume_f1354_0()
{    return inflightTakes;}
 File flume_f1355_0()
{    return queueSetDir;}
 void flume_f1356_0()
{    FileUtils.deleteQuietly(baseDir);}
public static Collection<Object[]> flume_f1357_0() throws Exception
{    Object[][] data = new Object[][] { { new EventQueueBackingStoreSupplier() {        @Override        public EventQueueBackingStore get() throws Exception {            Assert.assertTrue(baseDir.isDirectory() || baseDir.mkdirs());            return new EventQueueBackingStoreFileV2(getCheckpoint(), 1000, "test", new FileChannelCounter("test"));        }    } }, { new EventQueueBackingStoreSupplier() {        @Override        public EventQueueBackingStore get() throws Exception {            Assert.assertTrue(baseDir.isDirectory() || baseDir.mkdirs());            return new EventQueueBackingStoreFileV3(getCheckpoint(), 1000, "test", new FileChannelCounter("test"));        }    } } };    return Arrays.asList(data);}
public EventQueueBackingStore flume_f1358_0() throws Exception
{    Assert.assertTrue(baseDir.isDirectory() || baseDir.mkdirs());    return new EventQueueBackingStoreFileV2(getCheckpoint(), 1000, "test", new FileChannelCounter("test"));}
public EventQueueBackingStore flume_f1359_0() throws Exception
{    Assert.assertTrue(baseDir.isDirectory() || baseDir.mkdirs());    return new EventQueueBackingStoreFileV3(getCheckpoint(), 1000, "test", new FileChannelCounter("test"));}
public void flume_f1360_0() throws Exception
{    backingStore = backingStoreSupplier.get();}
public void flume_f1361_0() throws IOException
{    if (backingStore != null) {        backingStore.close();    }    backingStoreSupplier.delete();}
public void flume_f1362_0() throws Exception
{    backingStore.close();    File checkpoint = backingStoreSupplier.getCheckpoint();    Assert.assertTrue(checkpoint.delete());    backingStore = new EventQueueBackingStoreFileV2(checkpoint, 1, "test", new FileChannelCounter("test"));    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addTail(pointer1));    Assert.assertFalse(queue.addTail(pointer2));}
public void flume_f1363_0() throws Exception
{    backingStore.close();    File checkpoint = backingStoreSupplier.getCheckpoint();    Assert.assertTrue(checkpoint.delete());    backingStore = new EventQueueBackingStoreFileV2(checkpoint, 0, "test", new FileChannelCounter("test"));    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());}
public void flume_f1364_0() throws Exception
{    backingStore.close();    File checkpoint = backingStoreSupplier.getCheckpoint();    Assert.assertTrue(checkpoint.delete());    backingStore = new EventQueueBackingStoreFileV2(checkpoint, -1, "test", new FileChannelCounter("test"));    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());}
public void flume_f1365_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertNull(queue.removeHead(0L));}
public void flume_f1366_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addTail(pointer1));    Assert.assertEquals(pointer1, queue.removeHead(0));    Assert.assertEquals(Sets.newHashSet(), queue.getFileIDs());}
public void flume_f1367_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addTail(pointer1));    Assert.assertTrue(queue.addTail(pointer2));    Assert.assertEquals(Sets.newHashSet(1, 2), queue.getFileIDs());    Assert.assertEquals(pointer1, queue.removeHead(0));    Assert.assertEquals(Sets.newHashSet(2), queue.getFileIDs());}
public void flume_f1368_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    int size = 500;    Set<Integer> fileIDs = Sets.newHashSet();    for (int i = 1; i <= size; i++) {        Assert.assertTrue(queue.addTail(new FlumeEventPointer(i, i)));        fileIDs.add(i);        Assert.assertEquals(fileIDs, queue.getFileIDs());    }    for (int i = 1; i <= size; i++) {        Assert.assertEquals(new FlumeEventPointer(i, i), queue.removeHead(0));        fileIDs.remove(i);        Assert.assertEquals(fileIDs, queue.getFileIDs());    }    Assert.assertEquals(Sets.newHashSet(), queue.getFileIDs());}
public void flume_f1369_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addHead(pointer1));    Assert.assertEquals(Sets.newHashSet(1), queue.getFileIDs());    Assert.assertEquals(pointer1, queue.removeHead(0));    Assert.assertEquals(Sets.newHashSet(), queue.getFileIDs());}
public void flume_f1370_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    queue.replayComplete();    Assert.assertTrue(queue.addHead(pointer1));    Assert.assertTrue(queue.addHead(pointer2));    Assert.assertEquals(Sets.newHashSet(1, 2), queue.getFileIDs());    Assert.assertEquals(pointer2, queue.removeHead(0));    Assert.assertEquals(Sets.newHashSet(1), queue.getFileIDs());}
public void flume_f1371_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    queue.replayComplete();    int size = 500;    Set<Integer> fileIDs = Sets.newHashSet();    for (int i = 1; i <= size; i++) {        Assert.assertTrue(queue.addHead(new FlumeEventPointer(i, i)));        fileIDs.add(i);        Assert.assertEquals(fileIDs, queue.getFileIDs());    }    for (int i = size; i > 0; i--) {        Assert.assertEquals(new FlumeEventPointer(i, i), queue.removeHead(0));        fileIDs.remove(i);        Assert.assertEquals(fileIDs, queue.getFileIDs());    }    Assert.assertEquals(Sets.newHashSet(), queue.getFileIDs());}
public void flume_f1372_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addTail(pointer1));    Assert.assertEquals(Sets.newHashSet(1), queue.getFileIDs());    Assert.assertTrue(queue.remove(pointer1));    queue.replayComplete();    Assert.assertEquals(Sets.newHashSet(), queue.getFileIDs());    Assert.assertNull(queue.removeHead(0));    Assert.assertEquals(Sets.newHashSet(), queue.getFileIDs());}
public void flume_f1373_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addTail(pointer1));    Assert.assertTrue(queue.addTail(pointer2));    Assert.assertTrue(queue.remove(pointer1));    queue.replayComplete();    Assert.assertEquals(pointer2, queue.removeHead(0));}
public void flume_f1374_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    queue.addHead(pointer1);    Assert.assertTrue(queue.remove(pointer1));    Assert.assertNull(queue.removeHead(0));}
public void flume_f1375_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addHead(pointer1));    Assert.assertTrue(queue.addHead(pointer2));    Assert.assertTrue(queue.remove(pointer1));    queue.replayComplete();    Assert.assertEquals(pointer2, queue.removeHead(0));}
public void flume_f1376_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addHead(pointer1));    Assert.assertTrue(queue.addHead(pointer2));        Assert.assertFalse(queue.remove(pointer3));    Assert.assertTrue(queue.remove(pointer1));    Assert.assertTrue(queue.remove(pointer2));    queue.replayComplete();    Assert.assertEquals(2, queue.getSearchCount());}
public void flume_f1377_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    queue.replayComplete();    queue.remove(pointer1);}
public void flume_f1378_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    int size = Integer.MAX_VALUE;    for (int i = 1; i <= size; i++) {        if (!queue.addHead(new FlumeEventPointer(i, i))) {            break;        }    }    for (int i = queue.getSize() / 2; i > 0; i--) {        Assert.assertNotNull(queue.removeHead(0));    }        for (int i = 1; i <= size; i++) {        if (!queue.addHead(new FlumeEventPointer(i, i))) {            break;        }    }}
public void flume_f1379_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    long txnID1 = new Random().nextInt(Integer.MAX_VALUE - 1);    long txnID2 = txnID1 + 1;    queue.addWithoutCommit(new FlumeEventPointer(1, 1), txnID1);    queue.addWithoutCommit(new FlumeEventPointer(2, 1), txnID1);    queue.addWithoutCommit(new FlumeEventPointer(2, 2), txnID2);    queue.checkpoint(true);    TimeUnit.SECONDS.sleep(3L);    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    SetMultimap<Long, Long> deserializedMap = queue.deserializeInflightPuts();    Assert.assertTrue(deserializedMap.get(txnID1).contains(new FlumeEventPointer(1, 1).toLong()));    Assert.assertTrue(deserializedMap.get(txnID1).contains(new FlumeEventPointer(2, 1).toLong()));    Assert.assertTrue(deserializedMap.get(txnID2).contains(new FlumeEventPointer(2, 2).toLong()));}
public void flume_f1380_0() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    long txnID1 = new Random().nextInt(Integer.MAX_VALUE - 1);    long txnID2 = txnID1 + 1;    queue.addTail(new FlumeEventPointer(1, 1));    queue.addTail(new FlumeEventPointer(2, 1));    queue.addTail(new FlumeEventPointer(2, 2));    queue.removeHead(txnID1);    queue.removeHead(txnID2);    queue.removeHead(txnID2);    queue.checkpoint(true);    TimeUnit.SECONDS.sleep(3L);    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    SetMultimap<Long, Long> deserializedMap = queue.deserializeInflightTakes();    Assert.assertTrue(deserializedMap.get(txnID1).contains(new FlumeEventPointer(1, 1).toLong()));    Assert.assertTrue(deserializedMap.get(txnID2).contains(new FlumeEventPointer(2, 1).toLong()));    Assert.assertTrue(deserializedMap.get(txnID2).contains(new FlumeEventPointer(2, 2).toLong()));}
public void flume_f1381_0() throws Exception
{    RandomAccessFile inflight = null;    try {        queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());        long txnID1 = new Random().nextInt(Integer.MAX_VALUE - 1);        long txnID2 = txnID1 + 1;        queue.addWithoutCommit(new FlumeEventPointer(1, 1), txnID1);        queue.addWithoutCommit(new FlumeEventPointer(2, 1), txnID1);        queue.addWithoutCommit(new FlumeEventPointer(2, 2), txnID2);        queue.checkpoint(true);        TimeUnit.SECONDS.sleep(3L);        inflight = new RandomAccessFile(backingStoreSupplier.getInflightPuts(), "rw");        inflight.seek(0);        inflight.writeInt(new Random().nextInt());        queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());        SetMultimap<Long, Long> deserializedMap = queue.deserializeInflightPuts();        Assert.assertTrue(deserializedMap.get(txnID1).contains(new FlumeEventPointer(1, 1).toLong()));        Assert.assertTrue(deserializedMap.get(txnID1).contains(new FlumeEventPointer(2, 1).toLong()));        Assert.assertTrue(deserializedMap.get(txnID2).contains(new FlumeEventPointer(2, 2).toLong()));    } finally {        inflight.close();    }}
public void flume_f1382_0() throws Exception
{    RandomAccessFile inflight = null;    try {        queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());        long txnID1 = new Random().nextInt(Integer.MAX_VALUE - 1);        long txnID2 = txnID1 + 1;        queue.addWithoutCommit(new FlumeEventPointer(1, 1), txnID1);        queue.addWithoutCommit(new FlumeEventPointer(2, 1), txnID1);        queue.addWithoutCommit(new FlumeEventPointer(2, 2), txnID2);        queue.checkpoint(true);        TimeUnit.SECONDS.sleep(3L);        inflight = new RandomAccessFile(backingStoreSupplier.getInflightTakes(), "rw");        inflight.seek(0);        inflight.writeInt(new Random().nextInt());        queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());        SetMultimap<Long, Long> deserializedMap = queue.deserializeInflightTakes();        Assert.assertTrue(deserializedMap.get(txnID1).contains(new FlumeEventPointer(1, 1).toLong()));        Assert.assertTrue(deserializedMap.get(txnID1).contains(new FlumeEventPointer(2, 1).toLong()));        Assert.assertTrue(deserializedMap.get(txnID2).contains(new FlumeEventPointer(2, 2).toLong()));    } finally {        inflight.close();    }}
public void flume_f1383_0()
{    baseDir = Files.createTempDir();    checkpointDir = new File(baseDir, "chkpt");    Assert.assertTrue(checkpointDir.mkdirs() || checkpointDir.isDirectory());    dataDirs = new File[3];    dataDir = "";    for (int i = 0; i < dataDirs.length; i++) {        dataDirs[i] = new File(baseDir, "data" + (i + 1));        Assert.assertTrue(dataDirs[i].mkdirs() || dataDirs[i].isDirectory());        dataDir += dataDirs[i].getAbsolutePath() + ",";    }    dataDir = dataDir.substring(0, dataDir.length() - 1);}
public void flume_f1384_0()
{    if (channel != null && channel.isOpen()) {        channel.stop();    }    FileUtils.deleteQuietly(baseDir);}
public void flume_f1385_1() throws IOException, InterruptedException
{            Context context = new Context();    context.put(FileChannelConfiguration.CHECKPOINT_DIR, checkpointDir.getAbsolutePath());    context.put(FileChannelConfiguration.DATA_DIRS, dataDir);    context.put(FileChannelConfiguration.CAPACITY, String.valueOf(10000));        context.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "5000");    context.put(FileChannelConfiguration.MAX_FILE_SIZE, String.valueOf(1024 * 1024 * 5));        channel = new FileChannel();    channel.setName("FileChannel-" + UUID.randomUUID());    Configurables.configure(channel, context);    channel.start();    Assert.assertTrue(channel.isOpen());    SequenceGeneratorSource source = new SequenceGeneratorSource();    CountingSourceRunner sourceRunner = new CountingSourceRunner(source, channel);    source.configure(context);    source.start();    NullSink sink = new NullSink();    sink.setChannel(channel);    CountingSinkRunner sinkRunner = new CountingSinkRunner(sink);    sinkRunner.start();    sourceRunner.start();    TimeUnit.SECONDS.sleep(30);        sourceRunner.shutdown();    while (sourceRunner.isAlive()) {        Thread.sleep(10L);    }        while (channel.getDepth() > 0) {        Thread.sleep(10L);    }        sinkRunner.shutdown();        TimeUnit.SECONDS.sleep(5);    List<File> logs = Lists.newArrayList();    for (int i = 0; i < dataDirs.length; i++) {        logs.addAll(LogUtils.getLogs(dataDirs[i]));    }        for (File logFile : logs) {            }            for (Exception ex : sourceRunner.getErrors()) {            }    for (Exception ex : sinkRunner.getErrors()) {            }    Assert.assertEquals(sinkRunner.getCount(), sinkRunner.getCount());    Assert.assertEquals(Collections.EMPTY_LIST, sinkRunner.getErrors());    Assert.assertEquals(Collections.EMPTY_LIST, sourceRunner.getErrors());}
public void flume_f1386_0() throws IOException
{    transactionID = 0;    checkpointDir = Files.createTempDir();    FileUtils.forceDeleteOnExit(checkpointDir);    Assert.assertTrue(checkpointDir.isDirectory());    dataDirs = new File[3];    for (int i = 0; i < dataDirs.length; i++) {        dataDirs[i] = Files.createTempDir();        Assert.assertTrue(dataDirs[i].isDirectory());    }    log = new Log.Builder().setCheckpointInterval(1L).setMaxFileSize(MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setCheckpointOnClose(false).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();}
public void flume_f1387_0() throws Exception
{    if (log != null) {        log.close();    }    FileUtils.deleteQuietly(checkpointDir);    for (int i = 0; i < dataDirs.length; i++) {        FileUtils.deleteQuietly(dataDirs[i]);    }}
public void flume_f1388_0() throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long transactionID = ++this.transactionID;    FlumeEventPointer eventPointer = log.put(transactionID, eventIn);        log.commitPut(transactionID);        FlumeEvent eventOut = log.get(eventPointer);    Assert.assertNotNull(eventOut);    Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());    Assert.assertArrayEquals(eventIn.getBody(), eventOut.getBody());}
public void flume_f1389_0() throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    log.shutdownWorker();    Thread.sleep(1000);    for (int i = 0; i < 1000; i++) {        FlumeEvent eventIn = TestUtils.newPersistableEvent();        long transactionID = ++this.transactionID;        FlumeEventPointer eventPointer = log.put(transactionID, eventIn);                FlumeEvent eventOut = log.get(eventPointer);        Assert.assertNotNull(eventOut);        Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());        Assert.assertArrayEquals(eventIn.getBody(), eventOut.getBody());    }    int logCount = 0;    for (File dataDir : dataDirs) {        for (File logFile : dataDir.listFiles()) {            if (logFile.getName().startsWith("log-")) {                logCount++;            }        }    }        Assert.assertEquals(186, logCount);}
public void flume_f1390_0() throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long transactionID = ++this.transactionID;    FlumeEventPointer eventPointerIn = log.put(transactionID, eventIn);    log.commitPut(transactionID);    log.close();    log = new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    takeAndVerify(eventPointerIn, eventIn);}
public void flume_f1391_0() throws IOException, InterruptedException
{    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long transactionID = ++this.transactionID;    log.put(transactionID, eventIn);        log.rollback(transactionID);    log.close();    log = new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    FlumeEventQueue queue = log.getFlumeEventQueue();    Assert.assertNull(queue.removeHead(transactionID));}
public void flume_f1392_0() throws IOException, InterruptedException
{    log.close();    log = new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setMinimumRequiredSpace(Long.MAX_VALUE).setChannelCounter(new FileChannelCounter("testlog")).build();    try {        log.replay();        Assert.fail();    } catch (IOException e) {        Assert.assertTrue(e.getMessage(), e.getMessage().startsWith("Usable space exhausted"));    }}
public void flume_f1393_1() throws IOException, InterruptedException
{    try {        doTestMinimumRequiredSpaceTooSmallForPut();    } catch (IOException e) {                doTestMinimumRequiredSpaceTooSmallForPut();    } catch (AssertionError e) {                doTestMinimumRequiredSpaceTooSmallForPut();    }}
public void flume_f1394_0() throws IOException, InterruptedException
{    long minimumRequiredSpace = checkpointDir.getUsableSpace() - (10L * 1024L * 1024L);    log.close();    log = new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setMinimumRequiredSpace(minimumRequiredSpace).setUsableSpaceRefreshInterval(1L).setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    File filler = new File(checkpointDir, "filler");    byte[] buffer = new byte[64 * 1024];    FileOutputStream out = new FileOutputStream(filler);    while (checkpointDir.getUsableSpace() > minimumRequiredSpace) {        out.write(buffer);    }    out.close();    try {        FlumeEvent eventIn = TestUtils.newPersistableEvent();        long transactionID = ++this.transactionID;        log.put(transactionID, eventIn);        Assert.fail();    } catch (IOException e) {        Assert.assertTrue(e.getMessage(), e.getMessage().startsWith("Usable space exhausted"));    }}
public void flume_f1395_0() throws IOException, InterruptedException
{    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long putTransactionID = ++transactionID;    FlumeEventPointer eventPointer = log.put(putTransactionID, eventIn);    log.commitPut(putTransactionID);    long takeTransactionID = ++transactionID;    log.take(takeTransactionID, eventPointer);    log.commitTake(takeTransactionID);    log.close();    new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(1).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    FlumeEventQueue queue = log.getFlumeEventQueue();    Assert.assertNull(queue.removeHead(0));}
public void flume_f1396_0() throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    doPutTakeRollback(true);}
public void flume_f1397_0() throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    doPutTakeRollback(false);}
public void flume_f1398_0(boolean useLogReplayV1) throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long putTransactionID = ++transactionID;    FlumeEventPointer eventPointerIn = log.put(putTransactionID, eventIn);    log.commitPut(putTransactionID);    long takeTransactionID = ++transactionID;    log.take(takeTransactionID, eventPointerIn);    log.rollback(takeTransactionID);    log.close();    new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(1).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setUseLogReplayV1(useLogReplayV1).setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    takeAndVerify(eventPointerIn, eventIn);}
public void flume_f1399_0() throws IOException, InterruptedException
{    long putTransactionID = ++transactionID;    log.commitPut(putTransactionID);    log.close();    new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(1).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    FlumeEventQueue queue = log.getFlumeEventQueue();    FlumeEventPointer eventPointerOut = queue.removeHead(0);    Assert.assertNull(eventPointerOut);}
public void flume_f1400_0() throws IOException, InterruptedException
{    long putTransactionID = ++transactionID;    log.commitTake(putTransactionID);    log.close();    new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(1).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    FlumeEventQueue queue = log.getFlumeEventQueue();    FlumeEventPointer eventPointerOut = queue.removeHead(0);    Assert.assertNull(eventPointerOut);}
public void flume_f1401_0() throws IOException, InterruptedException
{    long putTransactionID = ++transactionID;    log.rollback(putTransactionID);    log.close();    new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(1).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    FlumeEventQueue queue = log.getFlumeEventQueue();    FlumeEventPointer eventPointerOut = queue.removeHead(0);    Assert.assertNull(eventPointerOut);}
public void flume_f1402_0() throws IOException
{    File logDir = dataDirs[0];    List<File> expected = Lists.newArrayList();    for (int i = 0; i < 10; i++) {        File log = new File(logDir, Log.PREFIX + i);        expected.add(log);        Assert.assertTrue(log.isFile() || log.createNewFile());        File metaDataFile = Serialization.getMetaDataFile(log);        File metaDataTempFile = Serialization.getMetaDataTempFile(metaDataFile);        File logGzip = new File(logDir, Log.PREFIX + i + ".gz");        Assert.assertTrue(metaDataFile.isFile() || metaDataFile.createNewFile());        Assert.assertTrue(metaDataTempFile.isFile() || metaDataTempFile.createNewFile());        Assert.assertTrue(log.isFile() || logGzip.createNewFile());    }    List<File> actual = LogUtils.getLogs(logDir);    LogUtils.sort(actual);    LogUtils.sort(expected);    Assert.assertEquals(expected, actual);}
public void flume_f1403_0() throws IOException, InterruptedException
{    doTestReplayFailsWithAllEmptyLogMetaData(false);}
public void flume_f1404_0() throws IOException, InterruptedException
{    doTestReplayFailsWithAllEmptyLogMetaData(true);}
public void flume_f1405_0(boolean useFastReplay) throws IOException, InterruptedException
{        log.close();    log = new Log.Builder().setCheckpointInterval(1L).setMaxFileSize(MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setUseFastReplay(useFastReplay).setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long transactionID = ++this.transactionID;    log.put(transactionID, eventIn);    log.commitPut(transactionID);    log.close();    if (useFastReplay) {        FileUtils.deleteQuietly(checkpointDir);        Assert.assertTrue(checkpointDir.mkdir());    }    List<File> logFiles = Lists.newArrayList();    for (int i = 0; i < dataDirs.length; i++) {        logFiles.addAll(LogUtils.getLogs(dataDirs[i]));    }    Assert.assertTrue(logFiles.size() > 0);    for (File logFile : logFiles) {        File logFileMeta = Serialization.getMetaDataFile(logFile);        Assert.assertTrue(logFileMeta.delete());        Assert.assertTrue(logFileMeta.createNewFile());    }    log = new Log.Builder().setCheckpointInterval(1L).setMaxFileSize(MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setUseFastReplay(useFastReplay).setChannelCounter(new FileChannelCounter("testlog")).build();    try {        log.replay();        Assert.fail();    } catch (IllegalStateException expected) {        String msg = expected.getMessage();        Assert.assertNotNull(msg);        Assert.assertTrue(msg, msg.contains(".meta is empty, but log"));    }}
public void flume_f1406_0() throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long transactionID = ++this.transactionID;    FlumeEventPointer eventPointer = log.put(transactionID, eventIn);        log.commitPut(transactionID);    log.close();    log = new Log.Builder().setCheckpointInterval(1L).setMaxFileSize(MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    doTestReplaySucceedsWithUnusedEmptyLogMetaData(eventIn, eventPointer);}
public void flume_f1407_0() throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long transactionID = ++this.transactionID;    FlumeEventPointer eventPointer = log.put(transactionID, eventIn);        log.commitPut(transactionID);    log.close();    checkpointDir = Files.createTempDir();    FileUtils.forceDeleteOnExit(checkpointDir);    Assert.assertTrue(checkpointDir.isDirectory());    log = new Log.Builder().setCheckpointInterval(1L).setMaxFileSize(MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setUseFastReplay(true).setChannelCounter(new FileChannelCounter("testlog")).build();    doTestReplaySucceedsWithUnusedEmptyLogMetaData(eventIn, eventPointer);}
public void flume_f1408_0(FlumeEvent eventIn, FlumeEventPointer eventPointer) throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    for (int i = 0; i < dataDirs.length; i++) {        for (File logFile : LogUtils.getLogs(dataDirs[i])) {            if (logFile.length() == 0L) {                File logFileMeta = Serialization.getMetaDataFile(logFile);                Assert.assertTrue(logFileMeta.delete());                Assert.assertTrue(logFileMeta.createNewFile());            }        }    }    log.replay();    FlumeEvent eventOut = log.get(eventPointer);    Assert.assertNotNull(eventOut);    Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());    Assert.assertArrayEquals(eventIn.getBody(), eventOut.getBody());}
public void flume_f1409_0() throws Exception
{    File fs = mock(File.class);    when(fs.getUsableSpace()).thenReturn(Long.MAX_VALUE);    LogFile.CachedFSUsableSpace cachedFS = new LogFile.CachedFSUsableSpace(fs, 1000L);    Assert.assertEquals(cachedFS.getUsableSpace(), Long.MAX_VALUE);    cachedFS.decrement(Integer.MAX_VALUE);    Assert.assertEquals(cachedFS.getUsableSpace(), Long.MAX_VALUE - Integer.MAX_VALUE);    try {        cachedFS.decrement(-1);        Assert.fail();    } catch (IllegalArgumentException expected) {    }    when(fs.getUsableSpace()).thenReturn(Long.MAX_VALUE - 1L);    Thread.sleep(1100);    Assert.assertEquals(cachedFS.getUsableSpace(), Long.MAX_VALUE - 1L);}
public void flume_f1410_0() throws Exception
{    log.close();    log = new Log.Builder().setCheckpointInterval(1L).setMaxFileSize(MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setCheckpointOnClose(true).setChannelName("testLog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();        FlumeEvent eventIn = TestUtils.newPersistableEvent();    log.put(transactionID, eventIn);    log.commitPut(transactionID);        File checkPointMetaFile = FileUtils.listFiles(checkpointDir, new String[] { "meta" }, false).iterator().next();    long before = FileUtils.checksumCRC32(checkPointMetaFile);        log.close();        long after = FileUtils.checksumCRC32(checkPointMetaFile);    Assert.assertFalse(before == after);}
private void flume_f1411_0(FlumeEventPointer eventPointerIn, FlumeEvent eventIn) throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    FlumeEventQueue queue = log.getFlumeEventQueue();    FlumeEventPointer eventPointerOut = queue.removeHead(0);    Assert.assertNotNull(eventPointerOut);    Assert.assertNull(queue.removeHead(0));    Assert.assertEquals(eventPointerIn, eventPointerOut);    Assert.assertEquals(eventPointerIn.hashCode(), eventPointerOut.hashCode());    FlumeEvent eventOut = log.get(eventPointerOut);    Assert.assertNotNull(eventOut);    Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());    Assert.assertArrayEquals(eventIn.getBody(), eventOut.getBody());}
public void flume_f1412_0() throws IOException
{    fileID = 1;    transactionID = 1L;    dataDir = Files.createTempDir();    dataFile = new File(dataDir, String.valueOf(fileID));    Assert.assertTrue(dataDir.isDirectory());    logFileWriter = LogFileFactory.getWriter(dataFile, fileID, Integer.MAX_VALUE, null, null, null, Long.MAX_VALUE, true, 0);}
public void flume_f1413_0() throws IOException
{    try {        if (logFileWriter != null) {            logFileWriter.close();        }    } finally {        FileUtils.deleteQuietly(dataDir);    }}
public void flume_f1414_0() throws IOException
{    Assert.assertTrue(dataFile.isFile() || dataFile.createNewFile());    try {        LogFileFactory.getWriter(dataFile, fileID, Integer.MAX_VALUE, null, null, null, Long.MAX_VALUE, true, 0);        Assert.fail();    } catch (IllegalStateException e) {        Assert.assertEquals("File already exists " + dataFile.getAbsolutePath(), e.getMessage());    }}
public void flume_f1415_0() throws IOException
{    FileUtils.deleteQuietly(dataFile);    Assert.assertFalse(dataFile.exists());    Assert.assertTrue(dataFile.mkdirs());    try {        LogFileFactory.getWriter(dataFile, fileID, Integer.MAX_VALUE, null, null, null, Long.MAX_VALUE, true, 0);        Assert.fail();    } catch (IllegalStateException e) {        Assert.assertEquals("File already exists " + dataFile.getAbsolutePath(), e.getMessage());    }}
public void flume_f1416_0() throws InterruptedException, IOException
{    final List<Throwable> errors = Collections.synchronizedList(new ArrayList<Throwable>());    ExecutorService executorService = Executors.newFixedThreadPool(10);    CompletionService<Void> completionService = new ExecutorCompletionService<Void>(executorService);    final LogFile.RandomReader logFileReader = LogFileFactory.getRandomReader(dataFile, null, true);    for (int i = 0; i < 1000; i++) {                synchronized (errors) {            for (Throwable throwable : errors) {                Throwables.propagateIfInstanceOf(throwable, AssertionError.class);            }                        for (Throwable throwable : errors) {                Throwables.propagate(throwable);            }        }        final FlumeEvent eventIn = TestUtils.newPersistableEvent();        final Put put = new Put(++transactionID, WriteOrderOracle.next(), eventIn);        ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);        FlumeEventPointer ptr = logFileWriter.put(bytes);        final int offset = ptr.getOffset();        completionService.submit(new Runnable() {            @Override            public void run() {                try {                    FlumeEvent eventOut = logFileReader.get(offset);                    Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());                    Assert.assertTrue(Arrays.equals(eventIn.getBody(), eventOut.getBody()));                } catch (Throwable throwable) {                    synchronized (errors) {                        errors.add(throwable);                    }                }            }        }, null);    }    for (int i = 0; i < 1000; i++) {        completionService.take();    }        for (Throwable throwable : errors) {        Throwables.propagateIfInstanceOf(throwable, AssertionError.class);    }        for (Throwable throwable : errors) {        Throwables.propagate(throwable);    }}
public void flume_f1417_0()
{    try {        FlumeEvent eventOut = logFileReader.get(offset);        Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());        Assert.assertTrue(Arrays.equals(eventIn.getBody(), eventOut.getBody()));    } catch (Throwable throwable) {        synchronized (errors) {            errors.add(throwable);        }    }}
public void flume_f1418_0() throws InterruptedException, IOException, CorruptEventException
{    Map<Integer, Put> puts = Maps.newHashMap();    for (int i = 0; i < 1000; i++) {        FlumeEvent eventIn = TestUtils.newPersistableEvent();        Put put = new Put(++transactionID, WriteOrderOracle.next(), eventIn);        ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);        FlumeEventPointer ptr = logFileWriter.put(bytes);        puts.put(ptr.getOffset(), put);    }    LogFile.SequentialReader reader = LogFileFactory.getSequentialReader(dataFile, null, true);    LogRecord entry;    while ((entry = reader.next()) != null) {        Integer offset = entry.getOffset();        TransactionEventRecord record = entry.getEvent();        Put put = puts.get(offset);        FlumeEvent eventIn = put.getEvent();        Assert.assertEquals(put.getTransactionID(), record.getTransactionID());        Assert.assertTrue(record instanceof Put);        FlumeEvent eventOut = ((Put) record).getEvent();        Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());        Assert.assertTrue(Arrays.equals(eventIn.getBody(), eventOut.getBody()));    }}
public void flume_f1419_0() throws InterruptedException, IOException, CorruptEventException
{    Map<Integer, Put> puts = Maps.newHashMap();    for (int i = 0; i < 1000; i++) {        FlumeEvent eventIn = TestUtils.newPersistableEvent();        Put put = new Put(++transactionID, WriteOrderOracle.next(), eventIn);        ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);        FlumeEventPointer ptr = logFileWriter.put(bytes);        puts.put(ptr.getOffset(), put);    }        File metadataFile = Serialization.getMetaDataFile(dataFile);    File oldMetadataFile = Serialization.getOldMetaDataFile(dataFile);    if (!metadataFile.renameTo(oldMetadataFile)) {        Assert.fail("Renaming to meta.old failed");    }    LogFile.SequentialReader reader = LogFileFactory.getSequentialReader(dataFile, null, true);    Assert.assertTrue(metadataFile.exists());    Assert.assertFalse(oldMetadataFile.exists());    LogRecord entry;    while ((entry = reader.next()) != null) {        Integer offset = entry.getOffset();        TransactionEventRecord record = entry.getEvent();        Put put = puts.get(offset);        FlumeEvent eventIn = put.getEvent();        Assert.assertEquals(put.getTransactionID(), record.getTransactionID());        Assert.assertTrue(record instanceof Put);        FlumeEvent eventOut = ((Put) record).getEvent();        Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());        Assert.assertTrue(Arrays.equals(eventIn.getBody(), eventOut.getBody()));    }}
public void flume_f1420_0() throws InterruptedException, IOException, CorruptEventException
{    Map<Integer, Put> puts = Maps.newHashMap();    for (int i = 0; i < 1000; i++) {        FlumeEvent eventIn = TestUtils.newPersistableEvent();        Put put = new Put(++transactionID, WriteOrderOracle.next(), eventIn);        ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);        FlumeEventPointer ptr = logFileWriter.put(bytes);        puts.put(ptr.getOffset(), put);    }        File metadataFile = Serialization.getMetaDataFile(dataFile);    File tempMetadataFile = Serialization.getMetaDataTempFile(dataFile);    File oldMetadataFile = Serialization.getOldMetaDataFile(dataFile);        oldMetadataFile.createNewFile();    if (!metadataFile.renameTo(tempMetadataFile)) {        Assert.fail("Renaming to meta.temp failed");    }    LogFile.SequentialReader reader = LogFileFactory.getSequentialReader(dataFile, null, true);    Assert.assertTrue(metadataFile.exists());    Assert.assertFalse(tempMetadataFile.exists());    Assert.assertFalse(oldMetadataFile.exists());    LogRecord entry;    while ((entry = reader.next()) != null) {        Integer offset = entry.getOffset();        TransactionEventRecord record = entry.getEvent();        Put put = puts.get(offset);        FlumeEvent eventIn = put.getEvent();        Assert.assertEquals(put.getTransactionID(), record.getTransactionID());        Assert.assertTrue(record instanceof Put);        FlumeEvent eventOut = ((Put) record).getEvent();        Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());        Assert.assertTrue(Arrays.equals(eventIn.getBody(), eventOut.getBody()));    }}
public void flume_f1421_0() throws IOException
{    if (dataFile.isFile()) {        Assert.assertTrue(dataFile.delete());    }    Assert.assertTrue(dataFile.createNewFile());    ProtosFactory.LogFileMetaData.Builder metaDataBuilder = ProtosFactory.LogFileMetaData.newBuilder();    metaDataBuilder.setVersion(1);    metaDataBuilder.setLogFileID(2);    metaDataBuilder.setCheckpointPosition(3);    metaDataBuilder.setCheckpointWriteOrderID(4);    LogFileV3.writeDelimitedTo(metaDataBuilder.build(), dataFile);    ProtosFactory.LogFileMetaData metaData = ProtosFactory.LogFileMetaData.parseDelimitedFrom(new FileInputStream(dataFile));    Assert.assertEquals(1, metaData.getVersion());    Assert.assertEquals(2, metaData.getLogFileID());    Assert.assertEquals(3, metaData.getCheckpointPosition());    Assert.assertEquals(4, metaData.getCheckpointWriteOrderID());}
public void flume_f1422_0() throws Exception
{    final LogFile.RandomReader logFileReader = LogFileFactory.getRandomReader(dataFile, null, true);    final FlumeEvent eventIn = TestUtils.newPersistableEvent(2500);    final Put put = new Put(++transactionID, WriteOrderOracle.next(), eventIn);    ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);    FlumeEventPointer ptr = logFileWriter.put(bytes);    logFileWriter.commit(TransactionEventRecord.toByteBuffer(new Commit(transactionID, WriteOrderOracle.next())));    logFileWriter.sync();    final int offset = ptr.getOffset();    RandomAccessFile writer = new RandomAccessFile(dataFile, "rw");    writer.seek(offset + 1500);    writer.write((byte) 45);    writer.write((byte) 12);    writer.getFD().sync();    logFileReader.get(offset);        Assert.fail();}
public void flume_f1423_0() throws Exception
{    final LogFile.RandomReader logFileReader = LogFileFactory.getRandomReader(dataFile, null, true);    final FlumeEvent eventIn = TestUtils.newPersistableEvent(2500);    final Put put = new Put(++transactionID, WriteOrderOracle.next(), eventIn);    ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);    FlumeEventPointer ptr = logFileWriter.put(bytes);    logFileWriter.commit(TransactionEventRecord.toByteBuffer(new Commit(transactionID, WriteOrderOracle.next())));    logFileWriter.sync();    final int offset = ptr.getOffset();    LogFile.OperationRecordUpdater updater = new LogFile.OperationRecordUpdater(dataFile);    updater.markRecordAsNoop(offset);    logFileReader.get(offset);        Assert.fail();}
public void flume_f1424_0() throws Exception
{    File tempDir = Files.createTempDir();    File temp = new File(tempDir, "temp");    final RandomAccessFile tempFile = new RandomAccessFile(temp, "rw");    for (int i = 0; i < 5000; i++) {        tempFile.write(LogFile.OP_RECORD);    }    tempFile.seek(0);    LogFile.OperationRecordUpdater recordUpdater = new LogFile.OperationRecordUpdater(temp);        for (int i = 0; i < 5000; i += 10) {        recordUpdater.markRecordAsNoop(i);    }    recordUpdater.close();    tempFile.seek(0);        for (int i = 0; i < 5000; i += 10) {        tempFile.seek(i);        Assert.assertEquals(LogFile.OP_NOOP, tempFile.readByte());    }}
public void flume_f1425_0() throws Exception
{    final FlumeEvent eventIn = TestUtils.newPersistableEvent(2500);    final Put put = new Put(++transactionID, WriteOrderOracle.next(), eventIn);    ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);    FlumeEventPointer ptr = logFileWriter.put(bytes);    logFileWriter.commit(TransactionEventRecord.toByteBuffer(new Commit(transactionID, WriteOrderOracle.next())));    logFileWriter.sync();    final int offset = ptr.getOffset();    LogFile.OperationRecordUpdater updater = new LogFile.OperationRecordUpdater(dataFile);    updater.markRecordAsNoop(offset);    RandomAccessFile fileReader = new RandomAccessFile(dataFile, "rw");    Assert.assertEquals(LogFile.OP_NOOP, fileReader.readByte());}
public void flume_f1426_0() throws Exception
{    final FlumeEvent eventIn = TestUtils.newPersistableEvent(250);    final CyclicBarrier barrier = new CyclicBarrier(20);    ExecutorService executorService = Executors.newFixedThreadPool(20);    ExecutorCompletionService<Void> completionService = new ExecutorCompletionService<Void>(executorService);    final LogFile.Writer writer = logFileWriter;    final AtomicLong txnId = new AtomicLong(++transactionID);    for (int i = 0; i < 20; i++) {        completionService.submit(new Callable<Void>() {            @Override            public Void call() {                try {                    Put put = new Put(txnId.incrementAndGet(), WriteOrderOracle.next(), eventIn);                    ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);                    writer.put(bytes);                    writer.commit(TransactionEventRecord.toByteBuffer(new Commit(txnId.get(), WriteOrderOracle.next())));                    barrier.await();                    writer.sync();                } catch (Exception ex) {                    Throwables.propagate(ex);                }                return null;            }        });    }    for (int i = 0; i < 20; i++) {        completionService.take().get();    }        Assert.assertTrue(logFileWriter.position() >= 5000);    Assert.assertEquals(1, writer.getSyncCount());    Assert.assertTrue(logFileWriter.getLastCommitPosition() == logFileWriter.getLastSyncPosition());    executorService.shutdown();}
public Void flume_f1427_0()
{    try {        Put put = new Put(txnId.incrementAndGet(), WriteOrderOracle.next(), eventIn);        ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);        writer.put(bytes);        writer.commit(TransactionEventRecord.toByteBuffer(new Commit(txnId.get(), WriteOrderOracle.next())));        barrier.await();        writer.sync();    } catch (Exception ex) {        Throwables.propagate(ex);    }    return null;}
public void flume_f1428_0()
{    long now = System.currentTimeMillis();    Commit commit = new Commit(now, now + 1);    LogRecord logRecord = new LogRecord(1, 2, commit);    Assert.assertTrue(now == commit.getTransactionID());    Assert.assertTrue(now + 1 == commit.getLogWriteOrderID());    Assert.assertTrue(1 == logRecord.getFileID());    Assert.assertTrue(2 == logRecord.getOffset());    Assert.assertTrue(commit == logRecord.getEvent());}
public void flume_f1429_0()
{        long now = System.currentTimeMillis();    List<LogRecord> records = Lists.newArrayList();    for (int i = 0; i < 3; i++) {        Commit commit = new Commit((long) i, now - i);        LogRecord logRecord = new LogRecord(1, i, commit);        records.add(logRecord);    }    LogRecord logRecord;    logRecord = Collections.min(records);    Assert.assertTrue(String.valueOf(logRecord.getOffset()), 2 == logRecord.getOffset());    records.remove(logRecord);    logRecord = Collections.min(records);    Assert.assertTrue(String.valueOf(logRecord.getOffset()), 1 == logRecord.getOffset());    records.remove(logRecord);    logRecord = Collections.min(records);    Assert.assertTrue(String.valueOf(logRecord.getOffset()), 0 == logRecord.getOffset());    records.remove(logRecord);}
public void flume_f1430_0() throws IOException
{    Put put = new Put(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.PUT.get(), put.getRecordType());    Take take = new Take(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.TAKE.get(), take.getRecordType());    Rollback rollback = new Rollback(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.ROLLBACK.get(), rollback.getRecordType());    Commit commit = new Commit(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.COMMIT.get(), commit.getRecordType());}
public void flume_f1431_0() throws IOException
{    Put in = new Put(System.currentTimeMillis(), WriteOrderOracle.next(), new FlumeEvent(new HashMap<String, String>(), new byte[0]));    Put out = (Put) TransactionEventRecord.fromDataInputV2(toDataInput(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());    Assert.assertEquals(in.getEvent().getHeaders(), out.getEvent().getHeaders());    Assert.assertTrue(Arrays.equals(in.getEvent().getBody(), out.getEvent().getBody()));}
public void flume_f1432_0() throws IOException
{    Take in = new Take(System.currentTimeMillis(), WriteOrderOracle.next(), 10, 20);    Take out = (Take) TransactionEventRecord.fromDataInputV2(toDataInput(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());    Assert.assertEquals(in.getFileID(), out.getFileID());    Assert.assertEquals(in.getOffset(), out.getOffset());}
public void flume_f1433_0() throws IOException
{    Rollback in = new Rollback(System.currentTimeMillis(), WriteOrderOracle.next());    Rollback out = (Rollback) TransactionEventRecord.fromDataInputV2(toDataInput(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());}
public void flume_f1434_0() throws IOException
{    Commit in = new Commit(System.currentTimeMillis(), WriteOrderOracle.next());    Commit out = (Commit) TransactionEventRecord.fromDataInputV2(toDataInput(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());}
public void flume_f1435_0() throws IOException
{    Put in = new Put(System.currentTimeMillis(), WriteOrderOracle.next(), new FlumeEvent(new HashMap<String, String>(), new byte[0]));    try {        TransactionEventRecord.fromDataInputV2(toDataInput(0, in));        Assert.fail();    } catch (IOException e) {        Assert.assertEquals("Header 0 is not the required value: deadbeef", e.getMessage());    }}
public void flume_f1436_0() throws IOException
{    TransactionEventRecord in = mock(TransactionEventRecord.class);    when(in.getRecordType()).thenReturn(Short.MIN_VALUE);    try {        TransactionEventRecord.fromDataInputV2(toDataInput(in));        Assert.fail();    } catch (NullPointerException e) {        Assert.assertEquals("Unknown action ffff8000", e.getMessage());    }}
private DataInput flume_f1437_0(TransactionEventRecord record) throws IOException
{    ByteBuffer buffer = TransactionEventRecord.toByteBufferV2(record);    ByteArrayInputStream byteInput = new ByteArrayInputStream(buffer.array());    DataInputStream dataInput = new DataInputStream(byteInput);    return dataInput;}
private DataInput flume_f1438_0(int header, TransactionEventRecord record) throws IOException
{    ByteArrayOutputStream byteOutput = new ByteArrayOutputStream();    DataOutputStream dataOutput = new DataOutputStream(byteOutput);    dataOutput.writeInt(header);    dataOutput.writeShort(record.getRecordType());    dataOutput.writeLong(record.getTransactionID());    dataOutput.writeLong(record.getLogWriteOrderID());    record.write(dataOutput);    ByteArrayInputStream byteInput = new ByteArrayInputStream(byteOutput.toByteArray());    DataInputStream dataInput = new DataInputStream(byteInput);    return dataInput;}
public void flume_f1439_0() throws IOException
{    Put put = new Put(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.PUT.get(), put.getRecordType());    Take take = new Take(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.TAKE.get(), take.getRecordType());    Rollback rollback = new Rollback(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.ROLLBACK.get(), rollback.getRecordType());    Commit commit = new Commit(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.COMMIT.get(), commit.getRecordType());}
public void flume_f1440_0() throws IOException, CorruptEventException
{    Map<String, String> headers = new HashMap<String, String>();    headers.put("key", "value");    Put in = new Put(System.currentTimeMillis(), WriteOrderOracle.next(), new FlumeEvent(headers, new byte[0]));    Put out = (Put) TransactionEventRecord.fromByteArray(toByteArray(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());    Assert.assertEquals(in.getEvent().getHeaders(), out.getEvent().getHeaders());    Assert.assertEquals(headers, in.getEvent().getHeaders());    Assert.assertEquals(headers, out.getEvent().getHeaders());    Assert.assertTrue(Arrays.equals(in.getEvent().getBody(), out.getEvent().getBody()));}
public void flume_f1441_0() throws IOException, CorruptEventException
{    Put in = new Put(System.currentTimeMillis(), WriteOrderOracle.next(), new FlumeEvent(null, new byte[0]));    Put out = (Put) TransactionEventRecord.fromByteArray(toByteArray(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());    Assert.assertNull(in.getEvent().getHeaders());    Assert.assertNotNull(out.getEvent().getHeaders());    Assert.assertTrue(Arrays.equals(in.getEvent().getBody(), out.getEvent().getBody()));}
public void flume_f1442_0() throws IOException, CorruptEventException
{    Take in = new Take(System.currentTimeMillis(), WriteOrderOracle.next(), 10, 20);    Take out = (Take) TransactionEventRecord.fromByteArray(toByteArray(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());    Assert.assertEquals(in.getFileID(), out.getFileID());    Assert.assertEquals(in.getOffset(), out.getOffset());}
public void flume_f1443_0() throws IOException, CorruptEventException
{    Rollback in = new Rollback(System.currentTimeMillis(), WriteOrderOracle.next());    Rollback out = (Rollback) TransactionEventRecord.fromByteArray(toByteArray(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());}
public void flume_f1444_0() throws IOException, CorruptEventException
{    Commit in = new Commit(System.currentTimeMillis(), WriteOrderOracle.next());    Commit out = (Commit) TransactionEventRecord.fromByteArray(toByteArray(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());}
public void flume_f1445_0() throws IOException, CorruptEventException
{    TransactionEventRecord in = mock(TransactionEventRecord.class);    when(in.getRecordType()).thenReturn(Short.MIN_VALUE);    try {        TransactionEventRecord.fromByteArray(toByteArray(in));        Assert.fail();    } catch (NullPointerException e) {        Assert.assertEquals("Unknown action ffff8000", e.getMessage());    }}
private byte[] flume_f1446_0(TransactionEventRecord record) throws IOException
{    ByteBuffer buffer = TransactionEventRecord.toByteBuffer(record);    return buffer.array();}
public void flume_f1447_0()
{    long current = TransactionIDOracle.next();    current += Integer.MAX_VALUE;    TransactionIDOracle.setSeed(current);    Assert.assertTrue(TransactionIDOracle.next() > System.currentTimeMillis());}
public static FlumeEvent flume_f1448_0()
{    Map<String, String> headers = Maps.newHashMap();    String timestamp = String.valueOf(System.currentTimeMillis());    headers.put("timestamp", timestamp);    FlumeEvent event = new FlumeEvent(headers, timestamp.getBytes());    return event;}
public static FlumeEvent flume_f1449_0(int size)
{    Map<String, String> headers = Maps.newHashMap();    String timestamp = String.valueOf(System.currentTimeMillis());    headers.put("timestamp", timestamp);    byte[] data = new byte[size];    Arrays.fill(data, (byte) 54);    FlumeEvent event = new FlumeEvent(headers, data);    return event;}
public static DataInput flume_f1450_0(Writable writable) throws IOException
{    ByteArrayOutputStream byteOutput = new ByteArrayOutputStream();    DataOutputStream dataOutput = new DataOutputStream(byteOutput);    writable.write(dataOutput);    ByteArrayInputStream byteInput = new ByteArrayInputStream(byteOutput.toByteArray());    DataInputStream dataInput = new DataInputStream(byteInput);    return dataInput;}
public static void flume_f1451_0(Set<String> in, Set<String> out)
{    Assert.assertNotNull(in);    Assert.assertNotNull(out);    Assert.assertEquals(in.size(), out.size());    Assert.assertTrue(in.equals(out));}
public static Set<String> flume_f1452_0(Channel channel, Transaction tx, String prefix, int number)
{    Set<String> events = Sets.newHashSet();    tx.begin();    for (int i = 0; i < number; i++) {        String eventData = (prefix + UUID.randomUUID()).toString();        Event event = EventBuilder.withBody(eventData.getBytes());        channel.put(event);        events.add(eventData);    }    return events;}
public static Set<String> flume_f1453_0(Channel channel, Transaction tx, int number)
{    Set<String> events = Sets.newHashSet();    tx.begin();    for (int i = 0; i < number; i++) {        Event e = channel.take();        if (e == null) {            break;        }        events.add(new String(e.getBody()));    }    return events;}
public static List<File> flume_f1454_0(File[] dataDirs)
{    List<File> result = Lists.newArrayList();    for (File dataDir : dataDirs) {        result.addAll(LogUtils.getLogs(dataDir));    }    return result;}
public static void flume_f1455_0(FileChannel channel)
{    Log log = field("log").ofType(Log.class).in(channel).get();    Assert.assertTrue("writeCheckpoint returned false", method("writeCheckpoint").withReturnType(Boolean.class).withParameterTypes(Boolean.class).in(log).invoke(true));}
public static Set<String> flume_f1456_0(Channel channel, int batchSize) throws Exception
{    return takeEvents(channel, batchSize, false);}
public static Set<String> flume_f1457_0(Channel channel, int batchSize, boolean checkForCorruption) throws Exception
{    return takeEvents(channel, batchSize, Integer.MAX_VALUE, checkForCorruption);}
public static Set<String> flume_f1458_0(Channel channel, int batchSize, int numEvents) throws Exception
{    return takeEvents(channel, batchSize, numEvents, false);}
public static Set<String> flume_f1459_0(Channel channel, int batchSize, int numEvents, boolean checkForCorruption) throws Exception
{    Set<String> result = Sets.newHashSet();    for (int i = 0; i < numEvents; i += batchSize) {        Transaction transaction = channel.getTransaction();        try {            transaction.begin();            for (int j = 0; j < batchSize; j++) {                Event event;                try {                    event = channel.take();                } catch (ChannelException ex) {                    Throwable th = ex;                    String msg;                    if (checkForCorruption) {                        msg = "Corrupt event found. Please run File Channel";                        th = ex.getCause();                    } else {                        msg = "Take list for FileBackedTransaction, capacity";                    }                    Assert.assertTrue(th.getMessage().startsWith(msg));                    if (checkForCorruption) {                        throw (Exception) th;                    }                    transaction.commit();                    return result;                }                if (event == null) {                    transaction.commit();                    return result;                }                result.add(new String(event.getBody(), Charsets.UTF_8));            }            transaction.commit();        } catch (Throwable ex) {            transaction.rollback();            throw new RuntimeException(ex);        } finally {            transaction.close();        }    }    return result;}
public static Set<String> flume_f1460_0(Channel channel) throws Exception
{    return consumeChannel(channel, false);}
public static Set<String> flume_f1461_0(Channel channel, boolean checkForCorruption) throws Exception
{    Set<String> result = Sets.newHashSet();    int[] batchSizes = new int[] { 1000, 100, 10, 1 };    for (int i = 0; i < batchSizes.length; i++) {        while (true) {            Set<String> batch = takeEvents(channel, batchSizes[i], checkForCorruption);            if (batch.isEmpty()) {                break;            }            result.addAll(batch);        }    }    return result;}
public static Set<String> flume_f1462_0(Channel channel, String prefix) throws Exception
{    Set<String> result = Sets.newHashSet();    int[] batchSizes = new int[] { 1000, 100, 10, 1 };    for (int i = 0; i < batchSizes.length; i++) {        try {            while (true) {                Set<String> batch = putEvents(channel, prefix, batchSizes[i], Integer.MAX_VALUE, true);                if (batch.isEmpty()) {                    break;                }                result.addAll(batch);            }        } catch (ChannelException e) {            Assert.assertTrue(("The channel has reached it's capacity. This might " + "be the result of a sink on the channel having too low of batch " + "size, a downstream system running slower than normal, or that " + "the channel capacity is just too low. [channel=" + channel.getName() + "]").equals(e.getMessage()) || e.getMessage().startsWith("Put queue for FileBackedTransaction of capacity "));        }    }    return result;}
public static Set<String> flume_f1463_0(Channel channel, String prefix, int batchSize, int numEvents) throws Exception
{    return putEvents(channel, prefix, batchSize, numEvents, false);}
public static Set<String> flume_f1464_0(Channel channel, String prefix, int batchSize, int numEvents, boolean untilCapacityIsReached) throws Exception
{    Set<String> result = Sets.newHashSet();    for (int i = 0; i < numEvents; i += batchSize) {        Transaction transaction = channel.getTransaction();        transaction.begin();        try {            Set<String> batch = Sets.newHashSet();            for (int j = 0; j < batchSize; j++) {                String s = prefix + "-" + i + "-" + j + "-" + UUID.randomUUID();                Event event = EventBuilder.withBody(s.getBytes(Charsets.UTF_8));                channel.put(event);                batch.add(s);            }            transaction.commit();            result.addAll(batch);        } catch (Exception ex) {            transaction.rollback();            if (untilCapacityIsReached && ex instanceof ChannelException && ("The channel has reached it's capacity. " + "This might be the result of a sink on the channel having too " + "low of batch size, a downstream system running slower than " + "normal, or that the channel capacity is just too low. " + "[channel=" + channel.getName() + "]").equals(ex.getMessage())) {                break;            }            throw ex;        } finally {            transaction.close();        }    }    return result;}
public static void flume_f1465_0(String resource, File output) throws IOException
{    URL input = Resources.getResource(resource);    FileOutputStream fos = new FileOutputStream(output);    GZIPInputStream gzis = new GZIPInputStream(input.openStream());    ByteStreams.copy(gzis, fos);    fos.close();    gzis.close();}
public static Context flume_f1466_0(String checkpointDir, String dataDir, String backupDir, Map<String, String> overrides)
{    Context context = new Context();    context.put(FileChannelConfiguration.CHECKPOINT_DIR, checkpointDir);    if (backupDir != null) {        context.put(FileChannelConfiguration.BACKUP_CHECKPOINT_DIR, backupDir);    }    context.put(FileChannelConfiguration.DATA_DIRS, dataDir);    context.put(FileChannelConfiguration.KEEP_ALIVE, String.valueOf(1));    context.put(FileChannelConfiguration.CAPACITY, String.valueOf(10000));    context.putAll(overrides);    return context;}
public static FileChannel flume_f1467_0(String checkpointDir, String dataDir, Map<String, String> overrides)
{    return createFileChannel(checkpointDir, dataDir, null, overrides);}
public static FileChannel flume_f1468_0(String checkpointDir, String dataDir, String backupDir, Map<String, String> overrides)
{    FileChannel channel = new FileChannel();    channel.setName("FileChannel-" + UUID.randomUUID());    Context context = createFileChannelContext(checkpointDir, dataDir, backupDir, overrides);    Configurables.configure(channel, context);    return channel;}
public static File flume_f1469_0(File baseDir, String name, String text) throws IOException
{    File passwordFile = new File(baseDir, name);    Files.write(text, passwordFile, Charsets.UTF_8);    return passwordFile;}
public void flume_f1470_0()
{    long current = WriteOrderOracle.next();    current += Integer.MAX_VALUE;    WriteOrderOracle.setSeed(current);    Assert.assertTrue(WriteOrderOracle.next() > System.currentTimeMillis());}
public String flume_f1471_0()
{    return getName();}
public String flume_f1472_0()
{    return name;}
public String flume_f1473_0()
{    return validationQuery;}
public static DatabaseType flume_f1474_0(String dbName)
{    DatabaseType type = null;    try {        type = DatabaseType.valueOf(dbName.trim().toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException ex) {        type = DatabaseType.OTHER;    }    return type;}
public boolean flume_f1475_1()
{    Connection connection = null;    Statement stmt = null;    try {        connection = dataSource.getConnection();        stmt = connection.createStatement();        ResultSet rset = stmt.executeQuery(QUREY_SYSCHEMA_FLUME);        if (!rset.next()) {                        return false;        }        String flumeSchemaId = rset.getString(1);                connection.commit();    } catch (SQLException ex) {        try {            connection.rollback();        } catch (SQLException ex2) {                    }        throw new JdbcChannelException("Unable to query schema", ex);    } finally {        if (stmt != null) {            try {                stmt.close();            } catch (SQLException ex) {                            }        }        if (connection != null) {            try {                connection.close();            } catch (SQLException ex) {                            }        }    }    return true;}
public void flume_f1476_0(boolean createForeignKeys, boolean createIndex)
{    runQuery(QUERY_CREATE_SCHEMA_FLUME);    runQuery(QUERY_CREATE_TABLE_FL_EVENT);    if (createForeignKeys) {        runQuery(QUERY_CREATE_TABLE_FL_PLSPILL_FK);        runQuery(QUERY_CREATE_TABLE_FL_HEADER_FK);        runQuery(QUERY_CREATE_TABLE_FL_NMSPILL_FK);        runQuery(QUERY_CREATE_TABLE_FL_VLSPILL_FK);    } else {        runQuery(QUERY_CREATE_TABLE_FL_PLSPILL_NOFK);        runQuery(QUERY_CREATE_TABLE_FL_HEADER_NOFK);        runQuery(QUERY_CREATE_TABLE_FL_NMSPILL_NOFK);        runQuery(QUERY_CREATE_TABLE_FL_VLSPILL_NOFK);    }    if (createIndex) {        runQuery(QUERY_CREATE_INDEX_FLE_CHANNEL);        runQuery(QUERY_CREATE_INDEX_FLH_EVENT);        runQuery(QUERY_CREATE_INDEX_FLP_EVENT);        runQuery(QUERY_CREATE_INDEX_FLN_HEADER);        runQuery(QUERY_CREATE_INDEX_FLV_HEADER);    }}
public void flume_f1477_0()
{    verifyTableStructure(SCHEMA_FLUME, TABLE_FL_EVENT_NAME, COLUMN_FLE_ID, COLUMN_FLE_PAYLOAD, COLUMN_FLE_CHANNEL, COLUMN_FLE_SPILL);    verifyTableStructure(SCHEMA_FLUME, TABLE_FL_PLSPILL_NAME, COLUMN_FLP_EVENT, COLUMN_FLP_SPILL);    verifyTableStructure(SCHEMA_FLUME, TABLE_FL_HEADER_NAME, COLUMN_FLH_ID, COLUMN_FLH_EVENT, COLUMN_FLH_NAME, COLUMN_FLH_VALUE, COLUMN_FLH_NMSPILL, COLUMN_FLH_VLSPILL);    verifyTableStructure(SCHEMA_FLUME, TABLE_FL_NMSPILL_NAME, COLUMN_FLN_HEADER, COLUMN_FLN_SPILL);    verifyTableStructure(SCHEMA_FLUME, TABLE_FL_VLSPILL_NAME, COLUMN_FLV_HEADER, COLUMN_FLV_SPILL);}
private void flume_f1478_1(String schemaName, String tableName, String... columns)
{    Set<String> columnNames = new HashSet<String>();    Connection connection = null;    PreparedStatement pStmt = null;    try {        connection = dataSource.getConnection();        pStmt = connection.prepareStatement(COLUMN_LOOKUP_QUERY);        pStmt.setString(1, tableName);        pStmt.setString(2, schemaName);        ResultSet rset = pStmt.executeQuery();        while (rset.next()) {            columnNames.add(rset.getString(1));        }        connection.commit();    } catch (SQLException ex) {        try {            connection.rollback();        } catch (SQLException ex2) {                    }        throw new JdbcChannelException("Unable to run query: " + COLUMN_LOOKUP_QUERY + ": 1=" + tableName + ", 2=" + schemaName, ex);    } finally {        if (pStmt != null) {            try {                pStmt.close();            } catch (SQLException ex) {                            }            if (connection != null) {                try {                    connection.close();                } catch (SQLException ex) {                                    }            }        }    }    Set<String> columnDiff = new HashSet<String>();    columnDiff.addAll(columnNames);        StringBuilder sb = new StringBuilder("{");    boolean first = true;    for (String column : columns) {        columnDiff.remove(column);        if (first) {            first = false;        } else {            sb.append(", ");        }        sb.append(column);    }    sb.append("}");    String expectedColumns = sb.toString();    if (LOGGER.isDebugEnabled()) {            }    if (columnNames.size() != columns.length || columnDiff.size() != 0) {        throw new JdbcChannelException("Expected table " + schemaName + "." + tableName + " to have columns: " + expectedColumns + ". Instead " + "found columns: " + columnNames);    }}
private void flume_f1479_1(String query)
{    Connection connection = null;    Statement stmt = null;    try {        connection = dataSource.getConnection();        stmt = connection.createStatement();        if (stmt.execute(query)) {            ResultSet rset = stmt.getResultSet();            int count = 0;            while (rset.next()) {                count++;            }                    } else {            int updateCount = stmt.getUpdateCount();                    }        connection.commit();    } catch (SQLException ex) {        try {            connection.rollback();        } catch (SQLException ex2) {                    }        throw new JdbcChannelException("Unable to run query: " + query, ex);    } finally {        if (stmt != null) {            try {                stmt.close();            } catch (SQLException ex) {                            }            if (connection != null) {                try {                    connection.close();                } catch (SQLException ex) {                                    }            }        }    }}
public void flume_f1480_1(PersistableEvent pe, Connection connection)
{        byte[] basePayload = pe.getBasePayload();    byte[] spillPayload = pe.getSpillPayload();    boolean hasSpillPayload = (spillPayload != null);    String channelName = pe.getChannelName();        PreparedStatement baseEventStmt = null;    PreparedStatement spillEventStmt = null;    PreparedStatement baseHeaderStmt = null;    PreparedStatement headerNameSpillStmt = null;    PreparedStatement headerValueSpillStmt = null;    try {        baseEventStmt = connection.prepareStatement(STMT_INSERT_EVENT_BASE, Statement.RETURN_GENERATED_KEYS);        baseEventStmt.setBytes(1, basePayload);        baseEventStmt.setString(2, channelName);        baseEventStmt.setBoolean(3, hasSpillPayload);        int baseEventCount = baseEventStmt.executeUpdate();        if (baseEventCount != 1) {            throw new JdbcChannelException("Invalid update count on base " + "event insert: " + baseEventCount);        }                ResultSet eventIdResult = baseEventStmt.getGeneratedKeys();        if (!eventIdResult.next()) {            throw new JdbcChannelException("Unable to retrieive inserted event-id");        }        long eventId = eventIdResult.getLong(1);        pe.setEventId(eventId);                if (hasSpillPayload) {            spillEventStmt = connection.prepareStatement(STMT_INSERT_EVENT_SPILL);            spillEventStmt.setLong(1, eventId);            spillEventStmt.setBinaryStream(2, new ByteArrayInputStream(spillPayload), spillPayload.length);            int spillEventCount = spillEventStmt.executeUpdate();            if (spillEventCount != 1) {                throw new JdbcChannelException("Invalid update count on spill " + "event insert: " + spillEventCount);            }        }                List<HeaderEntry> headers = pe.getHeaderEntries();        if (headers != null && headers.size() > 0) {            List<HeaderEntry> headerWithNameSpill = new ArrayList<HeaderEntry>();            List<HeaderEntry> headerWithValueSpill = new ArrayList<HeaderEntry>();            baseHeaderStmt = connection.prepareStatement(STMT_INSERT_HEADER_BASE, Statement.RETURN_GENERATED_KEYS);            Iterator<HeaderEntry> it = headers.iterator();            while (it.hasNext()) {                HeaderEntry entry = it.next();                SpillableString name = entry.getName();                SpillableString value = entry.getValue();                baseHeaderStmt.setLong(1, eventId);                baseHeaderStmt.setString(2, name.getBase());                baseHeaderStmt.setString(3, value.getBase());                baseHeaderStmt.setBoolean(4, name.hasSpill());                baseHeaderStmt.setBoolean(5, value.hasSpill());                int updateCount = baseHeaderStmt.executeUpdate();                if (updateCount != 1) {                    throw new JdbcChannelException("Unexpected update header count: " + updateCount);                }                ResultSet headerIdResultSet = baseHeaderStmt.getGeneratedKeys();                if (!headerIdResultSet.next()) {                    throw new JdbcChannelException("Unable to retrieve inserted header id");                }                long headerId = headerIdResultSet.getLong(1);                entry.setId(headerId);                if (name.hasSpill()) {                    headerWithNameSpill.add(entry);                }                if (value.hasSpill()) {                    headerWithValueSpill.add(entry);                }            }                        if (headerWithNameSpill.size() > 0) {                                headerNameSpillStmt = connection.prepareStatement(STMT_INSERT_HEADER_NAME_SPILL);                for (HeaderEntry entry : headerWithNameSpill) {                    String nameSpill = entry.getName().getSpill();                    headerNameSpillStmt.setLong(1, entry.getId());                    headerNameSpillStmt.setString(2, nameSpill);                    headerNameSpillStmt.addBatch();                }                int[] nameSpillUpdateCount = headerNameSpillStmt.executeBatch();                if (nameSpillUpdateCount.length != headerWithNameSpill.size()) {                    throw new JdbcChannelException("Unexpected update count for header " + "name spills: expected " + headerWithNameSpill.size() + ", " + "found " + nameSpillUpdateCount.length);                }                for (int i = 0; i < nameSpillUpdateCount.length; i++) {                    if (nameSpillUpdateCount[i] != 1) {                        throw new JdbcChannelException("Unexpected update count for " + "header name spill at position " + i + ", value: " + nameSpillUpdateCount[i]);                    }                }            }                        if (headerWithValueSpill.size() > 0) {                                headerValueSpillStmt = connection.prepareStatement(STMT_INSERT_HEADER_VALUE_SPILL);                for (HeaderEntry entry : headerWithValueSpill) {                    String valueSpill = entry.getValue().getSpill();                    headerValueSpillStmt.setLong(1, entry.getId());                    headerValueSpillStmt.setString(2, valueSpill);                    headerValueSpillStmt.addBatch();                }                int[] valueSpillUpdateCount = headerValueSpillStmt.executeBatch();                if (valueSpillUpdateCount.length != headerWithValueSpill.size()) {                    throw new JdbcChannelException("Unexpected update count for header " + "value spills: expected " + headerWithValueSpill.size() + ", " + "found " + valueSpillUpdateCount.length);                }                for (int i = 0; i < valueSpillUpdateCount.length; i++) {                    if (valueSpillUpdateCount[i] != 1) {                        throw new JdbcChannelException("Unexpected update count for " + "header value spill at position " + i + ", value: " + valueSpillUpdateCount[i]);                    }                }            }        }    } catch (SQLException ex) {        throw new JdbcChannelException("Unable to persist event: " + pe, ex);    } finally {        if (baseEventStmt != null) {            try {                baseEventStmt.close();            } catch (SQLException ex) {                            }        }        if (spillEventStmt != null) {            try {                spillEventStmt.close();            } catch (SQLException ex) {                            }        }        if (baseHeaderStmt != null) {            try {                baseHeaderStmt.close();            } catch (SQLException ex) {                            }        }        if (headerNameSpillStmt != null) {            try {                headerNameSpillStmt.close();            } catch (SQLException ex) {                            }        }        if (headerValueSpillStmt != null) {            try {                headerValueSpillStmt.close();            } catch (SQLException ex) {                            }        }    }    }
public PersistableEvent flume_f1481_1(String channel, Connection connection)
{    PersistableEvent.Builder peBuilder = null;    PreparedStatement baseEventFetchStmt = null;    PreparedStatement spillEventFetchStmt = null;    InputStream payloadInputStream = null;    PreparedStatement baseHeaderFetchStmt = null;    PreparedStatement nameSpillHeaderStmt = null;    PreparedStatement valueSpillHeaderStmt = null;    PreparedStatement deleteSpillEventStmt = null;    PreparedStatement deleteNameSpillHeaderStmt = null;    PreparedStatement deleteValueSpillHeaderStmt = null;    PreparedStatement deleteBaseHeaderStmt = null;    PreparedStatement deleteBaseEventStmt = null;    try {        baseEventFetchStmt = connection.prepareStatement(STMT_FETCH_PAYLOAD_BASE);        baseEventFetchStmt.setString(1, channel);        ResultSet rsetBaseEvent = baseEventFetchStmt.executeQuery();        if (!rsetBaseEvent.next()) {                                    return null;        }                long eventId = rsetBaseEvent.getLong(1);        peBuilder = new PersistableEvent.Builder(channel, eventId);        peBuilder.setBasePayload(rsetBaseEvent.getBytes(2));        boolean hasSpill = rsetBaseEvent.getBoolean(3);        if (hasSpill) {            spillEventFetchStmt = connection.prepareStatement(STMT_FETCH_PAYLOAD_SPILL);            spillEventFetchStmt.setLong(1, eventId);            ResultSet rsetSpillEvent = spillEventFetchStmt.executeQuery();            if (!rsetSpillEvent.next()) {                throw new JdbcChannelException("Payload spill expected but not " + "found for event: " + eventId);            }            Blob payloadSpillBlob = rsetSpillEvent.getBlob(1);            payloadInputStream = payloadSpillBlob.getBinaryStream();            ByteArrayOutputStream spillStream = new ByteArrayOutputStream();            byte[] buffer = new byte[1024];            int length = 0;            while ((length = payloadInputStream.read(buffer)) != -1) {                spillStream.write(buffer, 0, length);            }            peBuilder.setSpillPayload(spillStream.toByteArray());                        deleteSpillEventStmt = connection.prepareStatement(STMT_DELETE_EVENT_SPILL);            deleteSpillEventStmt.setLong(1, eventId);            int updateCount = deleteSpillEventStmt.executeUpdate();            if (updateCount != 1) {                throw new JdbcChannelException("Unexpected row count for spill " + "delete: " + updateCount);            }        }        if (rsetBaseEvent.next()) {            throw new JdbcChannelException("More than expected events retrieved");        }                List<Long> nameSpillHeaders = null;        List<Long> valueSpillHeaders = null;        baseHeaderFetchStmt = connection.prepareStatement(STMT_FETCH_HEADER_BASE);        baseHeaderFetchStmt.setLong(1, eventId);                int headerCount = 0;        ResultSet rsetBaseHeader = baseHeaderFetchStmt.executeQuery();        while (rsetBaseHeader.next()) {            headerCount++;            long headerId = rsetBaseHeader.getLong(1);            String baseName = rsetBaseHeader.getString(2);            String baseValue = rsetBaseHeader.getString(3);            boolean hasNameSpill = rsetBaseHeader.getBoolean(4);            boolean hasValueSpill = rsetBaseHeader.getBoolean(5);            peBuilder.setHeader(headerId, baseName, baseValue);            if (hasNameSpill) {                if (nameSpillHeaders == null) {                    nameSpillHeaders = new ArrayList<Long>();                }                nameSpillHeaders.add(headerId);            }            if (hasValueSpill) {                if (valueSpillHeaders == null) {                    valueSpillHeaders = new ArrayList<Long>();                }                valueSpillHeaders.add(headerId);            }        }        if (nameSpillHeaders != null) {            nameSpillHeaderStmt = connection.prepareStatement(STMT_FETCH_HEADER_NAME_SPILL);            deleteNameSpillHeaderStmt = connection.prepareStatement(STMT_DELETE_HEADER_NAME_SPILL);            for (long headerId : nameSpillHeaders) {                nameSpillHeaderStmt.setLong(1, headerId);                ResultSet rsetHeaderNameSpill = nameSpillHeaderStmt.executeQuery();                if (!rsetHeaderNameSpill.next()) {                    throw new JdbcChannelException("Name spill was set for header " + headerId + " but was not found");                }                String nameSpill = rsetHeaderNameSpill.getString(1);                peBuilder.setHeaderNameSpill(headerId, nameSpill);                deleteNameSpillHeaderStmt.setLong(1, headerId);                deleteNameSpillHeaderStmt.addBatch();            }                        int[] headerNameSpillDelete = deleteNameSpillHeaderStmt.executeBatch();            if (headerNameSpillDelete.length != nameSpillHeaders.size()) {                throw new JdbcChannelException("Unexpected number of header name " + "spill deletes: expected " + nameSpillHeaders.size() + ", found: " + headerNameSpillDelete.length);            }            for (int numRowsAffected : headerNameSpillDelete) {                if (numRowsAffected != 1) {                    throw new JdbcChannelException("Unexpected number of deleted rows " + "for header name spill deletes: " + numRowsAffected);                }            }        }        if (valueSpillHeaders != null) {            valueSpillHeaderStmt = connection.prepareStatement(STMT_FETCH_HEADER_VALUE_SPILL);            deleteValueSpillHeaderStmt = connection.prepareStatement(STMT_DELETE_HEADER_VALUE_SPILL);            for (long headerId : valueSpillHeaders) {                valueSpillHeaderStmt.setLong(1, headerId);                ResultSet rsetHeaderValueSpill = valueSpillHeaderStmt.executeQuery();                if (!rsetHeaderValueSpill.next()) {                    throw new JdbcChannelException("Value spill was set for header " + headerId + " but was not found");                }                String valueSpill = rsetHeaderValueSpill.getString(1);                peBuilder.setHeaderValueSpill(headerId, valueSpill);                deleteValueSpillHeaderStmt.setLong(1, headerId);                deleteValueSpillHeaderStmt.addBatch();            }                        int[] headerValueSpillDelete = deleteValueSpillHeaderStmt.executeBatch();            if (headerValueSpillDelete.length != valueSpillHeaders.size()) {                throw new JdbcChannelException("Unexpected number of header value " + "spill deletes: expected " + valueSpillHeaders.size() + ", found: " + headerValueSpillDelete.length);            }            for (int numRowsAffected : headerValueSpillDelete) {                if (numRowsAffected != 1) {                    throw new JdbcChannelException("Unexpected number of deleted rows " + "for header value spill deletes: " + numRowsAffected);                }            }        }                if (headerCount > 0) {            deleteBaseHeaderStmt = connection.prepareStatement(STMT_DELETE_HEADER_BASE);            deleteBaseHeaderStmt.setLong(1, eventId);            int rowCount = deleteBaseHeaderStmt.executeUpdate();            if (rowCount != headerCount) {                throw new JdbcChannelException("Unexpected base header delete count: " + "expected: " + headerCount + ", found: " + rowCount);            }        }                deleteBaseEventStmt = connection.prepareStatement(STMT_DELETE_EVENT_BASE);        deleteBaseEventStmt.setLong(1, eventId);        int rowCount = deleteBaseEventStmt.executeUpdate();        if (rowCount != 1) {            throw new JdbcChannelException("Unexpected row count for delete of " + "event-id: " + eventId + ", count: " + rowCount);        }    } catch (SQLException ex) {        throw new JdbcChannelException("Unable to retrieve event", ex);    } catch (IOException ex) {        throw new JdbcChannelException("Unable to read data", ex);    } finally {        if (payloadInputStream != null) {            try {                payloadInputStream.close();            } catch (IOException ex) {                            }        }        if (baseEventFetchStmt != null) {            try {                baseEventFetchStmt.close();            } catch (SQLException ex) {                            }        }        if (spillEventFetchStmt != null) {            try {                spillEventFetchStmt.close();            } catch (SQLException ex) {                            }        }        if (deleteSpillEventStmt != null) {            try {                deleteSpillEventStmt.close();            } catch (SQLException ex) {                            }        }        if (baseHeaderFetchStmt != null) {            try {                baseHeaderFetchStmt.close();            } catch (SQLException ex) {                            }        }        if (nameSpillHeaderStmt != null) {            try {                nameSpillHeaderStmt.close();            } catch (SQLException ex) {                            }        }        if (valueSpillHeaderStmt != null) {            try {                valueSpillHeaderStmt.close();            } catch (SQLException ex) {                            }        }        if (deleteNameSpillHeaderStmt != null) {            try {                deleteNameSpillHeaderStmt.close();            } catch (SQLException ex) {                            }        }        if (deleteValueSpillHeaderStmt != null) {            try {                deleteValueSpillHeaderStmt.close();            } catch (SQLException ex) {                            }        }        if (deleteBaseHeaderStmt != null) {            try {                deleteBaseHeaderStmt.close();            } catch (SQLException ex) {                            }        }        if (deleteBaseEventStmt != null) {            try {                deleteBaseEventStmt.close();            } catch (SQLException ex) {                            }        }    }    return peBuilder.build();}
public long flume_f1482_1(Connection connection)
{    long size = 0L;    Statement stmt = null;    try {        stmt = connection.createStatement();        stmt.execute(QUERY_CHANNEL_SIZE);        ResultSet rset = stmt.getResultSet();        if (!rset.next()) {            throw new JdbcChannelException("Failed to determine channel size: " + "Query (" + QUERY_CHANNEL_SIZE + ") did not produce any results");        }        size = rset.getLong(1);        connection.commit();    } catch (SQLException ex) {        try {            connection.rollback();        } catch (SQLException ex2) {                    }        throw new JdbcChannelException("Unable to run query: " + QUERY_CHANNEL_SIZE, ex);    } finally {        if (stmt != null) {            try {                stmt.close();            } catch (SQLException ex) {                            }        }    }    return size;}
public void flume_f1483_1(Context context)
{        initializeSystemProperties(context);    initializeDataSource(context);    initializeSchema(context);    initializeChannelState(context);}
private void flume_f1484_1(Context context)
{    Map<String, String> sysProps = new HashMap<String, String>();    Map<String, String> sysPropsOld = context.getSubProperties(ConfigurationConstants.OLD_CONFIG_JDBC_SYSPROP_PREFIX);    if (sysPropsOld.size() > 0) {                sysProps.putAll(sysPropsOld);    }    Map<String, String> sysPropsNew = context.getSubProperties(ConfigurationConstants.CONFIG_JDBC_SYSPROP_PREFIX);        if (sysPropsNew.size() > 0) {        sysProps.putAll(sysPropsNew);    }    for (String key : sysProps.keySet()) {        String value = sysProps.get(key);        if (key != null && value != null) {            System.setProperty(key, value);        }    }}
private void flume_f1485_1(Context context)
{    String maxCapacityStr = getConfigurationString(context, ConfigurationConstants.CONFIG_MAX_CAPACITY, ConfigurationConstants.OLD_CONFIG_MAX_CAPACITY, "0");    long maxCapacitySpecified = 0;    try {        maxCapacitySpecified = Long.parseLong(maxCapacityStr);    } catch (NumberFormatException nfe) {            }    if (maxCapacitySpecified > 0) {        this.maxCapacity = maxCapacitySpecified;            } else {            }    if (maxCapacity > 0) {                JdbcTransactionImpl tx = null;        try {            tx = getTransaction();            tx.begin();            Connection conn = tx.getConnection();            currentSize.set(schemaHandler.getChannelSize(conn));            tx.commit();        } catch (Exception ex) {            tx.rollback();            throw new JdbcChannelException("Failed to initialize current size", ex);        } finally {            if (tx != null) {                tx.close();            }        }        long currentSizeLong = currentSize.get();        if (currentSizeLong > maxCapacity) {                    }            }}
private void flume_f1486_1(Context context)
{    String createSchemaFlag = getConfigurationString(context, ConfigurationConstants.CONFIG_CREATE_SCHEMA, ConfigurationConstants.OLD_CONFIG_CREATE_SCHEMA, "true");    boolean createSchema = Boolean.valueOf(createSchemaFlag);            schemaHandler = SchemaHandlerFactory.getHandler(databaseType, dataSource);    if (!schemaHandler.schemaExists()) {        if (!createSchema) {            throw new JdbcChannelException("Schema does not exist and " + "auto-generation is disabled. Please enable auto-generation of " + "schema and try again.");        }        String createIndexFlag = getConfigurationString(context, ConfigurationConstants.CONFIG_CREATE_INDEX, ConfigurationConstants.OLD_CONFIG_CREATE_INDEX, "true");        String createForeignKeysFlag = getConfigurationString(context, ConfigurationConstants.CONFIG_CREATE_FK, ConfigurationConstants.OLD_CONFIG_CREATE_FK, "true");        boolean createIndex = Boolean.valueOf(createIndexFlag);        if (!createIndex) {                    }        boolean createForeignKeys = Boolean.valueOf(createForeignKeysFlag);        if (createForeignKeys) {                    } else {                    }                schemaHandler.createSchemaObjects(createForeignKeys, createIndex);    }        schemaHandler.validateSchema();}
public void flume_f1487_1()
{    try {        connectionPool.close();    } catch (Exception ex) {        throw new JdbcChannelException("Unable to close connection pool", ex);    }    if (databaseType.equals(DatabaseType.DERBY) && driverClassName.equals(EMBEDDED_DERBY_DRIVER_CLASSNAME)) {                if (connectUrl.startsWith("jdbc:derby:")) {            int index = connectUrl.indexOf(";");            String baseUrl = null;            if (index != -1) {                baseUrl = connectUrl.substring(0, index + 1);            } else {                baseUrl = connectUrl + ";";            }            String shutDownUrl = baseUrl + "shutdown=true";                        try {                DriverManager.getConnection(shutDownUrl);            } catch (SQLException ex) {                                if (ex.getErrorCode() != 45000) {                    throw new JdbcChannelException("Unable to shutdown embedded Derby: " + shutDownUrl + " Error Code: " + ex.getErrorCode(), ex);                }                            }        } else {                    }    }    dataSource = null;    txFactory = null;    schemaHandler = null;}
public void flume_f1488_1(String channel, Event event)
{    PersistableEvent persistableEvent = new PersistableEvent(channel, event);    JdbcTransactionImpl tx = null;    try {        tx = getTransaction();        tx.begin();        if (maxCapacity > 0) {            long currentSizeLong = currentSize.get();            if (currentSizeLong >= maxCapacity) {                throw new JdbcChannelException("Channel capacity reached: " + "maxCapacity: " + maxCapacity + ", currentSize: " + currentSizeLong);            }        }                schemaHandler.storeEvent(persistableEvent, tx.getConnection());        tx.incrementPersistedEventCount();        tx.commit();    } catch (Exception ex) {        tx.rollback();        throw new JdbcChannelException("Failed to persist event", ex);    } finally {        if (tx != null) {            tx.close();        }    }    }
public Event flume_f1489_1(String channelName)
{    PersistableEvent result = null;    JdbcTransactionImpl tx = null;    try {        tx = getTransaction();        tx.begin();                result = schemaHandler.fetchAndDeleteEvent(channelName, tx.getConnection());        if (result != null) {            tx.incrementRemovedEventCount();        }        tx.commit();    } catch (Exception ex) {        tx.rollback();        throw new JdbcChannelException("Failed to persist event", ex);    } finally {        if (tx != null) {            tx.close();        }    }    if (result != null) {            } else {            }    return result;}
public JdbcTransactionImpl flume_f1490_0()
{    return txFactory.get();}
private void flume_f1491_1(Context context)
{    driverClassName = getConfigurationString(context, ConfigurationConstants.CONFIG_JDBC_DRIVER_CLASS, ConfigurationConstants.OLD_CONFIG_JDBC_DRIVER_CLASS, null);    connectUrl = getConfigurationString(context, ConfigurationConstants.CONFIG_URL, ConfigurationConstants.OLD_CONFIG_URL, null);    String userName = getConfigurationString(context, ConfigurationConstants.CONFIG_USERNAME, ConfigurationConstants.OLD_CONFIG_USERNAME, null);    String password = getConfigurationString(context, ConfigurationConstants.CONFIG_PASSWORD, ConfigurationConstants.OLD_CONFIG_PASSWORD, null);    String jdbcPropertiesFile = getConfigurationString(context, ConfigurationConstants.CONFIG_JDBC_PROPS_FILE, ConfigurationConstants.OLD_CONFIG_JDBC_PROPS_FILE, null);    String dbTypeName = getConfigurationString(context, ConfigurationConstants.CONFIG_DATABASE_TYPE, ConfigurationConstants.OLD_CONFIG_DATABASE_TYPE, null);        if (connectUrl == null || connectUrl.trim().length() == 0) {                driverClassName = DEFAULT_DRIVER_CLASSNAME;        userName = DEFAULT_USERNAME;        password = DEFAULT_PASSWORD;        dbTypeName = DEFAULT_DBTYPE;        String homePath = System.getProperty("user.home").replace('\\', '/');        String defaultDbDir = homePath + "/.flume/jdbc-channel";        File dbDir = new File(defaultDbDir);        String canonicalDbDirPath = null;        try {            canonicalDbDirPath = dbDir.getCanonicalPath();        } catch (IOException ex) {            throw new JdbcChannelException("Unable to find canonical path of dir: " + defaultDbDir, ex);        }        if (!dbDir.exists()) {            if (!dbDir.mkdirs()) {                throw new JdbcChannelException("unable to create directory: " + canonicalDbDirPath);            }        }        connectUrl = "jdbc:derby:" + canonicalDbDirPath + "/db;create=true";                jdbcPropertiesFile = null;            }        databaseType = DatabaseType.getByName(dbTypeName);    switch(databaseType) {        case DERBY:        case MYSQL:            break;        default:            throw new JdbcChannelException("Database " + databaseType + " not supported at this time");    }        if (driverClassName == null || driverClassName.trim().length() == 0) {        throw new JdbcChannelException("No jdbc driver specified");    }    try {        Class.forName(driverClassName);    } catch (ClassNotFoundException ex) {        throw new JdbcChannelException("Unable to load driver: " + driverClassName, ex);    }        Properties jdbcProps = new Properties();    if (jdbcPropertiesFile != null && jdbcPropertiesFile.trim().length() > 0) {        File jdbcPropsFile = new File(jdbcPropertiesFile.trim());        if (!jdbcPropsFile.exists()) {            throw new JdbcChannelException("Jdbc properties file does not exist: " + jdbcPropertiesFile);        }        InputStream inStream = null;        try {            inStream = new FileInputStream(jdbcPropsFile);            jdbcProps.load(inStream);        } catch (IOException ex) {            throw new JdbcChannelException("Unable to load jdbc properties " + "from file: " + jdbcPropertiesFile, ex);        } finally {            if (inStream != null) {                try {                    inStream.close();                } catch (IOException ex) {                                    }            }        }    }    if (userName != null) {        Object oldUser = jdbcProps.put("user", userName);        if (oldUser != null) {                    }    }    if (password != null) {        Object oldPass = jdbcProps.put("password", password);        if (oldPass != null) {                    }    }    if (LOGGER.isDebugEnabled()) {        StringBuilder sb = new StringBuilder("JDBC Properties {");        boolean first = true;        Enumeration<?> propertyKeys = jdbcProps.propertyNames();        while (propertyKeys.hasMoreElements()) {            if (first) {                first = false;            } else {                sb.append(", ");            }            String key = (String) propertyKeys.nextElement();            sb.append(key).append("=");            if (key.equalsIgnoreCase("password")) {                sb.append("*******");            } else {                sb.append(jdbcProps.get(key));            }        }        sb.append("}");            }        String txIsolation = getConfigurationString(context, ConfigurationConstants.CONFIG_TX_ISOLATION_LEVEL, ConfigurationConstants.OLD_CONFIG_TX_ISOLATION_LEVEL, TransactionIsolation.READ_COMMITTED.getName());    TransactionIsolation txIsolationLevel = TransactionIsolation.getByName(txIsolation);            ConnectionFactory connFactory = new DriverManagerConnectionFactory(connectUrl, jdbcProps);    connectionPool = new GenericObjectPool();    String maxActiveConnections = getConfigurationString(context, ConfigurationConstants.CONFIG_MAX_CONNECTIONS, ConfigurationConstants.OLD_CONFIG_MAX_CONNECTIONS, "10");    int maxActive = 10;    if (maxActiveConnections != null && maxActiveConnections.length() > 0) {        try {            maxActive = Integer.parseInt(maxActiveConnections);        } catch (NumberFormatException nfe) {                    }    }        connectionPool.setMaxActive(maxActive);    statementPool = new GenericKeyedObjectPoolFactory(null);        new PoolableConnectionFactory(connFactory, connectionPool, statementPool, databaseType.getValidationQuery(), false, false, txIsolationLevel.getCode());    dataSource = new PoolingDataSource(connectionPool);    txFactory = new JdbcTransactionFactory(dataSource, this);}
protected void flume_f1492_1(long delta)
{    long currentSizeLong = currentSize.addAndGet(delta);    }
private String flume_f1493_1(Context context, String key, String oldKey, String defaultValue)
{    String oldValue = context.getString(oldKey);    if (oldValue != null && oldValue.length() > 0) {            }    String value = context.getString(key);    if (value == null) {        if (oldValue != null) {            value = oldValue;        } else {            value = defaultValue;        }    }    return value;}
protected JdbcTransactionImpl flume_f1494_0()
{    return new JdbcTransactionImpl(dataSource, this, providerImpl);}
public void flume_f1495_1()
{    if (!active) {        throw new JdbcChannelException("Inactive transaction");    }    if (count == 0) {                try {            connection = dataSource.getConnection();        } catch (SQLException ex) {            throw new JdbcChannelException("Unable to lease connection", ex);        }                try {            connection.clearWarnings();        } catch (SQLException ex) {                    }    }    count++;    LOGGER.trace("Tx count-begin: " + count + ", rollback: " + rollback);}
public void flume_f1496_0()
{    if (!active) {        throw new JdbcChannelException("Inactive transaction");    }    if (rollback) {        throw new JdbcChannelException("Cannot commit transaction marked for rollback");    }    LOGGER.trace("Tx count-commit: " + count + ", rollback: " + rollback);}
public void flume_f1497_1()
{    if (!active) {        throw new JdbcChannelException("Inactive transaction");    }        rollback = true;    LOGGER.trace("Tx count-rollback: " + count + ", rollback: " + rollback);}
public void flume_f1498_1()
{    if (!active) {        throw new JdbcChannelException("Inactive transaction");    }    count--;        if (count == 0) {        active = false;        try {            if (rollback) {                                connection.rollback();            } else {                                connection.commit();                                providerImpl.updateCurrentChannelSize(this.persistedEventCount - this.removedEventCount);                this.persistedEventCount = 0;                this.removedEventCount = 0;            }        } catch (SQLException ex) {            throw new JdbcChannelException("Unable to finalize transaction", ex);        } finally {            if (connection != null) {                                try {                    SQLWarning warning = connection.getWarnings();                    if (warning != null) {                        StringBuilder sb = new StringBuilder("Connection warnigns: ");                        boolean first = true;                        while (warning != null) {                            if (first) {                                first = false;                            } else {                                sb.append("; ");                            }                            sb.append("[").append(warning.getErrorCode()).append("] ");                            sb.append(warning.getMessage());                        }                                            }                } catch (SQLException ex) {                                    }                                try {                    connection.close();                } catch (SQLException ex) {                                    }            }                        txFactory.remove();                        connection = null;            txFactory = null;        }    }}
protected Connection flume_f1499_0()
{    if (!active) {        throw new JdbcChannelException("Inactive transaction");    }    return connection;}
protected void flume_f1500_0()
{    removedEventCount++;}
protected void flume_f1501_0()
{    persistedEventCount++;}
public boolean flume_f1502_0()
{        return false;}
public void flume_f1503_0()
{}
public void flume_f1504_0(PersistableEvent pe, Connection connection)
{}
public PersistableEvent flume_f1505_0(String channel, Connection connection)
{        return null;}
public long flume_f1506_0(Connection connection)
{        return 0;}
public void flume_f1507_0(boolean createForeignKeys, boolean createIndex)
{}
public String flume_f1508_0()
{    return channel;}
public byte[] flume_f1509_0()
{    return this.basePayload;}
public byte[] flume_f1510_0()
{    return this.spillPayload;}
protected void flume_f1511_0(long eventId)
{    this.eventId = eventId;}
protected long flume_f1512_0()
{    return this.eventId;}
public List<HeaderEntry> flume_f1513_0()
{    return headers;}
public String flume_f1514_0()
{    return name.getString();}
public SpillableString flume_f1515_0()
{    return name;}
public String flume_f1516_0()
{    return value.getString();}
public SpillableString flume_f1517_0()
{    return value;}
protected void flume_f1518_0(long headerId)
{    this.headerId = headerId;}
public long flume_f1519_0()
{    return headerId;}
public String flume_f1520_0()
{    return base;}
public String flume_f1521_0()
{    return spill;}
public String flume_f1522_0()
{    if (spill == null) {        return base;    }    return base + spill;}
public boolean flume_f1523_0()
{    return spill != null;}
public void flume_f1524_0(Map<String, String> headers)
{    throw new UnsupportedOperationException("Cannot update headers of " + "persistable event");}
public byte[] flume_f1525_0()
{    byte[] result = null;    if (spillPayload == null) {        result = Arrays.copyOf(basePayload, basePayload.length);    } else {        result = new byte[basePayload.length + spillPayload.length];        System.arraycopy(basePayload, 0, result, 0, basePayload.length);        System.arraycopy(spillPayload, 0, result, basePayload.length, spillPayload.length);    }    return result;}
public void flume_f1526_0(byte[] body)
{    throw new UnsupportedOperationException("Cannot update payload of " + "persistable event");}
public Map<String, String> flume_f1527_0()
{    Map<String, String> headerMap = null;    if (headers != null) {        headerMap = new HashMap<String, String>();        for (HeaderEntry entry : headers) {            headerMap.put(entry.getNameString(), entry.getValueString());        }    }    return headerMap;}
public Builder flume_f1528_0(long eventId)
{    bEventId = eventId;    return this;}
public Builder flume_f1529_0(String channel)
{    bChannelName = channel;    return this;}
public Builder flume_f1530_0(byte[] basePayload)
{    bBasePayload = basePayload;    return this;}
public Builder flume_f1531_0(byte[] spillPayload)
{    bSpillPayload = spillPayload;    return this;}
public Builder flume_f1532_0(long headerId, String baseName, String baseValue)
{    if (bHeaderParts == null) {        bHeaderParts = new HashMap<Long, HeaderPart>();    }    HeaderPart hp = new HeaderPart(baseName, baseValue);    if (bHeaderParts.put(headerId, hp) != null) {        throw new JdbcChannelException("Duplicate header found: " + "headerId: " + headerId + ", baseName: " + baseName + ", " + "baseValue: " + baseValue);    }    return this;}
public Builder flume_f1533_0(long headerId, String nameSpill)
{    HeaderPart hp = bHeaderParts.get(headerId);    if (hp == null) {        throw new JdbcChannelException("Header not found for spill: " + headerId);    }    hp.setSpillName(nameSpill);    return this;}
public Builder flume_f1534_0(long headerId, String valueSpill)
{    HeaderPart hp = bHeaderParts.get(headerId);    if (hp == null) {        throw new JdbcChannelException("Header not found for spill: " + headerId);    }    hp.setSpillValue(valueSpill);    return this;}
public PersistableEvent flume_f1535_0()
{    List<HeaderEntry> bHeaders = new ArrayList<HeaderEntry>();    if (bHeaderParts != null) {        for (long headerId : bHeaderParts.keySet()) {            HeaderPart part = bHeaderParts.get(headerId);            bHeaders.add(part.getEntry(headerId));        }    }    PersistableEvent pe = new PersistableEvent(bEventId, bChannelName, bBasePayload, bSpillPayload, bHeaders);    bEventId = 0L;    bChannelName = null;    bBasePayload = null;    bSpillPayload = null;    bHeaderParts = null;    return pe;}
 String flume_f1536_0()
{    return hBaseName;}
 String flume_f1537_0()
{    return hBaseValue;}
 String flume_f1538_0()
{    return hSpillName;}
 String flume_f1539_0()
{    return hSpillValue;}
 void flume_f1540_0(String spillName)
{    hSpillName = spillName;}
 void flume_f1541_0(String spillValue)
{    hSpillValue = spillValue;}
 HeaderEntry flume_f1542_0(long headerId)
{    return new HeaderEntry(headerId, hBaseName, hSpillName, hBaseValue, hSpillValue);}
public static SchemaHandler flume_f1543_0(DatabaseType dbType, DataSource dataSource)
{    SchemaHandler handler = null;    switch(dbType) {        case DERBY:            handler = new DerbySchemaHandler(dataSource);            break;        case MYSQL:            handler = new MySQLSchemaHandler(dataSource);            break;        default:            throw new JdbcChannelException("Database " + dbType + " not supported yet");    }    return handler;}
public void flume_f1544_0(Event event) throws ChannelException
{    getProvider().persistEvent(getName(), event);}
public Event flume_f1545_0() throws ChannelException
{    return getProvider().removeEvent(getName());}
public Transaction flume_f1546_0()
{    return getProvider().getTransaction();}
public void flume_f1547_0()
{    JdbcChannelProviderFactory.releaseProvider(getName());    provider = null;    super.stop();}
private JdbcChannelProvider flume_f1548_0()
{    return provider;}
public void flume_f1549_1(Context context)
{    provider = JdbcChannelProviderFactory.getProvider(context, getName());    }
public static synchronized JdbcChannelProvider flume_f1550_0(Context context, String name)
{    if (PROVIDER == null) {        PROVIDER = new JdbcChannelProviderImpl();        PROVIDER.initialize(context);    }    if (!INSTANCES.add(name)) {        throw new JdbcChannelException("Attempt to initialize multiple " + "channels with same name: " + name);    }    return PROVIDER;}
public static synchronized void flume_f1551_0(String name)
{    if (!INSTANCES.remove(name)) {        throw new JdbcChannelException("Attempt to release non-existant channel: " + name);    }    if (INSTANCES.size() == 0) {                PROVIDER.close();        PROVIDER = null;    }}
public int flume_f1552_0()
{    return code;}
public String flume_f1553_0()
{    return name;}
public String flume_f1554_0()
{    return getName();}
public static TransactionIsolation flume_f1555_0(String name)
{    return valueOf(name.trim().toUpperCase(Locale.ENGLISH));}
public void flume_f1556_1() throws IOException
{    derbyCtx.clear();    derbyCtx.put(ConfigurationConstants.CONFIG_CREATE_SCHEMA, "true");    derbyCtx.put(ConfigurationConstants.CONFIG_DATABASE_TYPE, "DERBY");    derbyCtx.put(ConfigurationConstants.CONFIG_JDBC_DRIVER_CLASS, "org.apache.derby.jdbc.EmbeddedDriver");    derbyCtx.put(ConfigurationConstants.CONFIG_PASSWORD, "");    derbyCtx.put(ConfigurationConstants.CONFIG_USERNAME, "sa");    File tmpDir = new File("target/test");    tmpDir.mkdirs();    File derbyLogFile = new File(tmpDir, "derbytest.log");    String derbyLogFilePath = derbyLogFile.getCanonicalPath();    derbyCtx.put(ConfigurationConstants.CONFIG_JDBC_SYSPROP_PREFIX + "derby.stream.error.file", derbyLogFilePath);        File tempFile = File.createTempFile("temp", "_db", tmpDir);    String absFileName = tempFile.getCanonicalPath();    tempFile.delete();    derbyDbDir = new File(absFileName + "_dir");    if (!derbyDbDir.exists()) {        derbyDbDir.mkdirs();    }    derbyCtx.put(ConfigurationConstants.CONFIG_URL, "jdbc:derby:memory:" + derbyDbDir.getCanonicalPath() + "/db;create=true");    configureChannel(derbyCtx);    }
public void flume_f1557_0()
{    provider = new JdbcChannelProviderImpl();    derbyCtx.put(ConfigurationConstants.CONFIG_MAX_CAPACITY, "10");    provider.initialize(derbyCtx);    Set<MockEvent> events = new HashSet<MockEvent>();    for (int i = 1; i < 12; i++) {        events.add(MockEventUtils.generateMockEvent(i, i, i, 61 % i, 1));    }    Iterator<MockEvent> meIt = events.iterator();    int count = 0;    while (meIt.hasNext()) {        count++;        MockEvent me = meIt.next();        String chName = me.getChannel();        try {            provider.persistEvent(chName, me);            if (count == 11) {                Assert.fail();            }        } catch (JdbcChannelException ex) {                        Assert.assertEquals(11, count);        }                Event e = provider.removeEvent(chName);        Assert.assertNotNull(e);                provider.persistEvent(chName, me);    }}
public void flume_f1558_0()
{    provider = new JdbcChannelProviderImpl();    provider.initialize(derbyCtx);    Transaction tx1 = provider.getTransaction();    tx1.begin();    Transaction tx2 = provider.getTransaction();    Assert.assertSame(tx1, tx2);    tx2.begin();    tx2.close();    tx1.close();    Transaction tx3 = provider.getTransaction();    Assert.assertNotSame(tx1, tx3);    tx3.begin();    tx3.close();    provider.close();    provider = null;}
public void flume_f1559_0() throws Exception
{    provider = new JdbcChannelProviderImpl();    provider.initialize(derbyCtx);    Map<String, List<MockEvent>> eventMap = new HashMap<String, List<MockEvent>>();    for (int i = 1; i < 121; i++) {        MockEvent me = MockEventUtils.generateMockEvent(i, i, i, 61 % i, 10);        List<MockEvent> meList = eventMap.get(me.getChannel());        if (meList == null) {            meList = new ArrayList<MockEvent>();            eventMap.put(me.getChannel(), meList);        }        meList.add(me);    }    List<MockSource> sourceList = new ArrayList<MockSource>();    List<MockSink> sinkList = new ArrayList<MockSink>();    for (String channel : eventMap.keySet()) {        List<MockEvent> meList = eventMap.get(channel);        sourceList.add(new MockSource(channel, meList, provider));        sinkList.add(new MockSink(channel, meList, provider));    }    ExecutorService sourceExecutor = Executors.newFixedThreadPool(10);    ExecutorService sinkExecutor = Executors.newFixedThreadPool(10);    List<Future<Integer>> srcResults = sourceExecutor.invokeAll(sourceList, 300, TimeUnit.SECONDS);    Thread.sleep(MockEventUtils.generateSleepInterval(3000));    List<Future<Integer>> sinkResults = sinkExecutor.invokeAll(sinkList, 300, TimeUnit.SECONDS);    int srcCount = 0;    for (Future<Integer> srcOutput : srcResults) {        srcCount += srcOutput.get();    }    Assert.assertEquals(120, srcCount);    int sinkCount = 0;    for (Future<Integer> sinkOutput : sinkResults) {        sinkCount += sinkOutput.get();    }    Assert.assertEquals(120, sinkCount);}
public void flume_f1560_0()
{    provider = new JdbcChannelProviderImpl();    provider.initialize(derbyCtx);    Map<String, List<MockEvent>> eventMap = new HashMap<String, List<MockEvent>>();    Set<MockEvent> events = new HashSet<MockEvent>();    for (int i = 1; i < 81; i++) {        events.add(MockEventUtils.generateMockEvent(i, i, i, 61 % i, 5));    }    Iterator<MockEvent> meIt = events.iterator();    while (meIt.hasNext()) {        MockEvent me = meIt.next();        String chName = me.getChannel();        List<MockEvent> eventList = eventMap.get(chName);        if (eventList == null) {            eventList = new ArrayList<MockEvent>();            eventMap.put(chName, eventList);        }        eventList.add(me);        provider.persistEvent(me.getChannel(), me);    }    for (String chName : eventMap.keySet()) {        List<MockEvent> meList = eventMap.get(chName);        Iterator<MockEvent> it = meList.iterator();        while (it.hasNext()) {            MockEvent me = it.next();            Event event = provider.removeEvent(chName);            assertEquals(me, event);        }                Event nullEvent = provider.removeEvent(chName);        Assert.assertNull(nullEvent);    }    provider.close();    provider = null;}
private static void flume_f1561_0(Event e1, Event e2)
{    byte[] pl1 = e1.getBody();    byte[] pl2 = e2.getBody();    Assert.assertArrayEquals(pl1, pl2);    Map<String, String> h1 = e1.getHeaders();    Map<String, String> h2 = e2.getHeaders();    if (h1 == null || h1.size() == 0) {        Assert.assertTrue(h2 == null || h2.size() == 0);    } else {        Assert.assertTrue(h1.size() == h2.size());        for (String key : h1.keySet()) {            Assert.assertTrue(h2.containsKey(key));            String v1 = h1.get(key);            String v2 = h2.remove(key);            Assert.assertEquals(v1, v2);        }        Assert.assertTrue(h2.size() == 0);    }}
public void flume_f1562_1() throws IOException
{    if (provider != null) {        try {            provider.close();        } catch (Exception ex) {                    }    }    provider = null;}
public Integer flume_f1563_1() throws Exception
{        if (events == null) {        return 0;    }    Iterator<MockEvent> it = events.iterator();    while (it.hasNext()) {        MockEvent me = it.next();        Event event = null;        while (event == null) {            event = provider.removeEvent(channel);            if (event == null) {                                try {                    Thread.sleep(MockEventUtils.generateSleepInterval(1000));                } catch (InterruptedException ex) {                    Thread.currentThread().interrupt();                }            } else {                            }        }        BaseJdbcChannelProviderTest.assertEquals(me, event);    }        return events.size();}
public Integer flume_f1564_1() throws Exception
{        if (events == null) {        return 0;    }    Iterator<MockEvent> it = events.iterator();    while (it.hasNext()) {        MockEvent me = it.next();        Assert.assertEquals(channel, me.getChannel());        provider.persistEvent(channel, me);        try {            Thread.sleep(MockEventUtils.generateSleepInterval(1000));        } catch (InterruptedException ex) {            Thread.currentThread().interrupt();        }    }        return events.size();}
public Map<String, String> flume_f1565_0()
{    return headers;}
public void flume_f1566_0(Map<String, String> headers)
{}
public byte[] flume_f1567_0()
{    return payload;}
public void flume_f1568_0(byte[] body)
{}
public String flume_f1569_0()
{    return channel;}
public static byte[] flume_f1570_0(int size)
{    byte[] result = new byte[size];    RANDOM.nextBytes(result);    return result;}
public static String flume_f1571_0(int size)
{    StringBuilder sb = new StringBuilder();    for (int i = 0; i < size; i++) {        int x = Math.abs(RANDOM.nextInt());        int y = x % CHARS.length;        sb.append(CHARS[y]);    }    return sb.toString();}
public static MockEvent flume_f1572_1(int payloadMargin, int headerNameMargin, int headerValueMargin, int numHeaders, int numChannels)
{    int chIndex = 0;    if (numChannels > 1) {        chIndex = Math.abs(RANDOM.nextInt()) % numChannels;    }    String channel = "test-" + chIndex;    StringBuilder sb = new StringBuilder("New Event[payload size:");    int plTh = ConfigurationConstants.PAYLOAD_LENGTH_THRESHOLD;    int plSize = Math.abs(RANDOM.nextInt()) % plTh + payloadMargin;    sb.append(plSize).append(", numHeaders:").append(numHeaders);    sb.append(", channel:").append(channel);    byte[] payload = generatePayload(plSize);    int nmTh = ConfigurationConstants.HEADER_NAME_LENGTH_THRESHOLD;    int vlTh = ConfigurationConstants.HEADER_VALUE_LENGTH_THRESHOLD;    Map<String, String> headers = new HashMap<String, String>();    for (int i = 0; i < numHeaders; i++) {        int nmSize = Math.abs(RANDOM.nextInt()) % nmTh + headerNameMargin;        int vlSize = Math.abs(RANDOM.nextInt()) % vlTh + headerValueMargin;        String name = generateHeaderString(nmSize);        String value = generateHeaderString(vlSize);        headers.put(name, value);        sb.append("{nm:").append(nmSize).append(",vl:").append(vlSize);        sb.append("} ");    }        return new MockEvent(payload, headers, channel);}
public static int flume_f1573_0(int upperBound)
{    return Math.abs(RANDOM.nextInt(upperBound));}
public void flume_f1574_0()
{    enumMap.clear();    enumMap.put(DBTYPE_OTHER, DatabaseType.OTHER);    enumMap.put(DBTYPE_DERBY, DatabaseType.DERBY);    enumMap.put(DBTYPE_MYSQL, DatabaseType.MYSQL);    enumMap.put(DBTYPE_PGSQL, DatabaseType.POSTGRESQL);    enumMap.put(DBTYPE_ORACLE, DatabaseType.ORACLE);}
public void flume_f1575_0()
{    for (String key : enumMap.keySet()) {        DatabaseType type = enumMap.get(key);        DatabaseType lookupType = DatabaseType.valueOf(key);        String lookupTypeName = lookupType.getName();        Assert.assertEquals(lookupTypeName, lookupType.toString());        Assert.assertSame(type, lookupType);        Assert.assertEquals(key, lookupTypeName);        DatabaseType lookupType2 = DatabaseType.getByName(key.toLowerCase(Locale.ENGLISH));        Assert.assertSame(type, lookupType2);    }}
public void flume_f1576_0()
{    String[] invalidTypes = new String[] { "foo", "bar", "abcd" };    for (String key : invalidTypes) {        DatabaseType type = DatabaseType.getByName(key);        Assert.assertSame(type, DatabaseType.OTHER);    }}
public void flume_f1577_0()
{    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_SCHEMA_FLUME, EXPECTED_QUERY_CREATE_SCHEMA_FLUME);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_EVENT, EXPECTED_QUERY_CREATE_TABLE_FL_EVENT);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_INDEX_FLE_CHANNEL, EXPECTED_QUERY_CREATE_INDEX_FLE_CHANNEL);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_PLSPILL_FK, EXPECTED_QUERY_CREATE_TABLE_FL_PLSPILL_FK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_PLSPILL_NOFK, EXPECTED_QUERY_CREATE_TABLE_FL_PLSPILL_NOFK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_INDEX_FLP_EVENT, EXPECTED_QUERY_CREATE_INDEX_FLP_EVENT);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_HEADER_FK, EXPECTED_QUERY_CREATE_TABLE_FL_HEADER_FK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_HEADER_NOFK, EXPECTED_QUERY_CREATE_TABLE_FL_HEADER_NOFK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_INDEX_FLH_EVENT, EXPECTED_QUERY_CREATE_INDEX_FLH_EVENT);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_NMSPILL_FK, EXPECTED_QUERY_CREATE_TABLE_FL_NMSPILL_FK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_NMSPILL_NOFK, EXPECTED_QUERY_CREATE_TABLE_FL_NMSPILL_NOFK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_INDEX_FLN_HEADER, EXPECTED_QUERY_CREATE_INDEX_FLN_HEADER);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_VLSPILL_FK, EXPECTED_QUERY_CREATE_TABLE_FL_VLSPILL_FK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_VLSPILL_NOFK, EXPECTED_QUERY_CREATE_TABLE_FL_VLSPILL_NOFK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_INDEX_FLV_HEADER, EXPECTED_QUERY_CREATE_INDEX_FLV_HEADER);    Assert.assertEquals(DerbySchemaHandler.COLUMN_LOOKUP_QUERY, EXPECTED_COLUMN_LOOKUP_QUERY);    Assert.assertEquals(DerbySchemaHandler.QUERY_CHANNEL_SIZE, EXPECTED_QUERY_CHANNEL_SIZE);    Assert.assertEquals(DerbySchemaHandler.STMT_INSERT_EVENT_BASE, EXPECTED_STMT_INSERT_EVENT_BASE);    Assert.assertEquals(DerbySchemaHandler.STMT_INSERT_EVENT_SPILL, EXPECTED_STMT_INSERT_EVENT_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_INSERT_HEADER_BASE, EXPECTED_STMT_INSERT_HEADER_BASE);    Assert.assertEquals(DerbySchemaHandler.STMT_INSERT_HEADER_NAME_SPILL, EXPECTED_STMT_INSERT_HEADER_NAME_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_INSERT_HEADER_VALUE_SPILL, EXPECTED_STMT_INSERT_HEADER_VALUE_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_FETCH_PAYLOAD_BASE, EXPECTED_STMT_FETCH_PAYLOAD_BASE);    Assert.assertEquals(DerbySchemaHandler.STMT_FETCH_PAYLOAD_SPILL, EXPECTED_STMT_FETCH_PAYLOAD_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_FETCH_HEADER_BASE, EXPECTED_STMT_FETCH_HEADER_BASE);    Assert.assertEquals(DerbySchemaHandler.STMT_FETCH_HEADER_NAME_SPILL, EXPECTED_STMT_FETCH_HEADER_NAME_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_FETCH_HEADER_VALUE_SPILL, EXPECTED_STMT_FETCH_HEADER_VALUE_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_DELETE_HEADER_VALUE_SPILL, EXPECTED_STMT_DELETE_HEADER_VALUE_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_DELETE_HEADER_NAME_SPILL, EXPECTED_STMT_DELETE_HEADER_NAME_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_DELETE_EVENT_SPILL, EXPECTED_STMT_DELETE_EVENT_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_DELETE_HEADER_BASE, EXPECTED_STMT_DELETE_HEADER_BASE);    Assert.assertEquals(DerbySchemaHandler.STMT_DELETE_EVENT_BASE, EXPECTED_STMT_DELETE_EVENT_BASE);}
protected void flume_f1578_0(Context context)
{}
protected void flume_f1579_0(Context context)
{    context.put(ConfigurationConstants.CONFIG_CREATE_FK, "false");}
public void flume_f1580_0()
{    int nameLimit = ConfigurationConstants.HEADER_NAME_LENGTH_THRESHOLD;    int valLimit = ConfigurationConstants.HEADER_VALUE_LENGTH_THRESHOLD;    byte[] s1 = MockEventUtils.generatePayload(1);    runTest(s1, null);    byte[] s2 = MockEventUtils.generatePayload(2);    runTest(s2, new HashMap<String, String>());    int th = ConfigurationConstants.PAYLOAD_LENGTH_THRESHOLD;    byte[] s3 = MockEventUtils.generatePayload(th - 2);    Map<String, String> m3 = new HashMap<String, String>();    m3.put(MockEventUtils.generateHeaderString(1), MockEventUtils.generateHeaderString(1));    runTest(s3, m3);    byte[] s4 = MockEventUtils.generatePayload(th - 1);    Map<String, String> m4 = new HashMap<String, String>();    m4.put(MockEventUtils.generateHeaderString(nameLimit - 21), "w");    m4.put(MockEventUtils.generateHeaderString(nameLimit - 2), "x");    m4.put(MockEventUtils.generateHeaderString(nameLimit - 1), "y");    m4.put(MockEventUtils.generateHeaderString(nameLimit), "z");    m4.put(MockEventUtils.generateHeaderString(nameLimit + 1), "a");    m4.put(MockEventUtils.generateHeaderString(nameLimit + 2), "b");    m4.put(MockEventUtils.generateHeaderString(nameLimit + 21), "c");    runTest(s4, m4);    byte[] s5 = MockEventUtils.generatePayload(th);    Map<String, String> m5 = new HashMap<String, String>();    m5.put("w", MockEventUtils.generateHeaderString(valLimit - 21));    m5.put("x", MockEventUtils.generateHeaderString(valLimit - 2));    m5.put("y", MockEventUtils.generateHeaderString(valLimit - 1));    m5.put("z", MockEventUtils.generateHeaderString(valLimit));    m5.put("a", MockEventUtils.generateHeaderString(valLimit + 1));    m5.put("b", MockEventUtils.generateHeaderString(valLimit + 2));    m5.put("c", MockEventUtils.generateHeaderString(valLimit + 21));    runTest(s5, m5);    byte[] s6 = MockEventUtils.generatePayload(th + 1);    Map<String, String> m6 = new HashMap<String, String>();    m6.put(MockEventUtils.generateHeaderString(nameLimit - 21), MockEventUtils.generateHeaderString(valLimit - 21));    m6.put(MockEventUtils.generateHeaderString(nameLimit - 2), MockEventUtils.generateHeaderString(valLimit - 2));    m6.put(MockEventUtils.generateHeaderString(nameLimit - 1), MockEventUtils.generateHeaderString(valLimit - 1));    m6.put(MockEventUtils.generateHeaderString(nameLimit), MockEventUtils.generateHeaderString(valLimit));    m6.put(MockEventUtils.generateHeaderString(nameLimit + 1), MockEventUtils.generateHeaderString(valLimit + 1));    m6.put(MockEventUtils.generateHeaderString(nameLimit + 2), MockEventUtils.generateHeaderString(valLimit + 2));    m6.put(MockEventUtils.generateHeaderString(nameLimit + 21), MockEventUtils.generateHeaderString(valLimit + 21));    runTest(s6, m6);    byte[] s7 = MockEventUtils.generatePayload(th + 2);    runTest(s7, null);    byte[] s8 = MockEventUtils.generatePayload(th + 27);    runTest(s8, null);}
private void flume_f1581_0(byte[] payload, Map<String, String> headers)
{    PersistableEvent pe = new PersistableEvent("test", new MockEvent(payload, headers, null));    Assert.assertArrayEquals(payload, pe.getBody());    Map<String, String> h = pe.getHeaders();    if (h == null) {        Assert.assertTrue(headers == null || headers.size() == 0);    } else {        Assert.assertTrue(headers.size() == h.size());        for (String key : h.keySet()) {            Assert.assertTrue(headers.containsKey(key));            String value = h.get(key);            String expectedValue = headers.remove(key);            Assert.assertEquals(expectedValue, value);        }        Assert.assertTrue(headers.size() == 0);    }}
public void flume_f1582_0()
{    enumMap.clear();    enumMap.put(TX_READ_UNCOMMITTED, TransactionIsolation.READ_UNCOMMITTED);    enumMap.put(TX_READ_COMMITTED, TransactionIsolation.READ_COMMITTED);    enumMap.put(TX_REPEATABLE_READ, TransactionIsolation.REPEATABLE_READ);    enumMap.put(TX_SERIALIZABLE, TransactionIsolation.SERIALIZABLE);}
public void flume_f1583_0()
{    for (String key : enumMap.keySet()) {        TransactionIsolation txIsolation = enumMap.get(key);        TransactionIsolation lookupTxIsolation = TransactionIsolation.valueOf(key);        String lookupTxIsolationName = lookupTxIsolation.getName();        Assert.assertEquals(lookupTxIsolationName, lookupTxIsolation.toString());        Assert.assertSame(txIsolation, lookupTxIsolation);        Assert.assertEquals(key, lookupTxIsolationName);        TransactionIsolation lookupTxIsolation2 = TransactionIsolation.getByName(key.toLowerCase(Locale.ENGLISH));        Assert.assertSame(txIsolation, lookupTxIsolation2);    }}
public ConsumerAndRecords flume_f1584_0()
{    return createConsumerAndRecords();}
public void flume_f1585_1()
{            if (migrateZookeeperOffsets && zookeeperConnect != null && !zookeeperConnect.isEmpty()) {        migrateOffsets();    }    producer = new KafkaProducer<String, byte[]>(producerProps);            counter.start();    super.start();}
public void flume_f1586_1()
{    for (ConsumerAndRecords c : consumers) {        try {            decommissionConsumerAndRecords(c);        } catch (Exception ex) {                    }    }    producer.close();    counter.stop();    super.stop();    }
protected BasicTransactionSemantics flume_f1587_0()
{    return new KafkaTransaction();}
public void flume_f1588_1(Context ctx)
{        translateOldProps(ctx);    topicStr = ctx.getString(TOPIC_CONFIG);    if (topicStr == null || topicStr.isEmpty()) {        topicStr = DEFAULT_TOPIC;            }    topic.set(topicStr);    groupId = ctx.getString(KAFKA_CONSUMER_PREFIX + ConsumerConfig.GROUP_ID_CONFIG);    if (groupId == null || groupId.isEmpty()) {        groupId = DEFAULT_GROUP_ID;            }    String bootStrapServers = ctx.getString(BOOTSTRAP_SERVERS_CONFIG);    if (bootStrapServers == null || bootStrapServers.isEmpty()) {        throw new ConfigurationException("Bootstrap Servers must be specified");    }    setProducerProps(ctx, bootStrapServers);    setConsumerProps(ctx, bootStrapServers);    parseAsFlumeEvent = ctx.getBoolean(PARSE_AS_FLUME_EVENT, DEFAULT_PARSE_AS_FLUME_EVENT);    pollTimeout = ctx.getLong(POLL_TIMEOUT, DEFAULT_POLL_TIMEOUT);    staticPartitionId = ctx.getInteger(STATIC_PARTITION_CONF);    partitionHeader = ctx.getString(PARTITION_HEADER_NAME);    migrateZookeeperOffsets = ctx.getBoolean(MIGRATE_ZOOKEEPER_OFFSETS, DEFAULT_MIGRATE_ZOOKEEPER_OFFSETS);    zookeeperConnect = ctx.getString(ZOOKEEPER_CONNECT_FLUME_KEY);    if (logger.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {            }    if (counter == null) {        counter = new KafkaChannelCounter(getName());    }}
private void flume_f1589_1(Context ctx)
{    if (!(ctx.containsKey(TOPIC_CONFIG))) {        ctx.put(TOPIC_CONFIG, ctx.getString("topic"));            }        if (!(ctx.containsKey(BOOTSTRAP_SERVERS_CONFIG))) {        String brokerList = ctx.getString(BROKER_LIST_FLUME_KEY);        if (brokerList == null || brokerList.isEmpty()) {            throw new ConfigurationException("Bootstrap Servers must be specified");        } else {            ctx.put(BOOTSTRAP_SERVERS_CONFIG, brokerList);                    }    }        if (!(ctx.containsKey(KAFKA_CONSUMER_PREFIX + ConsumerConfig.GROUP_ID_CONFIG))) {        String oldGroupId = ctx.getString(GROUP_ID_FLUME);        if (oldGroupId != null && !oldGroupId.isEmpty()) {            ctx.put(KAFKA_CONSUMER_PREFIX + ConsumerConfig.GROUP_ID_CONFIG, oldGroupId);                    }    }    if (!(ctx.containsKey((KAFKA_CONSUMER_PREFIX + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG)))) {        Boolean oldReadSmallest = ctx.getBoolean(READ_SMALLEST_OFFSET);        String auto;        if (oldReadSmallest != null) {            if (oldReadSmallest) {                auto = "earliest";            } else {                auto = "latest";            }            ctx.put(KAFKA_CONSUMER_PREFIX + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, auto);                    }    }}
private void flume_f1590_0(Context ctx, String bootStrapServers)
{    producerProps.clear();    producerProps.put(ProducerConfig.ACKS_CONFIG, DEFAULT_ACKS);    producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, DEFAULT_KEY_SERIALIZER);    producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, DEFAULT_VALUE_SERIAIZER);        producerProps.putAll(ctx.getSubProperties(KAFKA_PRODUCER_PREFIX));    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootStrapServers);    KafkaSSLUtil.addGlobalSSLParameters(producerProps);}
protected Properties flume_f1591_0()
{    return producerProps;}
private void flume_f1592_0(Context ctx, String bootStrapServers)
{    consumerProps.clear();    consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, DEFAULT_KEY_DESERIALIZER);    consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, DEFAULT_VALUE_DESERIAIZER);    consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, DEFAULT_AUTO_OFFSET_RESET);        consumerProps.putAll(ctx.getSubProperties(KAFKA_CONSUMER_PREFIX));        consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootStrapServers);    consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);    consumerProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);    KafkaSSLUtil.addGlobalSSLParameters(consumerProps);}
protected Properties flume_f1593_0()
{    return consumerProps;}
private synchronized ConsumerAndRecords flume_f1594_1()
{    try {        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<String, byte[]>(consumerProps);        ConsumerAndRecords car = new ConsumerAndRecords(consumer, channelUUID);                car.consumer.subscribe(Arrays.asList(topic.get()), new ChannelRebalanceListener(rebalanceFlag));        car.offsets = new HashMap<TopicPartition, OffsetAndMetadata>();        consumers.add(car);        return car;    } catch (Exception e) {        throw new FlumeException("Unable to connect to Kafka", e);    }}
private void flume_f1595_1()
{    try (KafkaZkClient zkClient = KafkaZkClient.apply(zookeeperConnect, JaasUtils.isZkSecurityEnabled(), ZK_SESSION_TIMEOUT, ZK_CONNECTION_TIMEOUT, 10, Time.SYSTEM, "kafka.server", "SessionExpireListener");        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<>(consumerProps)) {        Map<TopicPartition, OffsetAndMetadata> kafkaOffsets = getKafkaOffsets(consumer);        if (kafkaOffsets == null) {                        return;        }        if (!kafkaOffsets.isEmpty()) {                                    return;        }                Map<TopicPartition, OffsetAndMetadata> zookeeperOffsets = getZookeeperOffsets(zkClient, consumer);        if (zookeeperOffsets.isEmpty()) {                        return;        }                        consumer.commitSync(zookeeperOffsets);                Map<TopicPartition, OffsetAndMetadata> newKafkaOffsets = getKafkaOffsets(consumer);                if (newKafkaOffsets == null || !newKafkaOffsets.keySet().containsAll(zookeeperOffsets.keySet())) {            throw new FlumeException("Offsets could not be committed");        }    }}
private Map<TopicPartition, OffsetAndMetadata> flume_f1596_0(KafkaConsumer<String, byte[]> client)
{    Map<TopicPartition, OffsetAndMetadata> offsets = null;    List<PartitionInfo> partitions = client.partitionsFor(topicStr);    if (partitions != null) {        offsets = new HashMap<>();        for (PartitionInfo partition : partitions) {            TopicPartition key = new TopicPartition(topicStr, partition.partition());            OffsetAndMetadata offsetAndMetadata = client.committed(key);            if (offsetAndMetadata != null) {                offsets.put(key, offsetAndMetadata);            }        }    }    return offsets;}
private Map<TopicPartition, OffsetAndMetadata> flume_f1597_0(KafkaZkClient zkClient, KafkaConsumer<String, byte[]> consumer)
{    Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();    List<PartitionInfo> partitions = consumer.partitionsFor(topicStr);    for (PartitionInfo partition : partitions) {        TopicPartition topicPartition = new TopicPartition(topicStr, partition.partition());        Option<Object> optionOffset = zkClient.getConsumerOffset(groupId, topicPartition);        if (optionOffset.nonEmpty()) {            Long offset = (Long) optionOffset.get();            OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(offset);            offsets.put(topicPartition, offsetAndMetadata);        }    }    return offsets;}
private void flume_f1598_0(ConsumerAndRecords c)
{    c.consumer.wakeup();    c.consumer.close();}
 void flume_f1599_1()
{    try {        consumerAndRecords.get();    } catch (Exception e) {                e.printStackTrace();    }}
protected void flume_f1600_0() throws InterruptedException
{    rebalanceFlag.set(false);}
protected void flume_f1601_0(Event event) throws InterruptedException
{    type = TransactionType.PUT;    if (!producerRecords.isPresent()) {        producerRecords = Optional.of(new LinkedList<ProducerRecord<String, byte[]>>());    }    String key = event.getHeaders().get(KEY_HEADER);    Integer partitionId = null;    try {        if (staticPartitionId != null) {            partitionId = staticPartitionId;        }                if (partitionHeader != null) {            String headerVal = event.getHeaders().get(partitionHeader);            if (headerVal != null) {                partitionId = Integer.parseInt(headerVal);            }        }        if (partitionId != null) {            producerRecords.get().add(new ProducerRecord<String, byte[]>(topic.get(), partitionId, key, serializeValue(event, parseAsFlumeEvent)));        } else {            producerRecords.get().add(new ProducerRecord<String, byte[]>(topic.get(), key, serializeValue(event, parseAsFlumeEvent)));        }        counter.incrementEventPutAttemptCount();    } catch (NumberFormatException e) {        throw new ChannelException("Non integer partition id specified", e);    } catch (Exception e) {        throw new ChannelException("Error while serializing event", e);    }}
protected Event flume_f1602_1() throws InterruptedException
{    logger.trace("Starting event take");    type = TransactionType.TAKE;    try {        if (!(consumerAndRecords.get().uuid.equals(channelUUID))) {                        decommissionConsumerAndRecords(consumerAndRecords.get());            consumerAndRecords.remove();        }    } catch (Exception ex) {            }    if (!events.isPresent()) {        events = Optional.of(new LinkedList<Event>());    }    Event e;        if (rebalanceFlag.get()) {                return null;    }    if (!consumerAndRecords.get().failedEvents.isEmpty()) {        e = consumerAndRecords.get().failedEvents.removeFirst();    } else {        if (logger.isTraceEnabled()) {            logger.trace("Assignment during take: {}", consumerAndRecords.get().consumer.assignment().toString());        }        try {            long startTime = System.nanoTime();            if (!consumerAndRecords.get().recordIterator.hasNext()) {                consumerAndRecords.get().poll();            }            if (consumerAndRecords.get().recordIterator.hasNext()) {                ConsumerRecord<String, byte[]> record = consumerAndRecords.get().recordIterator.next();                e = deserializeValue(record.value(), parseAsFlumeEvent);                TopicPartition tp = new TopicPartition(record.topic(), record.partition());                OffsetAndMetadata oam = new OffsetAndMetadata(record.offset() + 1, batchUUID);                consumerAndRecords.get().saveOffsets(tp, oam);                                if (record.key() != null) {                    e.getHeaders().put(KEY_HEADER, record.key());                }                long endTime = System.nanoTime();                counter.addToKafkaEventGetTimer((endTime - startTime) / (1000 * 1000));                if (logger.isDebugEnabled()) {                                    }            } else {                return null;            }            counter.incrementEventTakeAttemptCount();        } catch (Exception ex) {                        throw new ChannelException("Error while getting events from Kafka", ex);        }    }    eventTaken = true;    events.get().add(e);    return e;}
protected void flume_f1603_1() throws InterruptedException
{    logger.trace("Starting commit");    if (type.equals(TransactionType.NONE)) {        return;    }    if (type.equals(TransactionType.PUT)) {        if (!kafkaFutures.isPresent()) {            kafkaFutures = Optional.of(new LinkedList<Future<RecordMetadata>>());        }        try {            long batchSize = producerRecords.get().size();            long startTime = System.nanoTime();            int index = 0;            for (ProducerRecord<String, byte[]> record : producerRecords.get()) {                index++;                kafkaFutures.get().add(producer.send(record, new ChannelCallback(index, startTime)));            }                        producer.flush();            for (Future<RecordMetadata> future : kafkaFutures.get()) {                future.get();            }            long endTime = System.nanoTime();            counter.addToKafkaEventSendTimer((endTime - startTime) / (1000 * 1000));            counter.addToEventPutSuccessCount(batchSize);            producerRecords.get().clear();            kafkaFutures.get().clear();        } catch (Exception ex) {                        throw new ChannelException("Commit failed as send to Kafka failed", ex);        }    } else {                if (consumerAndRecords.get().failedEvents.isEmpty() && eventTaken) {            logger.trace("About to commit batch");            long startTime = System.nanoTime();            consumerAndRecords.get().commitOffsets();            long endTime = System.nanoTime();            counter.addToKafkaCommitTimer((endTime - startTime) / (1000 * 1000));            if (logger.isDebugEnabled()) {                            }        }        int takes = events.get().size();        if (takes > 0) {            counter.addToEventTakeSuccessCount(takes);            events.get().clear();        }    }}
protected void flume_f1604_0() throws InterruptedException
{    if (type.equals(TransactionType.NONE)) {        return;    }    if (type.equals(TransactionType.PUT)) {        producerRecords.get().clear();        kafkaFutures.get().clear();    } else {        counter.addToRollbackCounter(events.get().size());        consumerAndRecords.get().failedEvents.addAll(events.get());        events.get().clear();    }}
private byte[] flume_f1605_0(Event event, boolean parseAsFlumeEvent) throws IOException
{    byte[] bytes;    if (parseAsFlumeEvent) {        if (!tempOutStream.isPresent()) {            tempOutStream = Optional.of(new ByteArrayOutputStream());        }        if (!writer.isPresent()) {            writer = Optional.of(new SpecificDatumWriter<AvroFlumeEvent>(AvroFlumeEvent.class));        }        tempOutStream.get().reset();        AvroFlumeEvent e = new AvroFlumeEvent(toCharSeqMap(event.getHeaders()), ByteBuffer.wrap(event.getBody()));        encoder = EncoderFactory.get().directBinaryEncoder(tempOutStream.get(), encoder);        writer.get().write(e, encoder);        encoder.flush();        bytes = tempOutStream.get().toByteArray();    } else {        bytes = event.getBody();    }    return bytes;}
private Event flume_f1606_0(byte[] value, boolean parseAsFlumeEvent) throws IOException
{    Event e;    if (parseAsFlumeEvent) {        ByteArrayInputStream in = new ByteArrayInputStream(value);        decoder = DecoderFactory.get().directBinaryDecoder(in, decoder);        if (!reader.isPresent()) {            reader = Optional.of(new SpecificDatumReader<AvroFlumeEvent>(AvroFlumeEvent.class));        }        AvroFlumeEvent event = reader.get().read(null, decoder);        e = EventBuilder.withBody(event.getBody().array(), toStringMap(event.getHeaders()));    } else {        e = EventBuilder.withBody(value, Collections.EMPTY_MAP);    }    return e;}
private static Map<CharSequence, CharSequence> flume_f1607_0(Map<String, String> stringMap)
{    Map<CharSequence, CharSequence> charSeqMap = new HashMap<CharSequence, CharSequence>();    for (Map.Entry<String, String> entry : stringMap.entrySet()) {        charSeqMap.put(entry.getKey(), entry.getValue());    }    return charSeqMap;}
private static Map<String, String> flume_f1608_0(Map<CharSequence, CharSequence> charSeqMap)
{    Map<String, String> stringMap = new HashMap<String, String>();    for (Map.Entry<CharSequence, CharSequence> entry : charSeqMap.entrySet()) {        stringMap.put(entry.getKey().toString(), entry.getValue().toString());    }    return stringMap;}
private void flume_f1609_1()
{    logger.trace("Polling with timeout: {}ms channel-{}", pollTimeout, getName());    try {        records = consumer.poll(Duration.ofMillis(pollTimeout));        recordIterator = records.iterator();            } catch (WakeupException e) {        logger.trace("Consumer woken up for channel {}.", getName());    }}
private void flume_f1610_1()
{    try {        consumer.commitSync(offsets);    } catch (Exception e) {            } finally {        logger.trace("About to clear offsets map.");        offsets.clear();    }}
private String flume_f1611_0()
{    StringBuilder sb = new StringBuilder();    sb.append(getName()).append(" current offsets map: ");    for (TopicPartition tp : offsets.keySet()) {        sb.append("p").append(tp.partition()).append("-").append(offsets.get(tp).offset()).append(" ");    }    return sb.toString();}
private String flume_f1612_1()
{    StringBuilder sb = new StringBuilder();    sb.append(getName()).append(" committed: ");    for (TopicPartition tp : consumer.assignment()) {        try {            sb.append("[").append(tp).append(",").append(consumer.committed(tp).offset()).append("] ");        } catch (NullPointerException npe) {                    }    }    return sb.toString();}
private void flume_f1613_0(TopicPartition tp, OffsetAndMetadata oam)
{    offsets.put(tp, oam);    if (logger.isTraceEnabled()) {        logger.trace(getOffsetMapString());    }}
public void flume_f1614_1(RecordMetadata metadata, Exception exception)
{    if (exception != null) {        log.trace("Error sending message to Kafka due to " + exception.getMessage());    }    if (log.isDebugEnabled()) {        long batchElapsedTime = System.currentTimeMillis() - startTime;        if (metadata != null) {                    }    }}
public void flume_f1615_1(Collection<TopicPartition> partitions)
{    for (TopicPartition partition : partitions) {                rebalanceFlag.set(true);    }}
public void flume_f1616_1(Collection<TopicPartition> partitions)
{    for (TopicPartition partition : partitions) {            }}
public void flume_f1617_0() throws Exception
{    Context context = new Context();    context.put("kafka.producer.some-parameter", "1");    context.put("kafka.consumer.another-parameter", "1");    context.put(BOOTSTRAP_SERVERS_CONFIG, testUtil.getKafkaServerUrl());    context.put(TOPIC_CONFIG, topic);    final KafkaChannel channel = new KafkaChannel();    Configurables.configure(channel, context);    Properties consumerProps = channel.getConsumerProps();    Properties producerProps = channel.getProducerProps();    Assert.assertEquals(producerProps.getProperty("some-parameter"), "1");    Assert.assertEquals(consumerProps.getProperty("another-parameter"), "1");}
public void flume_f1618_0() throws Exception
{    Context context = new Context();    context.put(BROKER_LIST_FLUME_KEY, testUtil.getKafkaServerUrl());    context.put(GROUP_ID_FLUME, "flume-something");    context.put(READ_SMALLEST_OFFSET, "true");    context.put("topic", topic);    final KafkaChannel channel = new KafkaChannel();    Configurables.configure(channel, context);    Properties consumerProps = channel.getConsumerProps();    Properties producerProps = channel.getProducerProps();    Assert.assertEquals(producerProps.getProperty(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG), testUtil.getKafkaServerUrl());    Assert.assertEquals(consumerProps.getProperty(ConsumerConfig.GROUP_ID_CONFIG), "flume-something");    Assert.assertEquals(consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), "earliest");}
public void flume_f1619_0() throws Exception
{    doTestStopAndStart(false, false);}
public void flume_f1620_0() throws Exception
{    doTestStopAndStart(true, true);}
public void flume_f1621_0() throws Exception
{    doTestStopAndStart(true, false);}
public void flume_f1622_0() throws Exception
{    doTestNullKeyNoHeader();}
public void flume_f1623_0() throws Exception
{    String sampleProducerProp = "compression.type";    String sampleProducerVal = "snappy";    String sampleConsumerProp = "fetch.min.bytes";    String sampleConsumerVal = "99";    Context context = prepareDefaultContext(false);    context.put(KafkaChannelConfiguration.KAFKA_PRODUCER_PREFIX + sampleProducerProp, sampleProducerVal);    context.put(KafkaChannelConfiguration.KAFKA_CONSUMER_PREFIX + sampleConsumerProp, sampleConsumerVal);    final KafkaChannel channel = createChannel(context);    Assert.assertEquals(sampleProducerVal, channel.getProducerProps().getProperty(sampleProducerProp));    Assert.assertEquals(sampleConsumerVal, channel.getConsumerProps().getProperty(sampleConsumerProp));    context = prepareDefaultContext(false);    channel.configure(context);    Assert.assertNull(channel.getProducerProps().getProperty(sampleProducerProp));    Assert.assertNull(channel.getConsumerProps().getProperty(sampleConsumerProp));}
private void flume_f1624_0() throws Exception
{    final KafkaChannel channel = startChannel(false);    Properties props = channel.getProducerProps();    KafkaProducer<String, byte[]> producer = new KafkaProducer<>(props);    for (int i = 0; i < 50; i++) {        ProducerRecord<String, byte[]> data = new ProducerRecord<>(topic, null, String.valueOf(i).getBytes());        producer.send(data).get();    }    ExecutorCompletionService<Void> submitterSvc = new ExecutorCompletionService<>(Executors.newCachedThreadPool());    List<Event> events = pullEvents(channel, submitterSvc, 50, false, false);    wait(submitterSvc, 5);    List<String> finals = new ArrayList<>(50);    for (int i = 0; i < 50; i++) {        finals.add(i, events.get(i).getHeaders().get(KEY_HEADER));    }    for (int i = 0; i < 50; i++) {        Assert.assertTrue(finals.get(i) == null);    }    channel.stop();}
private void flume_f1625_0(boolean rollback, boolean retryAfterRollback) throws Exception
{    final KafkaChannel channel = startChannel(true);    ExecutorService underlying = Executors.newCachedThreadPool();    ExecutorCompletionService<Void> submitterSvc = new ExecutorCompletionService<>(underlying);    final List<List<Event>> events = createBaseList();    putEvents(channel, events, submitterSvc);    wait(submitterSvc, 5);    channel.stop();    final KafkaChannel channel2 = startChannel(true);    int total = 50;    if (rollback && !retryAfterRollback) {        total = 40;    }    final List<Event> eventsPulled = pullEvents(channel2, submitterSvc, total, rollback, retryAfterRollback);    wait(submitterSvc, 5);    channel2.stop();    if (!retryAfterRollback && rollback) {        final KafkaChannel channel3 = startChannel(true);        int expectedRemaining = 50 - eventsPulled.size();        final List<Event> eventsPulled2 = pullEvents(channel3, submitterSvc, expectedRemaining, false, false);        wait(submitterSvc, 5);        Assert.assertEquals(expectedRemaining, eventsPulled2.size());        eventsPulled.addAll(eventsPulled2);        channel3.stop();    }    underlying.shutdownNow();    verify(eventsPulled);}
public void flume_f1626_0() throws Exception
{    final KafkaChannel channel = startChannel(true);    ExecutorService underlying = Executors.newCachedThreadPool();    ExecutorCompletionService<Void> submitterSvc = new ExecutorCompletionService<Void>(underlying);    final List<List<Event>> events = createBaseList();    putEvents(channel, events, submitterSvc);    takeEventsWithCommittingTxn(channel, 50);    KafkaChannelCounter counter = (KafkaChannelCounter) Whitebox.getInternalState(channel, "counter");    Assert.assertEquals(50, counter.getEventPutAttemptCount());    Assert.assertEquals(50, counter.getEventPutSuccessCount());    Assert.assertEquals(50, counter.getEventTakeAttemptCount());    Assert.assertEquals(50, counter.getEventTakeSuccessCount());    channel.stop();}
private void flume_f1627_0(KafkaChannel channel, long eventsCount)
{    List<Event> takeEventsList = new ArrayList<>();    Transaction txn = channel.getTransaction();    txn.begin();    while (takeEventsList.size() < eventsCount) {        Event event = channel.take();        if (event != null) {            takeEventsList.add(event);        }    }    txn.commit();    txn.close();}
public static void flume_f1628_0() throws Exception
{    testUtil.prepare();    Thread.sleep(2500);}
public void flume_f1629_0() throws Exception
{    topic = findUnusedTopic();    createTopic(topic, DEFAULT_TOPIC_PARTITIONS);    Thread.sleep(2500);}
public static void flume_f1630_0()
{    testUtil.tearDown();}
 String flume_f1631_0()
{    String newTopic = null;    boolean topicFound = false;    while (!topicFound) {        newTopic = RandomStringUtils.randomAlphabetic(8);        if (!usedTopics.contains(newTopic)) {            usedTopics.add(newTopic);            topicFound = true;        }    }    return newTopic;}
 static void flume_f1632_0(String topicName, int numPartitions)
{    testUtil.createTopics(Collections.singletonList(topicName), numPartitions);}
 static void flume_f1633_0(String topicName)
{    testUtil.deleteTopic(topicName);}
 KafkaChannel flume_f1634_0(boolean parseAsFlume) throws Exception
{    Context context = prepareDefaultContext(parseAsFlume);    KafkaChannel channel = createChannel(context);    channel.start();    return channel;}
 Context flume_f1635_0(boolean parseAsFlume)
{        Context context = new Context();    context.put(BOOTSTRAP_SERVERS_CONFIG, testUtil.getKafkaServerUrl());    context.put(PARSE_AS_FLUME_EVENT, String.valueOf(parseAsFlume));    context.put(TOPIC_CONFIG, topic);    context.put(KAFKA_CONSUMER_PREFIX + "max.poll.interval.ms", "10000");    return context;}
 KafkaChannel flume_f1636_0(Context context) throws Exception
{    final KafkaChannel channel = new KafkaChannel();    Configurables.configure(channel, context);    return channel;}
 List<Event> flume_f1637_0(final KafkaChannel channel, ExecutorCompletionService<Void> submitterSvc, final int total, final boolean testRollbacks, final boolean retryAfterRollback)
{    final List<Event> eventsPulled = Collections.synchronizedList(new ArrayList<Event>(50));    final CyclicBarrier barrier = new CyclicBarrier(5);    final AtomicInteger counter = new AtomicInteger(0);    final AtomicInteger rolledBackCount = new AtomicInteger(0);    final AtomicBoolean startedGettingEvents = new AtomicBoolean(false);    final AtomicBoolean rolledBack = new AtomicBoolean(false);    for (int k = 0; k < 5; k++) {        final int index = k;        submitterSvc.submit(new Callable<Void>() {            @Override            public Void call() throws Exception {                Transaction tx = null;                final List<Event> eventsLocal = Lists.newLinkedList();                channel.registerThread();                Thread.sleep(1000);                barrier.await();                while (counter.get() < (total - rolledBackCount.get())) {                    if (tx == null) {                        tx = channel.getTransaction();                        tx.begin();                    }                    try {                        Event e = channel.take();                        if (e != null) {                            startedGettingEvents.set(true);                            eventsLocal.add(e);                        } else {                            if (testRollbacks && index == 4 && (!rolledBack.get()) && startedGettingEvents.get()) {                                tx.rollback();                                tx.close();                                tx = null;                                rolledBack.set(true);                                final int eventsLocalSize = eventsLocal.size();                                eventsLocal.clear();                                if (!retryAfterRollback) {                                    rolledBackCount.set(eventsLocalSize);                                    return null;                                }                            } else {                                tx.commit();                                tx.close();                                tx = null;                                eventsPulled.addAll(eventsLocal);                                counter.getAndAdd(eventsLocal.size());                                eventsLocal.clear();                            }                        }                    } catch (Exception ex) {                        eventsLocal.clear();                        if (tx != null) {                            tx.rollback();                            tx.close();                        }                        tx = null;                        ex.printStackTrace();                    }                }                                return null;            }        });    }    return eventsPulled;}
public Void flume_f1638_0() throws Exception
{    Transaction tx = null;    final List<Event> eventsLocal = Lists.newLinkedList();    channel.registerThread();    Thread.sleep(1000);    barrier.await();    while (counter.get() < (total - rolledBackCount.get())) {        if (tx == null) {            tx = channel.getTransaction();            tx.begin();        }        try {            Event e = channel.take();            if (e != null) {                startedGettingEvents.set(true);                eventsLocal.add(e);            } else {                if (testRollbacks && index == 4 && (!rolledBack.get()) && startedGettingEvents.get()) {                    tx.rollback();                    tx.close();                    tx = null;                    rolledBack.set(true);                    final int eventsLocalSize = eventsLocal.size();                    eventsLocal.clear();                    if (!retryAfterRollback) {                        rolledBackCount.set(eventsLocalSize);                        return null;                    }                } else {                    tx.commit();                    tx.close();                    tx = null;                    eventsPulled.addAll(eventsLocal);                    counter.getAndAdd(eventsLocal.size());                    eventsLocal.clear();                }            }        } catch (Exception ex) {            eventsLocal.clear();            if (tx != null) {                tx.rollback();                tx.close();            }            tx = null;            ex.printStackTrace();        }    }        return null;}
 void flume_f1639_0(ExecutorCompletionService<Void> submitterSvc, int max) throws Exception
{    int completed = 0;    while (completed < max) {        submitterSvc.take();        completed++;    }}
 List<List<Event>> flume_f1640_0()
{    final List<List<Event>> events = new ArrayList<>();    for (int i = 0; i < 5; i++) {        List<Event> eventList = new ArrayList<>(10);        events.add(eventList);        for (int j = 0; j < 10; j++) {            Map<String, String> hdrs = new HashMap<>();            String v = (String.valueOf(i) + " - " + String.valueOf(j));            hdrs.put("header", v);            eventList.add(EventBuilder.withBody(v.getBytes(), hdrs));        }    }    return events;}
 void flume_f1641_0(final KafkaChannel channel, final List<List<Event>> events, ExecutorCompletionService<Void> submitterSvc)
{    for (int i = 0; i < 5; i++) {        final int index = i;        submitterSvc.submit(new Callable<Void>() {            @Override            public Void call() {                Transaction tx = channel.getTransaction();                tx.begin();                List<Event> eventsToPut = events.get(index);                for (int j = 0; j < 10; j++) {                    channel.put(eventsToPut.get(j));                }                try {                    tx.commit();                } finally {                    tx.close();                }                return null;            }        });    }}
public Void flume_f1642_0()
{    Transaction tx = channel.getTransaction();    tx.begin();    List<Event> eventsToPut = events.get(index);    for (int j = 0; j < 10; j++) {        channel.put(eventsToPut.get(j));    }    try {        tx.commit();    } finally {        tx.close();    }    return null;}
 void flume_f1643_0(List<Event> eventsPulled)
{    Assert.assertFalse(eventsPulled.isEmpty());    Assert.assertEquals(50, eventsPulled.size());    Set<String> eventStrings = new HashSet<>();    for (Event e : eventsPulled) {        Assert.assertEquals(e.getHeaders().get("header"), new String(e.getBody()));        eventStrings.add(e.getHeaders().get("header"));    }    for (int i = 0; i < 5; i++) {        for (int j = 0; j < 10; j++) {            String v = String.valueOf(i) + " - " + String.valueOf(j);            Assert.assertTrue(eventStrings.contains(v));            eventStrings.remove(v);        }    }    Assert.assertTrue(eventStrings.isEmpty());}
public void flume_f1644_0() throws Exception
{    String message = "testOffsetsNotCommittedOnStop-" + System.nanoTime();    KafkaChannel channel = startChannel(false);    KafkaProducer<String, byte[]> producer = new KafkaProducer<>(channel.getProducerProps());    ProducerRecord<String, byte[]> data = new ProducerRecord<>(topic, "header-" + message, message.getBytes());    producer.send(data).get();    producer.flush();    producer.close();    Event event = takeEventWithoutCommittingTxn(channel);    Assert.assertNotNull(event);    Assert.assertTrue(Arrays.equals(message.getBytes(), event.getBody()));        channel.stop();    channel = startChannel(false);        event = takeEventWithoutCommittingTxn(channel);    Assert.assertNotNull(event);    Assert.assertTrue(Arrays.equals(message.getBytes(), event.getBody()));}
public void flume_f1645_0() throws Exception
{    doTestMigrateZookeeperOffsets(false, false, "testMigrateOffsets-none");}
public void flume_f1646_0() throws Exception
{    doTestMigrateZookeeperOffsets(true, false, "testMigrateOffsets-zookeeper");}
public void flume_f1647_0() throws Exception
{    doTestMigrateZookeeperOffsets(false, true, "testMigrateOffsets-kafka");}
public void flume_f1648_0() throws Exception
{    doTestMigrateZookeeperOffsets(true, true, "testMigrateOffsets-both");}
private Event flume_f1649_0(KafkaChannel channel)
{    for (int i = 0; i < 10; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        Event event = channel.take();        if (event != null) {            return event;        } else {            txn.commit();            txn.close();        }    }    return null;}
private void flume_f1650_0(boolean hasZookeeperOffsets, boolean hasKafkaOffsets, String group) throws Exception
{        topic = findUnusedTopic();    createTopic(topic, 1);    Context context = prepareDefaultContext(false);    context.put(ZOOKEEPER_CONNECT_FLUME_KEY, testUtil.getZkUrl());    context.put(GROUP_ID_FLUME, group);    final KafkaChannel channel = createChannel(context);        Long fifthOffset = 0L;    Long tenthOffset = 0L;    Properties props = channel.getProducerProps();    KafkaProducer<String, byte[]> producer = new KafkaProducer<>(props);    for (int i = 1; i <= 50; i++) {        ProducerRecord<String, byte[]> data = new ProducerRecord<>(topic, null, String.valueOf(i).getBytes());        RecordMetadata recordMetadata = producer.send(data).get();        if (i == 5) {            fifthOffset = recordMetadata.offset();        }        if (i == 10) {            tenthOffset = recordMetadata.offset();        }    }        if (hasZookeeperOffsets) {        KafkaZkClient zkClient = KafkaZkClient.apply(testUtil.getZkUrl(), JaasUtils.isZkSecurityEnabled(), 30000, 30000, 10, Time.SYSTEM, "kafka.server", "SessionExpireListener");        zkClient.getConsumerOffset(group, new TopicPartition(topic, 0));        Long offset = tenthOffset + 1;        zkClient.setOrCreateConsumerOffset(group, new TopicPartition(topic, 0), offset);        zkClient.close();    }        if (hasKafkaOffsets) {        Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();        offsets.put(new TopicPartition(topic, 0), new OffsetAndMetadata(fifthOffset + 1));        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<>(channel.getConsumerProps());        consumer.commitSync(offsets);        consumer.close();    }        channel.start();    ExecutorCompletionService<Void> submitterSvc = new ExecutorCompletionService<>(Executors.newCachedThreadPool());    List<Event> events = pullEvents(channel, submitterSvc, 20, false, false);    wait(submitterSvc, 5);    List<Integer> finals = new ArrayList<>(40);    for (Event event : events) {        finals.add(Integer.parseInt(new String(event.getBody())));    }    channel.stop();    if (!hasKafkaOffsets && !hasZookeeperOffsets) {                Assert.assertTrue("Channel should read the the first message", finals.contains(1));    } else if (hasKafkaOffsets && hasZookeeperOffsets) {                Assert.assertFalse("Channel should not read the 5th message", finals.contains(5));        Assert.assertTrue("Channel should read the 6th message", finals.contains(6));    } else if (hasKafkaOffsets) {                Assert.assertFalse("Channel should not read the 5th message", finals.contains(5));        Assert.assertTrue("Channel should read the 6th message", finals.contains(6));    } else {                Assert.assertFalse("Channel should not read the 10th message", finals.contains(10));        Assert.assertTrue("Channel should read the 11th message", finals.contains(11));    }}
public void flume_f1651_0() throws Exception
{    topic = findUnusedTopic();    Context context = prepareDefaultContext(false);    context.put(ZOOKEEPER_CONNECT_FLUME_KEY, testUtil.getZkUrl());    context.put(GROUP_ID_FLUME, "testMigrateOffsets-nonExistingTopic");    KafkaChannel channel = createChannel(context);    channel.start();    Assert.assertEquals(LifecycleState.START, channel.getLifecycleState());    channel.stop();}
public void flume_f1652_0() throws Exception
{    doParseAsFlumeEventFalse(false);}
public void flume_f1653_0() throws Exception
{    doParseAsFlumeEventFalse(true);}
public void flume_f1654_0() throws Exception
{    doParseAsFlumeEventFalseAsSource(false);}
public void flume_f1655_0() throws Exception
{    doParseAsFlumeEventFalseAsSource(true);}
private void flume_f1656_0(Boolean checkHeaders) throws Exception
{    final KafkaChannel channel = startChannel(false);    Properties props = channel.getProducerProps();    KafkaProducer<String, byte[]> producer = new KafkaProducer<>(props);    for (int i = 0; i < 50; i++) {        ProducerRecord<String, byte[]> data = new ProducerRecord<>(topic, String.valueOf(i) + "-header", String.valueOf(i).getBytes());        producer.send(data).get();    }    ExecutorCompletionService<Void> submitterSvc = new ExecutorCompletionService<>(Executors.newCachedThreadPool());    List<Event> events = pullEvents(channel, submitterSvc, 50, false, false);    wait(submitterSvc, 5);    Map<Integer, String> finals = new HashMap<>();    for (int i = 0; i < 50; i++) {        finals.put(Integer.parseInt(new String(events.get(i).getBody())), events.get(i).getHeaders().get(KEY_HEADER));    }    for (int i = 0; i < 50; i++) {        Assert.assertTrue(finals.keySet().contains(i));        if (checkHeaders) {            Assert.assertTrue(finals.containsValue(String.valueOf(i) + "-header"));        }        finals.remove(i);    }    Assert.assertTrue(finals.isEmpty());    channel.stop();}
private void flume_f1657_0(Boolean checkHeaders) throws Exception
{    final KafkaChannel channel = startChannel(false);    List<String> msgs = new ArrayList<>();    Map<String, String> headers = new HashMap<>();    for (int i = 0; i < 50; i++) {        msgs.add(String.valueOf(i));    }    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < msgs.size(); i++) {        headers.put(KEY_HEADER, String.valueOf(i) + "-header");        channel.put(EventBuilder.withBody(msgs.get(i).getBytes(), headers));    }    tx.commit();    ExecutorCompletionService<Void> submitterSvc = new ExecutorCompletionService<>(Executors.newCachedThreadPool());    List<Event> events = pullEvents(channel, submitterSvc, 50, false, false);    wait(submitterSvc, 5);    Map<Integer, String> finals = new HashMap<>();    for (int i = 0; i < 50; i++) {        finals.put(Integer.parseInt(new String(events.get(i).getBody())), events.get(i).getHeaders().get(KEY_HEADER));    }    for (int i = 0; i < 50; i++) {        Assert.assertTrue(finals.keySet().contains(i));        if (checkHeaders) {            Assert.assertTrue(finals.containsValue(String.valueOf(i) + "-header"));        }        finals.remove(i);    }    Assert.assertTrue(finals.isEmpty());    channel.stop();}
public void flume_f1658_0() throws Exception
{    doPartitionHeader(PartitionTestScenario.PARTITION_ID_HEADER_ONLY);}
public void flume_f1659_0() throws Exception
{    doPartitionHeader(PartitionTestScenario.NO_PARTITION_HEADERS);}
public void flume_f1660_0() throws Exception
{    doPartitionHeader(PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID);}
public void flume_f1661_0() throws Exception
{    doPartitionHeader(PartitionTestScenario.STATIC_HEADER_ONLY);}
public void flume_f1662_0() throws Exception
{    doPartitionErrors(PartitionOption.NOTSET);}
public void flume_f1663_0() throws Exception
{    doPartitionErrors(PartitionOption.VALIDBUTOUTOFRANGE);}
public void flume_f1664_0() throws Exception
{    doPartitionErrors(PartitionOption.NOTANUMBER);}
private void flume_f1665_0(PartitionTestScenario scenario) throws Exception
{    final int numPtns = DEFAULT_TOPIC_PARTITIONS;    final int numMsgs = numPtns * 10;    final Integer staticPtn = DEFAULT_TOPIC_PARTITIONS - 2;    Context context = prepareDefaultContext(false);    if (scenario == PartitionTestScenario.PARTITION_ID_HEADER_ONLY || scenario == PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID) {        context.put(PARTITION_HEADER_NAME, "partition-header");    }    if (scenario == PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID || scenario == PartitionTestScenario.STATIC_HEADER_ONLY) {        context.put(STATIC_PARTITION_CONF, staticPtn.toString());    }    final KafkaChannel channel = createChannel(context);    channel.start();            Map<Integer, List<Event>> partitionMap = new HashMap<>(numPtns);    for (int i = 0; i < numPtns; i++) {        partitionMap.put(i, new ArrayList<Event>());    }    Transaction tx = channel.getTransaction();    tx.begin();    List<Event> orderedEvents = KafkaPartitionTestUtil.generateSkewedMessageList(scenario, numMsgs, partitionMap, numPtns, staticPtn);    for (Event event : orderedEvents) {        channel.put(event);    }    tx.commit();    Map<Integer, List<byte[]>> resultsMap = KafkaPartitionTestUtil.retrieveRecordsFromPartitions(topic, numPtns, channel.getConsumerProps());    KafkaPartitionTestUtil.checkResultsAgainstSkew(scenario, partitionMap, resultsMap, staticPtn, numMsgs);    channel.stop();}
private void flume_f1666_0(PartitionOption option) throws Exception
{    Context context = prepareDefaultContext(false);    context.put(PARTITION_HEADER_NAME, KafkaPartitionTestUtil.PARTITION_HEADER);    String tempTopic = findUnusedTopic();    createTopic(tempTopic, 5);    final KafkaChannel channel = createChannel(context);    channel.start();    Transaction tx = channel.getTransaction();    tx.begin();    Map<String, String> headers = new HashMap<>();    switch(option) {        case VALIDBUTOUTOFRANGE:            headers.put(KafkaPartitionTestUtil.PARTITION_HEADER, String.valueOf(DEFAULT_TOPIC_PARTITIONS + 2));            break;        case NOTSET:            headers.put("wrong-header", "2");            break;        case NOTANUMBER:            headers.put(KafkaPartitionTestUtil.PARTITION_HEADER, "not-a-number");            break;        default:            break;    }    Event event = EventBuilder.withBody(String.valueOf(9).getBytes(), headers);    channel.put(event);    tx.commit();    deleteTopic(tempTopic);}
public void flume_f1667_0() throws Exception
{    doTestSuccessRollback(false, false);}
public void flume_f1668_0() throws Exception
{    doTestSuccessRollback(false, true);}
public void flume_f1669_0() throws Exception
{    doTestSuccessRollback(true, false);}
public void flume_f1670_0() throws Exception
{    doTestSuccessRollback(true, true);}
private void flume_f1671_0(final boolean rollback, final boolean interleave) throws Exception
{    final KafkaChannel channel = startChannel(true);    writeAndVerify(rollback, channel, interleave);    channel.stop();}
private void flume_f1672_0(final boolean testRollbacks, final KafkaChannel channel, final boolean interleave) throws Exception
{    final List<List<Event>> events = createBaseList();    ExecutorCompletionService<Void> submitterSvc = new ExecutorCompletionService<Void>(Executors.newCachedThreadPool());    putEvents(channel, events, submitterSvc);    if (interleave) {        wait(submitterSvc, 5);    }    ExecutorCompletionService<Void> submitterSvc2 = new ExecutorCompletionService<Void>(Executors.newCachedThreadPool());    final List<Event> eventsPulled = pullEvents(channel, submitterSvc2, 50, testRollbacks, true);    if (!interleave) {        wait(submitterSvc, 5);    }    wait(submitterSvc2, 5);    verify(eventsPulled);}
protected int flume_f1673_0()
{    return totalStored.availablePermits();}
public int flume_f1674_0()
{    return memoryCapacity;}
public int flume_f1675_0()
{    return overflowTimeout;}
public int flume_f1676_0()
{    return maxMemQueueSize;}
protected Integer flume_f1677_0()
{    return overflowCapacity;}
protected boolean flume_f1678_0()
{    return overflowDisabled;}
public int flume_f1679_0()
{    synchronized (queueLock) {        return memQueue.size();    }}
public void flume_f1680_0(int amount)
{    value += amount;}
public int flume_f1681_0()
{    return value;}
public String flume_f1682_0()
{    StringBuilder sb = new StringBuilder();    sb.append("  [ ");    for (MutableInteger i : queue) {        sb.append(i.intValue());        sb.append(" ");    }    sb.append("]");    return sb.toString();}
public void flume_f1683_0(Integer eventCount)
{    totalPuts += eventCount;    if ((queue.peekLast() == null) || queue.getLast().intValue() < 0) {        queue.addLast(new MutableInteger(eventCount));    } else {        queue.getLast().add(eventCount);    }}
public void flume_f1684_0(Integer eventCount)
{    if ((queue.peekFirst() == null) || queue.getFirst().intValue() < 0) {        queue.addFirst(new MutableInteger(eventCount));    } else {        queue.getFirst().add(eventCount);    }}
public void flume_f1685_0(Integer eventCount)
{    totalPuts += eventCount;    if ((queue.peekLast() == null) || queue.getLast().intValue() > 0) {        queue.addLast(new MutableInteger(-eventCount));    } else {        queue.getLast().add(-eventCount);    }    overflowCounter += eventCount;}
public void flume_f1686_0(Integer eventCount)
{    if ((queue.peekFirst() == null) || queue.getFirst().intValue() > 0) {        queue.addFirst(new MutableInteger(-eventCount));    } else {        queue.getFirst().add(-eventCount);    }    overflowCounter += eventCount;}
public int flume_f1687_0()
{    return queue.getFirst().intValue();}
public boolean flume_f1688_0()
{    return queue.isEmpty();}
public void flume_f1689_0(int takeCount)
{    MutableInteger headValue = queue.getFirst();        if (headValue.intValue() < takeCount) {        throw new IllegalStateException("Cannot take " + takeCount + " from " + headValue.intValue() + " in DrainOrder Queue");    }    headValue.add(-takeCount);    if (headValue.intValue() == 0) {        queue.removeFirst();    }}
public void flume_f1690_0(int takeCount)
{    MutableInteger headValue = queue.getFirst();    if (headValue.intValue() > -takeCount) {        throw new IllegalStateException("Cannot take " + takeCount + " from " + headValue.intValue() + " in DrainOrder Queue head ");    }    headValue.add(takeCount);    if (headValue.intValue() == 0) {        queue.removeFirst();    }    overflowCounter -= takeCount;}
public void flume_f1691_0()
{    super.begin();}
public void flume_f1692_0()
{    if (overflowTakeTx != null) {        overflowTakeTx.close();    }    if (overflowPutTx != null) {        overflowPutTx.close();    }    super.close();}
protected void flume_f1693_0(Event event) throws InterruptedException
{    channelCounter.incrementEventPutAttemptCount();    putCalled = true;    int eventByteSize = (int) Math.ceil(estimateEventSize(event) / avgEventSize);    if (!putList.offer(event)) {        throw new ChannelFullException("Put queue in " + getName() + " channel's Transaction having capacity " + putList.size() + " full, consider reducing batch size of sources");    }    putListByteCount += eventByteSize;}
protected Event flume_f1694_1() throws InterruptedException
{    channelCounter.incrementEventTakeAttemptCount();    if (!totalStored.tryAcquire(overflowTimeout, TimeUnit.SECONDS)) {                return null;    }    boolean takeSuceeded = false;    try {        Event event;        synchronized (queueLock) {            int drainOrderTop = drainOrder.front();            if (!takeCalled) {                takeCalled = true;                if (drainOrderTop < 0) {                    useOverflow = true;                    overflowTakeTx = getOverflowTx();                    overflowTakeTx.begin();                }            }            if (useOverflow) {                if (drainOrderTop > 0) {                                                            return null;                }                event = overflowTakeTx.take();                ++takeCount;                drainOrder.takeOverflow(1);            } else {                if (drainOrderTop < 0) {                                                            return null;                }                event = memQueue.poll();                ++takeCount;                drainOrder.takePrimary(1);                Preconditions.checkNotNull(event, "Queue.poll returned NULL despite" + " semaphore signalling existence of entry");            }        }        int eventByteSize = (int) Math.ceil(estimateEventSize(event) / avgEventSize);        if (!useOverflow) {                        takeList.offer(event);        }        takeListByteCount += eventByteSize;        takeSuceeded = true;        return event;    } finally {        if (!takeSuceeded) {            totalStored.release();        }    }}
protected void flume_f1695_1() throws InterruptedException
{    if (putCalled) {        putCommit();        if (LOGGER.isDebugEnabled()) {                    }    } else if (takeCalled) {        takeCommit();        if (LOGGER.isDebugEnabled()) {                    }    }}
private void flume_f1696_1()
{    if (takeCount > largestTakeTxSize) {        largestTakeTxSize = takeCount;    }    synchronized (queueLock) {        if (overflowTakeTx != null) {            overflowTakeTx.commit();        }        double memoryPercentFree = (memoryCapacity == 0) ? 0 : (memoryCapacity - memQueue.size() + takeCount) / (double) memoryCapacity;        if (overflowActivated && memoryPercentFree >= overflowDeactivationThreshold) {            overflowActivated = false;                    }        channelCounter.setChannelSize(getTotalStored());    }    if (!useOverflow) {        memQueRemaining.release(takeCount);        bytesRemaining.release(takeListByteCount);    }    channelCounter.addToEventTakeSuccessCount(takeCount);}
private void flume_f1697_0() throws InterruptedException
{        int timeout = overflowActivated ? 0 : overflowTimeout;    if (memoryCapacity != 0) {                if (!memQueRemaining.tryAcquire(putList.size(), timeout, TimeUnit.SECONDS)) {            if (overflowDisabled) {                throw new ChannelFullException("Spillable Memory Channel's " + "memory capacity has been reached and overflow is " + "disabled. Consider increasing memoryCapacity.");            }            overflowActivated = true;            useOverflow = true;                } else if (!bytesRemaining.tryAcquire(putListByteCount, overflowTimeout, TimeUnit.SECONDS)) {            memQueRemaining.release(putList.size());            if (overflowDisabled) {                throw new ChannelFullException("Spillable Memory Channel's " + "memory capacity has been reached.  " + (bytesRemaining.availablePermits() * (int) avgEventSize) + " bytes are free and overflow is disabled. Consider " + "increasing byteCapacity or capacity.");            }            overflowActivated = true;            useOverflow = true;        }    } else {        useOverflow = true;    }    if (putList.size() > largestPutTxSize) {        largestPutTxSize = putList.size();    }    if (useOverflow) {        commitPutsToOverflow();    } else {        commitPutsToPrimary();    }}
private void flume_f1698_0() throws InterruptedException
{    overflowPutTx = getOverflowTx();    overflowPutTx.begin();    for (Event event : putList) {        overflowPutTx.put(event);    }    commitPutsToOverflow_core(overflowPutTx);    totalStored.release(putList.size());    overflowPutCount += putList.size();    channelCounter.addToEventPutSuccessCount(putList.size());}
private void flume_f1699_0(Transaction overflowPutTx) throws InterruptedException
{        for (int i = 0; i < 2; ++i) {        try {            synchronized (queueLock) {                overflowPutTx.commit();                drainOrder.putOverflow(putList.size());                channelCounter.setChannelSize(memQueue.size() + drainOrder.overflowCounter);                break;            }        } catch (ChannelFullException e) {                        if (i == 0) {                Thread.sleep(overflowTimeout * 1000);            } else {                throw e;            }        }    }}
private void flume_f1700_0()
{    synchronized (queueLock) {        for (Event e : putList) {            if (!memQueue.offer(e)) {                throw new ChannelException("Unable to insert event into memory " + "queue in spite of spare capacity, this is very unexpected");            }        }        drainOrder.putPrimary(putList.size());        maxMemQueueSize = (memQueue.size() > maxMemQueueSize) ? memQueue.size() : maxMemQueueSize;        channelCounter.setChannelSize(memQueue.size() + drainOrder.overflowCounter);    }        totalStored.release(putList.size());    channelCounter.addToEventPutSuccessCount(putList.size());}
protected void flume_f1701_1()
{        if (putCalled) {        if (overflowPutTx != null) {            overflowPutTx.rollback();        }        if (!useOverflow) {            bytesRemaining.release(putListByteCount);            putList.clear();        }        putListByteCount = 0;    } else if (takeCalled) {        synchronized (queueLock) {            if (overflowTakeTx != null) {                overflowTakeTx.rollback();            }            if (useOverflow) {                drainOrder.putFirstOverflow(takeCount);            } else {                int remainingCapacity = memoryCapacity - memQueue.size();                Preconditions.checkState(remainingCapacity >= takeCount, "Not enough space in memory queue to rollback takes. This" + " should never happen, please report");                while (!takeList.isEmpty()) {                    memQueue.addFirst(takeList.removeLast());                }                drainOrder.putFirstPrimary(takeCount);            }        }        totalStored.release(takeCount);    } else {        overflowTakeTx.rollback();    }    channelCounter.setChannelSize(memQueue.size() + drainOrder.overflowCounter);}
public void flume_f1702_1(Context context)
{    if (    getLifecycleState() == LifecycleState.START || getLifecycleState() == LifecycleState.ERROR) {        stop();    }    if (totalStored == null) {        totalStored = new Semaphore(0);    }    if (channelCounter == null) {        channelCounter = new ChannelCounter(getName());    }        Integer newMemoryCapacity;    try {        newMemoryCapacity = context.getInteger(MEMORY_CAPACITY, defaultMemoryCapacity);        if (newMemoryCapacity == null) {            newMemoryCapacity = defaultMemoryCapacity;        }        if (newMemoryCapacity < 0) {            throw new NumberFormatException(MEMORY_CAPACITY + " must be >= 0");        }    } catch (NumberFormatException e) {        newMemoryCapacity = defaultMemoryCapacity;            }    try {        resizePrimaryQueue(newMemoryCapacity);    } catch (InterruptedException e) {        Thread.currentThread().interrupt();    }        try {        Integer newOverflowTimeout = context.getInteger(OVERFLOW_TIMEOUT, defaultOverflowTimeout);        overflowTimeout = (newOverflowTimeout != null) ? newOverflowTimeout : defaultOverflowTimeout;    } catch (NumberFormatException e) {                overflowTimeout = defaultOverflowTimeout;    }    try {        Integer newThreshold = context.getInteger(OVERFLOW_DEACTIVATION_THRESHOLD);        overflowDeactivationThreshold = (newThreshold != null) ? newThreshold / 100.0 : defaultOverflowDeactivationThreshold / 100.0;    } catch (NumberFormatException e) {                overflowDeactivationThreshold = defaultOverflowDeactivationThreshold / 100.0;    }        try {        byteCapacityBufferPercentage = context.getInteger(BYTE_CAPACITY_BUFFER_PERCENTAGE, defaultByteCapacityBufferPercentage);    } catch (NumberFormatException e) {                byteCapacityBufferPercentage = defaultByteCapacityBufferPercentage;    }    try {        avgEventSize = context.getInteger(AVG_EVENT_SIZE, defaultAvgEventSize);    } catch (NumberFormatException e) {                avgEventSize = defaultAvgEventSize;    }    try {        byteCapacity = (int) ((context.getLong(BYTE_CAPACITY, defaultByteCapacity) * (1 - byteCapacityBufferPercentage * .01)) / avgEventSize);        if (byteCapacity < 1) {            byteCapacity = Integer.MAX_VALUE;        }    } catch (NumberFormatException e) {                byteCapacity = (int) ((defaultByteCapacity * (1 - byteCapacityBufferPercentage * .01)) / avgEventSize);    }    if (bytesRemaining == null) {        bytesRemaining = new Semaphore(byteCapacity);        lastByteCapacity = byteCapacity;    } else {        if (byteCapacity > lastByteCapacity) {            bytesRemaining.release(byteCapacity - lastByteCapacity);            lastByteCapacity = byteCapacity;        } else {            try {                if (!bytesRemaining.tryAcquire(lastByteCapacity - byteCapacity, overflowTimeout, TimeUnit.SECONDS)) {                                    } else {                    lastByteCapacity = byteCapacity;                }            } catch (InterruptedException e) {                Thread.currentThread().interrupt();            }        }    }    try {                overflowCapacity = context.getInteger(OVERFLOW_CAPACITY, defaultOverflowCapacity);                if (memoryCapacity < 1 && overflowCapacity < 1) {                        overflowCapacity = defaultOverflowCapacity;        }        overflowDisabled = (overflowCapacity < 1);        if (overflowDisabled) {            overflowActivated = false;        }    } catch (NumberFormatException e) {        overflowCapacity = defaultOverflowCapacity;    }            context.put(KEEP_ALIVE, "0");        context.put(CAPACITY, Integer.toString(overflowCapacity));    super.configure(context);}
private void flume_f1703_1(int newMemoryCapacity) throws InterruptedException
{    if (memQueue != null && memoryCapacity == newMemoryCapacity) {        return;    }    if (memoryCapacity > newMemoryCapacity) {        int diff = memoryCapacity - newMemoryCapacity;        if (!memQueRemaining.tryAcquire(diff, overflowTimeout, TimeUnit.SECONDS)) {                        return;        }        synchronized (queueLock) {            ArrayDeque<Event> newQueue = new ArrayDeque<Event>(newMemoryCapacity);            newQueue.addAll(memQueue);            memQueue = newQueue;            memoryCapacity = newMemoryCapacity;        }    } else {                synchronized (queueLock) {            ArrayDeque<Event> newQueue = new ArrayDeque<Event>(newMemoryCapacity);            if (memQueue != null) {                newQueue.addAll(memQueue);            }            memQueue = newQueue;            if (memQueRemaining == null) {                memQueRemaining = new Semaphore(newMemoryCapacity);            } else {                int diff = newMemoryCapacity - memoryCapacity;                memQueRemaining.release(diff);            }            memoryCapacity = newMemoryCapacity;        }    }}
public synchronized void flume_f1704_0()
{    super.start();    int overFlowCount = super.getDepth();    if (drainOrder.isEmpty()) {        drainOrder.putOverflow(overFlowCount);        totalStored.release(overFlowCount);    }    channelCounter.start();    int totalCount = overFlowCount + memQueue.size();    channelCounter.setChannelCapacity(memoryCapacity + getOverflowCapacity());    channelCounter.setChannelSize(totalCount);}
public synchronized void flume_f1705_0()
{    if (getLifecycleState() == LifecycleState.STOP) {        return;    }    channelCounter.setChannelSize(memQueue.size() + drainOrder.overflowCounter);    channelCounter.stop();    super.stop();}
protected BasicTransactionSemantics flume_f1706_0()
{    return new SpillableMemoryTransaction(channelCounter);}
private BasicTransactionSemantics flume_f1707_0()
{    return super.createTransaction();}
private long flume_f1708_0(Event event)
{    byte[] body = event.getBody();    if (body != null && body.length != 0) {        return body.length;    }        return 1;}
private void flume_f1709_0(Map<String, String> overrides)
{    Context context = new Context();    File checkPointDir = fileChannelDir.newFolder("checkpoint");    File dataDir = fileChannelDir.newFolder("data");    context.put(FileChannelConfiguration.CHECKPOINT_DIR, checkPointDir.getAbsolutePath());    context.put(FileChannelConfiguration.DATA_DIRS, dataDir.getAbsolutePath());        context.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "5000");    if (overrides != null) {        context.putAll(overrides);    }    Configurables.configure(channel, context);}
private void flume_f1710_0(Map<String, String> overrides)
{    configureChannel(overrides);    channel.stop();    channel.start();}
private void flume_f1711_0(Map<String, String> params)
{    configureChannel(params);    channel.start();}
private void flume_f1712_0(Map<String, String> params)
{    channel.stop();    setUp();    startChannel(params);}
public void flume_f1713_0()
{    channel = new SpillableMemoryChannel();    channel.setName("spillChannel-" + UUID.randomUUID());}
public void flume_f1714_0()
{    channel.stop();}
private static void flume_f1715_0(int first, int count, AbstractChannel channel)
{    for (int i = 0; i < count; ++i) {        channel.put(EventBuilder.withBody(String.valueOf(first++).getBytes()));    }}
private static void flume_f1716_0(AbstractChannel channel)
{    channel.take();}
private static void flume_f1717_0(int first, int count, AbstractChannel channel)
{    int last = first + count;    for (int i = first; i < last; ++i) {        Event e = channel.take();        if (e == null) {            throw new NullFound(i);        }        Event expected = EventBuilder.withBody(String.valueOf(i).getBytes());        Assert.assertArrayEquals(e.getBody(), expected.getBody());    }}
private static int flume_f1718_0(int batchSize, AbstractChannel channel)
{    int i = 0;    for (; i < batchSize; ++i) {        Event e = channel.take();        if (e == null) {            try {                Thread.sleep(0);            } catch (InterruptedException ex) {            /* ignore */            }            return i;        }    }    return i;}
private static void flume_f1719_0(int first, int count, AbstractChannel channel)
{    Transaction tx = channel.getTransaction();    tx.begin();    try {        putN(first, count, channel);        tx.commit();    } catch (RuntimeException e) {        tx.rollback();        throw e;    } finally {        tx.close();    }}
private static void flume_f1720_0(int first, int count, AbstractChannel channel)
{    Transaction tx = channel.getTransaction();    tx.begin();    try {        takeN(first, count, channel);        tx.commit();    } catch (NullFound e) {        tx.commit();        throw e;    } catch (AssertionError e) {        tx.rollback();        throw e;    } catch (RuntimeException e) {        tx.rollback();        throw e;    } finally {        tx.close();    }}
private static int flume_f1721_0(int count, AbstractChannel channel)
{    Transaction tx = channel.getTransaction();    tx.begin();    try {        int eventCount = takeN_NoCheck(count, channel);        tx.commit();        return eventCount;    } catch (RuntimeException e) {        tx.rollback();        throw e;    } finally {        tx.close();    }}
private static void flume_f1722_0(int count, AbstractChannel channel)
{    Transaction tx = channel.getTransaction();    tx.begin();    try {        for (int i = 0; i < count; ++i) {            takeNull(channel);        }        tx.commit();    } catch (AssertionError e) {        tx.rollback();        throw e;    } catch (RuntimeException e) {        tx.rollback();        throw e;    } finally {        tx.close();    }}
private Thread flume_f1723_0(String threadName, final int first, final int count, final int batchSize, final AbstractChannel channel)
{    return new Thread(threadName) {        public void run() {            int maxdepth = 0;            StopWatch watch = new StopWatch();            for (int i = first; i < first + count; i = i + batchSize) {                transactionalPutN(i, batchSize, channel);            }            watch.elapsed();        }    };}
public void flume_f1724_0()
{    int maxdepth = 0;    StopWatch watch = new StopWatch();    for (int i = first; i < first + count; i = i + batchSize) {        transactionalPutN(i, batchSize, channel);    }    watch.elapsed();}
private static Thread flume_f1725_0(String threadName, final int first, final int count, final int batchSize, final AbstractChannel channel)
{    return new Thread(threadName) {        public void run() {            StopWatch watch = new StopWatch();            for (int i = first; i < first + count; ) {                try {                    transactionalTakeN(i, batchSize, channel);                    i = i + batchSize;                } catch (NullFound e) {                    i = e.expectedValue;                }            }            watch.elapsed();        }    };}
public void flume_f1726_0()
{    StopWatch watch = new StopWatch();    for (int i = first; i < first + count; ) {        try {            transactionalTakeN(i, batchSize, channel);            i = i + batchSize;        } catch (NullFound e) {            i = e.expectedValue;        }    }    watch.elapsed();}
private static Thread flume_f1727_0(String threadName, final int totalEvents, final int batchSize, final AbstractChannel channel)
{    return new Thread(threadName) {        public void run() {            int batchSz = batchSize;            StopWatch watch = new StopWatch();            int i = 0, attempts = 0;            while (i < totalEvents) {                int remaining = totalEvents - i;                batchSz = (remaining > batchSz) ? batchSz : remaining;                int takenCount = transactionalTakeN_NoCheck(batchSz, channel);                if (takenCount < batchSz) {                    try {                        Thread.sleep(20);                    } catch (InterruptedException ex) {                    /* ignore */                    }                }                i += takenCount;                ++attempts;                if (attempts > totalEvents * 3) {                    throw new TooManyNulls(attempts);                }            }            watch.elapsed(" items = " + i + ", attempts = " + attempts);        }    };}
public void flume_f1728_0()
{    int batchSz = batchSize;    StopWatch watch = new StopWatch();    int i = 0, attempts = 0;    while (i < totalEvents) {        int remaining = totalEvents - i;        batchSz = (remaining > batchSz) ? batchSz : remaining;        int takenCount = transactionalTakeN_NoCheck(batchSz, channel);        if (takenCount < batchSz) {            try {                Thread.sleep(20);            } catch (InterruptedException ex) {            /* ignore */            }        }        i += takenCount;        ++attempts;        if (attempts > totalEvents * 3) {            throw new TooManyNulls(attempts);        }    }    watch.elapsed(" items = " + i + ", attempts = " + attempts);}
public void flume_f1729_0()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "5");    params.put("overflowCapacity", "5");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "5");    startChannel(params);    Transaction tx = channel.getTransaction();    tx.begin();    putN(0, 2, channel);    tx.commit();    tx.close();    tx = channel.getTransaction();    tx.begin();    takeN(0, 2, channel);    tx.commit();    tx.close();}
public void flume_f1730_0()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "2");        params.put("overflowCapacity", "0");    params.put("overflowTimeout", "0");    startChannel(params);    transactionalPutN(0, 2, channel);    boolean threw = false;    try {        transactionalPutN(2, 1, channel);    } catch (ChannelException e) {        threw = true;    }    Assert.assertTrue("Expecting ChannelFullException to be thrown", threw);    transactionalTakeN(0, 2, channel);    Transaction tx = channel.getTransaction();    tx.begin();    Assert.assertNull(channel.take());    tx.commit();    tx.close();}
public void flume_f1731_0()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "2");    params.put("overflowCapacity", "4");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "3");    params.put("overflowTimeout", "0");    startChannel(params);    transactionalPutN(1, 2, channel);    transactionalPutN(3, 2, channel);    transactionalPutN(5, 2, channel);    boolean threw = false;    try {                transactionalPutN(7, 2, channel);    } catch (ChannelFullException e) {        threw = true;    }    Assert.assertTrue("Expecting ChannelFullException to be thrown", threw);    transactionalTakeN(1, 2, channel);    transactionalTakeN(3, 2, channel);    transactionalTakeN(5, 2, channel);}
public void flume_f1732_0()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "2");    params.put("overflowCapacity", "10");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "4");    params.put("overflowTimeout", "0");    startChannel(params);    transactionalPutN(1, 2, channel);        transactionalPutN(3, 2, channel);    restartChannel(params);        transactionalTakeN(3, 2, channel);}
public void flume_f1733_0()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "10000000");    params.put("overflowCapacity", "20000000");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "10");    params.put("overflowTimeout", "1");    startChannel(params);    transactionalPutN(1, 5, channel);    transactionalPutN(6, 5, channel);        transactionalPutN(11, 5, channel);    transactionalTakeN(1, 10, channel);    transactionalTakeN(11, 5, channel);}
public void flume_f1734_0()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "10");    params.put("overflowCapacity", "20");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "10");    params.put("overflowTimeout", "1");    startChannel(params);    transactionalPutN(1, 5, channel);    transactionalPutN(6, 5, channel);        transactionalPutN(11, 5, channel);    transactionalTakeN(1, 10, channel);    transactionalTakeN(11, 5, channel);}
public void flume_f1735_0()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "10");    params.put("overflowCapacity", "10");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "5");    params.put("overflowTimeout", "1");    startChannel(params);    transactionalPutN(1, 5, channel);    transactionalPutN(6, 5, channel);        transactionalPutN(11, 5, channel);        transactionalPutN(16, 5, channel);    transactionalTakeN(1, 1, channel);    transactionalTakeN(2, 5, channel);    transactionalTakeN(7, 4, channel);    transactionalPutN(20, 2, channel);    transactionalPutN(22, 3, channel);        transactionalTakeN(11, 3, channel);        transactionalTakeN(14, 5, channel);        transactionalTakeN(19, 2, channel);}
public void flume_f1736_0()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "1000");        params.put("byteCapacity", "100");    params.put("avgEventSize", "10");    params.put("overflowCapacity", "20");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "10");    params.put("overflowTimeout", "1");    startChannel(params);        transactionalPutN(1, 8, channel);    transactionalPutN(9, 10, channel);        transactionalPutN(19, 10, channel);    boolean threw = false;    try {                transactionalPutN(11, 1, channel);    } catch (ChannelFullException e) {        threw = true;    }    Assert.assertTrue("byteCapacity did not throw as expected", threw);}
public void flume_f1737_0()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "5");    params.put("overflowCapacity", "15");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "10");    params.put("overflowTimeout", "1");    startChannel(params);    transactionalPutN(1, 5, channel);        transactionalPutN(6, 5, channel);        transactionalPutN(11, 5, channel);        transactionalPutN(16, 5, channel);    transactionalTakeN(1, 3, channel);    Transaction tx = channel.getTransaction();    tx.begin();    takeN(4, 2, channel);        takeNull(channel);    tx.commit();    tx.close();        transactionalTakeN(6, 5, channel);        transactionalTakeN(11, 5, channel);        transactionalTakeN(16, 2, channel);    transactionalPutN(21, 5, channel);    tx = channel.getTransaction();    tx.begin();        takeN(18, 3, channel);        takeNull(channel);    tx.commit();    tx.close();    transactionalTakeN(21, 5, channel);}
public void flume_f1738_0()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "100");    params.put("overflowCapacity", "900");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "900");    params.put("overflowTimeout", "0");    startChannel(params);        transactionalPutN(1, 5, channel);    Transaction tx = channel.getTransaction();    tx.begin();    putN(6, 5, channel);    tx.rollback();    tx.close();    transactionalTakeN(1, 5, channel);    transactionalTakeNull(2, channel);        transactionalPutN(11, 5, channel);    transactionalTakeN(11, 5, channel);        transactionalPutN(16, 5, channel);    tx = channel.getTransaction();    tx.begin();    takeN(16, 5, channel);    takeNull(channel);    tx.rollback();    tx.close();    transactionalTakeN_NoCheck(5, channel);        transactionalPutN(21, 5, channel);    transactionalTakeN(21, 5, channel);}
public void flume_f1739_0()
{        Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "10");    params.put("overflowCapacity", "0");    params.put("overflowTimeout", "0");    startChannel(params);    Assert.assertTrue("overflowTimeout setting did not reconfigure correctly", channel.getOverflowTimeout() == 0);    Assert.assertTrue("memoryCapacity did not reconfigure correctly", channel.getMemoryCapacity() == 10);    Assert.assertTrue("overflowCapacity did not reconfigure correctly", channel.isOverflowDisabled());    transactionalPutN(1, 10, channel);    boolean threw = false;    try {                transactionalPutN(11, 10, channel);    } catch (ChannelException e) {        threw = true;    }    Assert.assertTrue("Expected the channel to fill up and throw an exception, " + "but it did not throw", threw);        params = new HashMap<String, String>();    params.put("memoryCapacity", "20");    params.put("overflowCapacity", "0");    reconfigureChannel(params);    Assert.assertTrue("overflowTimeout setting did not reconfigure correctly", channel.getOverflowTimeout() == SpillableMemoryChannel.defaultOverflowTimeout);    Assert.assertTrue("memoryCapacity did not reconfigure correctly", channel.getMemoryCapacity() == 20);    Assert.assertTrue("overflowCapacity did not reconfigure correctly", channel.isOverflowDisabled());        transactionalTakeN(1, 10, channel);    transactionalPutN(11, 10, channel);    transactionalPutN(21, 10, channel);    threw = false;    try {                transactionalPutN(31, 10, channel);    } catch (ChannelException e) {        threw = true;    }    Assert.assertTrue("Expected the channel to fill up and throw an exception, " + "but it did not throw", threw);    transactionalTakeN(11, 10, channel);    transactionalTakeN(21, 10, channel);        params = new HashMap<String, String>();    reconfigureChannel(params);    Assert.assertTrue("overflowTimeout setting did not reconfigure correctly", channel.getOverflowTimeout() == SpillableMemoryChannel.defaultOverflowTimeout);    Assert.assertTrue("memoryCapacity did not reconfigure correctly", channel.getMemoryCapacity() == SpillableMemoryChannel.defaultMemoryCapacity);    Assert.assertTrue("overflowCapacity did not reconfigure correctly", channel.getOverflowCapacity() == SpillableMemoryChannel.defaultOverflowCapacity);    Assert.assertFalse("overflowCapacity did not reconfigure correctly", channel.isOverflowDisabled());        params = new HashMap<String, String>();    params.put("memoryCapacity", "10");    params.put("overflowCapacity", "10");    params.put("transactionCapacity", "5");    params.put("overflowTimeout", "1");    reconfigureChannel(params);    transactionalPutN(1, 5, channel);    transactionalPutN(6, 5, channel);    transactionalPutN(11, 5, channel);    transactionalPutN(16, 5, channel);    threw = false;    try {                transactionalPutN(21, 5, channel);    } catch (ChannelException e) {        threw = true;    }    Assert.assertTrue("Expected the last insertion to fail, but it didn't.", threw);        params = new HashMap<String, String>();    params.put("memoryCapacity", "10");    params.put("overflowCapacity", "20");    params.put("transactionCapacity", "10");    params.put("overflowTimeout", "1");    reconfigureChannel(params);        transactionalPutN(21, 5, channel);    transactionalTakeN(1, 10, channel);    transactionalTakeN(11, 5, channel);    transactionalTakeN(16, 5, channel);    transactionalTakeN(21, 5, channel);}
public void flume_f1740_0() throws InterruptedException
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "1000020");    params.put("overflowCapacity", "0");    params.put("overflowTimeout", "3");    startChannel(params);        Thread sourceThd = makePutThread("src", 1, 500000, 100, channel);    Thread sinkThd = makeTakeThread("sink", 1, 500000, 100, channel);    StopWatch watch = new StopWatch();    sinkThd.start();    sourceThd.start();    sourceThd.join();    sinkThd.join();    watch.elapsed();    System.out.println("Max Queue size " + channel.getMaxMemQueueSize());}
public void flume_f1741_0() throws InterruptedException
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "5000");    params.put("overflowCapacity", "5000");    params.put("transactionCapacity", "5000");    params.put("overflowTimeout", "0");    startChannel(params);    Assert.assertTrue("channel.channelCounter should have started", channel.channelCounter.getStartTime() > 0);        Thread sourceThd = makePutThread("src", 1, 5000, 2500, channel);    sourceThd.start();    sourceThd.join();    Assert.assertEquals(5000, channel.getTotalStored());    Assert.assertEquals(5000, channel.channelCounter.getChannelSize());    Assert.assertEquals(5000, channel.channelCounter.getEventPutAttemptCount());    Assert.assertEquals(5000, channel.channelCounter.getEventPutSuccessCount());        Thread sinkThd = makeTakeThread("sink", 1, 5000, 1000, channel);    sinkThd.start();    sinkThd.join();    Assert.assertEquals(0, channel.getTotalStored());    Assert.assertEquals(0, channel.channelCounter.getChannelSize());    Assert.assertEquals(5000, channel.channelCounter.getEventTakeAttemptCount());    Assert.assertEquals(5000, channel.channelCounter.getEventTakeSuccessCount());        sourceThd = makePutThread("src", 1, 10000, 1000, channel);    sourceThd.start();    sourceThd.join();    Assert.assertEquals(10000, channel.getTotalStored());    Assert.assertEquals(10000, channel.channelCounter.getChannelSize());    Assert.assertEquals(15000, channel.channelCounter.getEventPutAttemptCount());    Assert.assertEquals(15000, channel.channelCounter.getEventPutSuccessCount());        sinkThd = makeTakeThread("sink", 1, 5000, 1000, channel);    sinkThd.start();    sinkThd.join();    Assert.assertEquals(5000, channel.getTotalStored());    Assert.assertEquals(5000, channel.channelCounter.getChannelSize());    Assert.assertEquals(10000, channel.channelCounter.getEventTakeAttemptCount());    Assert.assertEquals(10000, channel.channelCounter.getEventTakeSuccessCount());        transactionalTakeN(5001, 1000, channel);    transactionalTakeN(6001, 1000, channel);    transactionalTakeN(7001, 1000, channel);    transactionalTakeN(8001, 1000, channel);    transactionalTakeN(9001, 1000, channel);    Assert.assertEquals(0, channel.getTotalStored());    Assert.assertEquals(0, channel.channelCounter.getChannelSize());    Assert.assertEquals(15000, channel.channelCounter.getEventTakeAttemptCount());    Assert.assertEquals(15000, channel.channelCounter.getEventTakeSuccessCount());        sourceThd = makePutThread("src1", 1, 5000, 1000, channel);    Thread sourceThd2 = makePutThread("src2", 1, 5000, 500, channel);    sinkThd = makeTakeThread_noCheck("sink1", 5000, 1000, channel);    sourceThd.start();    sourceThd2.start();    sinkThd.start();    sourceThd.join();    sourceThd2.join();    sinkThd.join();    Assert.assertEquals(5000, channel.getTotalStored());    Assert.assertEquals(5000, channel.channelCounter.getChannelSize());    Thread sinkThd2 = makeTakeThread_noCheck("sink2", 2500, 500, channel);    Thread sinkThd3 = makeTakeThread_noCheck("sink3", 2500, 1000, channel);    sinkThd2.start();    sinkThd3.start();    sinkThd2.join();    sinkThd3.join();    Assert.assertEquals(0, channel.getTotalStored());    Assert.assertEquals(0, channel.channelCounter.getChannelSize());    Assert.assertEquals(25000, channel.channelCounter.getEventTakeSuccessCount());    Assert.assertEquals(25000, channel.channelCounter.getEventPutSuccessCount());    Assert.assertTrue("TakeAttempt channel counter value larger than expected", 25000 <= channel.channelCounter.getEventTakeAttemptCount());    Assert.assertTrue("PutAttempt channel counter value larger than expected", 25000 <= channel.channelCounter.getEventPutAttemptCount());}
public ArrayList<Thread> flume_f1742_0(int count, int totalEvents, int batchSize)
{    ArrayList<Thread> sourceThds = new ArrayList<Thread>();    for (int i = 0; i < count; ++i) {        sourceThds.add(makePutThread("src" + i, 1, totalEvents / count, batchSize, channel));    }    return sourceThds;}
public ArrayList<Thread> flume_f1743_0(int count, int totalEvents, int batchSize)
{    ArrayList<Thread> sinkThreads = new ArrayList<Thread>(count);    for (int i = 0; i < count; ++i) {        sinkThreads.add(makeTakeThread_noCheck("sink" + i, totalEvents / count, batchSize, channel));    }    return sinkThreads;}
public void flume_f1744_0(ArrayList<Thread> threads)
{    for (Thread thread : threads) {        thread.start();    }}
public void flume_f1745_0(ArrayList<Thread> threads) throws InterruptedException
{    for (Thread thread : threads) {        try {            thread.join();        } catch (InterruptedException e) {            System.out.println("Interrupted while waiting on " + thread.getName());            throw e;        }    }}
public void flume_f1746_0() throws InterruptedException
{    int sourceCount = 8;    int sinkCount = 8;    int eventCount = 1000000;    int batchSize = 100;    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "0");    params.put("overflowCapacity", String.valueOf(eventCount));    params.put("overflowTimeout", "3");    startChannel(params);    ArrayList<Thread> sinks = createSinkThreads(sinkCount, eventCount, batchSize);    ArrayList<Thread> sources = createSourceThreads(sourceCount, eventCount, batchSize);    StopWatch watch = new StopWatch();    startThreads(sinks);    startThreads(sources);    joinThreads(sources);    joinThreads(sinks);    watch.elapsed();    System.out.println("Max Queue size " + channel.getMaxMemQueueSize());    Assert.assertEquals(eventCount, channel.drainOrder.totalPuts);    Assert.assertEquals("Channel not fully drained", 0, channel.getTotalStored());    System.out.println("testParallelMultipleSourcesAndSinks done");}
public void flume_f1747_0()
{    elapsed(null);}
public void flume_f1748_0(String suffix)
{    long elapsed = System.currentTimeMillis() - startTime;    if (suffix == null) {        suffix = "";    } else {        suffix = "{ " + suffix + " }";    }    if (elapsed < 10000) {        System.out.println(Thread.currentThread().getName() + " : [ " + elapsed + " ms ].        " + suffix);    } else {        System.out.println(Thread.currentThread().getName() + " : [ " + elapsed / 1000 + " sec ].       " + suffix);    }}
public void flume_f1749_0(String hostNames)
{    this.hosts = hostNames;}
public void flume_f1750_0(String selector)
{    this.selector = selector;}
public void flume_f1751_0(String maxBackoff)
{    this.maxBackoff = maxBackoff;}
private Properties flume_f1754_0(String hosts, String selector, String maxBackoff, long timeout) throws FlumeException
{    if (StringUtils.isEmpty(hosts)) {        throw new FlumeException("hosts must not be null");    }    Properties props = new Properties();    String[] hostsAndPorts = hosts.split("\\s+");    StringBuilder names = new StringBuilder();    for (int i = 0; i < hostsAndPorts.length; i++) {        String hostAndPort = hostsAndPorts[i];        String name = "h" + i;        props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + name, hostAndPort);        names.append(name).append(" ");    }    props.put(RpcClientConfigurationConstants.CONFIG_HOSTS, names.toString());    props.put(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, ClientType.DEFAULT_LOADBALANCE.toString());    if (!StringUtils.isEmpty(selector)) {        props.put(RpcClientConfigurationConstants.CONFIG_HOST_SELECTOR, selector);    }    if (!StringUtils.isEmpty(maxBackoff)) {        long millis = Long.parseLong(maxBackoff.trim());        if (millis <= 0) {            throw new FlumeException("Misconfigured max backoff, value must be greater than 0");        }        props.put(RpcClientConfigurationConstants.CONFIG_BACKOFF, String.valueOf(true));        props.put(RpcClientConfigurationConstants.CONFIG_MAX_BACKOFF, maxBackoff);    }    props.setProperty(RpcClientConfigurationConstants.CONFIG_CONNECT_TIMEOUT, String.valueOf(timeout));    props.setProperty(RpcClientConfigurationConstants.CONFIG_REQUEST_TIMEOUT, String.valueOf(timeout));    return props;}
private List<Event> flume_f1756_0(LoggingEvent loggingEvent)
{    Map<String, String> headers = new HashMap<>();    headers.put(Log4jAvroHeaders.LOGGER_NAME.toString(), loggingEvent.getLoggerName());    headers.put(Log4jAvroHeaders.TIMESTAMP.toString(), String.valueOf(loggingEvent.timeStamp));    headers.put(Log4jAvroHeaders.ADDRESS.toString(), clientAddress);                headers.put(Log4jAvroHeaders.LOG_LEVEL.toString(), String.valueOf(loggingEvent.getLevel().toInt()));    Map<String, String> headersWithEncoding = null;    Collection<?> messages;    if (loggingEvent.getMessage() instanceof Collection) {        messages = (Collection) loggingEvent.getMessage();    } else {        messages = Collections.singleton(loggingEvent.getMessage());    }    List<Event> events = new LinkedList<>();    for (Object message : messages) {        if (message instanceof GenericRecord) {            GenericRecord record = (GenericRecord) message;            populateAvroHeaders(headers, record.getSchema());            events.add(EventBuilder.withBody(serialize(record, record.getSchema()), headers));        } else if (message instanceof SpecificRecord || avroReflectionEnabled) {            Schema schema = ReflectData.get().getSchema(message.getClass());            populateAvroHeaders(headers, schema);            events.add(EventBuilder.withBody(serialize(message, schema), headers));        } else {            String msg;            if (layout != null) {                LoggingEvent singleLoggingEvent = new LoggingEvent(loggingEvent.getFQNOfLoggerClass(), loggingEvent.getLogger(), loggingEvent.getTimeStamp(), loggingEvent.getLevel(), message, loggingEvent.getThreadName(), loggingEvent.getThrowableInformation(), loggingEvent.getNDC(), loggingEvent.getLocationInformation(), loggingEvent.getProperties());                msg = layout.format(singleLoggingEvent);            } else {                msg = message.toString();            }            if (headersWithEncoding == null) {                headersWithEncoding = new HashMap<>(headers);                headersWithEncoding.put(Log4jAvroHeaders.MESSAGE_ENCODING.toString(), "UTF8");            }            events.add(EventBuilder.withBody(msg, Charset.forName("UTF8"), headersWithEncoding));        }    }    return events;}
private byte[] flume_f1758_0(Object datum, Schema datumSchema) throws FlumeException
{    if (schema == null || !datumSchema.equals(schema)) {        schema = datumSchema;        out = new ByteArrayOutputStream();        writer = new ReflectDatumWriter<>(schema);        encoder = EncoderFactory.get().binaryEncoder(out, null);    }    out.reset();    try {        writer.write(datum, encoder);        encoder.flush();        return out.toByteArray();    } catch (IOException e) {        throw new FlumeException(e);    }}
public boolean flume_f1760_0()
{        return true;}
public void flume_f1761_0(String hostname)
{    this.hostname = hostname;}
public void flume_f1762_0(int port)
{    this.port = port;}
public void flume_f1763_0(boolean unsafeMode)
{    this.unsafeMode = unsafeMode;}
public boolean flume_f1764_0()
{    return unsafeMode;}
public void flume_f1765_0(long timeout)
{    this.timeout = timeout;}
public long flume_f1766_0()
{    return this.timeout;}
public void flume_f1767_0(boolean avroReflectionEnabled)
{    this.avroReflectionEnabled = avroReflectionEnabled;}
public void flume_f1768_0(String avroSchemaUrl)
{    this.avroSchemaUrl = avroSchemaUrl;}
private void flume_f1771_0() throws FlumeException
{    close();    activateOptions();}
public String flume_f1772_0()
{    return headerName;}
public String flume_f1773_0()
{    return getName();}
public static Log4jAvroHeaders flume_f1774_0(String headerName)
{    Log4jAvroHeaders hdrs = null;    try {        hdrs = Log4jAvroHeaders.valueOf(headerName.toLowerCase(Locale.ENGLISH).trim());    } catch (IllegalArgumentException e) {        hdrs = Log4jAvroHeaders.OTHER;    }    return hdrs;}
private static List<Integer> flume_f1775_0(int numberOfPorts) throws IOException
{    List<Integer> ports = new ArrayList<>(numberOfPorts);    for (int index = 0; index < numberOfPorts; ++index) {        try (ServerSocket socket = new ServerSocket(0)) {            ports.add(socket.getLocalPort());        }    }    return ports;}
private static String flume_f1776_0(List<Integer> ports)
{    List<String> addresses = new ArrayList<String>(ports.size());    for (Integer port : ports) {        addresses.add("localhost:" + port);    }    String hostList = StringUtils.join(addresses, " ");    return hostList;}
public void flume_f1777_0() throws InterruptedException
{    ch = new MemoryChannel();    configureChannel();}
private void flume_f1778_0()
{    Configurables.configure(ch, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(ch);    rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);}
public void flume_f1779_0()
{    for (Source source : sources) {        source.stop();    }}
public void flume_f1780_0() throws IOException
{    int numberOfMsgs = 1000;    int expectedPerSource = 500;    String propertiesFile = "flume-loadbalancinglog4jtest.properties";    startSources(propertiesFile, false, getFreePorts(2));    sendAndAssertMessages(numberOfMsgs);    for (CountingAvroSource source : sources) {        Assert.assertEquals(expectedPerSource, source.appendCount.get());    }}
public void flume_f1781_0() throws IOException
{    int numberOfMsgs = 1000;    String propertiesFile = "flume-loadbalancing-rnd-log4jtest.properties";    startSources(propertiesFile, false, getFreePorts(10));    sendAndAssertMessages(numberOfMsgs);    int total = 0;    Set<Integer> counts = new HashSet<Integer>();    for (CountingAvroSource source : sources) {        total += source.appendCount.intValue();        counts.add(source.appendCount.intValue());    }        Assert.assertTrue("Very unusual distribution " + counts.size(), counts.size() > 2);    Assert.assertTrue("Missing events", total == numberOfMsgs);}
public void flume_f1782_0() throws Exception
{    String propertiesFile = "flume-loadbalancing-backoff-log4jtest.properties";    startSources(propertiesFile, false, getFreePorts(3));    sources.get(0).setFail();    sources.get(2).setFail();    sendAndAssertMessages(50);    Assert.assertEquals(50, sources.get(1).appendCount.intValue());    Assert.assertEquals(0, sources.get(0).appendCount.intValue());    Assert.assertEquals(0, sources.get(2).appendCount.intValue());    sources.get(0).setOk();        sources.get(1).setFail();    try {        send(1);                Assert.fail("Expected EventDeliveryException");    } catch (FlumeException e) {        Assert.assertTrue(e.getCause() instanceof EventDeliveryException);    }        Thread.sleep(2500);    sendAndAssertMessages(50);    Assert.assertEquals(50, sources.get(0).appendCount.intValue());    Assert.assertEquals(50, sources.get(1).appendCount.intValue());    Assert.assertEquals(0, sources.get(2).appendCount.intValue());}
public void flume_f1783_0() throws Exception
{    String propertiesFile = "flume-loadbalancing-backoff-log4jtest.properties";    startSources(propertiesFile, true, getFreePorts(3));    sources.get(0).setFail();    sources.get(1).setFail();    sources.get(2).setFail();    sendAndAssertFail();}
public void flume_f1784_0() throws Throwable
{    String propertiesFile = "flume-loadbalancinglog4jtest.properties";    ch = new TestLog4jAppender.SlowMemoryChannel(2000);    configureChannel();    slowDown = true;    startSources(propertiesFile, false, getFreePorts(3));    int level = 20000;    String msg = "This is log message number" + String.valueOf(level);    try {        fixture.log(Level.toLevel(level), msg);    } catch (FlumeException ex) {        throw ex.getCause();    }}
public void flume_f1785_0() throws Throwable
{    String propertiesFile = "flume-loadbalancing-backoff-log4jtest.properties";    startSources(propertiesFile, false, getFreePorts(3));    sources.get(0).setFail();    sources.get(1).setFail();    sources.get(2).setFail();    try {        sendAndAssertFail();    } catch (FlumeException ex) {        throw ex.getCause();    }}
private void flume_f1786_0(int numberOfMsgs) throws EventDeliveryException
{    for (int count = 0; count < numberOfMsgs; count++) {        int level = count % 5;        String msg = "This is log message number" + String.valueOf(count);        fixture.log(Level.toLevel(level), msg);    }}
private void flume_f1787_0() throws IOException
{    int level = 20000;    String msg = "This is log message number" + String.valueOf(level);    fixture.log(Level.toLevel(level), msg);    Transaction transaction = ch.getTransaction();    transaction.begin();    Event event = ch.take();    Assert.assertNull(event);    transaction.commit();    transaction.close();}
private void flume_f1788_0(int numberOfMsgs) throws IOException
{    for (int count = 0; count < numberOfMsgs; count++) {        int level = count % 5;        String msg = "This is log message number" + String.valueOf(count);        fixture.log(Level.toLevel(level), msg);        Transaction transaction = ch.getTransaction();        transaction.begin();        Event event = ch.take();        Assert.assertNotNull(event);        Assert.assertEquals(new String(event.getBody(), "UTF8"), msg);        Map<String, String> hdrs = event.getHeaders();        Assert.assertNotNull(hdrs.get(Log4jAvroHeaders.TIMESTAMP.toString()));        Assert.assertNotNull(hdrs.get(Log4jAvroHeaders.ADDRESS.toString()));        Assert.assertEquals(Level.toLevel(level), Level.toLevel(hdrs.get(Log4jAvroHeaders.LOG_LEVEL.toString())));        Assert.assertEquals(fixture.getName(), hdrs.get(Log4jAvroHeaders.LOGGER_NAME.toString()));        Assert.assertEquals("UTF8", hdrs.get(Log4jAvroHeaders.MESSAGE_ENCODING.toString()));                System.out.println("Got body: " + new String(event.getBody(), "UTF8"));        transaction.commit();        transaction.close();    }}
private void flume_f1789_0(String log4jProps, boolean unsafeMode, List<Integer> ports) throws IOException
{    for (int port : ports) {        CountingAvroSource source = new CountingAvroSource(port);        Context context = new Context();        context.put("port", String.valueOf(port));        context.put("bind", "0.0.0.0");        Configurables.configure(source, context);        sources.add(source);        source.setChannelProcessor(new ChannelProcessor(rcs));    }    for (Source source : sources) {        source.start();    }                        Reader reader = new InputStreamReader(getClass().getResourceAsStream("/" + log4jProps));    Properties props = new Properties();    props.load(reader);    props.setProperty("log4j.appender.out2.Hosts", toHostList(ports));    props.setProperty("log4j.appender.out2.UnsafeMode", String.valueOf(unsafeMode));    if (slowDown) {        props.setProperty("log4j.appender.out2.Timeout", String.valueOf(1000));    }    PropertyConfigurator.configure(props);    fixture = LogManager.getLogger(TestLoadBalancingLog4jAppender.class);}
public void flume_f1790_0()
{    this.isFail = false;}
public void flume_f1791_0()
{    this.isFail = true;}
public String flume_f1792_0()
{    return "testing..." + port2;}
public Status flume_f1793_0(AvroFlumeEvent avroEvent)
{    if (isFail) {        return Status.FAILED;    }    appendCount.incrementAndGet();    return super.append(avroEvent);}
public Status flume_f1794_0(List<AvroFlumeEvent> events)
{    if (isFail) {        return Status.FAILED;    }    appendCount.addAndGet(events.size());    return super.appendBatch(events);}
private static int flume_f1795_0() throws Exception
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
public void flume_f1796_0() throws Exception
{    int port = getFreePort();    source = Mockito.spy(new AvroSource());    ch = new MemoryChannel();    Configurables.configure(ch, new Context());    Context context = new Context();    context.put("port", String.valueOf(port));    context.put("bind", "localhost");    Configurables.configure(source, context);    File TESTFILE = new File(TestLog4jAppender.class.getClassLoader().getResource("flume-log4jtest.properties").getFile());    FileReader reader = new FileReader(TESTFILE);    props = new Properties();    props.load(reader);    props.put("log4j.appender.out2.Port", String.valueOf(port));    reader.close();}
private void flume_f1797_0()
{    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(Collections.singletonList(ch));    source.setChannelProcessor(new ChannelProcessor(rcs));    source.start();}
public void flume_f1798_0() throws IOException
{    configureSource();    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(TestLog4jAppender.class);    for (int count = 0; count <= 1000; count++) {        /*       * Log4j internally defines levels as multiples of 10000. So if we       * create levels directly using count, the level will be set as the       * default.       */        int level = ((count % 5) + 1) * 10000;        String msg = "This is log message number" + String.valueOf(count);        logger.log(Level.toLevel(level), msg);        Transaction transaction = ch.getTransaction();        transaction.begin();        Event event = ch.take();        Assert.assertNotNull(event);        Assert.assertEquals(new String(event.getBody(), "UTF8"), msg);        Map<String, String> hdrs = event.getHeaders();        Assert.assertNotNull(hdrs.get(Log4jAvroHeaders.TIMESTAMP.toString()));        Assert.assertEquals(Level.toLevel(level), Level.toLevel(Integer.valueOf(hdrs.get(Log4jAvroHeaders.LOG_LEVEL.toString()))));        Assert.assertNotNull(hdrs.get(Log4jAvroHeaders.ADDRESS.toString()));        Assert.assertEquals(logger.getName(), hdrs.get(Log4jAvroHeaders.LOGGER_NAME.toString()));        Assert.assertEquals("UTF8", hdrs.get(Log4jAvroHeaders.MESSAGE_ENCODING.toString()));        transaction.commit();        transaction.close();    }}
private void flume_f1799_1(int numEvents)
{    configureSource();    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(getClass());    List<String> events = IntStream.range(0, numEvents).mapToObj(String::valueOf).collect(Collectors.toList());        Transaction tx = ch.getTransaction();    tx.begin();    for (String s : events) {        Event e = ch.take();        Assert.assertNotNull(e);        Assert.assertEquals(s, new String(e.getBody()));    }    Assert.assertNull("There should be no more events in the channel", ch.take());}
public void flume_f1800_0()
{    testBatchedSending(5);    Mockito.verify(source, Mockito.times(1)).appendBatch(Mockito.anyList());    Mockito.verify(source, Mockito.times(0)).append(Mockito.any(AvroFlumeEvent.class));}
public void flume_f1801_1()
{    configureSource();    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(getClass());        Transaction tx = ch.getTransaction();    tx.begin();    Event e = ch.take();    Assert.assertNotNull(e);    Assert.assertEquals("test", new String(e.getBody()));    Mockito.verify(source, Mockito.times(0)).appendBatch(Mockito.anyList());    Mockito.verify(source, Mockito.times(1)).append(Mockito.any(AvroFlumeEvent.class));}
public void flume_f1802_0()
{    testBatchedSending(1);    Mockito.verify(source, Mockito.times(0)).appendBatch(Mockito.anyList());    Mockito.verify(source, Mockito.times(1)).append(Mockito.any(AvroFlumeEvent.class));}
public void flume_f1803_0()
{    testBatchedSending(0);    Mockito.verify(source, Mockito.times(0)).appendBatch(Mockito.anyList());    Mockito.verify(source, Mockito.times(0)).append(Mockito.any(AvroFlumeEvent.class));}
public void flume_f1804_0() throws Throwable
{    configureSource();    props.setProperty("log4j.appender.out2.UnsafeMode", String.valueOf(true));    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(TestLog4jAppender.class);    source.stop();    sendAndAssertFail(logger);}
public void flume_f1805_0() throws Throwable
{    configureSource();    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(TestLog4jAppender.class);    source.stop();    sendAndAssertFail(logger);}
private void flume_f1806_0(Logger logger) throws Throwable
{    /*     * Log4j internally defines levels as multiples of 10000. So if we     * create levels directly using count, the level will be set as the     * default.     */    int level = 20000;    try {        logger.log(Level.toLevel(level), "Test Msg");    } catch (FlumeException ex) {        ex.printStackTrace();        throw ex.getCause();    }    Transaction transaction = ch.getTransaction();    transaction.begin();    Event event = ch.take();    Assert.assertNull(event);    transaction.commit();    transaction.close();}
public void flume_f1807_0() throws IOException
{    configureSource();    props.put("log4j.appender.out2.layout", "org.apache.log4j.PatternLayout");    props.put("log4j.appender.out2.layout.ConversionPattern", "%-5p [%t]: %m%n");    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(TestLog4jAppender.class);    Thread.currentThread().setName("Log4jAppenderTest");    for (int count = 0; count <= 100; count++) {        /*       * Log4j internally defines levels as multiples of 10000. So if we       * create levels directly using count, the level will be set as the       * default.       */        int level = ((count % 5) + 1) * 10000;        String msg = "This is log message number" + String.valueOf(count);        logger.log(Level.toLevel(level), msg);        Transaction transaction = ch.getTransaction();        transaction.begin();        Event event = ch.take();        Assert.assertNotNull(event);        StringBuilder builder = new StringBuilder();        builder.append("[").append("Log4jAppenderTest").append("]: ").append(msg);                String eventBody = new String(event.getBody(), "UTF-8");        String eventLevel = eventBody.split("\\s+")[0];        Assert.assertEquals(Level.toLevel(level).toString(), eventLevel);        Assert.assertEquals(new String(event.getBody(), "UTF8").trim().substring(eventLevel.length()).trim(), builder.toString());        Map<String, String> hdrs = event.getHeaders();        Assert.assertNotNull(hdrs.get(Log4jAvroHeaders.TIMESTAMP.toString()));        Assert.assertEquals(Level.toLevel(level), Level.toLevel(Integer.parseInt(hdrs.get(Log4jAvroHeaders.LOG_LEVEL.toString()))));        Assert.assertEquals(logger.getName(), hdrs.get(Log4jAvroHeaders.LOGGER_NAME.toString()));        Assert.assertEquals("UTF8", hdrs.get(Log4jAvroHeaders.MESSAGE_ENCODING.toString()));        transaction.commit();        transaction.close();    }}
public void flume_f1808_0() throws Throwable
{    ch = new SlowMemoryChannel(2000);    Configurables.configure(ch, new Context());    configureSource();    props.put("log4j.appender.out2.Timeout", "1000");    props.put("log4j.appender.out2.layout", "org.apache.log4j.PatternLayout");    props.put("log4j.appender.out2.layout.ConversionPattern", "%-5p [%t]: %m%n");    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(TestLog4jAppender.class);    Thread.currentThread().setName("Log4jAppenderTest");    int level = 10000;    String msg = "This is log message number" + String.valueOf(1);    try {        logger.log(Level.toLevel(level), msg);    } catch (FlumeException ex) {        throw ex.getCause();    }}
public void flume_f1809_0() throws Throwable
{    props.setProperty("log4j.appender.out2.UnsafeMode", String.valueOf(true));    testSlowness();}
public void flume_f1810_0()
{    source.stop();    ch.stop();    props.clear();}
public void flume_f1811_0(Event e)
{    try {        TimeUnit.MILLISECONDS.sleep(slowTime);    } catch (Exception ex) {        throw new RuntimeException(ex);    }    super.put(e);}
private static int flume_f1812_0() throws Exception
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
public void flume_f1813_0() throws Exception
{    URL schemaUrl = getClass().getClassLoader().getResource("myrecord.avsc");    Files.copy(Resources.newInputStreamSupplier(schemaUrl), new File("/tmp/myrecord.avsc"));    port = getFreePort();    source = new AvroSource();    ch = new MemoryChannel();    Configurables.configure(ch, new Context());    Context context = new Context();    context.put("port", String.valueOf(port));    context.put("bind", "localhost");    Configurables.configure(source, context);    List<Channel> channels = new ArrayList<>();    channels.add(ch);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    source.start();}
private void flume_f1814_0(String file) throws IOException
{    File TESTFILE = new File(TestLog4jAppenderWithAvro.class.getClassLoader().getResource(file).getFile());    FileReader reader = new FileReader(TESTFILE);    props = new Properties();    props.load(reader);    reader.close();}
public void flume_f1815_1() throws IOException
{    loadProperties("flume-log4jtest-avro-generic.properties");    props.put("log4j.appender.out2.Port", String.valueOf(port));    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(TestLog4jAppenderWithAvro.class);    String msg = "This is log message number " + String.valueOf(0);    Schema schema = new Schema.Parser().parse(getClass().getClassLoader().getResource("myrecord.avsc").openStream());    GenericRecordBuilder builder = new GenericRecordBuilder(schema);    GenericRecord record = builder.set("message", msg).build();        Transaction transaction = ch.getTransaction();    transaction.begin();    Event event = ch.take();    Assert.assertNotNull(event);    GenericDatumReader<GenericRecord> reader = new GenericDatumReader<>(schema);    BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(event.getBody(), null);    GenericRecord recordFromEvent = reader.read(null, decoder);    Assert.assertEquals(msg, recordFromEvent.get("message").toString());    Map<String, String> hdrs = event.getHeaders();    Assert.assertNull(hdrs.get(Log4jAvroHeaders.MESSAGE_ENCODING.toString()));    Assert.assertEquals("Schema URL should be set", "file:///tmp/myrecord.avsc", hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_URL.toString()));    Assert.assertNull("Schema string should not be set", hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_LITERAL.toString()));    transaction.commit();    transaction.close();}
public void flume_f1816_1() throws IOException
{    loadProperties("flume-log4jtest-avro-reflect.properties");    props.put("log4j.appender.out2.Port", String.valueOf(port));    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(TestLog4jAppenderWithAvro.class);    String msg = "This is log message number " + String.valueOf(0);    AppEvent appEvent = new AppEvent();    appEvent.setMessage(msg);        Transaction transaction = ch.getTransaction();    transaction.begin();    Event event = ch.take();    Assert.assertNotNull(event);    Schema schema = ReflectData.get().getSchema(appEvent.getClass());    ReflectDatumReader<AppEvent> reader = new ReflectDatumReader<>(AppEvent.class);    BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(event.getBody(), null);    AppEvent recordFromEvent = reader.read(null, decoder);    Assert.assertEquals(msg, recordFromEvent.getMessage());    Map<String, String> hdrs = event.getHeaders();    Assert.assertNull(hdrs.get(Log4jAvroHeaders.MESSAGE_ENCODING.toString()));    Assert.assertNull("Schema URL should not be set", hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_URL.toString()));    Assert.assertEquals("Schema string should be set", schema.toString(), hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_LITERAL.toString()));    transaction.commit();    transaction.close();}
public void flume_f1817_1() throws IOException
{    loadProperties("flume-log4jtest-avro-reflect.properties");    props.put("log4j.appender.out2.Port", String.valueOf(port));    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(getClass());    List<Object> events = Arrays.asList("string", new AppEvent("appEvent"));        Transaction transaction = ch.getTransaction();    transaction.begin();    for (Object o : events) {        Event e = ch.take();        Assert.assertNotNull(e);        ReflectDatumReader<?> reader = new ReflectDatumReader<>(o.getClass());        BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(e.getBody(), null);        Object readObject = reader.read(null, decoder);        Assert.assertEquals(o, readObject);        Map<String, String> hdrs = e.getHeaders();        Assert.assertNull(hdrs.get(Log4jAvroHeaders.MESSAGE_ENCODING.toString()));        Assert.assertNull("Schema URL should not be set", hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_URL.toString()));        Assert.assertEquals("Schema string should be set", ReflectData.get().getSchema(readObject.getClass()).toString(), hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_LITERAL.toString()));    }    Assert.assertNull("There should be no more events in the channel", ch.take());}
public void flume_f1818_1() throws IOException
{    loadProperties("flume-log4jtest-avro-generic.properties");    props.put("log4j.appender.out2.Port", String.valueOf(port));    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(getClass());    String msg = "Avro log message";    Schema schema = new Schema.Parser().parse(getClass().getClassLoader().getResource("myrecord.avsc").openStream());    GenericRecordBuilder builder = new GenericRecordBuilder(schema);    GenericRecord record = builder.set("message", msg).build();    List<Object> events = Arrays.asList("string", record);        Transaction transaction = ch.getTransaction();    transaction.begin();    Event event = ch.take();    Assert.assertNotNull(event);    Assert.assertEquals("string", new String(event.getBody()));    event = ch.take();    Assert.assertNotNull(event);    GenericDatumReader<GenericRecord> reader = new GenericDatumReader<>(schema);    BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(event.getBody(), null);    GenericRecord recordFromEvent = reader.read(null, decoder);    Assert.assertEquals(msg, recordFromEvent.get("message").toString());    Map<String, String> hdrs = event.getHeaders();    Assert.assertNull(hdrs.get(Log4jAvroHeaders.MESSAGE_ENCODING.toString()));    Assert.assertEquals("Schema URL should be set", "file:///tmp/myrecord.avsc", hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_URL.toString()));    Assert.assertNull("Schema string should not be set", hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_LITERAL.toString()));    transaction.commit();    transaction.close();}
public void flume_f1819_0()
{    source.stop();    ch.stop();    props.clear();}
public String flume_f1820_0()
{    return message;}
public void flume_f1821_0(String message)
{    this.message = message;}
public boolean flume_f1822_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    AppEvent appEvent = (AppEvent) o;    return message != null ? message.equals(appEvent.message) : appEvent.message == null;}
public int flume_f1823_0()
{    return message != null ? message.hashCode() : 0;}
public void flume_f1824_0(String name)
{    this.name = name;}
public String flume_f1825_0()
{    return name;}
public String flume_f1826_0(String key)
{    return System.getenv(key);}
public void flume_f1827_0(Map<String, String> configuration)
{}
public void flume_f1828_0()
{    environmentVariables.set(MY_PASSWORD_KEY, FILTERED);    environmentVariables.set(MY_PASSWORD_KEY_2, FILTERED_2);    ConfigFilter configFilter = new EnvironmentVariableConfigFilter();    assertEquals(FILTERED, configFilter.filter(MY_PASSWORD_KEY));    assertEquals(FILTERED_2, configFilter.filter(MY_PASSWORD_KEY_2));}
public void flume_f1829_0()
{    ConfigFilter configFilter = new EnvironmentVariableConfigFilter();    assertNull(configFilter.filter("unknown"));}
public String flume_f1830_1(String key)
{    try {        return execCommand(key);    } catch (InterruptedException | IllegalStateException | IOException ex) {            }    return null;}
public void flume_f1831_0(Map<String, String> configuration)
{    String charsetName = configuration.getOrDefault(CHARSET_KEY, CHARSET_DEFAULT);    try {        charset = Charset.forName(charsetName);    } catch (UnsupportedCharsetException ex) {        throw new RuntimeException("Unsupported charset: " + charsetName, ex);    }    command = configuration.get(COMMAND_KEY);    if (command == null) {        throw new IllegalArgumentException(COMMAND_KEY + " must be set for " + "ExternalProcessConfigFilter");    }}
private String flume_f1832_0(String key) throws IOException, InterruptedException
{    String[] split = command.split("\\s+");    int newLength = split.length + 1;    String[] commandParts = Arrays.copyOf(split, newLength);    commandParts[newLength - 1] = key;    Process p = Runtime.getRuntime().exec(commandParts);    p.waitFor();    if (p.exitValue() != 0) {        String stderr;        try {            stderr = getResultFromStream(p.getErrorStream());        } catch (Throwable t) {            stderr = null;        }        throw new IllegalStateException(String.format("Process (%s) exited with non-zero (%s) status code. Sterr: %s", this.command, p.exitValue(), stderr));    }    return getResultFromStream(p.getInputStream());}
private String flume_f1833_1(InputStream inputStream)
{    try (Scanner scanner = new Scanner(inputStream, charset.name())) {        String result = null;        if (scanner.hasNextLine()) {            result = scanner.nextLine();            if (scanner.hasNextLine()) {                            }        } else {                    }        return result;    }}
public void flume_f1834_0()
{    configFilter = new ExternalProcessConfigFilter();}
public void flume_f1835_0()
{    String file = Thread.currentThread().getContextClassLoader().getResource("test.sh").getFile();    File testExecutable = new File(file);    testExecutable.setExecutable(true);    HashMap<String, String> configuration = new HashMap<>();    configuration.put("command", file);    configFilter.initializeWithConfiguration(configuration);    assertEquals(FILTERED, configFilter.filter(MY_PASSWORD_KEY));    assertEquals(FILTERED_2, configFilter.filter(MY_PASSWORD_KEY_2));}
public void flume_f1836_0()
{    String file = Thread.currentThread().getContextClassLoader().getResource("test_error.sh").getFile();    File testExecutable = new File(file);    testExecutable.setExecutable(true);    HashMap<String, String> configuration = new HashMap<>();    configuration.put("command", file);    configFilter.initializeWithConfiguration(configuration);    assertNull(configFilter.filter(MY_PASSWORD_KEY));}
public void flume_f1837_1(Map<String, String> configuration)
{        hadoopConfiguration = new Configuration();    hadoopConfiguration.set(HADOOP_SECURITY + CREDENTIAL_PROVIDER_PATH, configuration.get(CREDENTIAL_PROVIDER_PATH));    String passwordFile = configuration.get(PASSWORD_FILE_CONFIG_KEY);    if (passwordFile != null && !passwordFile.isEmpty()) {        checkPasswordFile(passwordFile);        hadoopConfiguration.set(HADOOP_SECURITY + PASSWORD_FILE_CONFIG_KEY, passwordFile);    }}
private void flume_f1838_1(String passwordFile)
{    if (Thread.currentThread().getContextClassLoader().getResource(passwordFile) == null) {            }}
public String flume_f1839_1(String key)
{    char[] result = null;    try {        result = hadoopConfiguration.getPassword(key);    } catch (IOException e) {            }    return result == null ? null : new String(result);}
public static void flume_f1840_0() throws Exception
{    generateTempFileNames();    fillCredStoreWithDefaultPassword();    fillCredStoreWithPasswordFile();    fillCredStoreWithEnvironmentVariablePassword();}
public static void flume_f1841_0()
{    fileDefault.deleteOnExit();    fileEnvPassword.deleteOnExit();    fileFilePassword.deleteOnExit();}
public void flume_f1842_0()
{    String[] objects = System.getenv().keySet().toArray(new String[0]);    environmentVariables.clear(objects);    configFilter = new HadoopCredentialStoreConfigFilter();}
public void flume_f1843_0()
{    HashMap<String, String> configuration = new HashMap<>();    configuration.put(CREDENTIAL_PROVIDER_PATH, providerPathDefault);    configFilter.initializeWithConfiguration(configuration);    assertEquals("filtered_default", configFilter.filter("password"));}
public void flume_f1844_0()
{    environmentVariables.set("HADOOP_CREDSTORE_PASSWORD", "envSecret");    HashMap<String, String> configuration = new HashMap<>();    configuration.put(CREDENTIAL_PROVIDER_PATH, providerPathEnv);    configFilter.initializeWithConfiguration(configuration);    assertEquals("filtered_env", configFilter.filter("password"));}
public void flume_f1845_0()
{    HashMap<String, String> configuration = new HashMap<>();    configuration.put(CREDENTIAL_PROVIDER_PATH, providerPathPwdFile);    configuration.put(PASSWORD_FILE_CONFIG_KEY, "test-password.txt");    configFilter.initializeWithConfiguration(configuration);    assertEquals("filtered_file", configFilter.filter("password"));}
public void flume_f1846_0()
{    HashMap<String, String> configuration = new HashMap<>();    configuration.put(CREDENTIAL_PROVIDER_PATH, providerPathEnv);    configFilter.initializeWithConfiguration(configuration);    assertNull(configFilter.filter("password"));}
public void flume_f1847_0()
{    HashMap<String, String> configuration = new HashMap<>();    configuration.put(CREDENTIAL_PROVIDER_PATH, providerPathPwdFile);    configuration.put(PASSWORD_FILE_CONFIG_KEY, "test-password2.txt");    configFilter.initializeWithConfiguration(configuration);    assertNull(configFilter.filter("password"));}
public void flume_f1848_0()
{    HashMap<String, String> configuration = new HashMap<>();    configuration.put(CREDENTIAL_PROVIDER_PATH, providerPathPwdFile);    configFilter.initializeWithConfiguration(configuration);    assertNull(configFilter.filter("password"));}
public void flume_f1849_0()
{    HashMap<String, String> configuration = new HashMap<>();    configFilter.initializeWithConfiguration(configuration);}
private static void flume_f1850_0() throws Exception
{    environmentVariables.set("HADOOP_CREDSTORE_PASSWORD", "envSecret");    runCommand("create password -value filtered_env -provider " + providerPathEnv, new Configuration());}
private static void flume_f1851_0() throws Exception
{    Configuration conf = new Configuration();    conf.set(HADOOP_SECURITY + PASSWORD_FILE_CONFIG_KEY, "test-password.txt");    runCommand("create password -value filtered_file -provider " + providerPathPwdFile, conf);}
private static void flume_f1852_0() throws Exception
{    runCommand("create password -value filtered_default -provider " + providerPathDefault, new Configuration());}
private static void flume_f1853_0() throws IOException
{    fileDefault = Files.createTempFile("test-default-pwd-", ".jceks").toFile();    boolean deleted = fileDefault.delete();    fileEnvPassword = Files.createTempFile("test-env-pwd-", ".jceks").toFile();    deleted &= fileEnvPassword.delete();    fileFilePassword = Files.createTempFile("test-file-pwd-", ".jceks").toFile();    deleted &= fileFilePassword.delete();    if (!deleted) {        fail("Could not delete temporary files");    }    providerPathDefault = "jceks://file/" + fileDefault.getAbsolutePath();    providerPathEnv = "jceks://file/" + fileEnvPassword.getAbsolutePath();    providerPathPwdFile = "jceks://file/" + fileFilePassword.getAbsolutePath();}
private static void flume_f1854_0(String c, Configuration conf) throws Exception
{    ToolRunner.run(conf, new CredentialShell(), c.split(" "));}
public String flume_f1855_0()
{    return channelConfigurationType;}
public ChannelConfiguration flume_f1856_0(String name) throws ConfigurationException
{    if (this == OTHER) {        return new ChannelConfiguration(name);    }    Class<? extends ChannelConfiguration> clazz;    ChannelConfiguration instance = null;    try {        if (channelConfigurationType != null) {            clazz = (Class<? extends ChannelConfiguration>) Class.forName(channelConfigurationType);            instance = clazz.getConstructor(String.class).newInstance(name);        } else {            return new ChannelConfiguration(name);        }    } catch (ClassNotFoundException e) {                instance = new ChannelConfiguration(name);                instance.setNotFoundConfigClass();    } catch (Exception e) {        throw new ConfigurationException(e);    }    return instance;}
public Set<String> flume_f1857_0()
{    return channelNames;}
public void flume_f1858_0(Set<String> channelNames)
{    this.channelNames = channelNames;}
public String flume_f1859_0()
{    return this.selectorType;}
public ChannelSelectorConfiguration flume_f1860_0(String name) throws ConfigurationException
{    if (this == OTHER) {        return new ChannelSelectorConfiguration(name);    }    Class<? extends ChannelSelectorConfiguration> clazz;    ChannelSelectorConfiguration instance = null;    try {                if (this.selectorType != null) {            clazz = (Class<? extends ChannelSelectorConfiguration>) Class.forName(this.selectorType);            instance = clazz.getConstructor(String.class).newInstance(name);        } else {            return new ChannelSelectorConfiguration(name);        }    } catch (ClassNotFoundException e) {                instance = new ChannelSelectorConfiguration(name);                instance.setNotFoundConfigClass();    } catch (Exception e) {        throw new ConfigurationException("Configuration error!", e);    }    return instance;}
public String flume_f1861_0()
{    return channelSelectorClassName;}
public String flume_f1862_0()
{    return channelSelectorClassName;}
public String flume_f1863_0()
{    return channelClassName;}
public String flume_f1864_0()
{    return channelClassName;}
public boolean flume_f1865_0()
{    return notFoundConfigClass;}
public void flume_f1866_0()
{    this.notFoundConfigClass = true;}
public List<FlumeConfigurationError> flume_f1867_0()
{    return errors;}
public void flume_f1868_0(Context context) throws ConfigurationException
{    failIfConfigured();    String confType = context.getString(BasicConfigurationConstants.CONFIG_TYPE);    if (confType != null && !confType.isEmpty()) {        this.type = confType;    }        if (this.type == null || this.type.isEmpty()) {        errors.add(new FlumeConfigurationError(componentName, BasicConfigurationConstants.CONFIG_TYPE, FlumeConfigurationErrorType.ATTRS_MISSING, ErrorOrWarning.ERROR));        throw new ConfigurationException("Component has no type. Cannot configure. " + componentName);    }}
protected void flume_f1869_0() throws ConfigurationException
{    if (configured) {        throw new ConfigurationException("Already configured component." + componentName);    }}
public String flume_f1870_0()
{    return type;}
public void flume_f1871_0(String type)
{    this.type = type;}
public String flume_f1872_0()
{    return toString(0);}
public String flume_f1873_0(int indentCount)
{    StringBuilder indentSb = new StringBuilder();    for (int i = 0; i < indentCount; i++) {        indentSb.append(FlumeConfiguration.INDENTSTEP);    }    String indent = indentSb.toString();    StringBuilder sb = new StringBuilder(indent);    sb.append("ComponentConfiguration[").append(componentName).append("]");    sb.append(FlumeConfiguration.NEWLINE).append(indent).append(FlumeConfiguration.INDENTSTEP).append("CONFIG: ");    sb.append(FlumeConfiguration.NEWLINE).append(indent).append(FlumeConfiguration.INDENTSTEP);    return sb.toString();}
public String flume_f1874_0()
{    return componentName;}
protected void flume_f1875_0()
{    configured = true;}
public String flume_f1876_0()
{    return componentType;}
public static ComponentConfiguration flume_f1877_0(String name, String type, ComponentType component) throws ConfigurationException
{    Class<? extends ComponentConfiguration> confType = null;    if (type == null) {        throw new ConfigurationException("Cannot create component without knowing its type!");    }    try {        confType = (Class<? extends ComponentConfiguration>) Class.forName(type);        return confType.getConstructor(String.class).newInstance(type);    } catch (Exception ignored) {        try {            type = type.toUpperCase(Locale.ENGLISH);            switch(component) {                case SOURCE:                    return SourceConfigurationType.valueOf(type.toUpperCase(Locale.ENGLISH)).getConfiguration(name);                case CONFIG_FILTER:                    return ConfigFilterConfigurationType.valueOf(type.toUpperCase(Locale.ENGLISH)).getConfiguration(name);                case SINK:                    return SinkConfigurationType.valueOf(type.toUpperCase(Locale.ENGLISH)).getConfiguration(name);                case CHANNEL:                    return ChannelConfigurationType.valueOf(type.toUpperCase(Locale.ENGLISH)).getConfiguration(name);                case SINK_PROCESSOR:                    return SinkProcessorConfigurationType.valueOf(type.toUpperCase(Locale.ENGLISH)).getConfiguration(name);                case CHANNELSELECTOR:                    return ChannelSelectorConfigurationType.valueOf(type.toUpperCase(Locale.ENGLISH)).getConfiguration(name);                case SINKGROUP:                    return new SinkGroupConfiguration(name);                default:                    throw new ConfigurationException("Cannot create configuration. Unknown Type specified: " + type);            }        } catch (ConfigurationException e) {            throw e;        } catch (Exception e) {            throw new ConfigurationException("Could not create configuration! " + " Due to " + e.getClass().getSimpleName() + ": " + e.getMessage(), e);        }    }}
public String flume_f1878_0()
{    return configurationName;}
public ConfigFilterConfiguration flume_f1879_0(String name) throws ConfigurationException
{    if (this == OTHER) {        return new ConfigFilterConfiguration(name);    }    Class<? extends ConfigFilterConfiguration> clazz;    ConfigFilterConfiguration instance = null;    try {        if (configurationName != null) {            clazz = (Class<? extends ConfigFilterConfiguration>) Class.forName(configurationName);            instance = clazz.getConstructor(String.class).newInstance(name);        } else {            return new ConfigFilterConfiguration(name);        }    } catch (ClassNotFoundException e) {                instance = new ConfigFilterConfiguration(name);                instance.setNotFoundConfigClass();    } catch (Exception e) {        throw new ConfigurationException("Couldn't create configuration", e);    }    return instance;}
public String flume_f1880_0()
{    return className;}
public static ConfigFilter flume_f1881_1(String name, String type) throws FlumeException
{    Preconditions.checkNotNull(name, "name");    Preconditions.checkNotNull(type, "type");        Class<? extends ConfigFilter> aClass = getClass(type);    try {        ConfigFilter configFilter = aClass.newInstance();        configFilter.setName(name);        return configFilter;    } catch (Exception ex) {        throw new FlumeException("Unable to create configfilter: " + name + ", type: " + type + ", class: " + aClass.getName(), ex);    }}
public static Class<? extends ConfigFilter> flume_f1882_1(String type) throws FlumeException
{    String classname = type;    ConfigFilterType srcType = ConfigFilterType.OTHER;    try {        srcType = ConfigFilterType.valueOf(type.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException ex) {            }    if (srcType != ConfigFilterType.OTHER) {        classname = srcType.getClassName();    }    try {        return (Class<? extends ConfigFilter>) Class.forName(classname);    } catch (Exception ex) {        throw new FlumeException("Unable to load configfilter type: " + type + ", class: " + classname, ex);    }}
public List<FlumeConfigurationError> flume_f1883_0()
{    return errors;}
public AgentConfiguration flume_f1884_0(String hostname)
{    return agentConfigMap.get(hostname);}
private void flume_f1885_1()
{    Set<Entry<String, AgentConfiguration>> entries = agentConfigMap.entrySet();    Iterator<Entry<String, AgentConfiguration>> it = entries.iterator();    while (it.hasNext()) {        Entry<String, AgentConfiguration> next = it.next();        String agentName = next.getKey();        AgentConfiguration aconf = next.getValue();        if (!aconf.isValid()) {                        addError(agentName, AGENT_CONFIGURATION_INVALID, ERROR);            it.remove();        }                            }    }
private boolean flume_f1886_0(String rawName, String rawValue)
{        if (rawName == null || rawValue == null) {        addError("", AGENT_NAME_MISSING, ERROR);        return false;    }        String name = rawName.trim();    String value = rawValue.trim();        if (value.isEmpty()) {        addError(name, PROPERTY_VALUE_NULL, ERROR);        return false;    }    int index = name.indexOf('.');        if (index == -1) {        addError(name, AGENT_NAME_MISSING, ERROR);        return false;    }    String agentName = name.substring(0, index);        if (agentName.isEmpty()) {        addError(name, AGENT_NAME_MISSING, ERROR);        return false;    }    String configKey = name.substring(index + 1);        if (configKey.isEmpty()) {        addError(name, PROPERTY_NAME_NULL, ERROR);        return false;    }    AgentConfiguration aconf = agentConfigMap.get(agentName);    if (aconf == null) {        aconf = new AgentConfiguration(agentName, errors);        agentConfigMap.put(agentName, aconf);    }        return aconf.addProperty(configKey, value);}
private void flume_f1887_0(String component, FlumeConfigurationErrorType errorType, ErrorOrWarning level)
{    errors.add(new FlumeConfigurationError(component, "", errorType, level));}
public Map<String, ComponentConfiguration> flume_f1888_0()
{    return channelConfigMap;}
public Map<String, ComponentConfiguration> flume_f1889_0()
{    return sourceConfigMap;}
public Map<String, ComponentConfiguration> flume_f1890_0()
{    return configFilterConfigMap;}
public Map<String, ComponentConfiguration> flume_f1891_0()
{    return sinkConfigMap;}
public Map<String, ComponentConfiguration> flume_f1892_0()
{    return sinkgroupConfigMap;}
public Map<String, Context> flume_f1893_0()
{    return configFilterContextMap;}
public Map<String, Context> flume_f1894_0()
{    return sourceContextMap;}
public Map<String, Context> flume_f1895_0()
{    return sinkContextMap;}
public Map<String, Context> flume_f1896_0()
{    return channelContextMap;}
public Set<String> flume_f1897_0()
{    return sinkSet;}
public Set<String> flume_f1898_0()
{    return configFilterSet;}
public Set<String> flume_f1899_0()
{    return sourceSet;}
public Set<String> flume_f1900_0()
{    return channelSet;}
public Set<String> flume_f1901_0()
{    return sinkgroupSet;}
private boolean flume_f1902_1()
{        if (LOGGER.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {            }    configFilterSet = validateConfigFilterSet();    createConfigFilters();    runFiltersThroughConfigs();        if (channels == null || channels.trim().isEmpty()) {                addError(CONFIG_CHANNELS, PROPERTY_VALUE_NULL, ERROR);        return false;    }    channelSet = new HashSet<>(Arrays.asList(channels.split("\\s+")));    channelSet = validateChannels(channelSet);    if (channelSet.isEmpty()) {                addError(CONFIG_CHANNELS, PROPERTY_VALUE_NULL, ERROR);        return false;    }    sourceSet = validateSources(channelSet);    sinkSet = validateSinks(channelSet);    sinkgroupSet = validateGroups(sinkSet);        if (sourceSet.isEmpty() && sinkSet.isEmpty()) {                addError(CONFIG_SOURCES, PROPERTY_VALUE_NULL, ERROR);        addError(CONFIG_SINKS, PROPERTY_VALUE_NULL, ERROR);        return false;    }        this.configFilters = getSpaceDelimitedList(configFilterSet);    sources = getSpaceDelimitedList(sourceSet);    channels = getSpaceDelimitedList(channelSet);    sinks = getSpaceDelimitedList(sinkSet);    sinkgroups = getSpaceDelimitedList(sinkgroupSet);    if (LOGGER.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {                    }    return true;}
private void flume_f1903_0()
{    runFiltersOnContextMaps(sourceContextMap, channelContextMap, sinkContextMap, sinkGroupContextMap);}
private void flume_f1904_0(Map<String, Context>... maps)
{    for (Map<String, Context> map : maps) {        for (Context context : map.values()) {            for (String key : context.getParameters().keySet()) {                filterValue(context, key);            }        }    }}
private void flume_f1905_1()
{    for (String name : configFilterSet) {        Context context = configFilterContextMap.get(name);        ComponentConfiguration componentConfiguration = configFilterConfigMap.get(name);        try {            if (context != null) {                ConfigFilter configFilter = ConfigFilterFactory.create(name, context.getString(BasicConfigurationConstants.CONFIG_TYPE));                configFilter.initializeWithConfiguration(context.getParameters());                configFiltersInstances.add(configFilter);                configFilterPatternCache.put(configFilter.getName(), createConfigFilterPattern(configFilter));            } else if (componentConfiguration != null) {                ConfigFilter configFilter = ConfigFilterFactory.create(componentConfiguration.getComponentName(), componentConfiguration.getType());                configFiltersInstances.add(configFilter);                configFilterPatternCache.put(configFilter.getName(), createConfigFilterPattern(configFilter));            }        } catch (Exception e) {                    }    }}
private Pattern flume_f1906_0(ConfigFilter configFilter)
{        return Pattern.compile(    "\\$\\{" +     Pattern.quote(configFilter.getName()) +     "\\[(|'|\")" +     "(?<key>[-_a-zA-Z0-9]+)" +     "\\1\\]" +     "\\}");}
private void flume_f1907_1(Context c, String contextKey)
{    for (ConfigFilter configFilter : configFiltersInstances) {        try {            Pattern pattern = configFilterPatternCache.get(configFilter.getName());            String currentValue = c.getString(contextKey);            Matcher matcher = pattern.matcher(currentValue);            String filteredValue = currentValue;            while (matcher.find()) {                String key = matcher.group("key");                                String filtered = configFilter.filter(key);                if (filtered == null) {                    continue;                }                String fullMatch = matcher.group();                filteredValue = filteredValue.replace(fullMatch, filtered);            }            c.put(contextKey, filteredValue);        } catch (Exception e) {            e.printStackTrace();                    }    }}
private void flume_f1908_0(String key, FlumeConfigurationErrorType errorType, ErrorOrWarning level)
{    errorList.add(new FlumeConfigurationError(agentName, key, errorType, level));}
private ChannelType flume_f1909_0(String type)
{    return getKnownComponent(type, ChannelType.values());}
private SinkType flume_f1910_0(String type)
{    return getKnownComponent(type, SinkType.values());}
private SourceType flume_f1911_0(String type)
{    return getKnownComponent(type, SourceType.values());}
private ConfigFilterType flume_f1912_0(String type)
{    return getKnownComponent(type, ConfigFilterType.values());}
private T flume_f1913_0(String type, T[] values)
{    for (T value : values) {        if (value.toString().equalsIgnoreCase(type))            return value;        String src = value.getClassName();        if (src != null && src.equalsIgnoreCase(type))            return value;    }    return null;}
private Set<String> flume_f1914_1(Set<String> channelSet)
{    Iterator<String> iter = channelSet.iterator();    Map<String, Context> newContextMap = new HashMap<>();    ChannelConfiguration conf = null;    /*       * The logic for the following code:       *       * Is it a known component?       *  -Yes: Get the ChannelType and set the string name of that to       *        config and set configSpecified to true.       *  -No.Look for config type for the given component:       *      -Config Found:       *        Set config to the type mentioned, set configSpecified to true       *      -No Config found:       *        Set config to OTHER, configSpecified to false,       *        do basic validation. Leave the context in the       *        contextMap to process later. Setting it to other returns       *        a vanilla configuration(Source/Sink/Channel Configuration),       *        which does basic syntactic validation. This object is not       *        put into the map, so the context is retained which can be       *        picked up - this is meant for older classes which don't       *        implement ConfigurableComponent.       */    while (iter.hasNext()) {        String channelName = iter.next();        Context channelContext = channelContextMap.get(channelName);                if (channelContext != null) {                        ChannelType chType = getKnownChannel(channelContext.getString(BasicConfigurationConstants.CONFIG_TYPE));            boolean configSpecified = false;            String config = null;                        if (chType == null) {                config = channelContext.getString(CONFIG_CONFIG);                if (config == null || config.isEmpty()) {                    config = "OTHER";                } else {                    configSpecified = true;                }            } else {                config = chType.toString().toUpperCase(Locale.ENGLISH);                configSpecified = true;            }            try {                conf = (ChannelConfiguration) ComponentConfigurationFactory.create(channelName, config, ComponentType.CHANNEL);                                if (conf != null) {                    conf.configure(channelContext);                }                if ((configSpecified && conf.isNotFoundConfigClass()) || !configSpecified) {                    newContextMap.put(channelName, channelContext);                } else if (configSpecified) {                    channelConfigMap.put(channelName, conf);                }                if (conf != null) {                    errorList.addAll(conf.getErrors());                }            } catch (ConfigurationException e) {                                if (conf != null)                    errorList.addAll(conf.getErrors());                iter.remove();                            }        } else {            iter.remove();            addError(channelName, CONFIG_ERROR, ERROR);        }    }    channelContextMap = newContextMap;    Set<String> tempchannelSet = new HashSet<String>();    tempchannelSet.addAll(channelConfigMap.keySet());    tempchannelSet.addAll(channelContextMap.keySet());    channelSet.retainAll(tempchannelSet);    return channelSet;}
private Set<String> flume_f1915_1()
{    if (configFilters == null || configFilters.isEmpty()) {                return new HashSet<>();    }    Set<String> configFilterSet = new HashSet<>(Arrays.asList(configFilters.split("\\s+")));    Map<String, Context> newContextMap = new HashMap<>();    Iterator<String> iter = configFilterSet.iterator();    ConfigFilterConfiguration conf = null;    while (iter.hasNext()) {        String configFilterName = iter.next();        Context configFilterContext = configFilterContextMap.get(configFilterName);        if (configFilterContext != null) {                        ConfigFilterType chType = getKnownConfigFilter(configFilterContext.getString(BasicConfigurationConstants.CONFIG_TYPE));            boolean configSpecified = false;            String config = null;                        if (chType == null) {                config = configFilterContext.getString(CONFIG_CONFIG);                if (config == null || config.isEmpty()) {                    config = "OTHER";                } else {                    configSpecified = true;                }            } else {                config = chType.toString().toUpperCase(Locale.ENGLISH);                configSpecified = true;            }            try {                conf = (ConfigFilterConfiguration) ComponentConfigurationFactory.create(configFilterName, config, ComponentType.CONFIG_FILTER);                                if (conf != null) {                    conf.configure(configFilterContext);                }                if ((configSpecified && conf.isNotFoundConfigClass()) || !configSpecified) {                    newContextMap.put(configFilterName, configFilterContext);                } else if (configSpecified) {                    configFilterConfigMap.put(configFilterName, conf);                }                if (conf != null) {                    errorList.addAll(conf.getErrors());                }            } catch (ConfigurationException e) {                if (conf != null)                    errorList.addAll(conf.getErrors());                iter.remove();                            }        } else {            iter.remove();            addError(configFilterName, CONFIG_ERROR, ERROR);                    }    }    configFilterContextMap = newContextMap;    Set<String> tempchannelSet = new HashSet<String>();    tempchannelSet.addAll(configFilterConfigMap.keySet());    tempchannelSet.addAll(configFilterContextMap.keySet());    configFilterSet.retainAll(tempchannelSet);    return configFilterSet;}
private Set<String> flume_f1916_1(Set<String> channelSet)
{        if (sources == null || sources.isEmpty()) {                addError(CONFIG_SOURCES, PROPERTY_VALUE_NULL, WARNING);        return new HashSet<String>();    }    Set<String> sourceSet = new HashSet<String>(Arrays.asList(sources.split("\\s+")));    Map<String, Context> newContextMap = new HashMap<String, Context>();    Iterator<String> iter = sourceSet.iterator();    SourceConfiguration srcConf = null;    /*       * The logic for the following code:       *       * Is it a known component?       *  -Yes: Get the SourceType and set the string name of that to       *        config and set configSpecified to true.       *  -No.Look for config type for the given component:       *      -Config Found:       *        Set config to the type mentioned, set configSpecified to true       *      -No Config found:       *        Set config to OTHER, configSpecified to false,       *        do basic validation. Leave the context in the       *        contextMap to process later. Setting it to other returns       *        a vanilla configuration(Source/Sink/Channel Configuration),       *        which does basic syntactic validation. This object is not       *        put into the map, so the context is retained which can be       *        picked up - this is meant for older classes which don't       *        implement ConfigurableComponent.       */    while (iter.hasNext()) {        String sourceName = iter.next();        Context srcContext = sourceContextMap.get(sourceName);        String config = null;        boolean configSpecified = false;        if (srcContext != null) {            SourceType srcType = getKnownSource(srcContext.getString(BasicConfigurationConstants.CONFIG_TYPE));            if (srcType == null) {                config = srcContext.getString(CONFIG_CONFIG);                if (config == null || config.isEmpty()) {                    config = "OTHER";                } else {                    configSpecified = true;                }            } else {                config = srcType.toString().toUpperCase(Locale.ENGLISH);                configSpecified = true;            }            try {                                                srcConf = (SourceConfiguration) ComponentConfigurationFactory.create(sourceName, config, ComponentType.SOURCE);                if (srcConf != null) {                    srcConf.configure(srcContext);                    Set<String> channels = new HashSet<String>();                    if (srcConf.getChannels() != null) {                        channels.addAll(srcConf.getChannels());                    }                    channels.retainAll(channelSet);                    if (channels.isEmpty()) {                        throw new ConfigurationException("No Channels configured for " + sourceName);                    }                    srcContext.put(CONFIG_CHANNELS, this.getSpaceDelimitedList(channels));                }                if ((configSpecified && srcConf.isNotFoundConfigClass()) || !configSpecified) {                    newContextMap.put(sourceName, srcContext);                } else if (configSpecified) {                    sourceConfigMap.put(sourceName, srcConf);                }                if (srcConf != null)                    errorList.addAll(srcConf.getErrors());            } catch (ConfigurationException e) {                if (srcConf != null)                    errorList.addAll(srcConf.getErrors());                iter.remove();                            }        } else {            iter.remove();            addError(sourceName, CONFIG_ERROR, ERROR);                    }    }            sourceContextMap = newContextMap;    Set<String> tempsourceSet = new HashSet<String>();    tempsourceSet.addAll(sourceContextMap.keySet());    tempsourceSet.addAll(sourceConfigMap.keySet());    sourceSet.retainAll(tempsourceSet);    return sourceSet;}
private Set<String> flume_f1917_1(Set<String> channelSet)
{            Map<String, Context> newContextMap = new HashMap<String, Context>();    Set<String> sinkSet;    SinkConfiguration sinkConf = null;    if (sinks == null || sinks.isEmpty()) {                addError(CONFIG_SINKS, PROPERTY_VALUE_NULL, WARNING);        return new HashSet<String>();    } else {        sinkSet = new HashSet<String>(Arrays.asList(sinks.split("\\s+")));    }    Iterator<String> iter = sinkSet.iterator();    /*       * The logic for the following code:       *       * Is it a known component?       *  -Yes: Get the SinkType and set the string name of that to       *        config and set configSpecified to true.       *  -No.Look for config type for the given component:       *      -Config Found:       *        Set config to the type mentioned, set configSpecified to true       *      -No Config found:       *        Set config to OTHER, configSpecified to false,       *        do basic validation. Leave the context in the       *        contextMap to process later. Setting it to other returns       *        a vanilla configuration(Source/Sink/Channel Configuration),       *        which does basic syntactic validation. This object is not       *        put into the map, so the context is retained which can be       *        picked up - this is meant for older classes which don't       *        implement ConfigurableComponent.       */    while (iter.hasNext()) {        String sinkName = iter.next();        Context sinkContext = sinkContextMap.get(sinkName.trim());        if (sinkContext == null) {            iter.remove();                        addError(sinkName, CONFIG_ERROR, ERROR);        } else {            String config = null;            boolean configSpecified = false;            SinkType sinkType = getKnownSink(sinkContext.getString(BasicConfigurationConstants.CONFIG_TYPE));            if (sinkType == null) {                config = sinkContext.getString(CONFIG_CONFIG);                if (config == null || config.isEmpty()) {                    config = "OTHER";                } else {                    configSpecified = true;                }            } else {                config = sinkType.toString().toUpperCase(Locale.ENGLISH);                configSpecified = true;            }            try {                                sinkConf = (SinkConfiguration) ComponentConfigurationFactory.create(sinkName, config, ComponentType.SINK);                if (sinkConf != null) {                    sinkConf.configure(sinkContext);                }                if (!channelSet.contains(sinkConf.getChannel())) {                    throw new ConfigurationException("Channel " + sinkConf.getChannel() + " not in active set.");                }                if ((configSpecified && sinkConf.isNotFoundConfigClass()) || !configSpecified) {                    newContextMap.put(sinkName, sinkContext);                } else if (configSpecified) {                    sinkConfigMap.put(sinkName, sinkConf);                }                if (sinkConf != null)                    errorList.addAll(sinkConf.getErrors());            } catch (ConfigurationException e) {                iter.remove();                if (sinkConf != null)                    errorList.addAll(sinkConf.getErrors());                            }        }        }    sinkContextMap = newContextMap;    Set<String> tempSinkset = new HashSet<String>();    tempSinkset.addAll(sinkConfigMap.keySet());    tempSinkset.addAll(sinkContextMap.keySet());    sinkSet.retainAll(tempSinkset);    return sinkSet;}
private Set<String> flume_f1918_1(Set<String> sinkSet)
{    Set<String> sinkgroupSet = stringToSet(sinkgroups, " ");    Map<String, String> usedSinks = new HashMap<String, String>();    Iterator<String> iter = sinkgroupSet.iterator();    SinkGroupConfiguration conf;    while (iter.hasNext()) {        String sinkgroupName = iter.next();        Context context = this.sinkGroupContextMap.get(sinkgroupName);        if (context != null) {            try {                conf = (SinkGroupConfiguration) ComponentConfigurationFactory.create(sinkgroupName, "sinkgroup", ComponentType.SINKGROUP);                conf.configure(context);                Set<String> groupSinks = validGroupSinks(sinkSet, usedSinks, conf);                if (conf != null)                    errorList.addAll(conf.getErrors());                if (groupSinks != null && !groupSinks.isEmpty()) {                    List<String> sinkArray = new ArrayList<String>();                    sinkArray.addAll(groupSinks);                    conf.setSinks(sinkArray);                    sinkgroupConfigMap.put(sinkgroupName, conf);                } else {                    addError(sinkgroupName, CONFIG_ERROR, ERROR);                    if (conf != null)                        errorList.addAll(conf.getErrors());                    throw new ConfigurationException("No available sinks for sinkgroup: " + sinkgroupName + ". Sinkgroup will be removed");                }            } catch (ConfigurationException e) {                iter.remove();                addError(sinkgroupName, CONFIG_ERROR, ERROR);                            }        } else {            iter.remove();            addError(sinkgroupName, CONFIG_ERROR, ERROR);                    }    }    sinkgroupSet.retainAll(sinkgroupConfigMap.keySet());    return sinkgroupSet;}
private Set<String> flume_f1919_1(Set<String> sinkSet, Map<String, String> usedSinks, SinkGroupConfiguration groupConf)
{    Set<String> groupSinks = Collections.synchronizedSet(new HashSet<String>(groupConf.getSinks()));    if (groupSinks.isEmpty())        return null;    Iterator<String> sinkIt = groupSinks.iterator();    while (sinkIt.hasNext()) {        String curSink = sinkIt.next();        if (usedSinks.containsKey(curSink)) {                        addError(groupConf.getComponentName(), PROPERTY_PART_OF_ANOTHER_GROUP, ERROR);            sinkIt.remove();            continue;        } else if (!sinkSet.contains(curSink)) {                        addError(curSink, INVALID_PROPERTY, ERROR);            sinkIt.remove();            continue;        } else {            usedSinks.put(curSink, groupConf.getComponentName());        }    }    return groupSinks;}
private String flume_f1920_0(Set<String> entries)
{    if (entries.isEmpty()) {        return null;    }    StringBuilder sb = new StringBuilder();    for (String entry : entries) {        sb.append(" ").append(entry);    }    return sb.toString().trim();}
private static Set<String> flume_f1921_0(String target, String delim)
{    Set<String> out = new HashSet<String>();    if (target == null || target.trim().length() == 0) {        return out;    }    StringTokenizer t = new StringTokenizer(target, delim);    while (t.hasMoreTokens()) {        out.add(t.nextToken());    }    return out;}
public String flume_f1922_0()
{    StringBuilder sb = new StringBuilder("AgentConfiguration[");    sb.append(agentName).append("]").append(NEWLINE);    sb.append("CONFIG_FILTERS: ").append(configFilterContextMap).append(NEWLINE);    sb.append("SOURCES: ").append(sourceContextMap).append(NEWLINE);    sb.append("CHANNELS: ").append(channelContextMap).append(NEWLINE);    sb.append("SINKS: ").append(sinkContextMap).append(NEWLINE);    return sb.toString();}
public String flume_f1923_0()
{    StringBuilder sb = new StringBuilder("AgentConfiguration created without Configuration stubs " + "for which only basic syntactical validation was performed[");    sb.append(agentName).append("]").append(NEWLINE);    if (!sourceContextMap.isEmpty() || !sinkContextMap.isEmpty() || !channelContextMap.isEmpty()) {        if (!sourceContextMap.isEmpty()) {            sb.append("SOURCES: ").append(sourceContextMap).append(NEWLINE);        }        if (!channelContextMap.isEmpty()) {            sb.append("CHANNELS: ").append(channelContextMap).append(NEWLINE);        }        if (!sinkContextMap.isEmpty()) {            sb.append("SINKS: ").append(sinkContextMap).append(NEWLINE);        }    }    if (!sourceConfigMap.isEmpty() || !sinkConfigMap.isEmpty() || !channelConfigMap.isEmpty()) {        sb.append("AgentConfiguration created with Configuration stubs " + "for which full validation was performed[");        sb.append(agentName).append("]").append(NEWLINE);        if (!sourceConfigMap.isEmpty()) {            sb.append("SOURCES: ").append(sourceConfigMap).append(NEWLINE);        }        if (!channelConfigMap.isEmpty()) {            sb.append("CHANNELS: ").append(channelConfigMap).append(NEWLINE);        }        if (!sinkConfigMap.isEmpty()) {            sb.append("SINKS: ").append(sinkConfigMap).append(NEWLINE);        }    }    return sb.toString();}
private boolean flume_f1924_1(String key, String value)
{        if (CONFIG_CONFIGFILTERS.equals(key)) {        if (configFilters == null) {            configFilters = value;            return true;        } else {                        addError(CONFIG_CONFIGFILTERS, DUPLICATE_PROPERTY, ERROR);            return false;        }    }        if (CONFIG_SOURCES.equals(key)) {        if (sources == null) {            sources = value;            return true;        } else {                        addError(CONFIG_SOURCES, DUPLICATE_PROPERTY, ERROR);            return false;        }    }        if (CONFIG_SINKS.equals(key)) {        if (sinks == null) {            sinks = value;                        return true;        } else {                        addError(CONFIG_SINKS, DUPLICATE_PROPERTY, ERROR);            return false;        }    }        if (CONFIG_CHANNELS.equals(key)) {        if (channels == null) {            channels = value;            return true;        } else {                        addError(CONFIG_CHANNELS, DUPLICATE_PROPERTY, ERROR);            return false;        }    }        if (CONFIG_SINKGROUPS.equals(key)) {        if (sinkgroups == null) {            sinkgroups = value;            return true;        } else {                        addError(CONFIG_SINKGROUPS, DUPLICATE_PROPERTY, ERROR);            return false;        }    }    if (addAsSourceConfig(key, value) || addAsChannelValue(key, value) || addAsSinkConfig(key, value) || addAsSinkGroupConfig(key, value) || addAsConfigFilterConfig(key, value)) {        return true;    }        addError(key, INVALID_PROPERTY, ERROR);    return false;}
private boolean flume_f1925_0(String key, String value)
{    return addComponentConfig(key, value, CONFIG_CONFIGFILTERS_PREFIX, configFilterContextMap);}
private boolean flume_f1926_0(String key, String value)
{    return addComponentConfig(key, value, CONFIG_SINKGROUPS_PREFIX, sinkGroupContextMap);}
private boolean flume_f1927_0(String key, String value)
{    return addComponentConfig(key, value, CONFIG_SINKS_PREFIX, sinkContextMap);}
private boolean flume_f1928_0(String key, String value)
{    return addComponentConfig(key, value, CONFIG_CHANNELS_PREFIX, channelContextMap);}
private boolean flume_f1929_0(String key, String value)
{    return addComponentConfig(key, value, CONFIG_SOURCES_PREFIX, sourceContextMap);}
private boolean flume_f1930_1(String key, String value, String configPrefix, Map<String, Context> contextMap)
{    ComponentNameAndConfigKey parsed = parseConfigKey(key, configPrefix);    if (parsed != null) {        String name = parsed.getComponentName().trim();                Context context = contextMap.get(name);        if (context == null) {                        context = new Context();            contextMap.put(name, context);        }        context.put(parsed.getConfigKey(), value);        return true;    }    return false;}
private ComponentNameAndConfigKey flume_f1931_0(String key, String prefix)
{        if (!key.startsWith(prefix)) {        return null;    }            int index = key.indexOf('.', prefix.length() + 1);    if (index == -1) {        return null;    }    String name = key.substring(prefix.length(), index);    String configKey = key.substring(prefix.length() + name.length() + 1);        if (name.isEmpty() || configKey.isEmpty()) {        return null;    }    return new ComponentNameAndConfigKey(name, configKey);}
public String flume_f1932_0()
{    return componentName;}
public String flume_f1933_0()
{    return configKey;}
public String flume_f1934_0()
{    return componentName;}
public String flume_f1935_0()
{    return key;}
public FlumeConfigurationErrorType flume_f1936_0()
{    return errorType;}
public ErrorOrWarning flume_f1937_0()
{    return error;}
public String flume_f1938_0()
{    return error;}
public static boolean flume_f1939_0()
{    return Boolean.getBoolean(LOG_RAWDATA_PROP);}
public static boolean flume_f1940_0()
{    return Boolean.getBoolean(LOG_PRINTCONFIG_PROP);}
public String flume_f1941_0()
{    return channel;}
public void flume_f1942_0(String channel)
{    this.channel = channel;}
public void flume_f1943_0(Context context) throws ConfigurationException
{    super.configure(context);    this.channel = context.getString("channel");    if (this.channel == null || this.channel.isEmpty()) {        errors.add(new FlumeConfigurationError(componentName, "channel", FlumeConfigurationErrorType.PROPERTY_VALUE_NULL, ErrorOrWarning.ERROR));        throw new ConfigurationException("No channel configured for sink: " + this.getComponentName());    }}
public String flume_f1944_0(int indentCount)
{    String basicStr = super.toString(indentCount);    StringBuilder sb = new StringBuilder();    sb.append(basicStr).append(FlumeConfiguration.INDENTSTEP).append("CHANNEL:").append(this.channel).append(FlumeConfiguration.NEWLINE);    return sb.toString();}
public String flume_f1945_0()
{    return this.sinkConfigurationName;}
public SinkConfiguration flume_f1946_0(String name) throws ConfigurationException
{    if (this == OTHER) {        return new SinkConfiguration(name);    }    Class<? extends SinkConfiguration> clazz;    SinkConfiguration instance = null;    try {        if (sinkConfigurationName != null) {            clazz = (Class<? extends SinkConfiguration>) Class.forName(sinkConfigurationName);            instance = clazz.getConstructor(String.class).newInstance(name);        } else {            return new SinkConfiguration(name);        }    } catch (ClassNotFoundException e) {                instance = new SinkConfiguration(name);                instance.setNotFoundConfigClass();    } catch (Exception e) {        throw new ConfigurationException("Couldn't create configuration", e);    }    return instance;}
public void flume_f1947_0(List<String> sinks)
{    this.sinks = sinks;}
public List<String> flume_f1948_0()
{    return sinks;}
public void flume_f1949_0(Context context) throws ConfigurationException
{    super.configure(context);    sinks = Arrays.asList(context.getString(BasicConfigurationConstants.CONFIG_SINKS).split("\\s+"));    Map<String, String> params = context.getSubProperties(BasicConfigurationConstants.CONFIG_SINK_PROCESSOR_PREFIX);    processorContext = new Context();    processorContext.putAll(params);    SinkProcessorType spType = getKnownSinkProcessor(processorContext.getString(BasicConfigurationConstants.CONFIG_TYPE));    if (spType != null) {        processorConf = (SinkProcessorConfiguration) ComponentConfigurationFactory.create(this.getComponentName() + "-processor", spType.toString(), ComponentType.SINK_PROCESSOR);        if (processorConf != null) {            processorConf.setSinks(new HashSet<String>(sinks));            processorConf.configure(processorContext);        }    }    setConfigured();}
public Context flume_f1950_0()
{    return processorContext;}
public void flume_f1951_0(Context processorContext)
{    this.processorContext = processorContext;}
public SinkProcessorConfiguration flume_f1952_0()
{    return processorConf;}
public void flume_f1953_0(SinkProcessorConfiguration conf)
{    this.processorConf = conf;}
private SinkProcessorType flume_f1954_0(String type)
{    SinkProcessorType[] values = SinkProcessorType.values();    for (SinkProcessorType value : values) {        if (value.toString().equalsIgnoreCase(type))            return value;        String sinkProcessClassName = value.getClassName();        if (sinkProcessClassName != null && sinkProcessClassName.equalsIgnoreCase(type)) {            return value;        }    }    return null;}
public void flume_f1955_0(Context context) throws ConfigurationException
{}
public Set<String> flume_f1956_0()
{    return sinks;}
public void flume_f1957_0(Set<String> sinks)
{    this.sinks = sinks;}
public String flume_f1958_0()
{    return processorClassName;}
public SinkProcessorConfiguration flume_f1959_0(String name) throws ConfigurationException
{    Class<? extends SinkProcessorConfiguration> clazz;    SinkProcessorConfiguration instance = null;    try {        if (processorClassName != null) {            clazz = (Class<? extends SinkProcessorConfiguration>) Class.forName(processorClassName);            instance = clazz.getConstructor(String.class).newInstance(name);        } else {            return new SinkProcessorConfiguration(name);        }    } catch (ClassNotFoundException e) {                instance = new SinkProcessorConfiguration(name);                instance.setNotFoundConfigClass();    } catch (Exception e) {        throw new ConfigurationException("Could not instantiate configuration!", e);    }    return instance;}
public String flume_f1960_0()
{    return processorClassName;}
public String flume_f1961_0()
{    return processorClassName;}
public String flume_f1962_0()
{    return sinkClassName;}
public String flume_f1963_0()
{    return sinkClassName;}
public Set<String> flume_f1964_0()
{    return channels;}
public ChannelSelectorConfiguration flume_f1965_0()
{    return selectorConf;}
public void flume_f1966_0(Context context) throws ConfigurationException
{    super.configure(context);    try {        String channelList = context.getString(BasicConfigurationConstants.CONFIG_CHANNELS);        if (channelList != null) {            this.channels = new HashSet<String>(Arrays.asList(channelList.split("\\s+")));        }        if (channels.isEmpty()) {            errors.add(new FlumeConfigurationError(componentName, ComponentType.CHANNEL.getComponentType(), FlumeConfigurationErrorType.PROPERTY_VALUE_NULL, ErrorOrWarning.ERROR));            throw new ConfigurationException("No channels set for " + this.getComponentName());        }        Map<String, String> selectorParams = context.getSubProperties(BasicConfigurationConstants.CONFIG_SOURCE_CHANNELSELECTOR_PREFIX);        String selType;        if (selectorParams != null && !selectorParams.isEmpty()) {            selType = selectorParams.get(BasicConfigurationConstants.CONFIG_TYPE);        } else {            selType = ChannelSelectorConfigurationType.REPLICATING.toString();        }        if (selType == null || selType.isEmpty()) {            selType = ChannelSelectorConfigurationType.REPLICATING.toString();        }        ChannelSelectorType selectorType = this.getKnownChannelSelector(selType);        Context selectorContext = new Context();        selectorContext.putAll(selectorParams);        String config = null;        if (selectorType == null) {            config = selectorContext.getString(BasicConfigurationConstants.CONFIG_CONFIG);            if (config == null || config.isEmpty()) {                config = "OTHER";            }        } else {            config = selectorType.toString().toUpperCase(Locale.ENGLISH);        }        this.selectorConf = (ChannelSelectorConfiguration) ComponentConfigurationFactory.create(ComponentType.CHANNELSELECTOR.getComponentType(), config, ComponentType.CHANNELSELECTOR);        selectorConf.setChannels(channels);        selectorConf.configure(selectorContext);    } catch (Exception e) {        errors.add(new FlumeConfigurationError(componentName, ComponentType.CHANNELSELECTOR.getComponentType(), FlumeConfigurationErrorType.CONFIG_ERROR, ErrorOrWarning.ERROR));        throw new ConfigurationException("Failed to configure component!", e);    }}
public String flume_f1967_0(int indentCount)
{    String basicStr = super.toString(indentCount);    StringBuilder sb = new StringBuilder();    sb.append(basicStr).append("CHANNELS:");    for (String channel : this.channels) {        sb.append(FlumeConfiguration.INDENTSTEP).append(channel).append(FlumeConfiguration.NEWLINE);    }    return sb.toString();}
private ChannelSelectorType flume_f1968_0(String type)
{    ChannelSelectorType[] values = ChannelSelectorType.values();    for (ChannelSelectorType value : values) {        if (value.toString().equalsIgnoreCase(type))            return value;        String clName = value.getClassName();        if (clName != null && clName.equalsIgnoreCase(type))            return value;    }    return null;}
public String flume_f1969_0()
{    return this.srcConfigurationName;}
public SourceConfiguration flume_f1970_0(String name) throws ConfigurationException
{    if (this == OTHER) {        return new SourceConfiguration(name);    }    Class<? extends SourceConfiguration> clazz = null;    SourceConfiguration instance = null;    try {        if (srcConfigurationName != null) {            clazz = (Class<? extends SourceConfiguration>) Class.forName(srcConfigurationName);            instance = clazz.getConstructor(String.class).newInstance(name);        } else {                        instance = new SourceConfiguration(name);                        instance.setNotFoundConfigClass();        }    } catch (ClassNotFoundException e) {                instance = new SourceConfiguration(name);                instance.setNotFoundConfigClass();    } catch (Exception e) {        throw new ConfigurationException("Error creating configuration", e);    }    return instance;}
public String flume_f1971_0()
{    return sourceClassName;}
public String flume_f1972_0()
{    return sourceClassName;}
public Map<String, String> flume_f1973_0()
{    synchronized (parameters) {        return ImmutableMap.copyOf(parameters);    }}
public void flume_f1974_0()
{    parameters.clear();}
public Map<String, String> flume_f1975_0(String prefix)
{    Preconditions.checkArgument(prefix.endsWith("."), "The given prefix does not end with a period (" + prefix + ")");    Map<String, String> result = Maps.newHashMap();    synchronized (parameters) {        for (Entry<String, String> entry : parameters.entrySet()) {            String key = entry.getKey();            if (key.startsWith(prefix)) {                String name = key.substring(prefix.length());                result.put(name, entry.getValue());            }        }    }    return ImmutableMap.copyOf(result);}
public void flume_f1976_0(Map<String, String> map)
{    parameters.putAll(map);}
public void flume_f1977_0(String key, String value)
{    parameters.put(key, value);}
public boolean flume_f1978_0(String key)
{    return parameters.containsKey(key);}
public Boolean flume_f1979_0(String key, Boolean defaultValue)
{    String value = get(key);    if (value != null) {        return Boolean.valueOf(Boolean.parseBoolean(value.trim()));    }    return defaultValue;}
public Boolean flume_f1980_0(String key)
{    return getBoolean(key, null);}
public Integer flume_f1981_0(String key, Integer defaultValue)
{    String value = get(key);    if (value != null) {        return Integer.valueOf(Integer.parseInt(value.trim()));    }    return defaultValue;}
public Integer flume_f1982_0(String key)
{    return getInteger(key, null);}
public Long flume_f1983_0(String key, Long defaultValue)
{    String value = get(key);    if (value != null) {        return Long.valueOf(Long.parseLong(value.trim()));    }    return defaultValue;}
public Long flume_f1984_0(String key)
{    return getLong(key, null);}
public String flume_f1985_0(String key, String defaultValue)
{    return get(key, defaultValue);}
public String flume_f1986_0(String key)
{    return get(key);}
public Float flume_f1987_0(String key, Float defaultValue)
{    String value = get(key);    if (value != null) {        return Float.parseFloat(value.trim());    }    return defaultValue;}
public Float flume_f1988_0(String key)
{    return getFloat(key, null);}
public Double flume_f1989_0(String key, Double defaultValue)
{    String value = get(key);    if (value != null) {        return Double.parseDouble(value.trim());    }    return defaultValue;}
public Double flume_f1990_0(String key)
{    return getDouble(key, null);}
private String flume_f1991_0(String key, String defaultValue)
{    String result = parameters.get(key);    if (result != null) {        return result;    }    return defaultValue;}
private String flume_f1992_0(String key)
{    return get(key, null);}
public String flume_f1993_0()
{    return "{ parameters:" + parameters + " }";}
public String flume_f1994_0(String key)
{    if (key.equals("null")) {        return null;    }    if (key.equals("throw")) {        throw new IllegalStateException("Test exception");    }    return "filtered_" + key;}
public void flume_f1995_0(Map<String, String> configuration)
{}
public void flume_f1996_0() throws Exception
{    Context context = new Context();    context.put("type", "something");    SourceConfiguration sourceConfig = new SourceConfiguration("src");    sourceConfig.configure(context);}
public static void flume_f1997_0()
{    PROPERTIES.put(SOURCES, "s1 s2");    PROPERTIES.put(SOURCES + ".s1.type", "s1_type");    PROPERTIES.put(SOURCES + ".s1.channels", "c1");    PROPERTIES.put(SOURCES + ".s2.type", "jms");    PROPERTIES.put(SOURCES + ".s2.channels", "c2");    PROPERTIES.put(CHANNELS, "c1 c2");    PROPERTIES.put(CHANNELS + ".c1.type", "c1_type");    PROPERTIES.put(CHANNELS + ".c2.type", "memory");    PROPERTIES.put(SINKS, "k1 k2");    PROPERTIES.put(SINKS + ".k1.type", "k1_type");    PROPERTIES.put(SINKS + ".k2.type", "null");    PROPERTIES.put(SINKS + ".k1.channel", "c1");    PROPERTIES.put(SINKS + ".k2.channel", "c2");    PROPERTIES.put(AGENT + ".sinkgroups", "g1");    PROPERTIES.put(AGENT + ".sinkgroups.g1.sinks", "k1 k2");    PROPERTIES.put(AGENT + ".configfilters", "f1 f2");    PROPERTIES.put(AGENT + ".configfilters.f1.type", "f1_type");    PROPERTIES.put(AGENT + ".configfilters.f2.type", "env");}
public void flume_f1998_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    assertTrue(configuration.getConfigurationErrors().isEmpty());}
public void flume_f1999_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Set<String> sourceSet = configuration.getConfigurationFor(AGENT).getSourceSet();    assertEquals(new HashSet<>(Arrays.asList("s1", "s2")), sourceSet);}
public void flume_f2000_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Set<String> configFilterSet = configuration.getConfigurationFor(AGENT).getConfigFilterSet();    assertEquals(new HashSet<>(Arrays.asList("f1", "f2")), configFilterSet);}
public void flume_f2001_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Set<String> sinkSet = configuration.getConfigurationFor(AGENT).getSinkSet();    assertEquals(new HashSet<>(Arrays.asList("k1", "k2")), sinkSet);}
public void flume_f2002_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Set<String> channelSet = configuration.getConfigurationFor(AGENT).getChannelSet();    assertEquals(new HashSet<>(Arrays.asList("c1", "c2")), channelSet);}
public void flume_f2003_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Set<String> sinkSet = configuration.getConfigurationFor(AGENT).getSinkgroupSet();    assertEquals(new HashSet<>(Arrays.asList("g1")), sinkSet);}
public void flume_f2004_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, Context> contextMap = configuration.getConfigurationFor(AGENT).getConfigFilterContext();    assertEquals("f1_type", contextMap.get("f1").getString("type"));}
public void flume_f2005_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, Context> contextMap = configuration.getConfigurationFor(AGENT).getSourceContext();    assertEquals("s1_type", contextMap.get("s1").getString("type"));}
public void flume_f2006_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, Context> contextMap = configuration.getConfigurationFor(AGENT).getSinkContext();    assertEquals("k1_type", contextMap.get("k1").getString("type"));}
public void flume_f2007_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, Context> contextMap = configuration.getConfigurationFor(AGENT).getChannelContext();    assertEquals("c1_type", contextMap.get("c1").getString("type"));}
public void flume_f2008_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, ComponentConfiguration> configMap = configuration.getConfigurationFor(AGENT).getChannelConfigMap();    assertEquals("memory", configMap.get("c2").getType());}
public void flume_f2009_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, ComponentConfiguration> configMap = configuration.getConfigurationFor(AGENT).getConfigFilterConfigMap();    assertEquals("env", configMap.get("f2").getType());}
public void flume_f2010_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, ComponentConfiguration> configMap = configuration.getConfigurationFor(AGENT).getSourceConfigMap();    assertEquals("jms", configMap.get("s2").getType());}
public void flume_f2011_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, ComponentConfiguration> configMap = configuration.getConfigurationFor(AGENT).getSinkConfigMap();    assertEquals("null", configMap.get("k2").getType());}
public void flume_f2012_0()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, ComponentConfiguration> configMap = configuration.getConfigurationFor(AGENT).getSinkGroupConfigMap();    assertEquals("Sinkgroup", configMap.get("g1").getType());}
public void flume_f2013_0()
{    Map<String, String> properties = new HashMap<>(PROPERTIES);    properties.put(CHANNELS, "");    FlumeConfiguration flumeConfiguration = new FlumeConfiguration(properties);    assertFalse(flumeConfiguration.getConfigurationErrors().isEmpty());    assertNull(flumeConfiguration.getConfigurationFor(AGENT));}
public void flume_f2014_0()
{    Map<String, String> properties = new HashMap<>(PROPERTIES);    properties.remove(SOURCES);    properties.remove(SOURCES + ".s1.type");    properties.remove(SOURCES + ".s1.channels");    properties.remove(SOURCES + ".s2.type");    properties.remove(SOURCES + ".s2.channels");    FlumeConfiguration flumeConfiguration = new FlumeConfiguration(properties);    assertConfigHasNoError(flumeConfiguration);    assertNotNull(flumeConfiguration.getConfigurationFor(AGENT));}
public void flume_f2015_0()
{    Map<String, String> properties = new HashMap<>(PROPERTIES);    properties.remove(SINKS);    properties.remove(SINKS + ".k1.type", "k1_type");    properties.remove(SINKS + ".k2.type", "null");    properties.remove(SINKS + ".k1.channel", "c1");    properties.remove(SINKS + ".k2.channel", "c2");    properties.remove(AGENT + ".sinkgroups", "g1");    properties.remove(AGENT + ".sinkgroups.g1.sinks", "k1 k2");    FlumeConfiguration flumeConfiguration = new FlumeConfiguration(properties);    assertConfigHasNoError(flumeConfiguration);    assertNotNull(flumeConfiguration.getConfigurationFor(AGENT));}
private void flume_f2016_0(FlumeConfiguration configuration)
{    List<FlumeConfigurationError> configurationErrors = configuration.getConfigurationErrors();    long count = 0L;    for (FlumeConfigurationError e : configurationErrors) {        if (e.getErrorOrWarning() == ERROR) {            count++;        }    }    assertTrue(count == 0);}
public void flume_f2017_0()
{    Map<String, String> properties = new HashMap<>(PROPERTIES);    properties.put(SOURCES, "");    properties.put(SINKS, "");    FlumeConfiguration flumeConfiguration = new FlumeConfiguration(properties);    assertFalse(flumeConfiguration.getConfigurationErrors().isEmpty());    assertNull(flumeConfiguration.getConfigurationFor(AGENT));}
public void flume_f2018_0()
{    Properties properties = new Properties();    properties.put("agent1.channels", "ch0");    properties.put("agent1.channels.ch0.type", "memory");    properties.put("agent1.sources", "src0");    properties.put("agent1.sources.src0.type", "multiport_syslogtcp");    properties.put("agent1.sources.src0.channels", "ch0");    properties.put("agent1.sources.src0.host", "localhost");    properties.put("agent1.sources.src0.ports", "10001 10002 10003");    properties.put("agent1.sources.src0.portHeader", "port");    properties.put("agent1.sinks", "sink0");    properties.put("agent1.sinks.sink0.type", "null");    properties.put("agent1.sinks.sink0.channel", "ch0");    properties.put("agent1.configfilters", "f1");    properties.put("agent1.configfilters.f1.type", "env");    FlumeConfiguration conf = new FlumeConfiguration(properties);    AgentConfiguration agentConfiguration = conf.getConfigurationFor("agent1");    Assert.assertEquals(String.valueOf(agentConfiguration.getSourceSet()), 1, agentConfiguration.getSourceSet().size());    Assert.assertEquals(String.valueOf(agentConfiguration.getChannelSet()), 1, agentConfiguration.getChannelSet().size());    Assert.assertEquals(String.valueOf(agentConfiguration.getSinkSet()), 1, agentConfiguration.getSinkSet().size());    Assert.assertTrue(agentConfiguration.getSourceSet().contains("src0"));    Assert.assertTrue(agentConfiguration.getChannelSet().contains("ch0"));    Assert.assertTrue(agentConfiguration.getSinkSet().contains("sink0"));    Assert.assertTrue(agentConfiguration.getConfigFilterSet().contains("f1"));}
public void flume_f2019_0()
{    HashMap<String, String> properties = new HashMap<>();    properties.put(null, "something");    FlumeConfiguration config = new FlumeConfiguration(properties);    List<FlumeConfigurationError> configurationErrors = config.getConfigurationErrors();    assertEquals(1, configurationErrors.size());    assertError(configurationErrors.get(0), AGENT_NAME_MISSING, "", "", ERROR);}
public void flume_f2020_0()
{    HashMap<String, String> properties = new HashMap<>();    properties.put("something", null);    FlumeConfiguration config = new FlumeConfiguration(properties);    List<FlumeConfigurationError> configurationErrors = config.getConfigurationErrors();    assertEquals(1, configurationErrors.size());    assertError(configurationErrors.get(0), AGENT_NAME_MISSING, "", "", ERROR);}
public void flume_f2021_0()
{    HashMap<String, String> properties = new HashMap<>();    properties.put("something", "");    FlumeConfiguration config = new FlumeConfiguration(properties);    List<FlumeConfigurationError> configurationErrors = config.getConfigurationErrors();    assertEquals(1, configurationErrors.size());    assertError(configurationErrors.get(0), PROPERTY_VALUE_NULL, "something", "", ERROR);}
public void flume_f2022_0()
{    HashMap<String, String> properties = new HashMap<>();    properties.put("something", "value");    FlumeConfiguration config = new FlumeConfiguration(properties);    List<FlumeConfigurationError> configurationErrors = config.getConfigurationErrors();    assertEquals(1, configurationErrors.size());    assertError(configurationErrors.get(0), AGENT_NAME_MISSING, "something", "", ERROR);}
public void flume_f2023_0()
{    Properties properties = new Properties();    properties.put(".something", "value");    FlumeConfiguration config = new FlumeConfiguration(properties);    List<FlumeConfigurationError> configurationErrors = config.getConfigurationErrors();    assertEquals(1, configurationErrors.size());    assertError(configurationErrors.get(0), AGENT_NAME_MISSING, ".something", "", ERROR);}
public void flume_f2024_0()
{    HashMap<String, String> properties = new HashMap<>();    properties.put("agent.", "something");    FlumeConfiguration config = new FlumeConfiguration(properties);    List<FlumeConfigurationError> configurationErrors = config.getConfigurationErrors();    assertEquals(1, configurationErrors.size());    assertError(configurationErrors.get(0), PROPERTY_NAME_NULL, "agent.", "", ERROR);}
public void flume_f2025_0()
{    HashMap<String, String> properties = new HashMap<>();    properties.put("agent.channels", "c1");    properties.put("agent.channel.c1", "cc1");    FlumeConfiguration config = new FlumeConfiguration(properties);    List<FlumeConfigurationError> configurationErrors = config.getConfigurationErrors();    assertEquals(4, configurationErrors.size());    assertError(configurationErrors.get(0), INVALID_PROPERTY, "agent", "channel.c1", ERROR);    assertError(configurationErrors.get(1), CONFIG_ERROR, "agent", "c1", ERROR);    assertError(configurationErrors.get(2), PROPERTY_VALUE_NULL, "agent", "channels", ERROR);    assertError(configurationErrors.get(3), AGENT_CONFIGURATION_INVALID, "agent", "", ERROR);}
private void flume_f2026_0(FlumeConfigurationError error, FlumeConfigurationErrorType agentNameMissing, String componentName, String key, ErrorOrWarning eow)
{    assertEquals(agentNameMissing, error.getErrorType());    assertEquals("ComponentName mismatch.", componentName, error.getComponentName());    assertEquals("Key mismatch.", key, error.getKey());    assertEquals(eow, error.getErrorOrWarning());}
public void flume_f2027_0()
{    Properties properties = new Properties();    properties.put("agent1.channels", "ch0");    properties.put("agent1.channels.ch0.type", "file");    properties.put("agent1.channels.ch0.param1", "${f1['param']}");    properties.put("agent1.channels.ch0.param2", "${f1['param\"]}");    properties.put("agent1.channels.ch0.param3", "${f1['null']}");    properties.put("agent1.channels.ch0.param4", "${f1['throw']}");    properties.put("agent1.sources", "src0");    properties.put("agent1.sources.src0.type", "multiport_syslogtcp");    properties.put("agent1.sources.src0.channels", "ch0");    properties.put("agent1.sources.src0.host", "${f1[host]}");    properties.put("agent1.sources.src0.ports", "10001 10002 10003");    properties.put("agent1.sources.src0.portHeader", "${f2[\"port\"]}-${f1['header']}");    properties.put("agent1.sinks", "sink0");    properties.put("agent1.sinks.sink0.type", "thrift");    properties.put("agent1.sinks.sink0.param", "${f2['param']}");    properties.put("agent1.sinks.sink0.channel", "ch0");    properties.put("agent1.configfilters", "f1 f2");    properties.put("agent1.configfilters.f1.type", "org.apache.flume.conf.configfilter.MockConfigFilter");    properties.put("agent1.configfilters.f2.type", "org.apache.flume.conf.configfilter.MockConfigFilter");    FlumeConfiguration conf = new FlumeConfiguration(properties);    AgentConfiguration agentConfiguration = conf.getConfigurationFor("agent1");    Context src0 = agentConfiguration.getSourceContext().get("src0");    assertEquals("filtered_host", src0.getString("host"));    assertEquals("filtered_port-filtered_header", src0.getString("portHeader"));    Context sink0 = agentConfiguration.getSinkContext().get("sink0");    assertEquals("filtered_param", sink0.getString("param"));    Context ch0 = agentConfiguration.getChannelContext().get("ch0");    assertEquals("filtered_param", ch0.getString("param1"));    assertEquals("${f1['param\"]}", ch0.getString("param2"));    assertEquals("${f1['null']}", ch0.getString("param3"));    assertEquals("${f1['throw']}", ch0.getString("param4"));}
public synchronized void flume_f2028_0(String name)
{    this.name = name;}
public synchronized void flume_f2029_0()
{    lifecycleState = LifecycleState.START;}
public synchronized void flume_f2030_0()
{    lifecycleState = LifecycleState.STOP;}
public synchronized LifecycleState flume_f2031_0()
{    return lifecycleState;}
public synchronized String flume_f2032_0()
{    return name;}
public void flume_f2033_0(Context context)
{}
public String flume_f2034_0()
{    return this.getClass().getName() + "{name: " + name + "}";}
public List<Channel> flume_f2035_0()
{    return channels;}
public void flume_f2036_0(List<Channel> channels)
{    this.channels = channels;}
public synchronized void flume_f2037_0(String name)
{    this.name = name;}
public synchronized String flume_f2038_0()
{    return name;}
protected Map<String, Channel> flume_f2039_0()
{    Map<String, Channel> channelNameMap = new HashMap<String, Channel>();    for (Channel ch : getAllChannels()) {        channelNameMap.put(ch.getName(), ch);    }    return channelNameMap;}
protected List<Channel> flume_f2040_0(String channels, Map<String, Channel> channelNameMap)
{    List<Channel> configuredChannels = new ArrayList<Channel>();    if (channels == null || channels.isEmpty()) {        return configuredChannels;    }    String[] chNames = channels.split(" ");    for (String name : chNames) {        Channel ch = channelNameMap.get(name);        if (ch != null) {            configuredChannels.add(ch);        } else {            throw new FlumeException("Selector channel not found: " + name);        }    }    return configuredChannels;}
protected void flume_f2041_0()
{}
public void flume_f2042_0(Event event) throws ChannelException
{    BasicTransactionSemantics transaction = currentTransaction.get();    Preconditions.checkState(transaction != null, "No transaction exists for this thread");    transaction.put(event);}
public Event flume_f2043_0() throws ChannelException
{    BasicTransactionSemantics transaction = currentTransaction.get();    Preconditions.checkState(transaction != null, "No transaction exists for this thread");    return transaction.take();}
public Transaction flume_f2044_0()
{    if (!initialized) {        synchronized (this) {            if (!initialized) {                initialize();                initialized = true;            }        }    }    BasicTransactionSemantics transaction = currentTransaction.get();    if (transaction == null || transaction.getState().equals(BasicTransactionSemantics.State.CLOSED)) {        transaction = createTransaction();        currentTransaction.set(transaction);    }    return transaction;}
protected void flume_f2045_0() throws InterruptedException
{}
protected void flume_f2046_0()
{}
protected void flume_f2047_0(Event event)
{    Preconditions.checkState(Thread.currentThread().getId() == initialThreadId, "put() called from different thread than getTransaction()!");    Preconditions.checkState(state.equals(State.OPEN), "put() called when transaction is %s!", state);    Preconditions.checkArgument(event != null, "put() called with null event!");    try {        doPut(event);    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new ChannelException(e.toString(), e);    }}
protected Event flume_f2048_0()
{    Preconditions.checkState(Thread.currentThread().getId() == initialThreadId, "take() called from different thread than getTransaction()!");    Preconditions.checkState(state.equals(State.OPEN), "take() called when transaction is %s!", state);    try {        return doTake();    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        return null;    }}
protected State flume_f2049_0()
{    return state;}
public void flume_f2050_0()
{    Preconditions.checkState(Thread.currentThread().getId() == initialThreadId, "begin() called from different thread than getTransaction()!");    Preconditions.checkState(state.equals(State.NEW), "begin() called when transaction is " + state + "!");    try {        doBegin();    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new ChannelException(e.toString(), e);    }    state = State.OPEN;}
public void flume_f2051_0()
{    Preconditions.checkState(Thread.currentThread().getId() == initialThreadId, "commit() called from different thread than getTransaction()!");    Preconditions.checkState(state.equals(State.OPEN), "commit() called when transaction is %s!", state);    try {        doCommit();    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new ChannelException(e.toString(), e);    }    state = State.COMPLETED;}
public void flume_f2052_0()
{    Preconditions.checkState(Thread.currentThread().getId() == initialThreadId, "rollback() called from different thread than getTransaction()!");    Preconditions.checkState(state.equals(State.OPEN), "rollback() called when transaction is %s!", state);    state = State.COMPLETED;    try {        doRollback();    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new ChannelException(e.toString(), e);    }}
public void flume_f2053_0()
{    Preconditions.checkState(Thread.currentThread().getId() == initialThreadId, "close() called from different thread than getTransaction()!");    Preconditions.checkState(state.equals(State.NEW) || state.equals(State.COMPLETED), "close() called when transaction is %s" + " - you must either commit or rollback first", state);    state = State.CLOSED;    doClose();}
public String flume_f2054_0()
{    StringBuilder builder = new StringBuilder();    builder.append("BasicTransactionSemantics: {");    builder.append(" state:").append(state);    builder.append(" initialThreadId:").append(initialThreadId);    builder.append(" }");    return builder.toString();}
public void flume_f2055_0()
{    interceptorChain.initialize();}
public void flume_f2056_0()
{    interceptorChain.close();}
public void flume_f2057_0(Context context)
{    configureInterceptors(context);}
private void flume_f2058_1(Context context)
{    List<Interceptor> interceptors = Lists.newLinkedList();    String interceptorListStr = context.getString("interceptors", "");    if (interceptorListStr.isEmpty()) {        return;    }    String[] interceptorNames = interceptorListStr.split("\\s+");    Context interceptorContexts = new Context(context.getSubProperties("interceptors."));        InterceptorBuilderFactory factory = new InterceptorBuilderFactory();    for (String interceptorName : interceptorNames) {        Context interceptorContext = new Context(interceptorContexts.getSubProperties(interceptorName + "."));        String type = interceptorContext.getString("type");        if (type == null) {                        throw new FlumeException("Interceptor.Type not specified for " + interceptorName);        }        try {            Interceptor.Builder builder = factory.newInstance(type);            builder.configure(interceptorContext);            interceptors.add(builder.build());        } catch (ClassNotFoundException e) {                        throw new FlumeException("Interceptor.Builder not found.", e);        } catch (InstantiationException e) {                        throw new FlumeException("Interceptor.Builder not constructable.", e);        } catch (IllegalAccessException e) {                        throw new FlumeException("Unable to access Interceptor.Builder.", e);        }    }    interceptorChain.setInterceptors(interceptors);}
public ChannelSelector flume_f2059_0()
{    return selector;}
public void flume_f2060_1(List<Event> events)
{    Preconditions.checkNotNull(events, "Event list must not be null");    events = interceptorChain.intercept(events);    Map<Channel, List<Event>> reqChannelQueue = new LinkedHashMap<Channel, List<Event>>();    Map<Channel, List<Event>> optChannelQueue = new LinkedHashMap<Channel, List<Event>>();    for (Event event : events) {        List<Channel> reqChannels = selector.getRequiredChannels(event);        for (Channel ch : reqChannels) {            List<Event> eventQueue = reqChannelQueue.get(ch);            if (eventQueue == null) {                eventQueue = new ArrayList<Event>();                reqChannelQueue.put(ch, eventQueue);            }            eventQueue.add(event);        }        List<Channel> optChannels = selector.getOptionalChannels(event);        for (Channel ch : optChannels) {            List<Event> eventQueue = optChannelQueue.get(ch);            if (eventQueue == null) {                eventQueue = new ArrayList<Event>();                optChannelQueue.put(ch, eventQueue);            }            eventQueue.add(event);        }    }        for (Channel reqChannel : reqChannelQueue.keySet()) {        Transaction tx = reqChannel.getTransaction();        Preconditions.checkNotNull(tx, "Transaction object must not be null");        try {            tx.begin();            List<Event> batch = reqChannelQueue.get(reqChannel);            for (Event event : batch) {                reqChannel.put(event);            }            tx.commit();        } catch (Throwable t) {            tx.rollback();            if (t instanceof Error) {                                throw (Error) t;            } else if (t instanceof ChannelException) {                throw (ChannelException) t;            } else {                throw new ChannelException("Unable to put batch on required " + "channel: " + reqChannel, t);            }        } finally {            if (tx != null) {                tx.close();            }        }    }        for (Channel optChannel : optChannelQueue.keySet()) {        Transaction tx = optChannel.getTransaction();        Preconditions.checkNotNull(tx, "Transaction object must not be null");        try {            tx.begin();            List<Event> batch = optChannelQueue.get(optChannel);            for (Event event : batch) {                optChannel.put(event);            }            tx.commit();        } catch (Throwable t) {            tx.rollback();                        if (t instanceof Error) {                throw (Error) t;            }        } finally {            if (tx != null) {                tx.close();            }        }    }}
public void flume_f2061_1(Event event)
{    event = interceptorChain.intercept(event);    if (event == null) {        return;    }        List<Channel> requiredChannels = selector.getRequiredChannels(event);    for (Channel reqChannel : requiredChannels) {        Transaction tx = reqChannel.getTransaction();        Preconditions.checkNotNull(tx, "Transaction object must not be null");        try {            tx.begin();            reqChannel.put(event);            tx.commit();        } catch (Throwable t) {            tx.rollback();            if (t instanceof Error) {                                throw (Error) t;            } else if (t instanceof ChannelException) {                throw (ChannelException) t;            } else {                throw new ChannelException("Unable to put event on required " + "channel: " + reqChannel, t);            }        } finally {            if (tx != null) {                tx.close();            }        }    }        List<Channel> optionalChannels = selector.getOptionalChannels(event);    for (Channel optChannel : optionalChannels) {        Transaction tx = null;        try {            tx = optChannel.getTransaction();            tx.begin();            optChannel.put(event);            tx.commit();        } catch (Throwable t) {            tx.rollback();                        if (t instanceof Error) {                throw (Error) t;            }        } finally {            if (tx != null) {                tx.close();            }        }    }}
public static ChannelSelector flume_f2062_0(List<Channel> channels, Map<String, String> config)
{    ChannelSelector selector = getSelectorForType(config.get(BasicConfigurationConstants.CONFIG_TYPE));    selector.setChannels(channels);    Context context = new Context();    context.putAll(config);    Configurables.configure(selector, context);    return selector;}
public static ChannelSelector flume_f2063_0(List<Channel> channels, ChannelSelectorConfiguration conf)
{    String type = ChannelSelectorType.REPLICATING.toString();    if (conf != null) {        type = conf.getType();    }    ChannelSelector selector = getSelectorForType(type);    selector.setChannels(channels);    Configurables.configure(selector, conf);    return selector;}
private static ChannelSelector flume_f2064_1(String type)
{    if (type == null || type.trim().length() == 0) {        return new ReplicatingChannelSelector();    }    String selectorClassName = type;    ChannelSelectorType selectorType = ChannelSelectorType.OTHER;    try {        selectorType = ChannelSelectorType.valueOf(type.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException ex) {            }    if (!selectorType.equals(ChannelSelectorType.OTHER)) {        selectorClassName = selectorType.getChannelSelectorClassName();    }    ChannelSelector selector = null;    try {        @SuppressWarnings("unchecked")        Class<? extends ChannelSelector> selectorClass = (Class<? extends ChannelSelector>) Class.forName(selectorClassName);        selector = selectorClass.newInstance();    } catch (Exception ex) {        throw new FlumeException("Unable to load selector type: " + type + ", class: " + selectorClassName, ex);    }    return selector;}
public static void flume_f2065_0(final Channel channel, final Event event) throws ChannelException
{    transact(channel, new Runnable() {        @Override        public void run() {            channel.put(event);        }    });}
public void flume_f2066_0()
{    channel.put(event);}
public static void flume_f2067_0(final Channel channel, final Collection<Event> events) throws ChannelException
{    transact(channel, new Runnable() {        @Override        public void run() {            for (Event event : events) {                channel.put(event);            }        }    });}
public void flume_f2068_0()
{    for (Event event : events) {        channel.put(event);    }}
public static Event flume_f2069_0(final Channel channel) throws ChannelException
{    return transact(channel, new Callable<Event>() {        @Override        public Event call() {            return channel.take();        }    });}
public Event flume_f2070_0()
{    return channel.take();}
public static List<Event> flume_f2071_0(final Channel channel, final int max) throws ChannelException
{    return transact(channel, new Callable<List<Event>>() {        @Override        public List<Event> call() {            List<Event> events = new ArrayList<Event>(max);            while (events.size() < max) {                Event event = channel.take();                if (event == null) {                    break;                }                events.add(event);            }            return events;        }    });}
public List<Event> flume_f2072_0()
{    List<Event> events = new ArrayList<Event>(max);    while (events.size() < max) {        Event event = channel.take();        if (event == null) {            break;        }        events.add(event);    }    return events;}
public static void flume_f2073_0(Channel channel, Runnable transactor) throws ChannelException
{    transact(channel, Executors.callable(transactor));}
public static T flume_f2074_1(Channel channel, Callable<T> transactor) throws ChannelException
{    Transaction transaction = channel.getTransaction();    boolean committed = false;    boolean interrupted = false;    try {        transaction.begin();        T value = transactor.call();        transaction.commit();        committed = true;        return value;    } catch (Throwable e) {        interrupted = Thread.currentThread().isInterrupted();        try {            transaction.rollback();        } catch (Throwable e2) {                    }        if (e instanceof InterruptedException) {            interrupted = true;        } else if (e instanceof Error) {            throw (Error) e;        } else if (e instanceof RuntimeException) {            throw (RuntimeException) e;        }        throw new ChannelException(e);    } finally {        interrupted = interrupted || Thread.currentThread().isInterrupted();        try {            transaction.close();        } catch (Throwable e) {            if (committed) {                if (e instanceof Error) {                    throw (Error) e;                } else if (e instanceof RuntimeException) {                    throw (RuntimeException) e;                } else {                    throw new ChannelException(e);                }            } else {                            }        } finally {            if (interrupted) {                Thread.currentThread().interrupt();            }        }    }}
public Channel flume_f2075_1(String name, String type) throws FlumeException
{    Preconditions.checkNotNull(name, "name");    Preconditions.checkNotNull(type, "type");        Class<? extends Channel> channelClass = getClass(type);    try {        return channelClass.newInstance();    } catch (Exception ex) {        throw new FlumeException("Unable to create channel: " + name + ", type: " + type + ", class: " + channelClass.getName(), ex);    }}
public Class<? extends Channel> flume_f2076_1(String type) throws FlumeException
{    String channelClassName = type;    ChannelType channelType = ChannelType.OTHER;    try {        channelType = ChannelType.valueOf(type.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException ex) {            }    if (!channelType.equals(ChannelType.OTHER)) {        channelClassName = channelType.getChannelClassName();    }    try {        return (Class<? extends Channel>) Class.forName(channelClassName);    } catch (Exception ex) {        throw new FlumeException("Unable to load channel type: " + type + ", class: " + channelClassName, ex);    }}
protected void flume_f2077_0(Event event) throws InterruptedException
{    channelCounter.incrementEventPutAttemptCount();    int eventByteSize = (int) Math.ceil(estimateEventSize(event) / byteCapacitySlotSize);    if (!putList.offer(event)) {        throw new ChannelException("Put queue for MemoryTransaction of capacity " + putList.size() + " full, consider committing more frequently, " + "increasing capacity or increasing thread count");    }    putByteCounter += eventByteSize;}
protected Event flume_f2078_0() throws InterruptedException
{    channelCounter.incrementEventTakeAttemptCount();    if (takeList.remainingCapacity() == 0) {        throw new ChannelException("Take list for MemoryTransaction, capacity " + takeList.size() + " full, consider committing more frequently, " + "increasing capacity, or increasing thread count");    }    if (!queueStored.tryAcquire(keepAlive, TimeUnit.SECONDS)) {        return null;    }    Event event;    synchronized (queueLock) {        event = queue.poll();    }    Preconditions.checkNotNull(event, "Queue.poll returned NULL despite semaphore " + "signalling existence of entry");    takeList.put(event);    int eventByteSize = (int) Math.ceil(estimateEventSize(event) / byteCapacitySlotSize);    takeByteCounter += eventByteSize;    return event;}
protected void flume_f2079_0() throws InterruptedException
{    int remainingChange = takeList.size() - putList.size();    if (remainingChange < 0) {        if (!bytesRemaining.tryAcquire(putByteCounter, keepAlive, TimeUnit.SECONDS)) {            throw new ChannelException("Cannot commit transaction. Byte capacity " + "allocated to store event body " + byteCapacity * byteCapacitySlotSize + "reached. Please increase heap space/byte capacity allocated to " + "the channel as the sinks may not be keeping up with the sources");        }        if (!queueRemaining.tryAcquire(-remainingChange, keepAlive, TimeUnit.SECONDS)) {            bytesRemaining.release(putByteCounter);            throw new ChannelFullException("Space for commit to queue couldn't be acquired." + " Sinks are likely not keeping up with sources, or the buffer size is too tight");        }    }    int puts = putList.size();    int takes = takeList.size();    synchronized (queueLock) {        if (puts > 0) {            while (!putList.isEmpty()) {                if (!queue.offer(putList.removeFirst())) {                    throw new RuntimeException("Queue add failed, this shouldn't be able to happen");                }            }        }        putList.clear();        takeList.clear();    }    bytesRemaining.release(takeByteCounter);    takeByteCounter = 0;    putByteCounter = 0;    queueStored.release(puts);    if (remainingChange > 0) {        queueRemaining.release(remainingChange);    }    if (puts > 0) {        channelCounter.addToEventPutSuccessCount(puts);    }    if (takes > 0) {        channelCounter.addToEventTakeSuccessCount(takes);    }    channelCounter.setChannelSize(queue.size());}
protected void flume_f2080_0()
{    int takes = takeList.size();    synchronized (queueLock) {        Preconditions.checkState(queue.remainingCapacity() >= takeList.size(), "Not enough space in memory channel " + "queue to rollback takes. This should never happen, please report");        while (!takeList.isEmpty()) {            queue.addFirst(takeList.removeLast());        }        putList.clear();    }    putByteCounter = 0;    takeByteCounter = 0;    queueStored.release(takes);    channelCounter.setChannelSize(queue.size());}
public void flume_f2081_1(Context context)
{    Integer capacity = null;    try {        capacity = context.getInteger("capacity", defaultCapacity);    } catch (NumberFormatException e) {        capacity = defaultCapacity;            }    if (capacity <= 0) {        capacity = defaultCapacity;            }    try {        transCapacity = context.getInteger("transactionCapacity", defaultTransCapacity);    } catch (NumberFormatException e) {        transCapacity = defaultTransCapacity;            }    if (transCapacity <= 0) {        transCapacity = defaultTransCapacity;            }    Preconditions.checkState(transCapacity <= capacity, "Transaction Capacity of Memory Channel cannot be higher than " + "the capacity.");    try {        byteCapacityBufferPercentage = context.getInteger("byteCapacityBufferPercentage", defaultByteCapacityBufferPercentage);    } catch (NumberFormatException e) {        byteCapacityBufferPercentage = defaultByteCapacityBufferPercentage;    }    try {        byteCapacity = (int) ((context.getLong("byteCapacity", defaultByteCapacity).longValue() * (1 - byteCapacityBufferPercentage * .01)) / byteCapacitySlotSize);        if (byteCapacity < 1) {            byteCapacity = Integer.MAX_VALUE;        }    } catch (NumberFormatException e) {        byteCapacity = (int) ((defaultByteCapacity * (1 - byteCapacityBufferPercentage * .01)) / byteCapacitySlotSize);    }    try {        keepAlive = context.getInteger("keep-alive", defaultKeepAlive);    } catch (NumberFormatException e) {        keepAlive = defaultKeepAlive;    }    if (queue != null) {        try {            resizeQueue(capacity);        } catch (InterruptedException e) {            Thread.currentThread().interrupt();        }    } else {        synchronized (queueLock) {            queue = new LinkedBlockingDeque<Event>(capacity);            queueRemaining = new Semaphore(capacity);            queueStored = new Semaphore(0);        }    }    if (bytesRemaining == null) {        bytesRemaining = new Semaphore(byteCapacity);        lastByteCapacity = byteCapacity;    } else {        if (byteCapacity > lastByteCapacity) {            bytesRemaining.release(byteCapacity - lastByteCapacity);            lastByteCapacity = byteCapacity;        } else {            try {                if (!bytesRemaining.tryAcquire(lastByteCapacity - byteCapacity, keepAlive, TimeUnit.SECONDS)) {                                    } else {                    lastByteCapacity = byteCapacity;                }            } catch (InterruptedException e) {                Thread.currentThread().interrupt();            }        }    }    if (channelCounter == null) {        channelCounter = new ChannelCounter(getName());    }}
private void flume_f2082_1(int capacity) throws InterruptedException
{    int oldCapacity;    synchronized (queueLock) {        oldCapacity = queue.size() + queue.remainingCapacity();    }    if (oldCapacity == capacity) {        return;    } else if (oldCapacity > capacity) {        if (!queueRemaining.tryAcquire(oldCapacity - capacity, keepAlive, TimeUnit.SECONDS)) {                    } else {            synchronized (queueLock) {                LinkedBlockingDeque<Event> newQueue = new LinkedBlockingDeque<Event>(capacity);                newQueue.addAll(queue);                queue = newQueue;            }        }    } else {        synchronized (queueLock) {            LinkedBlockingDeque<Event> newQueue = new LinkedBlockingDeque<Event>(capacity);            newQueue.addAll(queue);            queue = newQueue;        }        queueRemaining.release(capacity - oldCapacity);    }}
public synchronized void flume_f2083_0()
{    channelCounter.start();    channelCounter.setChannelSize(queue.size());    channelCounter.setChannelCapacity(Long.valueOf(queue.size() + queue.remainingCapacity()));    super.start();}
public synchronized void flume_f2084_0()
{    channelCounter.setChannelSize(queue.size());    channelCounter.stop();    super.stop();}
protected BasicTransactionSemantics flume_f2085_0()
{    return new MemoryTransaction(transCapacity, channelCounter);}
private long flume_f2086_0(Event event)
{    byte[] body = event.getBody();    if (body != null && body.length != 0) {        return body.length;    }        return 1;}
 int flume_f2087_0()
{    return bytesRemaining.availablePermits();}
public long flume_f2088_0()
{    return transCapacity;}
public List<Channel> flume_f2089_0(Event event)
{    String headerValue = event.getHeaders().get(headerName);    if (headerValue == null || headerValue.trim().length() == 0) {        return defaultChannels;    }    List<Channel> channels = channelMapping.get(headerValue);        if (channels == null) {        channels = defaultChannels;    }    return channels;}
public List<Channel> flume_f2090_0(Event event)
{    String hdr = event.getHeaders().get(headerName);    List<Channel> channels = optionalChannels.get(hdr);    if (channels == null) {        channels = EMPTY_LIST;    }    return channels;}
public void flume_f2091_0(Context context)
{    this.headerName = context.getString(CONFIG_MULTIPLEX_HEADER_NAME, DEFAULT_MULTIPLEX_HEADER);    Map<String, Channel> channelNameMap = getChannelNameMap();    defaultChannels = getChannelListFromNames(context.getString(CONFIG_DEFAULT_CHANNEL), channelNameMap);    Map<String, String> mapConfig = context.getSubProperties(CONFIG_PREFIX_MAPPING);    channelMapping = new HashMap<String, List<Channel>>();    for (String headerValue : mapConfig.keySet()) {        List<Channel> configuredChannels = getChannelListFromNames(mapConfig.get(headerValue), channelNameMap);                if (configuredChannels.size() == 0) {            throw new FlumeException("No channel configured for when " + "header value is: " + headerValue);        }        if (channelMapping.put(headerValue, configuredChannels) != null) {            throw new FlumeException("Selector channel configured twice");        }    }            Map<String, String> optionalChannelsMapping = context.getSubProperties(CONFIG_PREFIX_OPTIONAL + ".");    optionalChannels = new HashMap<String, List<Channel>>();    for (String hdr : optionalChannelsMapping.keySet()) {        List<Channel> confChannels = getChannelListFromNames(optionalChannelsMapping.get(hdr), channelNameMap);        if (confChannels.isEmpty()) {            confChannels = EMPTY_LIST;        }                        List<Channel> reqdChannels = channelMapping.get(hdr);                if (reqdChannels == null || reqdChannels.isEmpty()) {            reqdChannels = defaultChannels;        }        for (Channel c : reqdChannels) {            if (confChannels.contains(c)) {                confChannels.remove(c);            }        }        if (optionalChannels.put(hdr, confChannels) != null) {            throw new FlumeException("Selector channel configured twice");        }    }}
public void flume_f2092_0(Context context)
{    Integer capacity = context.getInteger("capacity");    keepAlive = context.getInteger("keep-alive");    if (capacity == null) {        capacity = defaultCapacity;    }    if (keepAlive == null) {        keepAlive = defaultKeepAlive;    }    queue = new ArrayBlockingQueue<Event>(capacity);    if (channelCounter == null) {        channelCounter = new ChannelCounter(getName());    }}
public void flume_f2093_0()
{    channelCounter.start();    channelCounter.setChannelSize(queue.size());    channelCounter.setChannelSize(Long.valueOf(queue.size() + queue.remainingCapacity()));    super.start();}
public void flume_f2094_0()
{    channelCounter.setChannelSize(queue.size());    channelCounter.stop();    super.stop();}
public void flume_f2095_0(Event event)
{    Preconditions.checkState(queue != null, "No queue defined (Did you forget to configure me?");    channelCounter.incrementEventPutAttemptCount();    try {        queue.put(event);    } catch (InterruptedException ex) {        throw new ChannelException("Failed to put(" + event + ")", ex);    }    channelCounter.addToEventPutSuccessCount(1);    channelCounter.setChannelSize(queue.size());}
public Event flume_f2096_0()
{    Preconditions.checkState(queue != null, "No queue defined (Did you forget to configure me?");    channelCounter.incrementEventTakeAttemptCount();    try {        Event e = queue.poll(keepAlive, TimeUnit.SECONDS);        channelCounter.addToEventTakeSuccessCount(1);        channelCounter.setChannelSize(queue.size());        return e;    } catch (InterruptedException ex) {        throw new ChannelException("Failed to take()", ex);    }}
public Transaction flume_f2097_0()
{    return NoOpTransaction.sharedInstance();}
public static Transaction flume_f2098_0()
{    if (sharedInstance == null) {        sharedInstance = new NoOpTransaction();    }    return sharedInstance;}
public void flume_f2099_0()
{}
public void flume_f2100_0()
{}
public void flume_f2101_0()
{}
public void flume_f2102_0()
{}
public List<Channel> flume_f2103_0(Event event)
{    /*     * Seems like there are lot of components within flume that do not call     * configure method. It is conceiveable that custom component tests too     * do that. So in that case, revert to old behavior.     */    if (requiredChannels == null) {        return getAllChannels();    }    return requiredChannels;}
public List<Channel> flume_f2104_0(Event event)
{    return optionalChannels;}
public void flume_f2105_0(Context context)
{    String optionalList = context.getString(CONFIG_OPTIONAL);    requiredChannels = new ArrayList<Channel>(getAllChannels());    Map<String, Channel> channelNameMap = getChannelNameMap();    if (optionalList != null && !optionalList.isEmpty()) {        for (String optional : optionalList.split("\\s+")) {            Channel optionalChannel = channelNameMap.get(optional);            requiredChannels.remove(optionalChannel);            if (!optionalChannels.contains(optionalChannel)) {                optionalChannels.add(optionalChannel);            }        }    }}
public static void flume_f2106_1(String[] args)
{    SSLUtil.initGlobalSSLParameters();    AvroCLIClient client = new AvroCLIClient();    try {        if (client.parseCommandLine(args)) {            client.run();        }    } catch (ParseException e) {            } catch (IOException e) {            } catch (FlumeException e) {            } catch (EventDeliveryException e) {            }    }
private void flume_f2107_1(CommandLine commandLine)
{    String headerFile = commandLine.getOptionValue("headerFile");    FileInputStream fs = null;    try {        if (headerFile != null) {            fs = new FileInputStream(headerFile);            Properties properties = new Properties();            properties.load(fs);            for (Map.Entry<Object, Object> propertiesEntry : properties.entrySet()) {                String key = (String) propertiesEntry.getKey();                String value = (String) propertiesEntry.getValue();                                headers.put(key, value);            }        }    } catch (Exception e) {                return;    } finally {        if (fs != null) {            try {                fs.close();            } catch (Exception e) {                                return;            }        }    }}
private boolean flume_f2108_0(String[] args) throws ParseException
{    Options options = new Options();    options.addOption("P", "rpcProps", true, "RPC client properties file with " + "server connection params").addOption("p", "port", true, "port of the avro source").addOption("H", "host", true, "hostname of the avro source").addOption("F", "filename", true, "file to stream to avro source").addOption(null, "dirname", true, "directory to stream to avro source").addOption("R", "headerFile", true, ("file containing headers as " + "key/value pairs on each new line")).addOption("h", "help", false, "display help text");    CommandLineParser parser = new GnuParser();    CommandLine commandLine = parser.parse(options, args);    if (commandLine.hasOption('h')) {        new HelpFormatter().printHelp("flume-ng avro-client", "", options, "The --dirname option assumes that a spooling directory exists " + "where immutable log files are dropped.", true);        return false;    }    if (commandLine.hasOption("filename") && commandLine.hasOption("dirname")) {        throw new ParseException("--filename and --dirname options cannot be used simultaneously");    }    if (!commandLine.hasOption("port") && !commandLine.hasOption("host") && !commandLine.hasOption("rpcProps")) {        throw new ParseException("Either --rpcProps or both --host and --port " + "must be specified.");    }    if (commandLine.hasOption("rpcProps")) {        rpcClientPropsFile = commandLine.getOptionValue("rpcProps");        Preconditions.checkNotNull(rpcClientPropsFile, "RPC client properties " + "file must be specified after --rpcProps argument.");        Preconditions.checkArgument(new File(rpcClientPropsFile).exists(), "RPC client properties file %s does not exist!", rpcClientPropsFile);    }    if (rpcClientPropsFile == null) {        if (!commandLine.hasOption("port")) {            throw new ParseException("You must specify a port to connect to with --port");        }        port = Integer.parseInt(commandLine.getOptionValue("port"));        if (!commandLine.hasOption("host")) {            throw new ParseException("You must specify a hostname to connect to with --host");        }        hostname = commandLine.getOptionValue("host");    }    fileName = commandLine.getOptionValue("filename");    dirName = commandLine.getOptionValue("dirname");    if (commandLine.hasOption("headerFile")) {        parseHeaders(commandLine);    }    return true;}
private void flume_f2109_1() throws IOException, FlumeException, EventDeliveryException
{    EventReader reader = null;    RpcClient rpcClient;    if (rpcClientPropsFile != null) {        rpcClient = RpcClientFactory.getInstance(new File(rpcClientPropsFile));    } else {        rpcClient = RpcClientFactory.getDefaultInstance(hostname, port, BATCH_SIZE);    }    try {        if (fileName != null) {            reader = new SimpleTextLineEventReader(new FileReader(new File(fileName)));        } else if (dirName != null) {            reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(new File(dirName)).sourceCounter(new SourceCounter("avrocli")).build();        } else {            reader = new SimpleTextLineEventReader(new InputStreamReader(System.in));        }        long lastCheck = System.currentTimeMillis();        long sentBytes = 0;        int batchSize = rpcClient.getBatchSize();        List<Event> events;        while (!(events = reader.readEvents(batchSize)).isEmpty()) {            for (Event event : events) {                event.setHeaders(headers);                sentBytes += event.getBody().length;                sent++;                long now = System.currentTimeMillis();                if (now >= lastCheck + 5000) {                                        lastCheck = now;                }            }            rpcClient.appendBatch(events);            if (reader instanceof ReliableEventReader) {                ((ReliableEventReader) reader).commit();            }        }            } finally {        if (reader != null) {                        reader.close();        }                rpcClient.close();    }}
private List<File> flume_f2110_1(final Path directory)
{    Preconditions.checkNotNull(directory);    final List<File> candidateFiles = new ArrayList<>();    try {        final Set<Path> trackerDirCompletedFiles = getTrackerDirCompletedFiles();        Files.walkFileTree(directory, new SimpleFileVisitor<Path>() {            @Override            public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException {                if (directory.equals(dir)) {                                        return FileVisitResult.CONTINUE;                }                String directoryName = dir.getFileName().toString();                if (!recursiveDirectorySearch || directoryName.startsWith(".") || ignorePattern.matcher(directoryName).matches()) {                    return FileVisitResult.SKIP_SUBTREE;                }                return FileVisitResult.CONTINUE;            }            @Override            public FileVisitResult visitFile(Path candidate, BasicFileAttributes attrs) throws IOException {                String fileName = candidate.getFileName().toString();                if (!fileName.endsWith(completedSuffix) && !isFileInTrackerDir(trackerDirCompletedFiles, candidate) && !fileName.startsWith(".") && includePattern.matcher(fileName).matches() && !ignorePattern.matcher(fileName).matches()) {                    candidateFiles.add(candidate.toFile());                }                return FileVisitResult.CONTINUE;            }        });    } catch (IOException e) {                sourceCounter.incrementGenericProcessingFail();    }    return candidateFiles;}
public FileVisitResult flume_f2111_0(Path dir, BasicFileAttributes attrs) throws IOException
{    if (directory.equals(dir)) {                return FileVisitResult.CONTINUE;    }    String directoryName = dir.getFileName().toString();    if (!recursiveDirectorySearch || directoryName.startsWith(".") || ignorePattern.matcher(directoryName).matches()) {        return FileVisitResult.SKIP_SUBTREE;    }    return FileVisitResult.CONTINUE;}
public FileVisitResult flume_f2112_0(Path candidate, BasicFileAttributes attrs) throws IOException
{    String fileName = candidate.getFileName().toString();    if (!fileName.endsWith(completedSuffix) && !isFileInTrackerDir(trackerDirCompletedFiles, candidate) && !fileName.startsWith(".") && includePattern.matcher(fileName).matches() && !ignorePattern.matcher(fileName).matches()) {        candidateFiles.add(candidate.toFile());    }    return FileVisitResult.CONTINUE;}
private Set<Path> flume_f2113_0() throws IOException
{    final Set<Path> completedFiles = new HashSet<>();    if (TrackingPolicy.TRACKER_DIR != trackingPolicy) {        return completedFiles;    }    Path trackerDirPath = trackerDirectory.toPath();    Files.walkFileTree(trackerDirPath, new SimpleFileVisitor<Path>() {        @Override        public FileVisitResult visitFile(Path candidate, BasicFileAttributes attrs) throws IOException {            String fileName = candidate.getFileName().toString();            if (fileName.endsWith(completedSuffix)) {                completedFiles.add(candidate.toAbsolutePath());            }            return FileVisitResult.CONTINUE;        }    });    return completedFiles;}
public FileVisitResult flume_f2114_0(Path candidate, BasicFileAttributes attrs) throws IOException
{    String fileName = candidate.getFileName().toString();    if (fileName.endsWith(completedSuffix)) {        completedFiles.add(candidate.toAbsolutePath());    }    return FileVisitResult.CONTINUE;}
private boolean flume_f2115_0(Set<Path> completedFiles, Path path)
{    Path relPath = getRelPathToSpoolDir(path);    Path trackerPath = Paths.get(trackerDirectoryAbsolutePath, relPath.toString() + completedSuffix);    return completedFiles.contains(trackerPath);}
private Path flume_f2116_0(Path path)
{    return spoolDirPath.relativize(path.toAbsolutePath());}
 int flume_f2117_0()
{    return listFilesCount;}
public String flume_f2118_0()
{    if (!lastFileRead.isPresent()) {        return null;    }    return lastFileRead.get().getFile().getAbsolutePath();}
public Event flume_f2119_0() throws IOException
{    List<Event> events = readEvents(1);    if (!events.isEmpty()) {        return events.get(0);    } else {        return null;    }}
public List<Event> flume_f2120_1(int numEvents) throws IOException
{    if (!committed) {        if (!currentFile.isPresent()) {            throw new IllegalStateException("File should not roll when " + "commit is outstanding.");        }                currentFile.get().getDeserializer().reset();    } else {                if (!currentFile.isPresent()) {            currentFile = getNextFile();        }                if (!currentFile.isPresent()) {            return Collections.emptyList();        }    }    List<Event> events = readDeserializerEvents(numEvents);    /* It's possible that the last read took us just up to a file boundary.     * If so, try to roll to the next file, if there is one.     * Loop until events is not empty or there is no next file in case of 0 byte files */    while (events.isEmpty()) {                retireCurrentFile();        currentFile = getNextFile();        if (!currentFile.isPresent()) {            return Collections.emptyList();        }        events = readDeserializerEvents(numEvents);    }    fillHeader(events);    committed = false;    lastFileRead = currentFile;    return events;}
private List<Event> flume_f2121_0(int numEvents) throws IOException
{    EventDeserializer des = currentFile.get().getDeserializer();    List<Event> events = des.readEvents(numEvents);    if (events.isEmpty() && firstTimeRead) {        events.add(EventBuilder.withBody(new byte[0]));    }    firstTimeRead = false;    return events;}
private void flume_f2122_0(List<Event> events)
{    if (annotateFileName) {        String filename = currentFile.get().getFile().getAbsolutePath();        for (Event event : events) {            event.getHeaders().put(fileNameHeader, filename);        }    }    if (annotateBaseName) {        String basename = currentFile.get().getFile().getName();        for (Event event : events) {            event.getHeaders().put(baseNameHeader, basename);        }    }}
public void flume_f2123_0() throws IOException
{    if (currentFile.isPresent()) {        currentFile.get().getDeserializer().close();        currentFile = Optional.absent();    }}
public void flume_f2124_0() throws IOException
{    if (!committed && currentFile.isPresent()) {        currentFile.get().getDeserializer().mark();        committed = true;    }}
private void flume_f2125_0() throws IOException
{    Preconditions.checkState(currentFile.isPresent());    File fileToRoll = new File(currentFile.get().getFile().getAbsolutePath());    currentFile.get().getDeserializer().close();        if (fileToRoll.lastModified() != currentFile.get().getLastModified()) {        String message = "File has been modified since being read: " + fileToRoll;        throw new IllegalStateException(message);    }    if (fileToRoll.length() != currentFile.get().getLength()) {        String message = "File has changed size since being read: " + fileToRoll;        throw new IllegalStateException(message);    }    if (deletePolicy.equalsIgnoreCase(DeletePolicy.NEVER.name())) {        if (trackingPolicy == TrackingPolicy.RENAME) {            rollCurrentFile(fileToRoll);        } else {            rollCurrentFileInTrackerDir(fileToRoll);        }    } else if (deletePolicy.equalsIgnoreCase(DeletePolicy.IMMEDIATE.name())) {        deleteCurrentFile(fileToRoll);    } else {                throw new IllegalArgumentException("Unsupported delete policy: " + deletePolicy);    }}
private void flume_f2126_1(File fileToRoll) throws IOException
{    File dest = new File(fileToRoll.getPath() + completedSuffix);            if (dest.exists() && PlatformDetect.isWindows()) {        /*       * If we are here, it means the completed file already exists. In almost       * every case this means the user is violating an assumption of Flume       * (that log files are placed in the spooling directory with unique       * names). However, there is a corner case on Windows systems where the       * file was already rolled but the rename was not atomic. If that seems       * likely, we let it pass with only a warning.       */        if (com.google.common.io.Files.equal(currentFile.get().getFile(), dest)) {                        boolean deleted = fileToRoll.delete();            if (!deleted) {                                sourceCounter.incrementGenericProcessingFail();            }        } else {            String message = "File name has been re-used with different" + " files. Spooling assumptions violated for " + dest;            throw new IllegalStateException(message);        }        } else if (dest.exists()) {        String message = "File name has been re-used with different" + " files. Spooling assumptions violated for " + dest;        throw new IllegalStateException(message);        } else {        boolean renamed = fileToRoll.renameTo(dest);        if (renamed) {                                    deleteMetaFile();        } else {            /* If we are here then the file cannot be renamed for a reason other         * than that the destination file exists (actually, that remains         * possible w/ small probability due to TOC-TOU conditions).*/            String message = "Unable to move " + fileToRoll + " to " + dest + ". This will likely cause duplicate events. Please verify that " + "flume has sufficient permissions to perform these operations.";            throw new FlumeException(message);        }    }}
private void flume_f2127_1(File fileToRoll) throws IOException
{    Path path = fileToRoll.toPath();    Path relToRoll = getRelPathToSpoolDir(path);    File dest = new File(trackerDirectory.getPath(), relToRoll + completedSuffix);        if (dest.exists()) {        String message = "File name has been re-used with different" + " files. Spooling assumptions violated for " + dest;        throw new IllegalStateException(message);    }            dest.getParentFile().mkdirs();    if (!dest.createNewFile()) {        throw new IOException("Could not create tracker file: " + dest);    }}
private void flume_f2128_1(File fileToDelete) throws IOException
{        if (!fileToDelete.exists()) {                return;    }    if (!fileToDelete.delete()) {        throw new IOException("Unable to delete spool file: " + fileToDelete);    }        deleteMetaFile();}
private Optional<FileInfo> flume_f2129_0()
{    List<File> candidateFiles = Collections.emptyList();    if (consumeOrder != ConsumeOrder.RANDOM || candidateFileIter == null || !candidateFileIter.hasNext()) {        candidateFiles = getCandidateFiles(spoolDirectory.toPath());        listFilesCount++;        candidateFileIter = candidateFiles.iterator();    }    if (!candidateFileIter.hasNext()) {                return Optional.absent();    }    File selectedFile = candidateFileIter.next();    if (consumeOrder == ConsumeOrder.RANDOM) {                return openFile(selectedFile);    } else if (consumeOrder == ConsumeOrder.YOUNGEST) {        for (File candidateFile : candidateFiles) {            long compare = selectedFile.lastModified() - candidateFile.lastModified();            if (compare == 0) {                                selectedFile = smallerLexicographical(selectedFile, candidateFile);            } else if (compare < 0) {                                selectedFile = candidateFile;            }        }    } else {                for (File candidateFile : candidateFiles) {            long compare = selectedFile.lastModified() - candidateFile.lastModified();            if (compare == 0) {                                selectedFile = smallerLexicographical(selectedFile, candidateFile);            } else if (compare > 0) {                                selectedFile = candidateFile;            }        }    }    firstTimeRead = true;    return openFile(selectedFile);}
private File flume_f2130_0(File f1, File f2)
{    if (f1.getName().compareTo(f2.getName()) < 0) {        return f1;    }    return f2;}
private Optional<FileInfo> flume_f2131_1(File file)
{    try {                String nextPath = file.getPath();        PositionTracker tracker = DurablePositionTracker.getInstance(metaFile, nextPath);        if (!tracker.getTarget().equals(nextPath)) {            tracker.close();            deleteMetaFile();            tracker = DurablePositionTracker.getInstance(metaFile, nextPath);        }                Preconditions.checkState(tracker.getTarget().equals(nextPath), "Tracker target %s does not equal expected filename %s", tracker.getTarget(), nextPath);        ResettableInputStream in = new ResettableFileInputStream(file, tracker, ResettableFileInputStream.DEFAULT_BUF_SIZE, inputCharset, decodeErrorPolicy);        EventDeserializer deserializer = EventDeserializerFactory.getInstance(deserializerType, deserializerContext, in);        return Optional.of(new FileInfo(file, deserializer));    } catch (FileNotFoundException e) {                        return Optional.absent();    } catch (IOException e) {                sourceCounter.incrementGenericProcessingFail();        return Optional.absent();    }}
private void flume_f2132_0() throws IOException
{    if (metaFile.exists() && !metaFile.delete()) {        throw new IOException("Unable to delete old meta file " + metaFile);    }}
public long flume_f2133_0()
{    return length;}
public long flume_f2134_0()
{    return lastModified;}
public EventDeserializer flume_f2135_0()
{    return deserializer;}
public File flume_f2136_0()
{    return file;}
public Builder flume_f2137_0(File directory)
{    this.spoolDirectory = directory;    return this;}
public Builder flume_f2138_0(String completedSuffix)
{    this.completedSuffix = completedSuffix;    return this;}
public Builder flume_f2139_0(String includePattern)
{    this.includePattern = includePattern;    return this;}
public Builder flume_f2140_0(String ignorePattern)
{    this.ignorePattern = ignorePattern;    return this;}
public Builder flume_f2141_0(String trackerDirPath)
{    this.trackerDirPath = trackerDirPath;    return this;}
public Builder flume_f2142_0(Boolean annotateFileName)
{    this.annotateFileName = annotateFileName;    return this;}
public Builder flume_f2143_0(String fileNameHeader)
{    this.fileNameHeader = fileNameHeader;    return this;}
public Builder flume_f2144_0(Boolean annotateBaseName)
{    this.annotateBaseName = annotateBaseName;    return this;}
public Builder flume_f2145_0(String baseNameHeader)
{    this.baseNameHeader = baseNameHeader;    return this;}
public Builder flume_f2146_0(String deserializerType)
{    this.deserializerType = deserializerType;    return this;}
public Builder flume_f2147_0(Context deserializerContext)
{    this.deserializerContext = deserializerContext;    return this;}
public Builder flume_f2148_0(String deletePolicy)
{    this.deletePolicy = deletePolicy;    return this;}
public Builder flume_f2149_0(String trackingPolicy)
{    this.trackingPolicy = trackingPolicy;    return this;}
public Builder flume_f2150_0(String inputCharset)
{    this.inputCharset = inputCharset;    return this;}
public Builder flume_f2151_0(boolean recursiveDirectorySearch)
{    this.recursiveDirectorySearch = recursiveDirectorySearch;    return this;}
public Builder flume_f2152_0(DecodeErrorPolicy decodeErrorPolicy)
{    this.decodeErrorPolicy = decodeErrorPolicy;    return this;}
public Builder flume_f2153_0(ConsumeOrder consumeOrder)
{    this.consumeOrder = consumeOrder;    return this;}
public Builder flume_f2154_0(SourceCounter sourceCounter)
{    this.sourceCounter = sourceCounter;    return this;}
public ReliableSpoolingFileEventReader flume_f2155_0() throws IOException
{    return new ReliableSpoolingFileEventReader(spoolDirectory, completedSuffix, includePattern, ignorePattern, trackerDirPath, annotateFileName, fileNameHeader, annotateBaseName, baseNameHeader, deserializerType, deserializerContext, deletePolicy, trackingPolicy, inputCharset, decodeErrorPolicy, consumeOrder, recursiveDirectorySearch, sourceCounter);}
public Event flume_f2156_0() throws IOException
{    String line = reader.readLine();    if (line != null) {        return EventBuilder.withBody(line, Charsets.UTF_8);    } else {        return null;    }}
public List<Event> flume_f2157_0(int n) throws IOException
{    List<Event> events = Lists.newLinkedList();    while (events.size() < n) {        Event event = readEvent();        if (event != null) {            events.add(event);        } else {            break;        }    }    return events;}
public void flume_f2158_0() throws IOException
{    reader.close();}
public static boolean flume_f2159_0(Object target, Context context)
{    if (target instanceof Configurable) {        ((Configurable) target).configure(context);        return true;    }    return false;}
public static boolean flume_f2160_0(Object target, ComponentConfiguration conf)
{    if (target instanceof ConfigurableComponent) {        ((ConfigurableComponent) target).configure(conf);        return true;    }    return false;}
public static void flume_f2161_0(Context context, String... keys)
{    for (String key : keys) {        if (!context.getParameters().containsKey(key) || context.getParameters().get(key) == null) {            throw new IllegalArgumentException("Required parameter " + key + " must exist and may not be null");        }    }}
public static void flume_f2162_0(Context context, String... keys)
{    for (String key : keys) {        if (context.getParameters().containsKey(key) && context.getParameters().get(key) == null) {            throw new IllegalArgumentException("Optional parameter " + key + " may not be null");        }    }}
public synchronized Long flume_f2163_0(String name)
{    return getCounter(name).get();}
public synchronized Long flume_f2164_0(String name)
{    return getCounter(name).incrementAndGet();}
public synchronized Long flume_f2165_0(String name, Long delta)
{    return getCounter(name).addAndGet(delta);}
public synchronized void flume_f2166_0(CounterGroup counterGroup)
{    synchronized (counterGroup) {        for (Entry<String, AtomicLong> entry : counterGroup.getCounters().entrySet()) {            addAndGet(entry.getKey(), entry.getValue().get());        }    }}
public synchronized void flume_f2167_0(String name, Long value)
{    getCounter(name).set(value);}
public synchronized AtomicLong flume_f2168_0(String name)
{    if (!counters.containsKey(name)) {        counters.put(name, new AtomicLong());    }    return counters.get(name);}
public synchronized String flume_f2169_0()
{    return "{ name:" + name + " counters:" + counters + " }";}
public synchronized String flume_f2170_0()
{    return name;}
public synchronized void flume_f2171_0(String name)
{    this.name = name;}
public synchronized HashMap<String, AtomicLong> flume_f2172_0()
{    return counters;}
public synchronized void flume_f2173_0(HashMap<String, AtomicLong> counters)
{    this.counters = counters;}
public static String flume_f2174_0(Event event)
{    return dumpEvent(event, DEFAULT_MAX_BYTES);}
public static String flume_f2175_1(Event event, int maxBytes)
{    StringBuilder buffer = new StringBuilder();    if (event == null || event.getBody() == null) {        buffer.append("null");    } else if (event.getBody().length == 0) {        } else {        byte[] body = event.getBody();        byte[] data = Arrays.copyOf(body, Math.min(body.length, maxBytes));        ByteArrayOutputStream out = new ByteArrayOutputStream();        try {            HexDump.dump(data, 0, out, 0);            String hexDump = new String(out.toByteArray());                        if (hexDump.startsWith(HEXDUMP_OFFSET)) {                hexDump = hexDump.substring(HEXDUMP_OFFSET.length());            }            buffer.append(hexDump);        } catch (Exception e) {            if (LOGGER.isInfoEnabled()) {                            }            buffer.append("...Exception while dumping: ").append(e.getMessage());        }        String result = buffer.toString();        if (result.endsWith(EOL) && buffer.length() > EOL.length()) {            buffer.delete(buffer.length() - EOL.length(), buffer.length()).toString();        }    }    return "{ headers:" + event.getHeaders() + " body:" + buffer + " }";}
public static boolean flume_f2176_0(String in)
{    return tagPattern.matcher(in).find();}
public static String flume_f2177_0(char c)
{        switch(c) {        case 'a':            return "weekday_short";        case 'A':            return "weekday_full";        case 'b':            return "monthname_short";        case 'B':            return "monthname_full";        case 'c':            return "datetime";        case 'd':                        return "day_of_month_xx";        case 'e':                        return "day_of_month_x";        case 'D':                        return "date_short";        case 'H':            return "hour_24_xx";        case 'I':            return "hour_12_xx";        case 'j':                        return "day_of_year_xxx";        case 'k':                        return "hour_24";        case 'l':                        return "hour_12";        case 'm':            return "month_xx";        case 'n':                        return "month_x";        case 'M':            return "minute_xx";        case 'p':            return "am_pm";        case 's':            return "unix_seconds";        case 'S':            return "seconds_xx";        case 't':                        return "unix_millis";        case 'y':            return "year_xx";        case 'Y':            return "year_xxxx";        case 'z':            return "timezone_delta";        default:                        return "" + c;    }}
public static String flume_f2178_0(char c, Map<String, String> headers)
{    return replaceShorthand(c, headers, false, 0, 0);}
public static String flume_f2179_0(char c, Map<String, String> headers, boolean needRounding, int unit, int roundDown)
{    return replaceShorthand(c, headers, null, needRounding, unit, roundDown, false);}
public static String flume_f2180_0(char c, Map<String, String> headers, TimeZone timeZone, boolean needRounding, int unit, int roundDown, boolean useLocalTimestamp)
{    long ts = 0;    if (useLocalTimestamp) {        ts = clock.currentTimeMillis();    }    return replaceShorthand(c, headers, timeZone, needRounding, unit, roundDown, false, ts);}
protected HashMap<String, SimpleDateFormat> flume_f2181_0()
{    return new HashMap<String, SimpleDateFormat>();}
protected static SimpleDateFormat flume_f2182_0(String string)
{    HashMap<String, SimpleDateFormat> localCache = simpleDateFormatCache.get();    SimpleDateFormat simpleDateFormat = localCache.get(string);    if (simpleDateFormat == null) {        simpleDateFormat = new SimpleDateFormat(string);        localCache.put(string, simpleDateFormat);        simpleDateFormatCache.set(localCache);    }    return simpleDateFormat;}
protected static String flume_f2183_0(String key)
{    String replacementString = "";    switch(key.toLowerCase()) {        case "localhost":            replacementString = InetAddressCache.hostName;            break;        case "ip":            replacementString = InetAddressCache.hostAddress;            break;        case "fqdn":            replacementString = InetAddressCache.canonicalHostName;            break;        default:            throw new RuntimeException("The static escape string '" + key + "'" + " was provided but does not match any of (localhost,IP,FQDN)");    }    return replacementString;}
protected static String flume_f2184_0(char c, Map<String, String> headers, TimeZone timeZone, boolean needRounding, int unit, int roundDown, boolean useLocalTimestamp, long ts)
{    String timestampHeader = null;    try {        if (!useLocalTimestamp) {            timestampHeader = headers.get("timestamp");            Preconditions.checkNotNull(timestampHeader, "Expected timestamp in " + "the Flume event headers, but it was null");            ts = Long.valueOf(timestampHeader);        } else {            timestampHeader = String.valueOf(ts);        }    } catch (NumberFormatException e) {        throw new RuntimeException("Flume wasn't able to parse timestamp header" + " in the event to resolve time based bucketing. Please check that" + " you're correctly populating timestamp header (for example using" + " TimestampInterceptor source interceptor).", e);    }    if (needRounding) {        ts = roundDown(roundDown, unit, ts, timeZone);    }        String formatString = "";    switch(c) {        case '%':            return "%";        case 'a':            formatString = "EEE";            break;        case 'A':            formatString = "EEEE";            break;        case 'b':            formatString = "MMM";            break;        case 'B':            formatString = "MMMM";            break;        case 'c':            formatString = "EEE MMM d HH:mm:ss yyyy";            break;        case 'd':            formatString = "dd";            break;        case 'e':            formatString = "d";            break;        case 'D':            formatString = "MM/dd/yy";            break;        case 'H':            formatString = "HH";            break;        case 'I':            formatString = "hh";            break;        case 'j':            formatString = "DDD";            break;        case 'k':            formatString = "H";            break;        case 'l':            formatString = "h";            break;        case 'm':            formatString = "MM";            break;        case 'M':            formatString = "mm";            break;        case 'n':            formatString = "M";            break;        case 'p':            formatString = "a";            break;        case 's':            return "" + (ts / 1000);        case 'S':            formatString = "ss";            break;        case 't':                        return timestampHeader;        case 'y':            formatString = "yy";            break;        case 'Y':            formatString = "yyyy";            break;        case 'z':            formatString = "ZZZ";            break;        default:                        return "";    }    SimpleDateFormat format = getSimpleDateFormat(formatString);    if (timeZone != null) {        format.setTimeZone(timeZone);    } else {        format.setTimeZone(TimeZone.getDefault());    }    Date date = new Date(ts);    return format.format(date);}
private static long flume_f2185_0(int roundDown, int unit, long ts, TimeZone timeZone)
{    long timestamp = ts;    if (roundDown <= 0) {        roundDown = 1;    }    switch(unit) {        case Calendar.SECOND:            timestamp = TimestampRoundDownUtil.roundDownTimeStampSeconds(ts, roundDown, timeZone);            break;        case Calendar.MINUTE:            timestamp = TimestampRoundDownUtil.roundDownTimeStampMinutes(ts, roundDown, timeZone);            break;        case Calendar.HOUR_OF_DAY:            timestamp = TimestampRoundDownUtil.roundDownTimeStampHours(ts, roundDown, timeZone);            break;        default:            timestamp = ts;            break;    }    return timestamp;}
public static String flume_f2186_0(String in, Map<String, String> headers)
{    return escapeString(in, headers, false, 0, 0);}
public static String flume_f2187_0(String in, Map<String, String> headers, boolean needRounding, int unit, int roundDown)
{    return escapeString(in, headers, null, needRounding, unit, roundDown, false);}
public static String flume_f2188_0(String in, Map<String, String> headers, TimeZone timeZone, boolean needRounding, int unit, int roundDown, boolean useLocalTimeStamp)
{    long ts = clock.currentTimeMillis();    Matcher matcher = tagPattern.matcher(in);    StringBuffer sb = new StringBuffer();    while (matcher.find()) {        String replacement = "";                if (matcher.group(2) != null) {            replacement = headers.get(matcher.group(2));            if (replacement == null) {                replacement = "";                        }                } else if (matcher.group(3) != null) {            replacement = replaceStaticString(matcher.group(3));        } else {                                                Preconditions.checkState(matcher.group(1) != null && matcher.group(1).length() == 1, "Expected to match single character tag in string " + in);            char c = matcher.group(1).charAt(0);            replacement = replaceShorthand(c, headers, timeZone, needRounding, unit, roundDown, useLocalTimeStamp, ts);        }                                                                                replacement = replacement.replaceAll("\\\\", "\\\\\\\\");        replacement = replacement.replaceAll("\\$", "\\\\\\$");        matcher.appendReplacement(sb, replacement);    }    matcher.appendTail(sb);    return sb.toString();}
public static Map<String, String> flume_f2189_0(String in, Map<String, String> headers)
{    return getEscapeMapping(in, headers, false, 0, 0);}
public static Map<String, String> flume_f2190_0(String in, Map<String, String> headers, boolean needRounding, int unit, int roundDown)
{    Map<String, String> mapping = new HashMap<String, String>();    Matcher matcher = tagPattern.matcher(in);    while (matcher.find()) {        String replacement = "";                if (matcher.group(2) != null) {            replacement = headers.get(matcher.group(2));            if (replacement == null) {                replacement = "";                        }            mapping.put(matcher.group(2), replacement);        } else {                                                Preconditions.checkState(matcher.group(1) != null && matcher.group(1).length() == 1, "Expected to match single character tag in string " + in);            char c = matcher.group(1).charAt(0);            replacement = replaceShorthand(c, headers, needRounding, unit, roundDown);            mapping.put(expandShorthand(c), replacement);        }    }    return mapping;}
public static void flume_f2191_0(Clock clk)
{    clock = clk;}
public static Clock flume_f2192_0()
{    return clock;}
public File flume_f2193_0()
{    StringBuilder sb = new StringBuilder();    sb.append(filePrefix).append(seriesTimestamp).append("-");    sb.append(fileIndex.incrementAndGet());    if (extension.length() > 0) {        sb.append(".").append(extension);    }    currentFile = new File(baseDirectory, sb.toString());    return currentFile;}
public File flume_f2194_0()
{    if (currentFile == null) {        return nextFile();    }    return currentFile;}
public void flume_f2195_0()
{    currentFile = null;}
public File flume_f2196_0()
{    return baseDirectory;}
public void flume_f2197_0(File baseDirectory)
{    this.baseDirectory = baseDirectory;}
public long flume_f2198_0()
{    return seriesTimestamp;}
public String flume_f2199_0()
{    return filePrefix;}
public String flume_f2200_0()
{    return extension;}
public AtomicInteger flume_f2201_0()
{    return fileIndex;}
public PathManager flume_f2202_0(Context context)
{    return new DefaultPathManager(context);}
public static PathManager flume_f2203_1(String managerType, Context context)
{    Preconditions.checkNotNull(managerType, "path manager type must not be null");        PathManagerType type;    try {        type = PathManagerType.valueOf(managerType.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {                type = PathManagerType.OTHER;    }    Class<? extends PathManager.Builder> builderClass = type.getBuilderClass();        if (builderClass == null) {        try {            Class c = Class.forName(managerType);            if (c != null && PathManager.Builder.class.isAssignableFrom(c)) {                builderClass = (Class<? extends PathManager.Builder>) c;            } else {                String errMessage = "Unable to instantiate Builder from " + managerType + ": does not appear to implement " + PathManager.Builder.class.getName();                throw new FlumeException(errMessage);            }        } catch (ClassNotFoundException ex) {                        throw new FlumeException(ex);        }    }        PathManager.Builder builder;    try {        builder = builderClass.newInstance();    } catch (InstantiationException ex) {        String errMessage = "Cannot instantiate builder: " + managerType;                throw new FlumeException(errMessage, ex);    } catch (IllegalAccessException ex) {        String errMessage = "Cannot instantiate builder: " + managerType;                throw new FlumeException(errMessage, ex);    }    return builder.build(context);}
public Class<? extends PathManager.Builder> flume_f2204_0()
{    return builderClass;}
public File flume_f2205_0()
{    StringBuilder sb = new StringBuilder();    String date = formatter.print(LocalDateTime.now());    if (!date.equals(lastRoll)) {        getFileIndex().set(0);        lastRoll = date;    }    sb.append(getPrefix()).append(date).append("-");    sb.append(getFileIndex().incrementAndGet());    if (getExtension().length() > 0) {        sb.append(".").append(getExtension());    }    currentFile = new File(getBaseDirectory(), sb.toString());    return currentFile;}
public PathManager flume_f2206_0(Context context)
{    return new RollTimePathManager(context);}
public byte[] flume_f2207_0(Event event)
{    String body = event.getBody().length > 0 ? new String(event.getBody()) : "";    return (body + "\n").getBytes();}
public long flume_f2208_0()
{    return get(COUNTER_CHANNEL_SIZE);}
public void flume_f2209_0(long newSize)
{    set(COUNTER_CHANNEL_SIZE, newSize);}
public long flume_f2210_0()
{    return get(COUNTER_EVENT_PUT_ATTEMPT);}
public long flume_f2211_0()
{    return increment(COUNTER_EVENT_PUT_ATTEMPT);}
public long flume_f2212_0()
{    return get(COUNTER_EVENT_TAKE_ATTEMPT);}
public long flume_f2213_0()
{    return increment(COUNTER_EVENT_TAKE_ATTEMPT);}
public long flume_f2214_0()
{    return get(COUNTER_EVENT_PUT_SUCCESS);}
public long flume_f2215_0(long delta)
{    return addAndGet(COUNTER_EVENT_PUT_SUCCESS, delta);}
public long flume_f2216_0()
{    return get(COUNTER_EVENT_TAKE_SUCCESS);}
public long flume_f2217_0(long delta)
{    return addAndGet(COUNTER_EVENT_TAKE_SUCCESS, delta);}
public void flume_f2218_0(long capacity)
{    set(COUNTER_CHANNEL_CAPACITY, capacity);}
public long flume_f2219_0()
{    return get(COUNTER_CHANNEL_CAPACITY);}
public double flume_f2220_0()
{    long capacity = getChannelCapacity();    if (capacity != 0L) {        return (getChannelSize() / (double) capacity) * 100;    }    return Double.MAX_VALUE;}
protected void flume_f2221_0(String s)
{    byte[] bytes = s.getBytes();    int len = bytes.length;    xdr_int(len);    System.arraycopy(bytes, 0, buffer, offset, len);    offset += len;    pad();}
private void flume_f2222_0()
{    int newOffset = ((offset + 3) / 4) * 4;    while (offset < newOffset) {        buffer[offset++] = 0;    }}
protected void flume_f2223_0(int i)
{    buffer[offset++] = (byte) ((i >> 24) & 0xff);    buffer[offset++] = (byte) ((i >> 16) & 0xff);    buffer[offset++] = (byte) ((i >> 8) & 0xff);    buffer[offset++] = (byte) (i & 0xff);}
public synchronized void flume_f2224_1()
{    DatagramPacket packet;    for (SocketAddress addr : addresses) {        try {            packet = new DatagramPacket(buffer, offset, addr);            socket.send(packet);        } catch (Exception ex) {                    }    }    offset = 0;}
public void flume_f2225_1()
{    try {        socket = new DatagramSocket();        hostname = InetAddress.getLocalHost().getHostName();    } catch (SocketException ex) {                throw new FlumeException("Could not create socket for metrics collection.", ex);    } catch (Exception ex2) {            }    for (HostInfo host : hosts) {        addresses.add(new InetSocketAddress(host.getHostName(), host.getPortNumber()));    }    collectorRunnable.server = this;    if (service.isShutdown() || service.isTerminated()) {        service = Executors.newSingleThreadScheduledExecutor();    }    service.scheduleWithFixedDelay(collectorRunnable, 0, pollFrequency, TimeUnit.SECONDS);}
public void flume_f2226_1()
{    service.shutdown();    while (!service.isTerminated()) {        try {                        service.awaitTermination(500, TimeUnit.MILLISECONDS);        } catch (InterruptedException ex) {                        service.shutdownNow();        }    }    addresses.clear();}
public void flume_f2227_0(int pollFrequency)
{    this.pollFrequency = pollFrequency;}
public int flume_f2228_0()
{    return pollFrequency;}
public void flume_f2229_0(boolean isGanglia3)
{    this.isGanglia3 = isGanglia3;}
public boolean flume_f2230_0()
{    return this.isGanglia3;}
protected void flume_f2231_1(String name, String value)
{        name = hostname + "." + name;    xdr_int(0);    String type = "string";    try {        Float.parseFloat(value);        type = "float";    } catch (NumberFormatException ex) {        }        xdr_string(type);    xdr_string(name);    xdr_string(value);    xdr_string(DEFAULT_UNITS);    xdr_int(DEFAULT_SLOPE);    xdr_int(DEFAULT_TMAX);    xdr_int(DEFAULT_DMAX);}
protected void flume_f2232_1(String name, String value)
{            xdr_int(128);        xdr_string(hostname);        xdr_string(name);        xdr_int(0);    String type = "string";    try {        Float.parseFloat(value);        type = "float";    } catch (NumberFormatException ex) {        }        xdr_string(type);        xdr_string(name);        xdr_string(DEFAULT_UNITS);        xdr_int(DEFAULT_SLOPE);        xdr_int(DEFAULT_TMAX);        xdr_int(DEFAULT_DMAX);    xdr_int(1);    /*Num of the entries in extra_value field for Ganglia 3.1.x*/    xdr_string("GROUP");    /*Group attribute*/    xdr_string("flume");    /*Group value*/    this.sendToGangliaNodes();                        xdr_int(133);        xdr_string(hostname);        xdr_string(name);        xdr_int(0);        xdr_string("%s");        xdr_string(value);}
public void flume_f2233_0(Context context)
{    this.pollFrequency = context.getInteger(this.CONF_POLL_FREQUENCY, 60);    String localHosts = context.getString(this.CONF_HOSTS);    if (localHosts == null || localHosts.isEmpty()) {        throw new ConfigurationException("Hosts list cannot be empty.");    }    this.hosts = this.getHostsFromString(localHosts);    this.isGanglia3 = context.getBoolean(this.CONF_ISGANGLIA3, false);}
private List<HostInfo> flume_f2234_1(String hosts) throws FlumeException
{    List<HostInfo> hostInfoList = new ArrayList<HostInfo>();    String[] hostsAndPorts = hosts.split(",");    int i = 0;    for (String host : hostsAndPorts) {        String[] hostAndPort = host.split(":");        if (hostAndPort.length < 2) {                        continue;        }        try {            hostInfoList.add(new HostInfo("ganglia_host-" + String.valueOf(i), hostAndPort[0], Integer.parseInt(hostAndPort[1])));        } catch (Exception e) {                        continue;        }    }    if (hostInfoList.isEmpty()) {        throw new FlumeException("No valid ganglia hosts defined!");    }    return hostInfoList;}
public void flume_f2235_1()
{    try {        Map<String, Map<String, String>> metricsMap = JMXPollUtil.getAllMBeans();        for (String component : metricsMap.keySet()) {            Map<String, String> attributeMap = metricsMap.get(component);            for (String attribute : attributeMap.keySet()) {                if (isGanglia3) {                    server.createGangliaMessage(GANGLIA_CONTEXT + component + "." + attribute, attributeMap.get(attribute));                } else {                    server.createGangliaMessage31(GANGLIA_CONTEXT + component + "." + attribute, attributeMap.get(attribute));                }                server.sendToGangliaNodes();            }        }    } catch (Throwable t) {            }}
public void flume_f2236_1()
{    jettyServer = new Server();            HttpConfiguration httpConfiguration = new HttpConfiguration();    ServerConnector connector = new ServerConnector(jettyServer, new HttpConnectionFactory(httpConfiguration));    connector.setReuseAddress(true);    connector.setPort(port);    jettyServer.addConnector(connector);    jettyServer.setHandler(new HTTPMetricsHandler());    try {        jettyServer.start();        while (!jettyServer.isStarted()) {            Thread.sleep(500);        }    } catch (Exception ex) {            }}
public void flume_f2237_1()
{    try {        jettyServer.stop();        jettyServer.join();    } catch (Exception ex) {            }}
public void flume_f2238_0(Context context)
{    port = context.getInteger(CONFIG_PORT, DEFAULT_PORT);}
public void flume_f2239_0(String target, Request r1, HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException
{        if (request.getMethod().equalsIgnoreCase("TRACE") || request.getMethod().equalsIgnoreCase("OPTIONS")) {        response.sendError(HttpServletResponse.SC_FORBIDDEN);        response.flushBuffer();        ((Request) request).setHandled(true);        return;    }    if (target.equals("/")) {        response.setContentType("text/html;charset=utf-8");        response.setStatus(HttpServletResponse.SC_OK);        response.getWriter().write("For Flume metrics please click" + " <a href = \"./metrics\"> here</a>.");        response.flushBuffer();        ((Request) request).setHandled(true);        return;    } else if (target.equalsIgnoreCase("/metrics")) {        response.setContentType("application/json;charset=utf-8");        response.setStatus(HttpServletResponse.SC_OK);        Map<String, Map<String, String>> metricsMap = JMXPollUtil.getAllMBeans();        String json = gson.toJson(metricsMap, mapType);        response.getWriter().write(json);        response.flushBuffer();        ((Request) request).setHandled(true);        return;    }    response.sendError(HttpServletResponse.SC_NOT_FOUND);    response.flushBuffer();}
public long flume_f2240_0(long delta)
{    return addAndGet(TIMER_KAFKA_EVENT_GET, delta);}
public long flume_f2241_0(long delta)
{    return addAndGet(TIMER_KAFKA_EVENT_SEND, delta);}
public long flume_f2242_0(long delta)
{    return addAndGet(TIMER_KAFKA_COMMIT, delta);}
public long flume_f2243_0(long delta)
{    return addAndGet(COUNT_ROLLBACK, delta);}
public long flume_f2244_0()
{    return get(TIMER_KAFKA_EVENT_GET);}
public long flume_f2245_0()
{    return get(TIMER_KAFKA_EVENT_SEND);}
public long flume_f2246_0()
{    return get(TIMER_KAFKA_COMMIT);}
public long flume_f2247_0()
{    return get(COUNT_ROLLBACK);}
public long flume_f2248_0(long delta)
{    return addAndGet(TIMER_KAFKA_EVENT_SEND, delta);}
public long flume_f2249_0()
{    return increment(COUNT_ROLLBACK);}
public long flume_f2250_0()
{    return get(TIMER_KAFKA_EVENT_SEND);}
public long flume_f2251_0()
{    return get(COUNT_ROLLBACK);}
public long flume_f2252_0(long delta)
{    return addAndGet(TIMER_KAFKA_EVENT_GET, delta);}
public long flume_f2253_0(long delta)
{    return addAndGet(TIMER_KAFKA_COMMIT, delta);}
public long flume_f2254_0()
{    return increment(COUNTER_KAFKA_EMPTY);}
public long flume_f2255_0()
{    return get(TIMER_KAFKA_COMMIT);}
public long flume_f2256_0()
{    return get(TIMER_KAFKA_EVENT_GET);}
public long flume_f2257_0()
{    return get(COUNTER_KAFKA_EMPTY);}
public void flume_f2258_1()
{    register();    stopTime.set(0L);    for (String counter : counterMap.keySet()) {        counterMap.get(counter).set(0L);    }    startTime.set(System.currentTimeMillis());    }
 void flume_f2259_1()
{    if (!registered) {        try {            ObjectName objName = new ObjectName("org.apache.flume." + type.name().toLowerCase(Locale.ENGLISH) + ":type=" + this.name);            if (ManagementFactory.getPlatformMBeanServer().isRegistered(objName)) {                                ManagementFactory.getPlatformMBeanServer().unregisterMBean(objName);                            }            ManagementFactory.getPlatformMBeanServer().registerMBean(this, objName);                        registered = true;        } catch (Exception ex) {                    }    }}
public void flume_f2260_1()
{        stopTime.set(System.currentTimeMillis());                final String typePrefix = type.name().toLowerCase(Locale.ENGLISH);                        final List<String> mapKeys = new ArrayList<String>(counterMap.keySet());    Collections.sort(mapKeys);        for (final String counterMapKey : mapKeys) {                final long counterMapValue = get(counterMapKey);            }}
public long flume_f2261_0()
{    return startTime.get();}
public long flume_f2262_0()
{    return stopTime.get();}
public final String flume_f2263_0()
{    StringBuilder sb = new StringBuilder(type.name()).append(":");    sb.append(name).append("{");    boolean first = true;    Iterator<String> counterIterator = counterMap.keySet().iterator();    while (counterIterator.hasNext()) {        if (first) {            first = false;        } else {            sb.append(", ");        }        String counterName = counterIterator.next();        sb.append(counterName).append("=").append(get(counterName));    }    sb.append("}");    return sb.toString();}
protected long flume_f2264_0(String counter)
{    return counterMap.get(counter).get();}
protected void flume_f2265_0(String counter, long value)
{    counterMap.get(counter).set(value);}
protected long flume_f2266_0(String counter, long delta)
{    return counterMap.get(counter).addAndGet(delta);}
protected long flume_f2267_0(String counter)
{    return counterMap.get(counter).incrementAndGet();}
public String flume_f2268_0()
{    return type.name();}
public Class<? extends MonitorService> flume_f2269_0()
{    return this.monitoringClass;}
public long flume_f2270_0()
{    return get(COUNTER_CONNECTION_CREATED);}
public long flume_f2271_0()
{    return increment(COUNTER_CONNECTION_CREATED);}
public long flume_f2272_0()
{    return get(COUNTER_CONNECTION_CLOSED);}
public long flume_f2273_0()
{    return increment(COUNTER_CONNECTION_CLOSED);}
public long flume_f2274_0()
{    return get(COUNTER_CONNECTION_FAILED);}
public long flume_f2275_0()
{    return increment(COUNTER_CONNECTION_FAILED);}
public long flume_f2276_0()
{    return get(COUNTER_BATCH_EMPTY);}
public long flume_f2277_0()
{    return increment(COUNTER_BATCH_EMPTY);}
public long flume_f2278_0()
{    return get(COUNTER_BATCH_UNDERFLOW);}
public long flume_f2279_0()
{    return increment(COUNTER_BATCH_UNDERFLOW);}
public long flume_f2280_0()
{    return get(COUNTER_BATCH_COMPLETE);}
public long flume_f2281_0()
{    return increment(COUNTER_BATCH_COMPLETE);}
public long flume_f2282_0()
{    return get(COUNTER_EVENT_DRAIN_ATTEMPT);}
public long flume_f2283_0()
{    return increment(COUNTER_EVENT_DRAIN_ATTEMPT);}
public long flume_f2284_0(long delta)
{    return addAndGet(COUNTER_EVENT_DRAIN_ATTEMPT, delta);}
public long flume_f2285_0()
{    return get(COUNTER_EVENT_DRAIN_SUCCESS);}
public long flume_f2286_0()
{    return increment(COUNTER_EVENT_DRAIN_SUCCESS);}
public long flume_f2287_0(long delta)
{    return addAndGet(COUNTER_EVENT_DRAIN_SUCCESS, delta);}
public long flume_f2288_0()
{    return increment(COUNTER_EVENT_WRITE_FAIL);}
public long flume_f2289_0()
{    return get(COUNTER_EVENT_WRITE_FAIL);}
public long flume_f2290_0()
{    return increment(COUNTER_CHANNEL_READ_FAIL);}
public long flume_f2291_0()
{    return get(COUNTER_CHANNEL_READ_FAIL);}
public long flume_f2292_0(Throwable t)
{    if (t instanceof ChannelException) {        return incrementChannelReadFail();    }    return incrementEventWriteFail();}
public long flume_f2293_0()
{    return get(COUNTER_EVENTS_RECEIVED);}
public long flume_f2294_0()
{    return increment(COUNTER_EVENTS_RECEIVED);}
public long flume_f2295_0(long delta)
{    return addAndGet(COUNTER_EVENTS_RECEIVED, delta);}
public long flume_f2296_0()
{    return get(COUNTER_EVENTS_ACCEPTED);}
public long flume_f2297_0()
{    return increment(COUNTER_EVENTS_ACCEPTED);}
public long flume_f2298_0(long delta)
{    return addAndGet(COUNTER_EVENTS_ACCEPTED, delta);}
public long flume_f2299_0()
{    return get(COUNTER_APPEND_RECEIVED);}
public long flume_f2300_0()
{    return increment(COUNTER_APPEND_RECEIVED);}
public long flume_f2301_0()
{    return get(COUNTER_APPEND_ACCEPTED);}
public long flume_f2302_0()
{    return increment(COUNTER_APPEND_ACCEPTED);}
public long flume_f2303_0()
{    return get(COUNTER_APPEND_BATCH_RECEIVED);}
public long flume_f2304_0()
{    return increment(COUNTER_APPEND_BATCH_RECEIVED);}
public long flume_f2305_0()
{    return get(COUNTER_APPEND_BATCH_ACCEPTED);}
public long flume_f2306_0()
{    return increment(COUNTER_APPEND_BATCH_ACCEPTED);}
public long flume_f2307_0()
{    return get(COUNTER_OPEN_CONNECTION_COUNT);}
public void flume_f2308_0(long openConnectionCount)
{    set(COUNTER_OPEN_CONNECTION_COUNT, openConnectionCount);}
public long flume_f2309_0()
{    return increment(COUNTER_EVENT_READ_FAIL);}
public long flume_f2310_0()
{    return get(COUNTER_EVENT_READ_FAIL);}
public long flume_f2311_0()
{    return increment(COUNTER_CHANNEL_WRITE_FAIL);}
public long flume_f2312_0()
{    return get(COUNTER_CHANNEL_WRITE_FAIL);}
public long flume_f2313_0()
{    return increment(COUNTER_GENERIC_PROCESSING_FAIL);}
public long flume_f2314_0()
{    return get(COUNTER_GENERIC_PROCESSING_FAIL);}
public long flume_f2315_0(Throwable t)
{    if (t instanceof ChannelException) {        return incrementChannelWriteFail();    }    return incrementEventReadFail();}
public static Map<String, Map<String, String>> flume_f2316_1()
{    Map<String, Map<String, String>> mbeanMap = Maps.newHashMap();    Set<ObjectInstance> queryMBeans = null;    try {        queryMBeans = mbeanServer.queryMBeans(null, null);    } catch (Exception ex) {                Throwables.propagate(ex);    }    for (ObjectInstance obj : queryMBeans) {        try {            if (!obj.getObjectName().toString().startsWith("org.apache.flume")) {                continue;            }            MBeanAttributeInfo[] attrs = mbeanServer.getMBeanInfo(obj.getObjectName()).getAttributes();            String[] strAtts = new String[attrs.length];            for (int i = 0; i < strAtts.length; i++) {                strAtts[i] = attrs[i].getName();            }            AttributeList attrList = mbeanServer.getAttributes(obj.getObjectName(), strAtts);            String component = obj.getObjectName().toString().substring(obj.getObjectName().toString().indexOf('=') + 1);            Map<String, String> attrMap = Maps.newHashMap();            for (Object attr : attrList) {                Attribute localAttr = (Attribute) attr;                if (localAttr.getName().equalsIgnoreCase("type")) {                    component = localAttr.getValue() + "." + component;                }                attrMap.put(localAttr.getName(), localAttr.getValue().toString());            }            mbeanMap.put(component, attrMap);        } catch (Exception e) {                    }    }    return mbeanMap;}
public void flume_f2317_0()
{}
public Event flume_f2318_0(Event event)
{    Map<String, String> headers = event.getHeaders();    if (preserveExisting && headers.containsKey(header)) {        return event;    }    if (host != null) {        headers.put(header, host);    }    return event;}
public List<Event> flume_f2319_0(List<Event> events)
{    for (Event event : events) {        intercept(event);    }    return events;}
public void flume_f2320_0()
{}
public Interceptor flume_f2321_0()
{    return new HostInterceptor(preserveExisting, useIP, header);}
public void flume_f2322_0(Context context)
{    preserveExisting = context.getBoolean(PRESERVE, PRESERVE_DFLT);    useIP = context.getBoolean(USE_IP, USE_IP_DFLT);    header = context.getString(HOST_HEADER, HOST);}
private static Class<? extends Builder> flume_f2323_0(String name)
{    try {        return InterceptorType.valueOf(name.toUpperCase(Locale.ENGLISH)).getBuilderClass();    } catch (IllegalArgumentException e) {        return null;    }}
public static Builder flume_f2324_0(String name) throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Class<? extends Builder> clazz = lookup(name);    if (clazz == null) {        clazz = (Class<? extends Builder>) Class.forName(name);    }    return clazz.newInstance();}
public void flume_f2325_0(List<Interceptor> interceptors)
{    this.interceptors = interceptors;}
public Event flume_f2326_0(Event event)
{    for (Interceptor interceptor : interceptors) {        if (event == null) {            return null;        }        event = interceptor.intercept(event);    }    return event;}
public List<Event> flume_f2327_0(List<Event> events)
{    for (Interceptor interceptor : interceptors) {        if (events.isEmpty()) {            return events;        }        events = interceptor.intercept(events);        Preconditions.checkNotNull(events, "Event list returned null from interceptor %s", interceptor);    }    return events;}
public void flume_f2328_0()
{    Iterator<Interceptor> iter = interceptors.iterator();    while (iter.hasNext()) {        Interceptor interceptor = iter.next();        interceptor.initialize();    }}
public void flume_f2329_0()
{    Iterator<Interceptor> iter = interceptors.iterator();    while (iter.hasNext()) {        Interceptor interceptor = iter.next();        interceptor.close();    }}
public Class<? extends Interceptor.Builder> flume_f2330_0()
{    return builderClass;}
public void flume_f2331_0()
{}
public void flume_f2332_0()
{}
public Event flume_f2333_1(Event event)
{    Matcher matcher = regex.matcher(new String(event.getBody(), Charsets.UTF_8));    Map<String, String> headers = event.getHeaders();    if (matcher.find()) {        for (int group = 0, count = matcher.groupCount(); group < count; group++) {            int groupIndex = group + 1;            if (groupIndex > serializers.size()) {                if (logger.isDebugEnabled()) {                                    }                break;            }            NameAndSerializer serializer = serializers.get(group);            if (logger.isDebugEnabled()) {                            }            headers.put(serializer.headerName, serializer.serializer.serialize(matcher.group(groupIndex)));        }    }    return event;}
public List<Event> flume_f2334_0(List<Event> events)
{    List<Event> intercepted = Lists.newArrayListWithCapacity(events.size());    for (Event event : events) {        Event interceptedEvent = intercept(event);        if (interceptedEvent != null) {            intercepted.add(interceptedEvent);        }    }    return intercepted;}
public void flume_f2335_0(Context context)
{    String regexString = context.getString(REGEX);    Preconditions.checkArgument(!StringUtils.isEmpty(regexString), "Must supply a valid regex string");    regex = Pattern.compile(regexString);    regex.pattern();    regex.matcher("").groupCount();    configureSerializers(context);}
private void flume_f2336_0(Context context)
{    String serializerListStr = context.getString(SERIALIZERS);    Preconditions.checkArgument(!StringUtils.isEmpty(serializerListStr), "Must supply at least one name and serializer");    String[] serializerNames = serializerListStr.split("\\s+");    Context serializerContexts = new Context(context.getSubProperties(SERIALIZERS + "."));    serializerList = Lists.newArrayListWithCapacity(serializerNames.length);    for (String serializerName : serializerNames) {        Context serializerContext = new Context(serializerContexts.getSubProperties(serializerName + "."));        String type = serializerContext.getString("type", "DEFAULT");        String name = serializerContext.getString("name");        Preconditions.checkArgument(!StringUtils.isEmpty(name), "Supplied name cannot be empty.");        if ("DEFAULT".equals(type)) {            serializerList.add(new NameAndSerializer(name, defaultSerializer));        } else {            serializerList.add(new NameAndSerializer(name, getCustomSerializer(type, serializerContext)));        }    }}
private RegexExtractorInterceptorSerializer flume_f2337_1(String clazzName, Context context)
{    try {        RegexExtractorInterceptorSerializer serializer = (RegexExtractorInterceptorSerializer) Class.forName(clazzName).newInstance();        serializer.configure(context);        return serializer;    } catch (Exception e) {                Throwables.propagate(e);    }    return defaultSerializer;}
public Interceptor flume_f2338_0()
{    Preconditions.checkArgument(regex != null, "Regex pattern was misconfigured");    Preconditions.checkArgument(serializerList.size() > 0, "Must supply a valid group match id list");    return new RegexExtractorInterceptor(regex, serializerList);}
public void flume_f2339_0(Context context)
{    String pattern = context.getString("pattern");    Preconditions.checkArgument(!StringUtils.isEmpty(pattern), "Must configure with a valid pattern");    formatter = DateTimeFormat.forPattern(pattern);}
public String flume_f2340_0(String value)
{    DateTime dateTime = formatter.parseDateTime(value);    return Long.toString(dateTime.getMillis());}
public void flume_f2341_0(ComponentConfiguration conf)
{}
public String flume_f2342_0(String value)
{    return value;}
public void flume_f2343_0(Context context)
{}
public void flume_f2344_0(ComponentConfiguration conf)
{}
public void flume_f2345_0()
{}
public Event flume_f2346_0(Event event)
{    if (!excludeEvents) {        if (regex.matcher(new String(event.getBody())).find()) {            return event;        } else {            return null;        }    } else {        if (regex.matcher(new String(event.getBody())).find()) {            return null;        } else {            return event;        }    }}
public List<Event> flume_f2347_0(List<Event> events)
{    List<Event> out = Lists.newArrayList();    for (Event event : events) {        Event outEvent = intercept(event);        if (outEvent != null) {            out.add(outEvent);        }    }    return out;}
public void flume_f2348_0()
{}
public void flume_f2349_0(Context context)
{    String regexString = context.getString(REGEX, DEFAULT_REGEX);    regex = Pattern.compile(regexString);    excludeEvents = context.getBoolean(EXCLUDE_EVENTS, DEFAULT_EXCLUDE_EVENTS);}
public Interceptor flume_f2350_1()
{        return new RegexFilteringInterceptor(regex, excludeEvents);}
public void flume_f2351_0()
{}
public void flume_f2352_0()
{}
public List<Event> flume_f2353_0(final List<Event> events)
{    for (final Event event : events) {        intercept(event);    }    return events;}
public Event flume_f2354_1(final Event event)
{    assert event != null : "Missing Flume event while intercepting";    try {        final Map<String, String> headers = event.getHeaders();                if (withName != null && headers.remove(withName) != null) {            LOG.trace("Removed header \"{}\" for event: {}", withName, event);        }                if (fromList != null || matchRegex != null) {            final Iterator<String> headerIterator = headers.keySet().iterator();            List<String> removedHeaders = new LinkedList<>();            while (headerIterator.hasNext()) {                final String currentHeader = headerIterator.next();                if (fromList != null && fromList.contains(currentHeader)) {                    headerIterator.remove();                    removedHeaders.add(currentHeader);                } else if (matchRegex != null) {                    final Matcher matcher = matchRegex.matcher(currentHeader);                    if (matcher.matches()) {                        headerIterator.remove();                        removedHeaders.add(currentHeader);                    }                }            }            if (!removedHeaders.isEmpty() && LogPrivacyUtil.allowLogRawData()) {                LOG.trace("Removed headers \"{}\" for event: {}", removedHeaders, event);            }        }    } catch (final Exception e) {            }    return event;}
public Interceptor flume_f2355_1()
{    if (LOG.isDebugEnabled()) {            }    return new RemoveHeaderInterceptor(withName, fromList, listSeparator, matchRegex);}
public void flume_f2356_0(final Context context)
{    withName = context.getString(WITH_NAME);    fromList = context.getString(FROM_LIST);    listSeparator = context.getString(LIST_SEPARATOR, LIST_SEPARATOR_DEFAULT);    final String matchRegexStr = context.getString(MATCH_REGEX);    if (matchRegexStr != null) {        matchRegex = Pattern.compile(matchRegexStr);    }}
public void flume_f2357_0()
{}
public void flume_f2358_0()
{}
public Event flume_f2359_0(Event event)
{    String origBody = new String(event.getBody(), charset);    Matcher matcher = searchPattern.matcher(origBody);    String newBody = matcher.replaceAll(replaceString);    event.setBody(newBody.getBytes(charset));    return event;}
public List<Event> flume_f2360_0(List<Event> events)
{    for (Event event : events) {        intercept(event);    }    return events;}
public void flume_f2361_0(Context context)
{    String searchPattern = context.getString(SEARCH_PAT_KEY);    Preconditions.checkArgument(!StringUtils.isEmpty(searchPattern), "Must supply a valid search pattern " + SEARCH_PAT_KEY + " (may not be empty)");    replaceString = context.getString(REPLACE_STRING_KEY);        if (replaceString == null) {        replaceString = "";    }    searchRegex = Pattern.compile(searchPattern);    if (context.containsKey(CHARSET_KEY)) {                charset = Charset.forName(context.getString(CHARSET_KEY));    }}
public Interceptor flume_f2362_0()
{    Preconditions.checkNotNull(searchRegex, "Regular expression search pattern required");    Preconditions.checkNotNull(replaceString, "Replacement string required");    return new SearchAndReplaceInterceptor(searchRegex, replaceString, charset);}
public void flume_f2363_0()
{}
public Event flume_f2364_0(Event event)
{    Map<String, String> headers = event.getHeaders();    if (preserveExisting && headers.containsKey(key)) {        return event;    }    headers.put(key, value);    return event;}
public List<Event> flume_f2365_0(List<Event> events)
{    for (Event event : events) {        intercept(event);    }    return events;}
public void flume_f2366_0()
{}
public void flume_f2367_0(Context context)
{    preserveExisting = context.getBoolean(Constants.PRESERVE, Constants.PRESERVE_DEFAULT);    key = context.getString(Constants.KEY, Constants.KEY_DEFAULT);    value = context.getString(Constants.VALUE, Constants.VALUE_DEFAULT);}
public Interceptor flume_f2368_1()
{        return new StaticInterceptor(preserveExisting, key, value);}
public void flume_f2369_0()
{}
public Event flume_f2370_0(Event event)
{    Map<String, String> headers = event.getHeaders();    if (preserveExisting && headers.containsKey(header)) {        } else {        long now = System.currentTimeMillis();        headers.put(header, Long.toString(now));    }    return event;}
public List<Event> flume_f2371_0(List<Event> events)
{    for (Event event : events) {        intercept(event);    }    return events;}
public void flume_f2372_0()
{}
public Interceptor flume_f2373_0()
{    return new TimestampInterceptor(preserveExisting, header);}
public void flume_f2374_0(Context context)
{    preserveExisting = context.getBoolean(CONFIG_PRESERVE, DEFAULT_PRESERVE);    header = context.getString(CONFIG_HEADER_NAME, DEFAULT_HEADER_NAME);}
public static boolean flume_f2375_0(LifecycleAware delegate, LifecycleState state) throws InterruptedException
{    return waitForState(delegate, state, 0);}
public static boolean flume_f2376_0(LifecycleAware delegate, LifecycleState state, long timeout) throws InterruptedException
{    return waitForOneOf(delegate, new LifecycleState[] { state }, timeout);}
public static boolean flume_f2377_0(LifecycleAware delegate, LifecycleState[] states) throws InterruptedException
{    return waitForOneOf(delegate, states, 0);}
public static boolean flume_f2378_1(LifecycleAware delegate, LifecycleState[] states, long timeout) throws InterruptedException
{    if (logger.isDebugEnabled()) {            }    long sleepInterval = Math.max(shortestSleepDuration, timeout / maxNumberOfChecks);    long deadLine = System.currentTimeMillis() + timeout;    do {        for (LifecycleState state : states) {            if (delegate.getLifecycleState().equals(state)) {                return true;            }        }        Thread.sleep(sleepInterval);    } while (timeout == 0 || System.currentTimeMillis() < deadLine);        return false;}
public static void flume_f2379_0(List<LifecycleAware> services) throws InterruptedException
{    for (LifecycleAware service : services) {        waitForOneOf(service, LifecycleState.STOP_OR_ERROR);    }}
public synchronized void flume_f2380_1()
{        monitorService.scheduleWithFixedDelay(purger, 2, 2, TimeUnit.HOURS);    lifecycleState = LifecycleState.START;    }
public synchronized void flume_f2381_1()
{        if (monitorService != null) {        monitorService.shutdown();        try {            monitorService.awaitTermination(10, TimeUnit.SECONDS);        } catch (InterruptedException e) {                    }        if (!monitorService.isTerminated()) {            monitorService.shutdownNow();            try {                while (!monitorService.isTerminated()) {                    monitorService.awaitTermination(10, TimeUnit.SECONDS);                }            } catch (InterruptedException e) {                            }        }    }    for (final Entry<LifecycleAware, Supervisoree> entry : supervisedProcesses.entrySet()) {        if (entry.getKey().getLifecycleState().equals(LifecycleState.START)) {            entry.getValue().status.desiredState = LifecycleState.STOP;            entry.getKey().stop();        }    }    /* If we've failed, preserve the error state. */    if (lifecycleState.equals(LifecycleState.START)) {        lifecycleState = LifecycleState.STOP;    }    supervisedProcesses.clear();    monitorFutures.clear();    }
public synchronized void flume_f2382_0()
{    lifecycleState = LifecycleState.ERROR;}
public synchronized void flume_f2383_1(LifecycleAware lifecycleAware, SupervisorPolicy policy, LifecycleState desiredState)
{    if (this.monitorService.isShutdown() || this.monitorService.isTerminated() || this.monitorService.isTerminating()) {        throw new FlumeException("Supervise called on " + lifecycleAware + " " + "after shutdown has been initiated. " + lifecycleAware + " will not" + " be started");    }    Preconditions.checkState(!supervisedProcesses.containsKey(lifecycleAware), "Refusing to supervise " + lifecycleAware + " more than once");    if (logger.isDebugEnabled()) {            }    Supervisoree process = new Supervisoree();    process.status = new Status();    process.policy = policy;    process.status.desiredState = desiredState;    process.status.error = false;    MonitorRunnable monitorRunnable = new MonitorRunnable();    monitorRunnable.lifecycleAware = lifecycleAware;    monitorRunnable.supervisoree = process;    monitorRunnable.monitorService = monitorService;    supervisedProcesses.put(lifecycleAware, process);    ScheduledFuture<?> future = monitorService.scheduleWithFixedDelay(monitorRunnable, 0, 3, TimeUnit.SECONDS);    monitorFutures.put(lifecycleAware, future);}
public synchronized void flume_f2384_1(LifecycleAware lifecycleAware)
{    Preconditions.checkState(supervisedProcesses.containsKey(lifecycleAware), "Unaware of " + lifecycleAware + " - can not unsupervise");        synchronized (lifecycleAware) {        Supervisoree supervisoree = supervisedProcesses.get(lifecycleAware);        supervisoree.status.discard = true;        this.setDesiredState(lifecycleAware, LifecycleState.STOP);                lifecycleAware.stop();    }    supervisedProcesses.remove(lifecycleAware);            monitorFutures.get(lifecycleAware).cancel(false);        needToPurge = true;    monitorFutures.remove(lifecycleAware);}
public synchronized void flume_f2385_1(LifecycleAware lifecycleAware, LifecycleState desiredState)
{    Preconditions.checkState(supervisedProcesses.containsKey(lifecycleAware), "Unaware of " + lifecycleAware + " - can not set desired state to " + desiredState);        Supervisoree supervisoree = supervisedProcesses.get(lifecycleAware);    supervisoree.status.desiredState = desiredState;}
public synchronized LifecycleState flume_f2386_0()
{    return lifecycleState;}
public synchronized boolean flume_f2387_0(LifecycleAware component)
{    return supervisedProcesses.get(component).status.error;}
public void flume_f2388_1()
{        long now = System.currentTimeMillis();    try {        if (supervisoree.status.firstSeen == null) {                        supervisoree.status.firstSeen = now;        }        supervisoree.status.lastSeen = now;        synchronized (lifecycleAware) {            if (supervisoree.status.discard) {                                                return;            } else if (supervisoree.status.error) {                                return;            }            supervisoree.status.lastSeenState = lifecycleAware.getLifecycleState();            if (!lifecycleAware.getLifecycleState().equals(supervisoree.status.desiredState)) {                                switch(supervisoree.status.desiredState) {                    case START:                        try {                            lifecycleAware.start();                        } catch (Throwable e) {                                                        if (e instanceof Error) {                                                                supervisoree.status.desiredState = LifecycleState.STOP;                                try {                                    lifecycleAware.stop();                                                                    } catch (Throwable e1) {                                                                        supervisoree.status.error = true;                                    if (e1 instanceof Error) {                                        throw (Error) e1;                                    }                                                                                                }                            }                            supervisoree.status.failures++;                        }                        break;                    case STOP:                        try {                            lifecycleAware.stop();                        } catch (Throwable e) {                                                        if (e instanceof Error) {                                throw (Error) e;                            }                            supervisoree.status.failures++;                        }                        break;                    default:                                        }                if (!supervisoree.policy.isValid(lifecycleAware, supervisoree.status)) {                                    }            }        }    } catch (Throwable t) {            }    }
public void flume_f2389_0()
{    if (needToPurge) {        monitorService.purge();        needToPurge = false;    }}
public String flume_f2390_0()
{    return "{ lastSeen:" + lastSeen + " lastSeenState:" + lastSeenState + " desiredState:" + desiredState + " firstSeen:" + firstSeen + " failures:" + failures + " discard:" + discard + " error:" + error + " }";}
 boolean flume_f2391_0(LifecycleAware object, Status status)
{    return true;}
 boolean flume_f2392_0(LifecycleAware object, Status status)
{    return status.failures == 0;}
public String flume_f2393_0()
{    return "{ status:" + status + " policy:" + policy + " }";}
public void flume_f2394_1(Context context)
{    int syncIntervalBytes = context.getInteger(SYNC_INTERVAL_BYTES, DEFAULT_SYNC_INTERVAL_BYTES);    String compressionCodec = context.getString(COMPRESSION_CODEC, DEFAULT_COMPRESSION_CODEC);    writer = new ReflectDatumWriter<T>(getSchema());    dataFileWriter = new DataFileWriter<T>(writer);    dataFileWriter.setSyncInterval(syncIntervalBytes);    try {        CodecFactory codecFactory = CodecFactory.fromString(compressionCodec);        dataFileWriter.setCodec(codecFactory);    } catch (AvroRuntimeException e) {            }}
public void flume_f2395_0() throws IOException
{        dataFileWriter.create(getSchema(), getOutputStream());}
public void flume_f2396_0() throws IOException
{        throw new UnsupportedOperationException("Avro API doesn't support append");}
public void flume_f2397_0(Event event) throws IOException
{    T destType = convert(event);    dataFileWriter.append(destType);}
public void flume_f2398_0() throws IOException
{    dataFileWriter.flush();}
public void flume_f2399_0() throws IOException
{}
public boolean flume_f2400_0()
{    return false;}
private void flume_f2401_0() throws IOException, NoSuchAlgorithmException
{    SeekableResettableInputBridge in = new SeekableResettableInputBridge(ris);    long pos = in.tell();    in.seek(0L);    fileReader = new DataFileReader<GenericRecord>(in, new GenericDatumReader<GenericRecord>());    fileReader.sync(pos);    schema = fileReader.getSchema();    datumWriter = new GenericDatumWriter(schema);    out = new ByteArrayOutputStream();    encoder = EncoderFactory.get().binaryEncoder(out, encoder);    schemaHash = SchemaNormalization.parsingFingerprint("CRC-64-AVRO", schema);    schemaHashString = Hex.encodeHexString(schemaHash);}
public Event flume_f2402_0() throws IOException
{    if (fileReader.hasNext()) {        record = fileReader.next(record);        out.reset();        datumWriter.write(record, encoder);        encoder.flush();                Event event = EventBuilder.withBody(out.toByteArray());        if (schemaType == AvroSchemaType.HASH) {            event.getHeaders().put(AVRO_SCHEMA_HEADER_HASH, schemaHashString);        } else {            event.getHeaders().put(AVRO_SCHEMA_HEADER_LITERAL, schema.toString());        }        return event;    }    return null;}
public List<Event> flume_f2403_0(int numEvents) throws IOException
{    List<Event> events = Lists.newArrayList();    for (int i = 0; i < numEvents && fileReader.hasNext(); i++) {        Event event = readEvent();        if (event != null) {            events.add(event);        }    }    return events;}
public void flume_f2404_0() throws IOException
{    long pos = fileReader.previousSync() - DataFileConstants.SYNC_SIZE;    if (pos < 0)        pos = 0;    ((RemoteMarkable) ris).markPosition(pos);}
public void flume_f2405_0() throws IOException
{    long pos = ((RemoteMarkable) ris).getMarkPosition();    fileReader.sync(pos);}
public void flume_f2406_0() throws IOException
{    ris.close();}
public EventDeserializer flume_f2407_0(Context context, ResettableInputStream in)
{    if (!(in instanceof RemoteMarkable)) {        throw new IllegalArgumentException("Cannot use this deserializer " + "without a RemoteMarkable input stream");    }    AvroEventDeserializer deserializer = new AvroEventDeserializer(context, in);    try {        deserializer.initialize();    } catch (Exception e) {        throw new FlumeException("Cannot instantiate deserializer", e);    }    return deserializer;}
public void flume_f2408_0(long p) throws IOException
{    ris.seek(p);}
public long flume_f2409_0() throws IOException
{    return ris.tell();}
public long flume_f2410_0() throws IOException
{    if (ris instanceof LengthMeasurable) {        return ((LengthMeasurable) ris).length();    } else {                return Long.MAX_VALUE;    }}
public int flume_f2411_0(byte[] b, int off, int len) throws IOException
{    return ris.read(b, off, len);}
public void flume_f2412_0() throws IOException
{    ris.close();}
public boolean flume_f2413_0()
{    return true;}
public void flume_f2414_0()
{}
public void flume_f2415_0()
{}
public void flume_f2416_0()
{}
public void flume_f2417_0(Event e) throws IOException
{    out.write(e.getBody());    if (appendNewline) {        out.write('\n');    }}
public void flume_f2418_0() throws IOException
{}
public EventSerializer flume_f2419_0(Context context, OutputStream out)
{    BodyTextEventSerializer s = new BodyTextEventSerializer(out, context);    return s;}
public static DurablePositionTracker flume_f2420_0(File trackerFile, String target) throws IOException
{    if (!trackerFile.exists()) {        return new DurablePositionTracker(trackerFile, target);    }        DurablePositionTracker oldTracker = new DurablePositionTracker(trackerFile, target);    String existingTarget = oldTracker.getTarget();    long targetPosition = oldTracker.getPosition();    oldTracker.close();    File tmpMeta = File.createTempFile(trackerFile.getName(), ".tmp", trackerFile.getParentFile());    tmpMeta.delete();    DurablePositionTracker tmpTracker = new DurablePositionTracker(tmpMeta, existingTarget);    tmpTracker.storePosition(targetPosition);    tmpTracker.close();        if (PlatformDetect.isWindows()) {        if (!trackerFile.delete()) {            throw new IOException("Unable to delete existing meta file " + trackerFile);        }    }        if (!tmpMeta.renameTo(trackerFile)) {        throw new IOException("Unable to rename " + tmpMeta + " to " + trackerFile);    }        DurablePositionTracker newTracker = new DurablePositionTracker(trackerFile, existingTarget);    return newTracker;}
private void flume_f2421_0() throws IOException
{    long syncPos = trackerFile.length() - 256L;    if (syncPos < 0)        syncPos = 0L;    reader.sync(syncPos);    while (reader.hasNext()) {        reader.next(metaCache);    }}
public synchronized void flume_f2422_0(long position) throws IOException
{    metaCache.setOffset(position);    writer.append(metaCache);    writer.sync();    writer.flush();}
public synchronized long flume_f2423_0()
{    return metaCache.getOffset();}
public String flume_f2424_0()
{    return target;}
public void flume_f2425_0() throws IOException
{    if (isOpen) {        writer.close();        reader.close();        isOpen = false;    }}
public static EventDeserializer flume_f2426_1(String deserializerType, Context context, ResettableInputStream in)
{    Preconditions.checkNotNull(deserializerType, "serializer type must not be null");        EventDeserializerType type;    try {        type = EventDeserializerType.valueOf(deserializerType.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {                type = EventDeserializerType.OTHER;    }    Class<? extends EventDeserializer.Builder> builderClass = type.getBuilderClass();        if (builderClass == null) {        try {            Class c = Class.forName(deserializerType);            if (c != null && EventDeserializer.Builder.class.isAssignableFrom(c)) {                builderClass = (Class<? extends EventDeserializer.Builder>) c;            } else {                String errMessage = "Unable to instantiate Builder from " + deserializerType + ": does not appear to implement " + EventDeserializer.Builder.class.getName();                throw new FlumeException(errMessage);            }        } catch (ClassNotFoundException ex) {                        throw new FlumeException(ex);        }    }        EventDeserializer.Builder builder;    try {        builder = builderClass.newInstance();    } catch (InstantiationException ex) {        String errMessage = "Cannot instantiate builder: " + deserializerType;                throw new FlumeException(errMessage, ex);    } catch (IllegalAccessException ex) {        String errMessage = "Cannot instantiate builder: " + deserializerType;                throw new FlumeException(errMessage, ex);    }    return builder.build(context, in);}
public Class<? extends EventDeserializer.Builder> flume_f2427_0()
{    return builderClass;}
public static EventSerializer flume_f2428_1(String serializerType, Context context, OutputStream out)
{    Preconditions.checkNotNull(serializerType, "serializer type must not be null");        EventSerializerType type;    try {        type = EventSerializerType.valueOf(serializerType.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {                type = EventSerializerType.OTHER;    }    Class<? extends EventSerializer.Builder> builderClass = type.getBuilderClass();        if (builderClass == null) {        try {            Class c = Class.forName(serializerType);            if (c != null && EventSerializer.Builder.class.isAssignableFrom(c)) {                builderClass = (Class<? extends EventSerializer.Builder>) c;            } else {                String errMessage = "Unable to instantiate Builder from " + serializerType + ": does not appear to implement " + EventSerializer.Builder.class.getName();                throw new FlumeException(errMessage);            }        } catch (ClassNotFoundException ex) {                        throw new FlumeException(ex);        }    }        EventSerializer.Builder builder;    try {        builder = builderClass.newInstance();    } catch (InstantiationException ex) {        String errMessage = "Cannot instantiate builder: " + serializerType;                throw new FlumeException(errMessage, ex);    } catch (IllegalAccessException ex) {        String errMessage = "Cannot instantiate builder: " + serializerType;                throw new FlumeException(errMessage, ex);    }    return builder.build(context, out);}
public Class<? extends EventSerializer.Builder> flume_f2429_0()
{    return builderClass;}
protected Schema flume_f2430_0()
{    return SCHEMA;}
protected OutputStream flume_f2431_0()
{    return out;}
protected Event flume_f2432_0(Event event)
{    return event;}
public EventSerializer flume_f2433_0(Context context, OutputStream out)
{    FlumeEventAvroEventSerializer writer = new FlumeEventAvroEventSerializer(out);    writer.configure(context);    return writer;}
public boolean flume_f2434_0()
{    return true;}
public void flume_f2435_0()
{}
public void flume_f2436_0()
{}
public void flume_f2437_0()
{}
public void flume_f2438_0(Event e) throws IOException
{    out.write((e.getHeaders() + " ").getBytes());    out.write(e.getBody());    if (appendNewline) {        out.write('\n');    }}
public void flume_f2439_0() throws IOException
{}
public EventSerializer flume_f2440_0(Context context, OutputStream out)
{    HeaderAndBodyTextEventSerializer s = new HeaderAndBodyTextEventSerializer(out, context);    return s;}
public Event flume_f2441_0() throws IOException
{    ensureOpen();    String line = readLine();    if (line == null) {        return null;    } else {        return EventBuilder.withBody(line, outputCharset);    }}
public List<Event> flume_f2442_0(int numEvents) throws IOException
{    ensureOpen();    List<Event> events = Lists.newLinkedList();    for (int i = 0; i < numEvents; i++) {        Event event = readEvent();        if (event != null) {            events.add(event);        } else {            break;        }    }    return events;}
public void flume_f2443_0() throws IOException
{    ensureOpen();    in.mark();}
public void flume_f2444_0() throws IOException
{    ensureOpen();    in.reset();}
public void flume_f2445_0() throws IOException
{    if (isOpen) {        reset();        in.close();        isOpen = false;    }}
private void flume_f2446_0()
{    if (!isOpen) {        throw new IllegalStateException("Serializer has been closed");    }}
private String flume_f2447_1() throws IOException
{    StringBuilder sb = new StringBuilder();    int c;    int readChars = 0;    while ((c = in.readChar()) != -1) {        readChars++;                if (c == '\n') {            break;        }        sb.append((char) c);        if (readChars >= maxLineLength) {                        break;        }    }    if (readChars > 0) {        return sb.toString();    } else {        return null;    }}
public EventDeserializer flume_f2448_0(Context context, ResettableInputStream in)
{    return new LineDeserializer(context, in);}
public synchronized int flume_f2449_0() throws IOException
{    int len = read(byteBuf, 0, 1);    if (len == -1) {        return -1;        } else if (len == 0) {        return -1;    } else {        return byteBuf[0] & 0xFF;    }}
public synchronized int flume_f2450_0(byte[] b, int off, int len) throws IOException
{    logger.trace("read(buf, {}, {})", off, len);    if (position >= fileSize) {        return -1;    }    if (!buf.hasRemaining()) {        refillBuf();    }    int rem = buf.remaining();    if (len > rem) {        len = rem;    }    buf.get(b, off, len);    incrPosition(len, true);    return len;}
public synchronized int flume_f2451_1() throws IOException
{        if (hasLowSurrogate) {        hasLowSurrogate = false;        return lowSurrogate;    }        if (buf.remaining() < maxCharWidth) {        buf.clear();        buf.flip();        refillBuf();    }    int start = buf.position();    charBuf.clear();    charBuf.limit(1);    boolean isEndOfInput = false;    if (position >= fileSize) {        isEndOfInput = true;    }    CoderResult res = decoder.decode(buf, charBuf, isEndOfInput);    if (res.isMalformed() || res.isUnmappable()) {        res.throwException();    }    int delta = buf.position() - start;    charBuf.flip();        if (charBuf.hasRemaining()) {        char c = charBuf.get();        incrPosition(delta, true);        return c;    }        if (buf.hasRemaining()) {        charBuf.clear();                charBuf.limit(2);                res = decoder.decode(buf, charBuf, isEndOfInput);        if (res.isMalformed() || res.isUnmappable()) {            res.throwException();        }        charBuf.flip();                if (charBuf.remaining() == 2) {            char highSurrogate = charBuf.get();                        lowSurrogate = charBuf.get();                        if (!Character.isHighSurrogate(highSurrogate) || !Character.isLowSurrogate(lowSurrogate)) {                                            }            hasLowSurrogate = true;                        delta = buf.position() - start;            incrPosition(delta, true);                        return highSurrogate;        }    }        incrPosition(delta, false);    return -1;}
private void flume_f2452_0() throws IOException
{    buf.compact();        chan.position(position);    chan.read(buf);    buf.flip();}
public void flume_f2453_0() throws IOException
{    tracker.storePosition(tell());}
public void flume_f2454_0(long position) throws IOException
{    tracker.storePosition(position);}
public long flume_f2455_0() throws IOException
{    return tracker.getPosition();}
public void flume_f2456_0() throws IOException
{    seek(tracker.getPosition());}
public long flume_f2457_0() throws IOException
{    return file.length();}
public long flume_f2458_0() throws IOException
{    logger.trace("Tell position: {}", syncPosition);    return syncPosition;}
public synchronized void flume_f2459_0(long newPos) throws IOException
{    logger.trace("Seek to position: {}", newPos);        long relativeChange = newPos - position;        if (relativeChange == 0)        return;    long newBufPos = buf.position() + relativeChange;    if (newBufPos >= 0 && newBufPos < buf.limit()) {                buf.position((int) newBufPos);    } else {                buf.clear();        buf.flip();    }        decoder.reset();        chan.position(newPos);        position = syncPosition = newPos;}
private void flume_f2460_0(int incr, boolean updateSyncPosition)
{    position += incr;    if (updateSyncPosition) {        syncPosition = position;    }}
public void flume_f2461_0() throws IOException
{    tracker.close();    in.close();}
public void flume_f2462_1(Context context)
{    clientProps = new Properties();    hostname = context.getString("hostname");    port = context.getInteger("port");    Preconditions.checkState(hostname != null, "No hostname specified");    Preconditions.checkState(port != null, "No port specified");    clientProps.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "h1");    clientProps.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "h1", hostname + ":" + port);    for (Entry<String, String> entry : context.getParameters().entrySet()) {        clientProps.setProperty(entry.getKey(), entry.getValue());    }    batchSize = AbstractRpcClient.parseBatchSize(clientProps);    if (sinkCounter == null) {        sinkCounter = new SinkCounter(getName());    }    cxnResetInterval = context.getInteger("reset-connection-interval", DEFAULT_CXN_RESET_INTERVAL);    if (cxnResetInterval == DEFAULT_CXN_RESET_INTERVAL) {            }}
private void flume_f2463_1() throws FlumeException
{    if (client == null) {                try {            resetConnectionFlag = new AtomicBoolean(false);            client = initializeRpcClient(clientProps);            Preconditions.checkNotNull(client, "Rpc Client could not be " + "initialized. " + getName() + " could not be started");            sinkCounter.incrementConnectionCreatedCount();            if (cxnResetInterval > 0) {                cxnResetExecutor.schedule(new Runnable() {                    @Override                    public void run() {                        resetConnectionFlag.set(true);                    }                }, cxnResetInterval, TimeUnit.SECONDS);            }        } catch (Exception ex) {            sinkCounter.incrementConnectionFailedCount();            if (ex instanceof FlumeException) {                throw (FlumeException) ex;            } else {                throw new FlumeException(ex);            }        }            }}
public void flume_f2464_0()
{    resetConnectionFlag.set(true);}
private void flume_f2465_1()
{    try {        destroyConnection();        createConnection();    } catch (Throwable throwable) {                    }}
private void flume_f2466_1()
{    if (client != null) {                try {            client.close();            sinkCounter.incrementConnectionClosedCount();        } catch (FlumeException e) {            sinkCounter.incrementConnectionFailedCount();                    }    }    client = null;}
private void flume_f2467_0() throws FlumeException
{    if (client == null) {        createConnection();    } else if (!client.isActive()) {        destroyConnection();        createConnection();    }}
public void flume_f2468_1()
{        sinkCounter.start();    try {        createConnection();    } catch (FlumeException e) {                /* Try to prevent leaking resources. */        destroyConnection();    }    super.start();    }
public void flume_f2469_1()
{        destroyConnection();    cxnResetExecutor.shutdown();    try {        if (cxnResetExecutor.awaitTermination(5, TimeUnit.SECONDS)) {            cxnResetExecutor.shutdownNow();        }    } catch (Exception ex) {            }    sinkCounter.stop();    super.stop();    }
public String flume_f2470_0()
{    return "RpcSink " + getName() + " { host: " + hostname + ", port: " + port + " }";}
public Status flume_f2471_1() throws EventDeliveryException
{    Status status = Status.READY;    Channel channel = getChannel();    Transaction transaction = channel.getTransaction();    if (resetConnectionFlag.get()) {        resetConnection();                                resetConnectionFlag.set(false);    }    try {        transaction.begin();        verifyConnection();        List<Event> batch = Lists.newLinkedList();        for (int i = 0; i < client.getBatchSize(); i++) {            Event event = channel.take();            if (event == null) {                break;            }            batch.add(event);        }        int size = batch.size();        int batchSize = client.getBatchSize();        if (size == 0) {            sinkCounter.incrementBatchEmptyCount();            status = Status.BACKOFF;        } else {            if (size < batchSize) {                sinkCounter.incrementBatchUnderflowCount();            } else {                sinkCounter.incrementBatchCompleteCount();            }            sinkCounter.addToEventDrainAttemptCount(size);            client.appendBatch(batch);        }        transaction.commit();        sinkCounter.addToEventDrainSuccessCount(size);    } catch (Throwable t) {        transaction.rollback();        if (t instanceof Error) {            throw (Error) t;        } else if (t instanceof ChannelException) {                        sinkCounter.incrementChannelReadFail();            status = Status.BACKOFF;        } else {            sinkCounter.incrementEventWriteFail();            destroyConnection();            throw new EventDeliveryException("Failed to send events", t);        }    } finally {        transaction.close();    }    return status;}
 RpcClient flume_f2472_0()
{    return client;}
public long flume_f2473_0()
{    return batchSize;}
public synchronized void flume_f2474_0()
{    Preconditions.checkState(channel != null, "No channel configured");    lifecycleState = LifecycleState.START;}
public synchronized void flume_f2475_0()
{    lifecycleState = LifecycleState.STOP;}
public synchronized Channel flume_f2476_0()
{    return channel;}
public synchronized void flume_f2477_0(Channel channel)
{    this.channel = channel;}
public synchronized LifecycleState flume_f2478_0()
{    return lifecycleState;}
public synchronized void flume_f2479_0(String name)
{    this.name = name;}
public synchronized String flume_f2480_0()
{    return name;}
public String flume_f2481_0()
{    return this.getClass().getName() + "{name:" + name + ", channel:" + channel.getName() + "}";}
public void flume_f2482_0()
{    for (Sink s : sinkList) {        s.start();    }    state = LifecycleState.START;}
public void flume_f2483_0()
{    for (Sink s : sinkList) {        s.stop();    }    state = LifecycleState.STOP;}
public LifecycleState flume_f2484_0()
{    return state;}
public void flume_f2485_0(List<Sink> sinks)
{    List<Sink> list = new ArrayList<Sink>();    list.addAll(sinks);    sinkList = Collections.unmodifiableList(list);}
protected List<Sink> flume_f2486_0()
{    return sinkList;}
public void flume_f2487_0(Context context)
{    Long timeOut = context.getLong("maxTimeOut");    if (timeOut != null) {        maxTimeOut = timeOut;    }}
public void flume_f2488_0()
{    state = LifecycleState.START;}
public void flume_f2489_0()
{    state = LifecycleState.STOP;}
public LifecycleState flume_f2490_0()
{    return state;}
public void flume_f2491_0(List<Sink> sinks)
{    sinkList = new ArrayList<Sink>();    sinkList.addAll(sinks);}
protected List<Sink> flume_f2492_0()
{    return sinkList;}
public void flume_f2493_0(Sink failedSink)
{}
protected RpcClient flume_f2494_1(Properties props)
{        return RpcClientFactory.getInstance(props);}
public Sink flume_f2495_1(String name, String type) throws FlumeException
{    Preconditions.checkNotNull(name, "name");    Preconditions.checkNotNull(type, "type");        Class<? extends Sink> sinkClass = getClass(type);    try {        Sink sink = sinkClass.newInstance();        sink.setName(name);        return sink;    } catch (Exception ex) {        throw new FlumeException("Unable to create sink: " + name + ", type: " + type + ", class: " + sinkClass.getName(), ex);    }}
public Class<? extends Sink> flume_f2496_1(String type) throws FlumeException
{    String sinkClassName = type;    SinkType sinkType = SinkType.OTHER;    try {        sinkType = SinkType.valueOf(type.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException ex) {            }    if (!sinkType.equals(SinkType.OTHER)) {        sinkClassName = sinkType.getSinkClassName();    }    try {        return (Class<? extends Sink>) Class.forName(sinkClassName);    } catch (Exception ex) {        throw new FlumeException("Unable to load sink type: " + type + ", class: " + sinkClassName, ex);    }}
public void flume_f2497_0()
{    Preconditions.checkNotNull(sink, "DefaultSinkProcessor sink not set");    sink.start();    lifecycleState = LifecycleState.START;}
public void flume_f2498_0()
{    Preconditions.checkNotNull(sink, "DefaultSinkProcessor sink not set");    sink.stop();    lifecycleState = LifecycleState.STOP;}
public LifecycleState flume_f2499_0()
{    return lifecycleState;}
public void flume_f2500_0(Context context)
{}
public Status flume_f2501_0() throws EventDeliveryException
{    return sink.process();}
public void flume_f2502_0(List<Sink> sinks)
{    Preconditions.checkNotNull(sinks);    Preconditions.checkArgument(sinks.size() == 1, "DefaultSinkPolicy can " + "only handle one sink, " + "try using a policy that supports multiple sinks");    sink = sinks.get(0);}
public void flume_f2503_0(ComponentConfiguration conf)
{}
public int flume_f2504_0(FailedSink arg0)
{    return refresh.compareTo(arg0.refresh);}
public Long flume_f2505_0()
{    return refresh;}
public Sink flume_f2506_0()
{    return sink;}
public Integer flume_f2507_0()
{    return priority;}
public void flume_f2508_1()
{    sequentialFailures++;    adjustRefresh();    }
private void flume_f2509_0()
{    refresh = System.currentTimeMillis() + Math.min(maxPenalty, (1 << sequentialFailures) * FAILURE_PENALTY);}
public void flume_f2510_1(Context context)
{    liveSinks = new TreeMap<Integer, Sink>();    failedSinks = new PriorityQueue<FailedSink>();    Integer nextPrio = 0;    String maxPenaltyStr = context.getString(MAX_PENALTY_PREFIX);    if (maxPenaltyStr == null) {        maxPenalty = DEFAULT_MAX_PENALTY;    } else {        try {            maxPenalty = Integer.parseInt(maxPenaltyStr);        } catch (NumberFormatException e) {                        maxPenalty = DEFAULT_MAX_PENALTY;        }    }    for (Entry<String, Sink> entry : sinks.entrySet()) {        String priStr = PRIORITY_PREFIX + entry.getKey();        Integer priority;        try {            priority = Integer.parseInt(context.getString(priStr));        } catch (Exception e) {            priority = --nextPrio;        }        if (!liveSinks.containsKey(priority)) {            liveSinks.put(priority, sinks.get(entry.getKey()));        } else {                    }    }    activeSink = liveSinks.get(liveSinks.lastKey());}
public Status flume_f2511_1() throws EventDeliveryException
{        Long now = System.currentTimeMillis();    while (!failedSinks.isEmpty() && failedSinks.peek().getRefresh() < now) {        FailedSink cur = failedSinks.poll();        Status s;        try {            s = cur.getSink().process();            if (s == Status.READY) {                liveSinks.put(cur.getPriority(), cur.getSink());                activeSink = liveSinks.get(liveSinks.lastKey());                            } else {                                failedSinks.add(cur);            }            return s;        } catch (Exception e) {            cur.incFails();            failedSinks.add(cur);        }    }    Status ret = null;    while (activeSink != null) {        try {            ret = activeSink.process();            return ret;        } catch (Exception e) {                        activeSink = moveActiveToDeadAndGetNext();        }    }    throw new EventDeliveryException("All sinks failed to process, " + "nothing left to failover to");}
private Sink flume_f2512_0()
{    Integer key = liveSinks.lastKey();    failedSinks.add(new FailedSink(key, activeSink, 1));    liveSinks.remove(key);    if (liveSinks.isEmpty())        return null;    if (liveSinks.lastKey() != null) {        return liveSinks.get(liveSinks.lastKey());    } else {        return null;    }}
public void flume_f2513_0(List<Sink> sinks)
{        super.setSinks(sinks);    this.sinks = new HashMap<String, Sink>();    for (Sink sink : sinks) {        this.sinks.put(sink.getName(), sink);    }}
public void flume_f2514_1(Context context)
{    Preconditions.checkState(getSinks().size() > 1, "The LoadBalancingSinkProcessor cannot be used for a single sink. " + "Please configure more than one sinks and try again.");    String selectorTypeName = context.getString(CONFIG_SELECTOR, SELECTOR_NAME_ROUND_ROBIN);    Boolean shouldBackOff = context.getBoolean(CONFIG_BACKOFF, false);    selector = null;    if (selectorTypeName.equalsIgnoreCase(SELECTOR_NAME_ROUND_ROBIN)) {        selector = new RoundRobinSinkSelector(shouldBackOff);    } else if (selectorTypeName.equalsIgnoreCase(SELECTOR_NAME_RANDOM)) {        selector = new RandomOrderSinkSelector(shouldBackOff);    } else {        try {            @SuppressWarnings("unchecked")            Class<? extends SinkSelector> klass = (Class<? extends SinkSelector>) Class.forName(selectorTypeName);            selector = klass.newInstance();        } catch (Exception ex) {            throw new FlumeException("Unable to instantiate sink selector: " + selectorTypeName, ex);        }    }    selector.setSinks(getSinks());    selector.configure(new Context(context.getSubProperties(CONFIG_SELECTOR_PREFIX)));    }
public void flume_f2515_0()
{    super.start();    selector.start();}
public void flume_f2516_0()
{    super.stop();    selector.stop();}
public Status flume_f2517_1() throws EventDeliveryException
{    Status status = null;    Iterator<Sink> sinkIterator = selector.createSinkIterator();    while (sinkIterator.hasNext()) {        Sink sink = sinkIterator.next();        try {            status = sink.process();            break;        } catch (Exception ex) {            selector.informSinkFailed(sink);                    }    }    if (status == null) {        throw new EventDeliveryException("All configured sinks have failed");    }    return status;}
public void flume_f2518_0(Context context)
{    super.configure(context);    if (maxTimeOut != 0) {        selector.setMaxTimeOut(maxTimeOut);    }}
public Iterator<Sink> flume_f2519_0()
{    return selector.createIterator();}
public void flume_f2520_0(List<Sink> sinks)
{    selector.setObjects(sinks);}
public void flume_f2521_0(Sink failedSink)
{    selector.informFailure(failedSink);}
public void flume_f2522_0(Context context)
{    super.configure(context);    if (maxTimeOut != 0) {        selector.setMaxTimeOut(maxTimeOut);    }}
public void flume_f2523_0(List<Sink> sinks)
{    selector.setObjects(sinks);}
public Iterator<Sink> flume_f2524_0()
{    return selector.createIterator();}
public void flume_f2525_0(Sink failedSink)
{    selector.informFailure(failedSink);}
public void flume_f2526_1(Context context)
{    String strMaxBytes = context.getString(MAX_BYTES_DUMP_KEY);    if (!Strings.isNullOrEmpty(strMaxBytes)) {        try {            maxBytesToLog = Integer.parseInt(strMaxBytes);        } catch (NumberFormatException e) {                        maxBytesToLog = DEFAULT_MAX_BYTE_DUMP;        }    }}
public Status flume_f2527_1() throws EventDeliveryException
{    Status result = Status.READY;    Channel channel = getChannel();    Transaction transaction = channel.getTransaction();    Event event = null;    try {        transaction.begin();        event = channel.take();        if (event != null) {            if (logger.isInfoEnabled()) {                            }        } else {                        result = Status.BACKOFF;        }        transaction.commit();    } catch (Exception ex) {        transaction.rollback();        throw new EventDeliveryException("Failed to log event: " + event, ex);    } finally {        transaction.close();    }    return result;}
public void flume_f2528_1(Context context)
{    batchSize = context.getInteger("batchSize", DFLT_BATCH_SIZE);        Preconditions.checkArgument(batchSize > 0, "Batch size must be > 0");    logEveryNEvents = context.getInteger("logEveryNEvents", DFLT_LOG_EVERY_N_EVENTS);        Preconditions.checkArgument(logEveryNEvents > 0, "logEveryNEvents must be > 0");}
public Status flume_f2529_1() throws EventDeliveryException
{    Status status = Status.READY;    Channel channel = getChannel();    Transaction transaction = channel.getTransaction();    Event event = null;    long eventCounter = counterGroup.get("events.success");    try {        transaction.begin();        int i = 0;        for (i = 0; i < batchSize; i++) {            event = channel.take();            if (++eventCounter % logEveryNEvents == 0) {                            }            if (event == null) {                status = Status.BACKOFF;                break;            }        }        transaction.commit();        counterGroup.addAndGet("events.success", (long) Math.min(batchSize, i));        counterGroup.incrementAndGet("transaction.success");    } catch (Exception ex) {        transaction.rollback();        counterGroup.incrementAndGet("transaction.failed");                throw new EventDeliveryException("Failed to deliver event: " + event, ex);    } finally {        transaction.close();    }    return status;}
public void flume_f2530_1()
{        counterGroup.setName(this.getName());    super.start();    }
public void flume_f2531_1()
{        super.stop();    }
public String flume_f2532_0()
{    return "NullSink " + getName() + " { batchSize: " + batchSize + " }";}
public long flume_f2533_0()
{    return batchSize;}
public void flume_f2534_0(Context context)
{    String pathManagerType = context.getString("sink.pathManager", "DEFAULT");    String directory = context.getString("sink.directory");    String rollInterval = context.getString("sink.rollInterval");    serializerType = context.getString("sink.serializer", "TEXT");    serializerContext = new Context(context.getSubProperties("sink." + EventSerializer.CTX_PREFIX));    Context pathManagerContext = new Context(context.getSubProperties("sink." + PathManager.CTX_PREFIX));    pathController = PathManagerFactory.getInstance(pathManagerType, pathManagerContext);    Preconditions.checkArgument(directory != null, "Directory may not be null");    Preconditions.checkNotNull(serializerType, "Serializer type is undefined");    if (rollInterval == null) {        this.rollInterval = defaultRollInterval;    } else {        this.rollInterval = Long.parseLong(rollInterval);    }    batchSize = context.getInteger("sink.batchSize", defaultBatchSize);    this.directory = new File(directory);    if (sinkCounter == null) {        sinkCounter = new SinkCounter(getName());    }}
public void flume_f2535_1()
{        sinkCounter.start();    super.start();    pathController.setBaseDirectory(directory);    if (rollInterval > 0) {        rollService = Executors.newScheduledThreadPool(1, new ThreadFactoryBuilder().setNameFormat("rollingFileSink-roller-" + Thread.currentThread().getId() + "-%d").build());        /*       * Every N seconds, mark that it's time to rotate. We purposefully do NOT       * touch anything other than the indicator flag to avoid error handling       * issues (e.g. IO exceptions occuring in two different threads.       * Resist the urge to actually perform rotation in a separate thread!       */        rollService.scheduleAtFixedRate(new Runnable() {            @Override            public void run() {                                shouldRotate = true;            }        }, rollInterval, rollInterval, TimeUnit.SECONDS);    } else {            }    }
public void flume_f2536_1()
{        shouldRotate = true;}
public Status flume_f2537_1() throws EventDeliveryException
{    if (shouldRotate) {                if (outputStream != null) {                        try {                serializer.flush();                serializer.beforeClose();                outputStream.close();                sinkCounter.incrementConnectionClosedCount();                shouldRotate = false;            } catch (IOException e) {                sinkCounter.incrementConnectionFailedCount();                throw new EventDeliveryException("Unable to rotate file " + pathController.getCurrentFile() + " while delivering event", e);            } finally {                serializer = null;                outputStream = null;            }            pathController.rotate();        }    }    if (outputStream == null) {        File currentFile = pathController.getCurrentFile();                try {            outputStream = new BufferedOutputStream(new FileOutputStream(currentFile));            serializer = EventSerializerFactory.getInstance(serializerType, serializerContext, outputStream);            serializer.afterCreate();            sinkCounter.incrementConnectionCreatedCount();        } catch (IOException e) {            sinkCounter.incrementConnectionFailedCount();            throw new EventDeliveryException("Failed to open file " + pathController.getCurrentFile() + " while delivering event", e);        }    }    Channel channel = getChannel();    Transaction transaction = channel.getTransaction();    Event event = null;    Status result = Status.READY;    try {        transaction.begin();        int eventAttemptCounter = 0;        for (int i = 0; i < batchSize; i++) {            event = channel.take();            if (event != null) {                sinkCounter.incrementEventDrainAttemptCount();                eventAttemptCounter++;                serializer.write(event);            /*           * FIXME: Feature: Rotate on size and time by checking bytes written and           * setting shouldRotate = true if we're past a threshold.           */            /*           * FIXME: Feature: Control flush interval based on time or number of           * events. For now, we're super-conservative and flush on each write.           */            } else {                                result = Status.BACKOFF;                break;            }        }        serializer.flush();        outputStream.flush();        transaction.commit();        sinkCounter.addToEventDrainSuccessCount(eventAttemptCounter);    } catch (Exception ex) {        sinkCounter.incrementEventWriteOrChannelFail(ex);        transaction.rollback();        throw new EventDeliveryException("Failed to process transaction", ex);    } finally {        transaction.close();    }    return result;}
public void flume_f2538_1()
{        sinkCounter.stop();    super.stop();    if (outputStream != null) {                try {            serializer.flush();            serializer.beforeClose();            outputStream.close();            sinkCounter.incrementConnectionClosedCount();        } catch (IOException e) {            sinkCounter.incrementConnectionFailedCount();                    } finally {            outputStream = null;            serializer = null;        }    }    if (rollInterval > 0) {        rollService.shutdown();        while (!rollService.isTerminated()) {            try {                rollService.awaitTermination(1, TimeUnit.SECONDS);            } catch (InterruptedException e) {                            }        }    }    }
public File flume_f2539_0()
{    return directory;}
public void flume_f2540_0(File directory)
{    this.directory = directory;}
public long flume_f2541_0()
{    return rollInterval;}
public void flume_f2542_0(long rollInterval)
{    this.rollInterval = rollInterval;}
public long flume_f2543_0()
{    return batchSize;}
public void flume_f2544_0(Context context)
{    conf = new SinkGroupConfiguration("sinkgrp");    try {        conf.configure(context);    } catch (ConfigurationException e) {        throw new FlumeException("Invalid Configuration!", e);    }    configure(conf);}
public SinkProcessor flume_f2545_0()
{    return processor;}
public void flume_f2546_0(ComponentConfiguration conf)
{    this.conf = (SinkGroupConfiguration) conf;    processor = SinkProcessorFactory.getProcessor(this.conf.getProcessorContext(), sinks);}
public static SinkProcessor flume_f2547_1(Context context, List<Sink> sinks)
{    Preconditions.checkNotNull(context);    Preconditions.checkNotNull(sinks);    Preconditions.checkArgument(!sinks.isEmpty());    Map<String, String> params = context.getParameters();    SinkProcessor processor;    String typeStr = params.get(TYPE);    SinkProcessorType type = SinkProcessorType.OTHER;    String processorClassName = typeStr;    try {        type = SinkProcessorType.valueOf(typeStr.toUpperCase(Locale.ENGLISH));    } catch (Exception ex) {            }    if (!type.equals(SinkProcessorType.OTHER)) {        processorClassName = type.getSinkProcessorClassName();    }        Class<? extends SinkProcessor> processorClass = null;    try {        processorClass = (Class<? extends SinkProcessor>) Class.forName(processorClassName);    } catch (Exception ex) {        throw new FlumeException("Unable to load sink processor type: " + typeStr + ", class: " + type.getSinkProcessorClassName(), ex);    }    try {        processor = processorClass.newInstance();    } catch (Exception e) {        throw new FlumeException("Unable to create sink processor, type: " + typeStr + ", class: " + processorClassName, e);    }    processor.setSinks(sinks);    Configurables.configure(processor, context);    return processor;}
public static SinkProcessor flume_f2548_1(ComponentConfiguration conf, List<Sink> sinks)
{    String typeStr = conf.getType();    SinkProcessor processor;    SinkProcessorType type = SinkProcessorType.DEFAULT;    try {        type = SinkProcessorType.valueOf(typeStr.toUpperCase(Locale.ENGLISH));    } catch (Exception ex) {            }    Class<? extends SinkProcessor> processorClass = null;    try {        processorClass = (Class<? extends SinkProcessor>) Class.forName(type.getSinkProcessorClassName());    } catch (Exception ex) {        throw new FlumeException("Unable to load sink processor type: " + typeStr + ", class: " + type.getSinkProcessorClassName(), ex);    }    try {        processor = processorClass.newInstance();    } catch (Exception e) {        throw new FlumeException("Unable to create processor, type: " + typeStr + ", class: " + type.getSinkProcessorClassName(), e);    }    processor.setSinks(sinks);    Configurables.configure(processor, conf);    return processor;}
protected RpcClient flume_f2549_0(Properties props)
{            props.setProperty(RpcClientConfigurationConstants.CONFIG_CONNECTION_POOL_SIZE, String.valueOf(1));    boolean enableKerberos = Boolean.parseBoolean(props.getProperty(RpcClientConfigurationConstants.KERBEROS_KEY, "false"));    if (enableKerberos) {        return SecureRpcClientFactory.getThriftInstance(props);    } else {        props.setProperty(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, RpcClientFactory.ClientType.THRIFT.name());        return RpcClientFactory.getInstance(props);    }}
public SinkProcessor flume_f2550_0()
{    return policy;}
public void flume_f2551_0(SinkProcessor policy)
{    this.policy = policy;}
public void flume_f2552_0()
{    SinkProcessor policy = getPolicy();    policy.start();    runner = new PollingRunner();    runner.policy = policy;    runner.counterGroup = counterGroup;    runner.shouldStop = new AtomicBoolean();    runnerThread = new Thread(runner);    runnerThread.setName("SinkRunner-PollingRunner-" + policy.getClass().getSimpleName());    runnerThread.start();    lifecycleState = LifecycleState.START;}
public void flume_f2553_1()
{    if (runnerThread != null) {        runner.shouldStop.set(true);        runnerThread.interrupt();        while (runnerThread.isAlive()) {            try {                                runnerThread.join(500);            } catch (InterruptedException e) {                            }        }    }    getPolicy().stop();    lifecycleState = LifecycleState.STOP;}
public String flume_f2554_0()
{    return "SinkRunner: { policy:" + getPolicy() + " counterGroup:" + counterGroup + " }";}
public LifecycleState flume_f2555_0()
{    return lifecycleState;}
public void flume_f2556_1()
{        while (!shouldStop.get()) {        try {            if (policy.process().equals(Sink.Status.BACKOFF)) {                counterGroup.incrementAndGet("runner.backoffs");                Thread.sleep(Math.min(counterGroup.incrementAndGet("runner.backoffs.consecutive") * backoffSleepIncrement, maxBackoffSleep));            } else {                counterGroup.set("runner.backoffs.consecutive", 0L);            }        } catch (InterruptedException e) {                        counterGroup.incrementAndGet("runner.interruptions");        } catch (Exception e) {                        if (e instanceof EventDeliveryException) {                counterGroup.incrementAndGet("runner.deliveryErrors");            } else {                counterGroup.incrementAndGet("runner.errors");            }            try {                Thread.sleep(maxBackoffSleep);            } catch (InterruptedException ex) {                Thread.currentThread().interrupt();            }        }    }    }
public Status flume_f2557_0() throws EventDeliveryException
{    Exception exception = getStartException();    if (exception != null) {        throw new FlumeException("Source had error configuring or starting", exception);    }    if (!isStarted()) {        throw new EventDeliveryException("Source is not started.  It is in '" + getLifecycleState() + "' state");    }    return doProcess();}
public synchronized void flume_f2558_0(Context context)
{    super.configure(context);    backoffSleepIncrement = context.getLong(PollableSourceConstants.BACKOFF_SLEEP_INCREMENT, PollableSourceConstants.DEFAULT_BACKOFF_SLEEP_INCREMENT);    maxBackoffSleep = context.getLong(PollableSourceConstants.MAX_BACKOFF_SLEEP, PollableSourceConstants.DEFAULT_MAX_BACKOFF_SLEEP);}
public long flume_f2559_0()
{    return backoffSleepIncrement;}
public long flume_f2560_0()
{    return maxBackoffSleep;}
public synchronized void flume_f2561_0()
{    Preconditions.checkState(channelProcessor != null, "No channel processor configured");    lifecycleState = LifecycleState.START;}
public synchronized void flume_f2562_0()
{    lifecycleState = LifecycleState.STOP;}
public synchronized void flume_f2563_0(ChannelProcessor cp)
{    channelProcessor = cp;}
public synchronized ChannelProcessor flume_f2564_0()
{    return channelProcessor;}
public synchronized LifecycleState flume_f2565_0()
{    return lifecycleState;}
public synchronized void flume_f2566_0(String name)
{    this.name = name;}
public synchronized String flume_f2567_0()
{    return name;}
public String flume_f2568_0()
{    return this.getClass().getName() + "{name:" + name + ",state:" + lifecycleState + "}";}
public void flume_f2569_1(Context context)
{    configureSsl(context);    Configurables.ensureRequiredNonNull(context, PORT_KEY, BIND_KEY);    port = context.getInteger(PORT_KEY);    bindAddress = context.getString(BIND_KEY);    compressionType = context.getString(COMPRESSION_TYPE, "none");    try {        maxThreads = context.getInteger(THREADS, 0);    } catch (NumberFormatException e) {            }    enableIpFilter = context.getBoolean(IP_FILTER_KEY, false);    if (enableIpFilter) {        patternRuleConfigDefinition = context.getString(IP_FILTER_RULES_KEY);        if (patternRuleConfigDefinition == null || patternRuleConfigDefinition.trim().isEmpty()) {            throw new FlumeException("ipFilter is configured with true but ipFilterRules is not defined:" + " ");        }        String[] patternRuleDefinitions = patternRuleConfigDefinition.split(",");        rules = new ArrayList<IpFilterRule>(patternRuleDefinitions.length);        for (String patternRuleDefinition : patternRuleDefinitions) {            rules.add(generateRule(patternRuleDefinition));        }    }    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
public void flume_f2570_1()
{        try {        Responder responder = new SpecificResponder(AvroSourceProtocol.class, this);        socketChannelFactory = initSocketChannelFactory();        ChannelPipelineFactory pipelineFactory = initChannelPipelineFactory();        server = new NettyServer(responder, new InetSocketAddress(bindAddress, port), socketChannelFactory, pipelineFactory, null);    } catch (org.jboss.netty.channel.ChannelException nce) {                stop();        throw new FlumeException("Failed to set up server socket", nce);    }    connectionCountUpdater = Executors.newSingleThreadScheduledExecutor();    server.start();    sourceCounter.start();    super.start();    final NettyServer srv = (NettyServer) server;    connectionCountUpdater.scheduleWithFixedDelay(() -> sourceCounter.setOpenConnectionCount(Long.valueOf(srv.getNumActiveConnections())), 0, 60, TimeUnit.SECONDS);    }
private NioServerSocketChannelFactory flume_f2571_0()
{    NioServerSocketChannelFactory socketChannelFactory;    if (maxThreads <= 0) {        socketChannelFactory = new NioServerSocketChannelFactory(Executors.newCachedThreadPool(new ThreadFactoryBuilder().setNameFormat("Avro " + NettyTransceiver.class.getSimpleName() + " Boss-%d").build()), Executors.newCachedThreadPool(new ThreadFactoryBuilder().setNameFormat("Avro " + NettyTransceiver.class.getSimpleName() + "  I/O Worker-%d").build()));    } else {        socketChannelFactory = new NioServerSocketChannelFactory(Executors.newCachedThreadPool(new ThreadFactoryBuilder().setNameFormat("Avro " + NettyTransceiver.class.getSimpleName() + " Boss-%d").build()), Executors.newFixedThreadPool(maxThreads, new ThreadFactoryBuilder().setNameFormat("Avro " + NettyTransceiver.class.getSimpleName() + "  I/O Worker-%d").build()));    }    return socketChannelFactory;}
private ChannelPipelineFactory flume_f2572_0()
{    ChannelPipelineFactory pipelineFactory;    boolean enableCompression = compressionType.equalsIgnoreCase("deflate");    if (enableCompression || isSslEnabled() || enableIpFilter) {        pipelineFactory = new AdvancedChannelPipelineFactory(enableCompression, enableIpFilter, patternRuleConfigDefinition, getSslEngineSupplier(false));    } else {        pipelineFactory = Channels::pipeline;    }    return pipelineFactory;}
public void flume_f2573_1()
{        if (server != null) {        server.close();        try {            server.join();            server = null;        } catch (InterruptedException e) {                        Thread.currentThread().interrupt();        }    }    if (socketChannelFactory != null) {        socketChannelFactory.releaseExternalResources();        socketChannelFactory = null;    }    sourceCounter.stop();    if (connectionCountUpdater != null) {        connectionCountUpdater.shutdownNow();        connectionCountUpdater = null;    }    super.stop();    }
public String flume_f2574_0()
{    return "Avro source " + getName() + ": { bindAddress: " + bindAddress + ", port: " + port + " }";}
private static Map<String, String> flume_f2575_0(Map<CharSequence, CharSequence> charSeqMap)
{    Map<String, String> stringMap = new HashMap<String, String>();    for (Map.Entry<CharSequence, CharSequence> entry : charSeqMap.entrySet()) {        stringMap.put(entry.getKey().toString(), entry.getValue().toString());    }    return stringMap;}
public Status flume_f2576_1(AvroFlumeEvent avroEvent)
{    if (logger.isDebugEnabled()) {        if (LogPrivacyUtil.allowLogRawData()) {                    } else {                    }    }    sourceCounter.incrementAppendReceivedCount();    sourceCounter.incrementEventReceivedCount();    Event event = EventBuilder.withBody(avroEvent.getBody().array(), toStringMap(avroEvent.getHeaders()));    try {        getChannelProcessor().processEvent(event);    } catch (ChannelException ex) {                sourceCounter.incrementChannelWriteFail();        return Status.FAILED;    }    sourceCounter.incrementAppendAcceptedCount();    sourceCounter.incrementEventAcceptedCount();    return Status.OK;}
public Status flume_f2577_1(List<AvroFlumeEvent> events)
{        sourceCounter.incrementAppendBatchReceivedCount();    sourceCounter.addToEventReceivedCount(events.size());    List<Event> batch = new ArrayList<Event>();    for (AvroFlumeEvent avroEvent : events) {        Event event = EventBuilder.withBody(avroEvent.getBody().array(), toStringMap(avroEvent.getHeaders()));        batch.add(event);    }    try {        getChannelProcessor().processEventBatch(batch);    } catch (Throwable t) {                sourceCounter.incrementChannelWriteFail();        if (t instanceof Error) {            throw (Error) t;        }        return Status.FAILED;    }    sourceCounter.incrementAppendBatchAcceptedCount();    sourceCounter.addToEventAcceptedCount(events.size());    return Status.OK;}
private PatternRule flume_f2578_1(String patternRuleDefinition) throws FlumeException
{    patternRuleDefinition = patternRuleDefinition.trim();        int firstColonIndex = patternRuleDefinition.indexOf(":");    if (firstColonIndex == -1) {        throw new FlumeException("Invalid ipFilter patternRule '" + patternRuleDefinition + "' should look like <'allow'  or 'deny'>:<'ip' or " + "'name'>:<pattern>");    } else {        String ruleAccessFlag = patternRuleDefinition.substring(0, firstColonIndex);        int secondColonIndex = patternRuleDefinition.indexOf(":", firstColonIndex + 1);        if ((!ruleAccessFlag.equals("allow") && !ruleAccessFlag.equals("deny")) || secondColonIndex == -1) {            throw new FlumeException("Invalid ipFilter patternRule '" + patternRuleDefinition + "' should look like <'allow'  or 'deny'>:<'ip' or " + "'name'>:<pattern>");        }        String patternTypeFlag = patternRuleDefinition.substring(firstColonIndex + 1, secondColonIndex);        if ((!patternTypeFlag.equals("ip") && !patternTypeFlag.equals("name"))) {            throw new FlumeException("Invalid ipFilter patternRule '" + patternRuleDefinition + "' should look like <'allow'  or 'deny'>:<'ip' or " + "'name'>:<pattern>");        }        boolean isAllow = ruleAccessFlag.equals("allow");        String patternRuleString = (patternTypeFlag.equals("ip") ? "i" : "n") + ":" + patternRuleDefinition.substring(secondColonIndex + 1);                return new PatternRule(isAllow, patternRuleString);    }}
public ChannelPipeline flume_f2579_1() throws Exception
{    ChannelPipeline pipeline = Channels.pipeline();    if (enableCompression) {        ZlibEncoder encoder = new ZlibEncoder(6);        pipeline.addFirst("deflater", encoder);        pipeline.addFirst("inflater", new ZlibDecoder());    }    sslEngineSupplier.get().ifPresent(sslEngine -> {                                        pipeline.addFirst("ssl", new SslHandler(sslEngine));    });    if (enableIpFilter) {                IpFilterRuleHandler ipFilterHandler = new IpFilterRuleHandler();        ipFilterHandler.addAll(rules);                pipeline.addFirst("ipFilter", ipFilterHandler);    }    return pipeline;}
public synchronized void flume_f2580_0(Context context)
{    if (isStarted()) {        throw new IllegalStateException("Configure called when started");    } else {        try {            exception = null;            setLifecycleState(LifecycleState.IDLE);            doConfigure(context);        } catch (Exception e) {            exception = e;            setLifecycleState(LifecycleState.ERROR);                        Throwables.propagate(e);        }    }}
public synchronized void flume_f2581_1()
{    if (exception != null) {            } else {        try {            Preconditions.checkState(channelProcessor != null, "No channel processor configured");            doStart();            setLifecycleState(LifecycleState.START);        } catch (Exception e) {                        exception = e;            setLifecycleState(LifecycleState.ERROR);        }    }}
public synchronized void flume_f2582_1()
{    try {        doStop();        setLifecycleState(LifecycleState.STOP);    } catch (Exception e) {                setLifecycleState(LifecycleState.ERROR);    }}
public synchronized void flume_f2583_0(ChannelProcessor cp)
{    channelProcessor = cp;}
public synchronized ChannelProcessor flume_f2584_0()
{    return channelProcessor;}
public synchronized void flume_f2585_0(String name)
{    this.name = name;}
public synchronized String flume_f2586_0()
{    return name;}
public synchronized LifecycleState flume_f2587_0()
{    return lifecycleState;}
public String flume_f2588_0()
{    return this.getClass().getName() + "{name:" + name + ",state:" + lifecycleState + "}";}
protected boolean flume_f2589_0()
{    return getLifecycleState() == LifecycleState.START;}
protected Exception flume_f2590_0()
{    return exception;}
protected synchronized void flume_f2591_0(LifecycleState lifecycleState)
{    this.lifecycleState = lifecycleState;}
public Source flume_f2592_1(String name, String type) throws FlumeException
{    Preconditions.checkNotNull(name, "name");    Preconditions.checkNotNull(type, "type");        Class<? extends Source> sourceClass = getClass(type);    try {        Source source = sourceClass.newInstance();        source.setName(name);        return source;    } catch (Exception ex) {        throw new FlumeException("Unable to create source: " + name + ", type: " + type + ", class: " + sourceClass.getName(), ex);    }}
public Class<? extends Source> flume_f2593_1(String type) throws FlumeException
{    String sourceClassName = type;    SourceType srcType = SourceType.OTHER;    try {        srcType = SourceType.valueOf(type.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException ex) {            }    if (!srcType.equals(SourceType.OTHER)) {        sourceClassName = srcType.getSourceClassName();    }    try {        return (Class<? extends Source>) Class.forName(sourceClassName);    } catch (Exception ex) {        throw new FlumeException("Unable to load source type: " + type + ", class: " + sourceClassName, ex);    }}
public void flume_f2594_0()
{    Source source = getSource();    ChannelProcessor cp = source.getChannelProcessor();    cp.initialize();    source.start();    lifecycleState = LifecycleState.START;}
public void flume_f2595_0()
{    Source source = getSource();    source.stop();    ChannelProcessor cp = source.getChannelProcessor();    cp.close();    lifecycleState = LifecycleState.STOP;}
public String flume_f2596_0()
{    return "EventDrivenSourceRunner: { source:" + getSource() + " }";}
public LifecycleState flume_f2597_0()
{    return lifecycleState;}
public void flume_f2598_1()
{            sourceCounter.start();    executor = Executors.newSingleThreadExecutor();    runner = new ExecRunnable(shell, command, getChannelProcessor(), sourceCounter, restart, restartThrottle, logStderr, bufferCount, batchTimeout, charset);        runnerFuture = executor.submit(runner);        super.start();    }
public void flume_f2599_1()
{        if (runner != null) {        runner.setRestart(false);        runner.kill();    }    if (runnerFuture != null) {                runnerFuture.cancel(true);            }    executor.shutdown();    while (!executor.isTerminated()) {                try {            executor.awaitTermination(500, TimeUnit.MILLISECONDS);        } catch (InterruptedException e) {                        Thread.currentThread().interrupt();        }    }    sourceCounter.stop();    super.stop();    }
public void flume_f2600_0(Context context)
{    command = context.getString("command");    Preconditions.checkState(command != null, "The parameter command must be specified");    restartThrottle = context.getLong(ExecSourceConfigurationConstants.CONFIG_RESTART_THROTTLE, ExecSourceConfigurationConstants.DEFAULT_RESTART_THROTTLE);    restart = context.getBoolean(ExecSourceConfigurationConstants.CONFIG_RESTART, ExecSourceConfigurationConstants.DEFAULT_RESTART);    logStderr = context.getBoolean(ExecSourceConfigurationConstants.CONFIG_LOG_STDERR, ExecSourceConfigurationConstants.DEFAULT_LOG_STDERR);    bufferCount = context.getInteger(ExecSourceConfigurationConstants.CONFIG_BATCH_SIZE, ExecSourceConfigurationConstants.DEFAULT_BATCH_SIZE);    batchTimeout = context.getLong(ExecSourceConfigurationConstants.CONFIG_BATCH_TIME_OUT, ExecSourceConfigurationConstants.DEFAULT_BATCH_TIME_OUT);    charset = Charset.forName(context.getString(ExecSourceConfigurationConstants.CHARSET, ExecSourceConfigurationConstants.DEFAULT_CHARSET));    shell = context.getString(ExecSourceConfigurationConstants.CONFIG_SHELL, null);    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
public long flume_f2601_0()
{    return bufferCount;}
public void flume_f2602_1()
{    do {        String exitCode = "unknown";        BufferedReader reader = null;        String line = null;        final List<Event> eventList = new ArrayList<Event>();        timedFlushService = Executors.newSingleThreadScheduledExecutor(new ThreadFactoryBuilder().setNameFormat("timedFlushExecService" + Thread.currentThread().getId() + "-%d").build());        try {            if (shell != null) {                String[] commandArgs = formulateShellCommand(shell, command);                process = Runtime.getRuntime().exec(commandArgs);            } else {                String[] commandArgs = command.split("\\s+");                process = new ProcessBuilder(commandArgs).start();            }            reader = new BufferedReader(new InputStreamReader(process.getInputStream(), charset));                        StderrReader stderrReader = new StderrReader(new BufferedReader(new InputStreamReader(process.getErrorStream(), charset)), logStderr);            stderrReader.setName("StderrReader-[" + command + "]");            stderrReader.setDaemon(true);            stderrReader.start();            future = timedFlushService.scheduleWithFixedDelay(new Runnable() {                @Override                public void run() {                    try {                        synchronized (eventList) {                            if (!eventList.isEmpty() && timeout()) {                                flushEventBatch(eventList);                            }                        }                    } catch (Exception e) {                                                if (e instanceof InterruptedException) {                            Thread.currentThread().interrupt();                        }                    }                }            }, batchTimeout, batchTimeout, TimeUnit.MILLISECONDS);            while ((line = reader.readLine()) != null) {                sourceCounter.incrementEventReceivedCount();                synchronized (eventList) {                    eventList.add(EventBuilder.withBody(line.getBytes(charset)));                    if (eventList.size() >= bufferCount || timeout()) {                        flushEventBatch(eventList);                    }                }            }            synchronized (eventList) {                if (!eventList.isEmpty()) {                    flushEventBatch(eventList);                }            }        } catch (Exception e) {                        if (e instanceof InterruptedException) {                Thread.currentThread().interrupt();            }        } finally {            if (reader != null) {                try {                    reader.close();                } catch (IOException ex) {                                    }            }            exitCode = String.valueOf(kill());        }        if (restart) {                        try {                Thread.sleep(restartThrottle);            } catch (InterruptedException e) {                Thread.currentThread().interrupt();            }        } else {                    }    } while (restart);}
public void flume_f2603_1()
{    try {        synchronized (eventList) {            if (!eventList.isEmpty() && timeout()) {                flushEventBatch(eventList);            }        }    } catch (Exception e) {                if (e instanceof InterruptedException) {            Thread.currentThread().interrupt();        }    }}
private void flume_f2604_0(List<Event> eventList)
{    channelProcessor.processEventBatch(eventList);    sourceCounter.addToEventAcceptedCount(eventList.size());    eventList.clear();    lastPushToChannel = systemClock.currentTimeMillis();}
private boolean flume_f2605_0()
{    return (systemClock.currentTimeMillis() - lastPushToChannel) >= batchTimeout;}
private static String[] flume_f2606_0(String shell, String command)
{    String[] shellArgs = shell.split("\\s+");    String[] result = new String[shellArgs.length + 1];    System.arraycopy(shellArgs, 0, result, 0, shellArgs.length);    result[shellArgs.length] = command;    return result;}
public int flume_f2607_1()
{    if (process != null) {        synchronized (process) {            process.destroy();            try {                int exitValue = process.waitFor();                                if (future != null) {                    future.cancel(true);                }                if (timedFlushService != null) {                    timedFlushService.shutdown();                    while (!timedFlushService.isTerminated()) {                        try {                            timedFlushService.awaitTermination(500, TimeUnit.MILLISECONDS);                        } catch (InterruptedException e) {                                                        Thread.currentThread().interrupt();                        }                    }                }                return exitValue;            } catch (InterruptedException ex) {                Thread.currentThread().interrupt();            }        }        return Integer.MIN_VALUE;    }    return Integer.MIN_VALUE / 2;}
public void flume_f2608_0(boolean restart)
{    this.restart = restart;}
public void flume_f2609_1()
{    try {        int i = 0;        String line = null;        while ((line = input.readLine()) != null) {            if (logStderr) {                                                                            }        }    } catch (IOException e) {            } finally {        try {            if (input != null) {                input.close();            }        } catch (IOException ex) {                    }    }}
public List<Event> flume_f2610_1(HttpServletRequest request) throws Exception
{    Map<String, String> headers = new HashMap<String, String>();    InputStream inputStream = request.getInputStream();    Map<String, String[]> parameters = request.getParameterMap();    for (String parameter : parameters.keySet()) {        String value = parameters.get(parameter)[0];        if (LOG.isDebugEnabled() && LogPrivacyUtil.allowLogRawData()) {                    }        headers.put(parameter, value);    }    for (String header : mandatoryHeaders) {        Preconditions.checkArgument(headers.containsKey(header), "Please specify " + header + " parameter in the request.");    }    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();    try {        IOUtils.copy(inputStream, outputStream);                Event event = EventBuilder.withBody(outputStream.toByteArray(), headers);        event.setHeaders(headers);        List<Event> eventList = new ArrayList<Event>();        eventList.add(event);        return eventList;    } finally {        outputStream.close();        inputStream.close();    }}
public void flume_f2611_0(Context context)
{    this.commaSeparatedHeaders = context.getString(MANDATORY_PARAMETERS, DEFAULT_MANDATORY_PARAMETERS);    this.mandatoryHeaders = commaSeparatedHeaders.split(PARAMETER_SEPARATOR);}
public void flume_f2612_1(Context context)
{    configureSsl(context);    sourceContext = context;    try {        port = context.getInteger(HTTPSourceConfigurationConstants.CONFIG_PORT);        host = context.getString(HTTPSourceConfigurationConstants.CONFIG_BIND, HTTPSourceConfigurationConstants.DEFAULT_BIND);        Preconditions.checkState(host != null && !host.isEmpty(), "HTTPSource hostname specified is empty");        Preconditions.checkNotNull(port, "HTTPSource requires a port number to be" + " specified");        String handlerClassName = context.getString(HTTPSourceConfigurationConstants.CONFIG_HANDLER, HTTPSourceConfigurationConstants.DEFAULT_HANDLER).trim();        @SuppressWarnings("unchecked")        Class<? extends HTTPSourceHandler> clazz = (Class<? extends HTTPSourceHandler>) Class.forName(handlerClassName);        handler = clazz.getDeclaredConstructor().newInstance();        Map<String, String> subProps = context.getSubProperties(HTTPSourceConfigurationConstants.CONFIG_HANDLER_PREFIX);        handler.configure(new Context(subProps));    } catch (ClassNotFoundException ex) {                Throwables.propagate(ex);    } catch (ClassCastException ex) {                Throwables.propagate(ex);    } catch (Exception ex) {                Throwables.propagate(ex);    }    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
public void flume_f2613_1()
{    Preconditions.checkState(srv == null, "Running HTTP Server found in source: " + getName() + " before I started one." + "Will not attempt to start.");    QueuedThreadPool threadPool = new QueuedThreadPool();    if (sourceContext.getSubProperties("QueuedThreadPool.").size() > 0) {        FlumeBeanConfigurator.setConfigurationFields(threadPool, sourceContext);    }    srv = new Server(threadPool);        MBeanContainer mbContainer = new MBeanContainer(ManagementFactory.getPlatformMBeanServer());    srv.addEventListener(mbContainer);    srv.addBean(mbContainer);    HttpConfiguration httpConfiguration = new HttpConfiguration();    httpConfiguration.addCustomizer(new SecureRequestCustomizer());    FlumeBeanConfigurator.setConfigurationFields(httpConfiguration, sourceContext);    ServerConnector connector = getSslContextSupplier().get().map(sslContext -> {        SslContextFactory sslCtxFactory = new SslContextFactory();        sslCtxFactory.setSslContext(sslContext);        sslCtxFactory.setExcludeProtocols(getExcludeProtocols().toArray(new String[] {}));        sslCtxFactory.setIncludeProtocols(getIncludeProtocols().toArray(new String[] {}));        sslCtxFactory.setExcludeCipherSuites(getExcludeCipherSuites().toArray(new String[] {}));        sslCtxFactory.setIncludeCipherSuites(getIncludeCipherSuites().toArray(new String[] {}));        FlumeBeanConfigurator.setConfigurationFields(sslCtxFactory, sourceContext);        httpConfiguration.setSecurePort(port);        httpConfiguration.setSecureScheme("https");        return new ServerConnector(srv, new SslConnectionFactory(sslCtxFactory, HttpVersion.HTTP_1_1.asString()), new HttpConnectionFactory(httpConfiguration));    }).orElse(new ServerConnector(srv, new HttpConnectionFactory(httpConfiguration)));    connector.setPort(port);    connector.setHost(host);    connector.setReuseAddress(true);    FlumeBeanConfigurator.setConfigurationFields(connector, sourceContext);    srv.addConnector(connector);    try {        ServletContextHandler context = new ServletContextHandler(ServletContextHandler.SESSIONS);        context.setContextPath("/");        srv.setHandler(context);        context.addServlet(new ServletHolder(new FlumeHTTPServlet()), "/");        context.setSecurityHandler(HTTPServerConstraintUtil.enforceConstraints());        srv.start();    } catch (Exception ex) {                Throwables.propagate(ex);    }    Preconditions.checkArgument(srv.isRunning());    sourceCounter.start();    super.start();}
public void flume_f2614_1()
{    try {        srv.stop();        srv.join();        srv = null;    } catch (Exception ex) {            }    sourceCounter.stop();    }
public void flume_f2615_1(HttpServletRequest request, HttpServletResponse response) throws IOException
{        List<Event> events = Collections.emptyList();    try {        events = handler.getEvents(request);    } catch (HTTPBadRequestException ex) {                sourceCounter.incrementEventReadFail();        response.sendError(HttpServletResponse.SC_BAD_REQUEST, "Bad request from client. " + ex.getMessage());        return;    } catch (Exception ex) {                sourceCounter.incrementEventReadFail();        response.sendError(HttpServletResponse.SC_INTERNAL_SERVER_ERROR, "Deserializer threw unexpected exception. " + ex.getMessage());        return;    }    sourceCounter.incrementAppendBatchReceivedCount();    sourceCounter.addToEventReceivedCount(events.size());    try {        getChannelProcessor().processEventBatch(events);    } catch (ChannelException ex) {                sourceCounter.incrementChannelWriteFail();        response.sendError(HttpServletResponse.SC_SERVICE_UNAVAILABLE, "Error appending event to channel. Channel might be full." + ex.getMessage());        return;    } catch (Exception ex) {                sourceCounter.incrementGenericProcessingFail();        response.sendError(HttpServletResponse.SC_INTERNAL_SERVER_ERROR, "Unexpected error while appending event to channel. " + ex.getMessage());        return;    }    response.setCharacterEncoding(request.getCharacterEncoding());    response.setStatus(HttpServletResponse.SC_OK);    response.flushBuffer();    sourceCounter.incrementAppendBatchAcceptedCount();    sourceCounter.addToEventAcceptedCount(events.size());}
public void flume_f2616_0(HttpServletRequest request, HttpServletResponse response) throws IOException
{    doPost(request, response);}
protected void flume_f2617_0(Context context)
{    handleDeprecatedParameter(context, "ssl", "enableSSL");    handleDeprecatedParameter(context, "exclude-protocols", "excludeProtocols");    handleDeprecatedParameter(context, "keystore-password", "keystorePassword");    super.configureSsl(context);}
private void flume_f2618_0(Context context, String newParam, String oldParam)
{    if (!context.containsKey(newParam) && context.containsKey(oldParam)) {        context.put(newParam, context.getString(oldParam));    }}
public List<Event> flume_f2619_1(HttpServletRequest request) throws Exception
{    BufferedReader reader = request.getReader();    String charset = request.getCharacterEncoding();        if (charset == null) {                charset = "UTF-8";    } else if (!(charset.equalsIgnoreCase("utf-8") || charset.equalsIgnoreCase("utf-16") || charset.equalsIgnoreCase("utf-32"))) {                throw new UnsupportedCharsetException("JSON handler supports UTF-8, " + "UTF-16 and UTF-32 only.");    }    /*     * Gson throws Exception if the data is not parseable to JSON.     * Need not catch it since the source will catch it and return error.     */    List<Event> eventList = new ArrayList<Event>(0);    try {        eventList = gson.fromJson(reader, listType);    } catch (JsonSyntaxException ex) {        throw new HTTPBadRequestException("Request has invalid JSON Syntax.", ex);    }    for (Event e : eventList) {        ((JSONEvent) e).setCharset(charset);    }    return getSimpleEvents(eventList);}
public void flume_f2620_0(Context context)
{}
private List<Event> flume_f2621_0(List<Event> events)
{    List<Event> newEvents = new ArrayList<Event>(events.size());    for (Event e : events) {        newEvents.add(EventBuilder.withBody(e.getBody(), e.getHeaders()));    }    return newEvents;}
public void flume_f2622_0(Context context)
{    configureSsl(context);    String portsStr = context.getString(SyslogSourceConfigurationConstants.CONFIG_PORTS);    Preconditions.checkNotNull(portsStr, "Must define config " + "parameter for MultiportSyslogTCPSource: ports");    for (String portStr : portsStr.split("\\s+")) {        Integer port = Integer.parseInt(portStr);        ports.add(port);    }    host = context.getString(SyslogSourceConfigurationConstants.CONFIG_HOST);    numProcessors = context.getInteger(SyslogSourceConfigurationConstants.CONFIG_NUMPROCESSORS);    maxEventSize = context.getInteger(SyslogSourceConfigurationConstants.CONFIG_EVENTSIZE, SyslogUtils.DEFAULT_SIZE);    String defaultCharsetStr = context.getString(SyslogSourceConfigurationConstants.CONFIG_CHARSET, SyslogSourceConfigurationConstants.DEFAULT_CHARSET);    try {        defaultCharset = Charset.forName(defaultCharsetStr);    } catch (Exception ex) {        throw new IllegalArgumentException("Unable to parse charset " + "string (" + defaultCharsetStr + ") from port configuration.", ex);    }    defaultDecoder = new ThreadSafeDecoder(defaultCharset);        portCharsets.clear();    {        Map<String, String> portCharsetCfg = context.getSubProperties(SyslogSourceConfigurationConstants.CONFIG_PORT_CHARSET_PREFIX);        for (Map.Entry<String, String> entry : portCharsetCfg.entrySet()) {            String portStr = entry.getKey();            String charsetStr = entry.getValue();            Integer port = Integer.parseInt(portStr);            Preconditions.checkNotNull(port, "Invalid port number in config");            try {                Charset charset = Charset.forName(charsetStr);                portCharsets.put(port, new ThreadSafeDecoder(charset));            } catch (Exception ex) {                throw new IllegalArgumentException("Unable to parse charset " + "string (" + charsetStr + ") from port configuration.", ex);            }        }    }    batchSize = context.getInteger(SyslogSourceConfigurationConstants.CONFIG_BATCHSIZE, SyslogSourceConfigurationConstants.DEFAULT_BATCHSIZE);    portHeader = context.getString(SyslogSourceConfigurationConstants.CONFIG_PORT_HEADER);    clientIPHeader = context.getString(SyslogSourceConfigurationConstants.CONFIG_CLIENT_IP_HEADER);    clientHostnameHeader = context.getString(SyslogSourceConfigurationConstants.CONFIG_CLIENT_HOSTNAME_HEADER);    readBufferSize = context.getInteger(SyslogSourceConfigurationConstants.CONFIG_READBUF_SIZE, SyslogSourceConfigurationConstants.DEFAULT_READBUF_SIZE);    keepFields = SyslogUtils.chooseFieldsToKeep(context.getString(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS, SyslogSourceConfigurationConstants.DEFAULT_KEEP_FIELDS));    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
public void flume_f2623_1()
{            if (numProcessors != null) {        acceptor = new NioSocketAcceptor(numProcessors);    } else {        acceptor = new NioSocketAcceptor();    }    getSslContextSupplier().get().ifPresent(sslContext -> {        SslFilter filter = new SslFilter(sslContext);        SSLParameters sslParameters = sslContext.getDefaultSSLParameters();        filter.setEnabledProtocols(getFilteredProtocols(sslParameters));        filter.setEnabledCipherSuites(getFilteredCipherSuites(sslParameters));        acceptor.getFilterChain().addFirst("ssl", filter);    });    acceptor.setReuseAddress(true);    acceptor.getSessionConfig().setReadBufferSize(readBufferSize);    acceptor.getSessionConfig().setIdleTime(IdleStatus.BOTH_IDLE, 10);    acceptor.setHandler(new MultiportSyslogHandler(maxEventSize, batchSize, getChannelProcessor(), sourceCounter, portHeader, clientIPHeader, clientHostnameHeader, defaultDecoder, portCharsets, keepFields));    for (int port : ports) {        InetSocketAddress addr;        if (host != null) {            addr = new InetSocketAddress(host, port);        } else {            addr = new InetSocketAddress(port);        }        try {                                    acceptor.bind(addr);        } catch (IOException ex) {                    }    }    sourceCounter.start();    super.start();    }
public void flume_f2624_1()
{        acceptor.unbind();    acceptor.dispose();    sourceCounter.stop();    super.stop();    }
public String flume_f2625_0()
{    return "Multiport Syslog TCP source " + getName();}
public long flume_f2626_0()
{    return batchSize;}
public void flume_f2627_1(IoSession session, Throwable cause) throws Exception
{        sourceCounter.incrementGenericProcessingFail();    if (cause instanceof Error) {        Throwables.propagate(cause);    }}
public void flume_f2628_1(IoSession session)
{                    session.setAttribute(SAVED_BUF, IoBuffer.allocate(maxEventSize, false));}
public void flume_f2629_1(IoSession session)
{        }
public void flume_f2630_1(IoSession session)
{    }
public void flume_f2631_1(IoSession session, Object message)
{    IoBuffer buf = (IoBuffer) message;    IoBuffer savedBuf = (IoBuffer) session.getAttribute(SAVED_BUF);    ParsedBuffer parsedLine = new ParsedBuffer();    List<Event> events = Lists.newArrayList();        CharsetDecoder decoder = defaultDecoder.get();    int port = ((InetSocketAddress) session.getLocalAddress()).getPort();    if (portCharsets.containsKey(port)) {        decoder = portCharsets.get(port).get();    }        while (buf.hasRemaining()) {        events.clear();                for (int num = 0; num < batchSize && buf.hasRemaining(); num++) {            if (lineSplitter.parseLine(buf, savedBuf, parsedLine)) {                Event event = parseEvent(parsedLine, decoder);                if (portHeader != null) {                    event.getHeaders().put(portHeader, String.valueOf(port));                }                if (clientIPHeader != null) {                    event.getHeaders().put(clientIPHeader, SyslogUtils.getIP(session.getRemoteAddress()));                }                if (clientHostnameHeader != null) {                    event.getHeaders().put(clientHostnameHeader, SyslogUtils.getHostname(session.getRemoteAddress()));                }                events.add(event);            } else {                logger.trace("Parsed null event");            }        }                if (events.isEmpty()) {            logger.trace("Empty set!");            return;        }        int numEvents = events.size();        sourceCounter.addToEventReceivedCount(numEvents);                try {            channelProcessor.processEventBatch(events);            sourceCounter.addToEventAcceptedCount(numEvents);        } catch (Throwable t) {                        sourceCounter.incrementEventReadOrChannelFail(t);            if (t instanceof Error) {                Throwables.propagate(t);            }        }    }}
 Event flume_f2632_1(ParsedBuffer parsedBuf, CharsetDecoder decoder)
{    String msg = null;    try {        msg = parsedBuf.buffer.getString(decoder);    } catch (Throwable t) {                sourceCounter.incrementEventReadFail();        if (t instanceof Error) {            Throwables.propagate(t);        }                byte[] bytes = new byte[parsedBuf.buffer.remaining()];        parsedBuf.buffer.get(bytes);        Event event = EventBuilder.withBody(bytes);        event.getHeaders().put(SyslogUtils.EVENT_STATUS, SyslogUtils.SyslogStatus.INVALID.getSyslogStatus());        return event;    }    if (logger.isTraceEnabled()) {        if (LogPrivacyUtil.allowLogRawData()) {            logger.trace("Seen raw event: {}", msg);        } else {            logger.trace("Seen raw event.");        }    }    Event event;    try {        event = syslogParser.parseMessage(msg, decoder.charset(), keepFields);        if (parsedBuf.incomplete) {            event.getHeaders().put(SyslogUtils.EVENT_STATUS, SyslogUtils.SyslogStatus.INCOMPLETE.getSyslogStatus());        }    } catch (IllegalArgumentException ex) {        event = EventBuilder.withBody(msg, decoder.charset());        event.getHeaders().put(SyslogUtils.EVENT_STATUS, SyslogUtils.SyslogStatus.INVALID.getSyslogStatus());                sourceCounter.incrementEventReadFail();    }    return event;}
public boolean flume_f2633_1(IoBuffer buf, IoBuffer savedBuf, ParsedBuffer parsedBuf)
{        parsedBuf.buffer = null;    parsedBuf.incomplete = false;    byte curByte;    buf.mark();        int msgPos = savedBuf.position();    boolean seenNewline = false;    while (!seenNewline && buf.hasRemaining() && msgPos < maxLineLength) {        curByte = buf.get();                if (curByte == NEWLINE) {            seenNewline = true;        }        msgPos++;    }        if (seenNewline) {        int end = buf.position();        buf.reset();        int start = buf.position();        if (savedBuf.position() > 0) {                        byte[] tmp = new byte[end - start];            buf.get(tmp);            savedBuf.put(tmp);            int len = savedBuf.position() - 1;            savedBuf.flip();            parsedBuf.buffer = savedBuf.getSlice(len);            savedBuf.clear();        } else {            parsedBuf.buffer = buf.getSlice(end - start - 1);                        buf.get();        }        return true;        } else {                if (msgPos == maxLineLength) {            int end = buf.position();            buf.reset();            int start = buf.position();            if (savedBuf.position() > 0) {                                byte[] tmp = new byte[end - start];                buf.get(tmp);                savedBuf.put(tmp);                savedBuf.flip();                parsedBuf.buffer = savedBuf.getSlice(msgPos);                savedBuf.clear();            } else {                                parsedBuf.buffer = buf.getSlice(msgPos);            }                        parsedBuf.incomplete = true;            return true;                } else if (!buf.hasRemaining()) {            int end = buf.position();            buf.reset();            int start = buf.position();            byte[] tmp = new byte[end - start];            buf.get(tmp);            savedBuf.put(tmp);            return false;                } else {            throw new IllegalStateException("unexpected buffer state: " + "msgPos=" + msgPos + ", buf.hasRemaining=" + buf.hasRemaining() + ", savedBuf.hasRemaining=" + savedBuf.hasRemaining() + ", seenNewline=" + seenNewline + ", maxLen=" + maxLineLength);        }    }}
protected CharsetDecoder flume_f2634_0()
{    return charset.newDecoder();}
public void flume_f2635_0(Context context)
{    String hostKey = NetcatSourceConfigurationConstants.CONFIG_HOSTNAME;    String portKey = NetcatSourceConfigurationConstants.CONFIG_PORT;    String ackEventKey = NetcatSourceConfigurationConstants.CONFIG_ACKEVENT;    Configurables.ensureRequiredNonNull(context, hostKey, portKey);    hostName = context.getString(hostKey);    port = context.getInteger(portKey);    ackEveryEvent = context.getBoolean(ackEventKey, true);    maxLineLength = context.getInteger(NetcatSourceConfigurationConstants.CONFIG_MAX_LINE_LENGTH, NetcatSourceConfigurationConstants.DEFAULT_MAX_LINE_LENGTH);    sourceEncoding = context.getString(NetcatSourceConfigurationConstants.CONFIG_SOURCE_ENCODING, NetcatSourceConfigurationConstants.DEFAULT_ENCODING);}
public void flume_f2636_1()
{        counterGroup.incrementAndGet("open.attempts");    try {        SocketAddress bindPoint = new InetSocketAddress(hostName, port);        serverSocket = ServerSocketChannel.open();        serverSocket.socket().setReuseAddress(true);        serverSocket.socket().bind(bindPoint);            } catch (IOException e) {        counterGroup.incrementAndGet("open.errors");                stop();        throw new FlumeException(e);    }    handlerService = Executors.newCachedThreadPool(new ThreadFactoryBuilder().setNameFormat("netcat-handler-%d").build());    AcceptHandler acceptRunnable = new AcceptHandler(maxLineLength);    acceptThreadShouldStop.set(false);    acceptRunnable.counterGroup = counterGroup;    acceptRunnable.handlerService = handlerService;    acceptRunnable.shouldStop = acceptThreadShouldStop;    acceptRunnable.ackEveryEvent = ackEveryEvent;    acceptRunnable.source = this;    acceptRunnable.serverSocket = serverSocket;    acceptRunnable.sourceEncoding = sourceEncoding;    acceptThread = new Thread(acceptRunnable);    acceptThread.start();        super.start();}
public void flume_f2637_1()
{        acceptThreadShouldStop.set(true);    if (acceptThread != null) {                while (acceptThread.isAlive()) {            try {                                acceptThread.interrupt();                acceptThread.join(500);            } catch (InterruptedException e) {                                Thread.currentThread().interrupt();            }        }            }    if (serverSocket != null) {        try {            serverSocket.close();        } catch (IOException e) {                        return;        }    }    if (handlerService != null) {        handlerService.shutdown();                        try {            handlerService.awaitTermination(500, TimeUnit.MILLISECONDS);        } catch (InterruptedException e) {                        Thread.currentThread().interrupt();        }        if (!handlerService.isShutdown()) {            handlerService.shutdownNow();        }            }        super.stop();}
public void flume_f2638_1()
{        while (!shouldStop.get()) {        try {            SocketChannel socketChannel = serverSocket.accept();            NetcatSocketHandler request = new NetcatSocketHandler(maxLineLength);            request.socketChannel = socketChannel;            request.counterGroup = counterGroup;            request.source = source;            request.ackEveryEvent = ackEveryEvent;            request.sourceEncoding = sourceEncoding;            handlerService.submit(request);            counterGroup.incrementAndGet("accept.succeeded");        } catch (ClosedByInterruptException e) {                } catch (IOException e) {                        counterGroup.incrementAndGet("accept.failed");        }    }    }
public void flume_f2639_1()
{        Event event = null;    try {        Reader reader = Channels.newReader(socketChannel, sourceEncoding);        Writer writer = Channels.newWriter(socketChannel, sourceEncoding);        CharBuffer buffer = CharBuffer.allocate(maxLineLength);                buffer.flip();        while (true) {                        int charsRead = fill(buffer, reader);                                    int eventsProcessed = processEvents(buffer, writer);                        if (charsRead == -1) {                                break;            } else if (charsRead == 0 && eventsProcessed == 0) {                if (buffer.remaining() == buffer.capacity()) {                                                                                                                                                                counterGroup.incrementAndGet("events.failed");                    writer.write("FAILED: Event exceeds the maximum length (" + buffer.capacity() + " chars, including newline)\n");                    writer.flush();                    break;                }            }        }        socketChannel.close();        counterGroup.incrementAndGet("sessions.completed");    } catch (IOException e) {        counterGroup.incrementAndGet("sessions.broken");        try {            socketChannel.close();        } catch (IOException ex) {                    }    }    }
private int flume_f2640_1(CharBuffer buffer, Writer writer) throws IOException
{    int numProcessed = 0;    boolean foundNewLine = true;    while (foundNewLine) {        foundNewLine = false;        int limit = buffer.limit();        for (int pos = buffer.position(); pos < limit; pos++) {            if (buffer.get(pos) == '\n') {                                                buffer.limit(pos);                ByteBuffer bytes = Charsets.UTF_8.encode(buffer);                                buffer.limit(limit);                                byte[] body = new byte[bytes.remaining()];                bytes.get(body);                Event event = EventBuilder.withBody(body);                                ChannelException ex = null;                try {                    source.getChannelProcessor().processEvent(event);                } catch (ChannelException chEx) {                    ex = chEx;                }                if (ex == null) {                    counterGroup.incrementAndGet("events.processed");                    numProcessed++;                    if (true == ackEveryEvent) {                        writer.write("OK\n");                    }                } else {                    counterGroup.incrementAndGet("events.failed");                                        writer.write("FAILED: " + ex.getMessage() + "\n");                }                writer.flush();                                                buffer.position(pos + 1);                foundNewLine = true;                break;            }        }    }    return numProcessed;}
private int flume_f2641_0(CharBuffer buffer, Reader reader) throws IOException
{        buffer.compact();        int charsRead = reader.read(buffer);    counterGroup.addAndGet("characters.received", Long.valueOf(charsRead));        buffer.flip();    return charsRead;}
private Event flume_f2642_0(ChannelBuffer in, SocketAddress remoteAddress)
{    Map<String, String> headers = new HashMap<String, String>();    headers.put(remoteHostHeader, remoteAddress.toString());    byte b = 0;    ByteArrayOutputStream baos = new ByteArrayOutputStream();    Event e = null;    boolean doneReading = false;    try {        while (!doneReading && in.readable()) {            b = in.readByte();                        if (b == '\n') {                doneReading = true;            } else {                baos.write(b);            }        }        e = EventBuilder.withBody(baos.toByteArray(), headers);    } finally {        }    return e;}
public void flume_f2643_1(ChannelHandlerContext ctx, MessageEvent mEvent)
{    try {        Event e = extractEvent((ChannelBuffer) mEvent.getMessage(), mEvent.getRemoteAddress());        if (e == null) {            return;        }        getChannelProcessor().processEvent(e);        counterGroup.incrementAndGet("events.success");    } catch (ChannelException ex) {        counterGroup.incrementAndGet("events.dropped");            } catch (RuntimeException ex) {        counterGroup.incrementAndGet("events.dropped");            }}
public void flume_f2644_0()
{        ConnectionlessBootstrap serverBootstrap = new ConnectionlessBootstrap(new OioDatagramChannelFactory(Executors.newCachedThreadPool()));    final NetcatHandler handler = new NetcatHandler();    serverBootstrap.setOption("receiveBufferSizePredictorFactory", new AdaptiveReceiveBufferSizePredictorFactory(DEFAULT_MIN_SIZE, DEFAULT_INITIAL_SIZE, maxsize));    serverBootstrap.setPipelineFactory(new ChannelPipelineFactory() {        @Override        public ChannelPipeline getPipeline() {            return Channels.pipeline(handler);        }    });    if (host == null) {        nettyChannel = serverBootstrap.bind(new InetSocketAddress(port));    } else {        nettyChannel = serverBootstrap.bind(new InetSocketAddress(host, port));    }    super.start();}
public ChannelPipeline flume_f2645_0()
{    return Channels.pipeline(handler);}
public void flume_f2646_1()
{            if (nettyChannel != null) {        nettyChannel.close();        try {            nettyChannel.getCloseFuture().await(60, TimeUnit.SECONDS);        } catch (InterruptedException e) {                    } finally {            nettyChannel = null;        }    }    super.stop();}
public void flume_f2647_0(Context context)
{    Configurables.ensureRequiredNonNull(context, CONFIG_PORT);    port = context.getInteger(CONFIG_PORT);    host = context.getString(CONFIG_HOST);    remoteHostHeader = context.getString(REMOTE_ADDRESS_HEADER);}
public int flume_f2648_0()
{    SocketAddress localAddress = nettyChannel.getLocalAddress();    if (localAddress instanceof InetSocketAddress) {        InetSocketAddress addr = (InetSocketAddress) localAddress;        return addr.getPort();    }    return 0;}
public void flume_f2649_0()
{    PollableSource source = (PollableSource) getSource();    ChannelProcessor cp = source.getChannelProcessor();    cp.initialize();    source.start();    runner = new PollingRunner();    runner.source = source;    runner.counterGroup = counterGroup;    runner.shouldStop = shouldStop;    runnerThread = new Thread(runner);    runnerThread.setName(getClass().getSimpleName() + "-" + source.getClass().getSimpleName() + "-" + source.getName());    runnerThread.start();    lifecycleState = LifecycleState.START;}
public void flume_f2650_1()
{    runner.shouldStop.set(true);    try {        runnerThread.interrupt();        runnerThread.join();    } catch (InterruptedException e) {                Thread.currentThread().interrupt();    }    Source source = getSource();    source.stop();    ChannelProcessor cp = source.getChannelProcessor();    cp.close();    lifecycleState = LifecycleState.STOP;}
public String flume_f2651_0()
{    return "PollableSourceRunner: { source:" + getSource() + " counterGroup:" + counterGroup + " }";}
public LifecycleState flume_f2652_0()
{    return lifecycleState;}
public void flume_f2653_1()
{        while (!shouldStop.get()) {        counterGroup.incrementAndGet("runner.polls");        try {            if (source.process().equals(PollableSource.Status.BACKOFF)) {                counterGroup.incrementAndGet("runner.backoffs");                Thread.sleep(Math.min(counterGroup.incrementAndGet("runner.backoffs.consecutive") * source.getBackOffSleepIncrement(), source.getMaxBackOffSleepInterval()));            } else {                counterGroup.set("runner.backoffs.consecutive", 0L);            }        } catch (InterruptedException e) {                        counterGroup.incrementAndGet("runner.interruptions");        } catch (EventDeliveryException e) {                        counterGroup.incrementAndGet("runner.deliveryErrors");        } catch (Exception e) {            counterGroup.incrementAndGet("runner.errors");                        try {                Thread.sleep(source.getMaxBackOffSleepInterval());            } catch (InterruptedException ex) {                Thread.currentThread().interrupt();            }        }    }    }
protected void flume_f2654_0(Context context) throws FlumeException
{    batchSize = context.getInteger("batchSize", 1);    totalEvents = context.getLong("totalEvents", Long.MAX_VALUE);    Preconditions.checkArgument(batchSize > 0, "batchSize was %s but expected positive", batchSize);    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
protected Status flume_f2655_1() throws EventDeliveryException
{    Status status = Status.READY;    long eventsSentTX = eventsSent;    try {        if (batchSize == 1) {            if (eventsSentTX < totalEvents) {                getChannelProcessor().processEvent(EventBuilder.withBody(String.valueOf(eventsSentTX++).getBytes()));                sourceCounter.incrementEventAcceptedCount();            } else {                status = Status.BACKOFF;            }        } else {            List<Event> batchArrayList = new ArrayList<>(batchSize);            for (int i = 0; i < batchSize; i++) {                if (eventsSentTX < totalEvents) {                    batchArrayList.add(i, EventBuilder.withBody(String.valueOf(eventsSentTX++).getBytes()));                } else {                    status = Status.BACKOFF;                    break;                }            }            if (!batchArrayList.isEmpty()) {                getChannelProcessor().processEventBatch(batchArrayList);                sourceCounter.incrementAppendBatchAcceptedCount();                sourceCounter.addToEventAcceptedCount(batchArrayList.size());            }        }        eventsSent = eventsSentTX;    } catch (ChannelException ex) {                sourceCounter.incrementChannelWriteFail();    }    return status;}
protected void flume_f2656_1() throws FlumeException
{        sourceCounter.start();    }
protected void flume_f2657_1() throws FlumeException
{        sourceCounter.stop();    }
public long flume_f2658_0()
{    return batchSize;}
public static RateLimiter flume_f2659_0(double permitsPerSecond)
{    /*     * The default RateLimiter configuration can save the unused permits of up to one second.     * This is to avoid unnecessary stalls in situations like this: A RateLimiter of 1qps,     * and 4 threads, all calling acquire() at these moments:     *     * T0 at 0 seconds     * T1 at 1.05 seconds     * T2 at 2 seconds     * T3 at 3 seconds     *     * Due to the slight delay of T1, T2 would have to sleep till 2.05 seconds,     * and T3 would also have to sleep till 3.05 seconds.     */    return create(SleepingStopwatch.createFromSystemTimer(), permitsPerSecond);}
 static RateLimiter flume_f2660_0(SleepingStopwatch stopwatch, double permitsPerSecond)
{    RateLimiter rateLimiter = new SmoothBursty(stopwatch, 1.0);    rateLimiter.setRate(permitsPerSecond);    return rateLimiter;}
public static RateLimiter flume_f2661_0(double permitsPerSecond, long warmupPeriod, TimeUnit unit)
{    checkArgument(warmupPeriod >= 0, "warmupPeriod must not be negative: %s", warmupPeriod);    return create(SleepingStopwatch.createFromSystemTimer(), permitsPerSecond, warmupPeriod, unit);}
 static RateLimiter flume_f2662_0(SleepingStopwatch stopwatch, double permitsPerSecond, long warmupPeriod, TimeUnit unit)
{    RateLimiter rateLimiter = new SmoothWarmingUp(stopwatch, warmupPeriod, unit);    rateLimiter.setRate(permitsPerSecond);    return rateLimiter;}
private Object flume_f2663_0()
{    Object mutex = mutexDoNotUseDirectly;    if (mutex == null) {        synchronized (this) {            mutex = mutexDoNotUseDirectly;            if (mutex == null) {                mutexDoNotUseDirectly = mutex = new Object();            }        }    }    return mutex;}
public final void flume_f2664_0(double permitsPerSecond)
{    checkArgument(permitsPerSecond > 0.0 && !Double.isNaN(permitsPerSecond), "rate must be positive");    synchronized (mutex()) {        doSetRate(permitsPerSecond, stopwatch.readMicros());    }}
public final double flume_f2665_0()
{    synchronized (mutex()) {        return doGetRate();    }}
public double flume_f2666_0()
{    return acquire(1);}
public double flume_f2667_0(int permits)
{    long microsToWait = reserve(permits);    stopwatch.sleepMicrosUninterruptibly(microsToWait);    return 1.0 * microsToWait / SECONDS.toMicros(1L);}
 final long flume_f2668_0(int permits)
{    checkPermits(permits);    synchronized (mutex()) {        return reserveAndGetWaitLength(permits, stopwatch.readMicros());    }}
public boolean flume_f2669_0(long timeout, TimeUnit unit)
{    return tryAcquire(1, timeout, unit);}
public boolean flume_f2670_0(int permits)
{    return tryAcquire(permits, 0, MICROSECONDS);}
public boolean flume_f2671_0()
{    return tryAcquire(1, 0, MICROSECONDS);}
public boolean flume_f2672_0(int permits, long timeout, TimeUnit unit)
{    long timeoutMicros = max(unit.toMicros(timeout), 0);    checkPermits(permits);    long microsToWait;    synchronized (mutex()) {        long nowMicros = stopwatch.readMicros();        if (!canAcquire(nowMicros, timeoutMicros)) {            return false;        } else {            microsToWait = reserveAndGetWaitLength(permits, nowMicros);        }    }    stopwatch.sleepMicrosUninterruptibly(microsToWait);    return true;}
private boolean flume_f2673_0(long nowMicros, long timeoutMicros)
{    return queryEarliestAvailable(nowMicros) - timeoutMicros <= nowMicros;}
 final long flume_f2674_0(int permits, long nowMicros)
{    long momentAvailable = reserveEarliestAvailable(permits, nowMicros);    return max(momentAvailable - nowMicros, 0);}
public String flume_f2675_0()
{    return String.format("RateLimiter[stableRate=%3.1fqps]", getRate());}
 static final SleepingStopwatch flume_f2676_0()
{    return new SleepingStopwatch() {        final Stopwatch stopwatch = Stopwatch.createStarted();        @Override        long readMicros() {            return stopwatch.elapsed(MICROSECONDS);        }        @Override        void sleepMicrosUninterruptibly(long micros) {            if (micros > 0) {                Uninterruptibles.sleepUninterruptibly(micros, MICROSECONDS);            }        }    };}
 long flume_f2677_0()
{    return stopwatch.elapsed(MICROSECONDS);}
 void flume_f2678_0(long micros)
{    if (micros > 0) {        Uninterruptibles.sleepUninterruptibly(micros, MICROSECONDS);    }}
private static int flume_f2679_0(int permits)
{    checkArgument(permits > 0, "Requested permits (%s) must be positive", permits);    return permits;}
 void flume_f2680_0(double permitsPerSecond, double stableIntervalMicros)
{    double oldMaxPermits = maxPermits;    maxPermits = warmupPeriodMicros / stableIntervalMicros;    halfPermits = maxPermits / 2.0;        double coldIntervalMicros = stableIntervalMicros * 3.0;    slope = (coldIntervalMicros - stableIntervalMicros) / halfPermits;    if (oldMaxPermits == Double.POSITIVE_INFINITY) {                storedPermits = 0.0;    } else {        storedPermits = (oldMaxPermits == 0.0) ?         maxPermits : storedPermits * maxPermits / oldMaxPermits;    }}
 long flume_f2681_0(double storedPermits, double permitsToTake)
{    double availablePermitsAboveHalf = storedPermits - halfPermits;    long micros = 0;        if (availablePermitsAboveHalf > 0.0) {        double permitsAboveHalfToTake = min(availablePermitsAboveHalf, permitsToTake);        micros = (long) (permitsAboveHalfToTake * (permitsToTime(availablePermitsAboveHalf) + permitsToTime(availablePermitsAboveHalf - permitsAboveHalfToTake)) / 2.0);        permitsToTake -= permitsAboveHalfToTake;    }        micros += (stableIntervalMicros * permitsToTake);    return micros;}
private double flume_f2682_0(double permits)
{    return stableIntervalMicros + permits * slope;}
 void flume_f2683_0(double permitsPerSecond, double stableIntervalMicros)
{    double oldMaxPermits = this.maxPermits;    maxPermits = maxBurstSeconds * permitsPerSecond;    if (oldMaxPermits == Double.POSITIVE_INFINITY) {                storedPermits = maxPermits;    } else {        storedPermits = (oldMaxPermits == 0.0) ?         0.0 : storedPermits * maxPermits / oldMaxPermits;    }}
 long flume_f2684_0(double storedPermits, double permitsToTake)
{    return 0L;}
 final void flume_f2685_0(double permitsPerSecond, long nowMicros)
{    resync(nowMicros);    double stableIntervalMicros = SECONDS.toMicros(1L) / permitsPerSecond;    this.stableIntervalMicros = stableIntervalMicros;    doSetRate(permitsPerSecond, stableIntervalMicros);}
 final double flume_f2686_0()
{    return SECONDS.toMicros(1L) / stableIntervalMicros;}
 final long flume_f2687_0(long nowMicros)
{    return nextFreeTicketMicros;}
 final long flume_f2688_0(int requiredPermits, long nowMicros)
{    resync(nowMicros);    long returnValue = nextFreeTicketMicros;    double storedPermitsToSpend = min(requiredPermits, this.storedPermits);    double freshPermits = requiredPermits - storedPermitsToSpend;    long waitMicros = storedPermitsToWaitTime(this.storedPermits, storedPermitsToSpend) + (long) (freshPermits * stableIntervalMicros);    this.nextFreeTicketMicros = nextFreeTicketMicros + waitMicros;    this.storedPermits -= storedPermitsToSpend;    return returnValue;}
private void flume_f2689_0(long nowMicros)
{        if (nowMicros > nextFreeTicketMicros) {        storedPermits = min(maxPermits, storedPermits + (nowMicros - nextFreeTicketMicros) / stableIntervalMicros);        nextFreeTicketMicros = nowMicros;    }}
public static Stopwatch flume_f2690_0()
{    return new Stopwatch();}
public static Stopwatch flume_f2691_0(Ticker ticker)
{    return new Stopwatch(ticker);}
public static Stopwatch flume_f2692_0()
{    return new Stopwatch().start();}
public static Stopwatch flume_f2693_0(Ticker ticker)
{    return new Stopwatch(ticker).start();}
public boolean flume_f2694_0()
{    return isRunning;}
public Stopwatch flume_f2695_0()
{    checkState(!isRunning, "This stopwatch is already running.");    isRunning = true;    startTick = ticker.read();    return this;}
public Stopwatch flume_f2696_0()
{    long tick = ticker.read();    checkState(isRunning, "This stopwatch is already stopped.");    isRunning = false;    elapsedNanos += tick - startTick;    return this;}
public Stopwatch flume_f2697_0()
{    elapsedNanos = 0;    isRunning = false;    return this;}
private long flume_f2698_0()
{    return isRunning ? ticker.read() - startTick + elapsedNanos : elapsedNanos;}
public long flume_f2699_0(TimeUnit desiredUnit)
{    return desiredUnit.convert(elapsedNanos(), NANOSECONDS);}
public String flume_f2700_0()
{    long nanos = elapsedNanos();    TimeUnit unit = chooseUnit(nanos);    double value = (double) nanos / NANOSECONDS.convert(1, unit);        return String.format("%.4g %s", value, abbreviate(unit));}
private static TimeUnit flume_f2701_0(long nanos)
{    if (DAYS.convert(nanos, NANOSECONDS) > 0) {        return DAYS;    }    if (HOURS.convert(nanos, NANOSECONDS) > 0) {        return HOURS;    }    if (MINUTES.convert(nanos, NANOSECONDS) > 0) {        return MINUTES;    }    if (SECONDS.convert(nanos, NANOSECONDS) > 0) {        return SECONDS;    }    if (MILLISECONDS.convert(nanos, NANOSECONDS) > 0) {        return MILLISECONDS;    }    if (MICROSECONDS.convert(nanos, NANOSECONDS) > 0) {        return MICROSECONDS;    }    return NANOSECONDS;}
private static String flume_f2702_0(TimeUnit unit)
{    switch(unit) {        case NANOSECONDS:            return "ns";        case MICROSECONDS:                        return "\u03bcs";        case MILLISECONDS:            return "ms";        case SECONDS:            return "s";        case MINUTES:            return "min";        case HOURS:            return "h";        case DAYS:            return "d";        default:            throw new AssertionError();    }}
public static void flume_f2703_0(CountDownLatch latch)
{    boolean interrupted = false;    try {        while (true) {            try {                latch.await();                return;            } catch (InterruptedException e) {                interrupted = true;            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
public static boolean flume_f2704_0(CountDownLatch latch, long timeout, TimeUnit unit)
{    boolean interrupted = false;    try {        long remainingNanos = unit.toNanos(timeout);        long end = System.nanoTime() + remainingNanos;        while (true) {            try {                                return latch.await(remainingNanos, NANOSECONDS);            } catch (InterruptedException e) {                interrupted = true;                remainingNanos = end - System.nanoTime();            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
public static void flume_f2705_0(Thread toJoin)
{    boolean interrupted = false;    try {        while (true) {            try {                toJoin.join();                return;            } catch (InterruptedException e) {                interrupted = true;            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
public static void flume_f2706_0(Thread toJoin, long timeout, TimeUnit unit)
{    Preconditions.checkNotNull(toJoin);    boolean interrupted = false;    try {        long remainingNanos = unit.toNanos(timeout);        long end = System.nanoTime() + remainingNanos;        while (true) {            try {                                NANOSECONDS.timedJoin(toJoin, remainingNanos);                return;            } catch (InterruptedException e) {                interrupted = true;                remainingNanos = end - System.nanoTime();            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
public static V flume_f2707_0(Future<V> future) throws ExecutionException
{    boolean interrupted = false;    try {        while (true) {            try {                return future.get();            } catch (InterruptedException e) {                interrupted = true;            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
public static V flume_f2708_0(Future<V> future, long timeout, TimeUnit unit) throws ExecutionException, TimeoutException
{    boolean interrupted = false;    try {        long remainingNanos = unit.toNanos(timeout);        long end = System.nanoTime() + remainingNanos;        while (true) {            try {                                return future.get(remainingNanos, NANOSECONDS);            } catch (InterruptedException e) {                interrupted = true;                remainingNanos = end - System.nanoTime();            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
public static E flume_f2709_0(BlockingQueue<E> queue)
{    boolean interrupted = false;    try {        while (true) {            try {                return queue.take();            } catch (InterruptedException e) {                interrupted = true;            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
public static void flume_f2710_0(BlockingQueue<E> queue, E element)
{    boolean interrupted = false;    try {        while (true) {            try {                queue.put(element);                return;            } catch (InterruptedException e) {                interrupted = true;            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
public static void flume_f2711_0(long sleepFor, TimeUnit unit)
{    boolean interrupted = false;    try {        long remainingNanos = unit.toNanos(sleepFor);        long end = System.nanoTime() + remainingNanos;        while (true) {            try {                                NANOSECONDS.sleep(remainingNanos);                return;            } catch (InterruptedException e) {                interrupted = true;                remainingNanos = end - System.nanoTime();            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
public static boolean flume_f2712_0(Semaphore semaphore, long timeout, TimeUnit unit)
{    return tryAcquireUninterruptibly(semaphore, 1, timeout, unit);}
public static boolean flume_f2713_0(Semaphore semaphore, int permits, long timeout, TimeUnit unit)
{    boolean interrupted = false;    try {        long remainingNanos = unit.toNanos(timeout);        long end = System.nanoTime() + remainingNanos;        while (true) {            try {                                return semaphore.tryAcquire(permits, remainingNanos, NANOSECONDS);            } catch (InterruptedException e) {                interrupted = true;                remainingNanos = end - System.nanoTime();            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
public synchronized void flume_f2714_1()
{        executor = Executors.newSingleThreadScheduledExecutor();    File directory = new File(spoolDirectory);    try {        reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(directory).completedSuffix(completedSuffix).includePattern(includePattern).ignorePattern(ignorePattern).trackerDirPath(trackerDirPath).annotateFileName(fileHeader).fileNameHeader(fileHeaderKey).annotateBaseName(basenameHeader).baseNameHeader(basenameHeaderKey).deserializerType(deserializerType).deserializerContext(deserializerContext).deletePolicy(deletePolicy).inputCharset(inputCharset).decodeErrorPolicy(decodeErrorPolicy).consumeOrder(consumeOrder).recursiveDirectorySearch(recursiveDirectorySearch).trackingPolicy(trackingPolicy).sourceCounter(sourceCounter).build();    } catch (IOException ioe) {        throw new FlumeException("Error instantiating spooling event parser", ioe);    }    Runnable runner = new SpoolDirectoryRunnable(reader, sourceCounter);    executor.scheduleWithFixedDelay(runner, 0, pollDelay, TimeUnit.MILLISECONDS);    super.start();        sourceCounter.start();}
public synchronized void flume_f2715_1()
{    executor.shutdown();    try {        executor.awaitTermination(10L, TimeUnit.SECONDS);    } catch (InterruptedException ex) {            }    executor.shutdownNow();    super.stop();    sourceCounter.stop();    }
public String flume_f2716_0()
{    return "Spool Directory source " + getName() + ": { spoolDir: " + spoolDirectory + " }";}
public synchronized void flume_f2717_0(Context context)
{    spoolDirectory = context.getString(SPOOL_DIRECTORY);    Preconditions.checkState(spoolDirectory != null, "Configuration must specify a spooling directory");    completedSuffix = context.getString(SPOOLED_FILE_SUFFIX, DEFAULT_SPOOLED_FILE_SUFFIX);    deletePolicy = context.getString(DELETE_POLICY, DEFAULT_DELETE_POLICY);    fileHeader = context.getBoolean(FILENAME_HEADER, DEFAULT_FILE_HEADER);    fileHeaderKey = context.getString(FILENAME_HEADER_KEY, DEFAULT_FILENAME_HEADER_KEY);    basenameHeader = context.getBoolean(BASENAME_HEADER, DEFAULT_BASENAME_HEADER);    basenameHeaderKey = context.getString(BASENAME_HEADER_KEY, DEFAULT_BASENAME_HEADER_KEY);    batchSize = context.getInteger(BATCH_SIZE, DEFAULT_BATCH_SIZE);    inputCharset = context.getString(INPUT_CHARSET, DEFAULT_INPUT_CHARSET);    decodeErrorPolicy = DecodeErrorPolicy.valueOf(context.getString(DECODE_ERROR_POLICY, DEFAULT_DECODE_ERROR_POLICY).toUpperCase(Locale.ENGLISH));    includePattern = context.getString(INCLUDE_PAT, DEFAULT_INCLUDE_PAT);    ignorePattern = context.getString(IGNORE_PAT, DEFAULT_IGNORE_PAT);    trackerDirPath = context.getString(TRACKER_DIR, DEFAULT_TRACKER_DIR);    deserializerType = context.getString(DESERIALIZER, DEFAULT_DESERIALIZER);    deserializerContext = new Context(context.getSubProperties(DESERIALIZER + "."));    consumeOrder = ConsumeOrder.valueOf(context.getString(CONSUME_ORDER, DEFAULT_CONSUME_ORDER.toString()).toUpperCase(Locale.ENGLISH));    pollDelay = context.getInteger(POLL_DELAY, DEFAULT_POLL_DELAY);    recursiveDirectorySearch = context.getBoolean(RECURSIVE_DIRECTORY_SEARCH, DEFAULT_RECURSIVE_DIRECTORY_SEARCH);            Integer bufferMaxLineLength = context.getInteger(BUFFER_MAX_LINE_LENGTH);    if (bufferMaxLineLength != null && deserializerType != null && deserializerType.equalsIgnoreCase(DEFAULT_DESERIALIZER)) {        deserializerContext.put(LineDeserializer.MAXLINE_KEY, bufferMaxLineLength.toString());    }    maxBackoff = context.getInteger(MAX_BACKOFF, DEFAULT_MAX_BACKOFF);    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }    trackingPolicy = context.getString(TRACKING_POLICY, DEFAULT_TRACKING_POLICY);}
protected boolean flume_f2718_0()
{    return hasFatalError;}
protected void flume_f2719_0(boolean backoff)
{    this.backoff = backoff;}
protected boolean flume_f2720_0()
{    return hitChannelException;}
protected boolean flume_f2721_0()
{    return hitChannelFullException;}
protected SourceCounter flume_f2722_0()
{    return sourceCounter;}
protected boolean flume_f2723_0()
{    return recursiveDirectorySearch;}
public long flume_f2724_0()
{    return batchSize;}
public void flume_f2725_1()
{    int backoffInterval = 250;    boolean readingEvents = false;    try {        while (!Thread.interrupted()) {            readingEvents = true;            List<Event> events = reader.readEvents(batchSize);            readingEvents = false;            if (events.isEmpty()) {                break;            }            sourceCounter.addToEventReceivedCount(events.size());            sourceCounter.incrementAppendBatchReceivedCount();            try {                getChannelProcessor().processEventBatch(events);                reader.commit();            } catch (ChannelFullException ex) {                                sourceCounter.incrementChannelWriteFail();                hitChannelFullException = true;                backoffInterval = waitAndGetNewBackoffInterval(backoffInterval);                continue;            } catch (ChannelException ex) {                                sourceCounter.incrementChannelWriteFail();                hitChannelException = true;                backoffInterval = waitAndGetNewBackoffInterval(backoffInterval);                continue;            }            backoffInterval = 250;            sourceCounter.addToEventAcceptedCount(events.size());            sourceCounter.incrementAppendBatchAcceptedCount();        }    } catch (Throwable t) {                if (readingEvents) {            sourceCounter.incrementEventReadFail();        } else {            sourceCounter.incrementGenericProcessingFail();        }        hasFatalError = true;        Throwables.propagate(t);    }}
private int flume_f2726_0(int backoffInterval) throws InterruptedException
{    if (backoff) {        TimeUnit.MILLISECONDS.sleep(backoffInterval);        backoffInterval = backoffInterval << 1;        backoffInterval = backoffInterval >= maxBackoff ? maxBackoff : backoffInterval;    }    return backoffInterval;}
public String flume_f2727_0()
{    return keystore;}
public String flume_f2728_0()
{    return keystorePassword;}
public String flume_f2729_0()
{    return keystoreType;}
public Set<String> flume_f2730_0()
{    return excludeProtocols;}
public Set<String> flume_f2731_0()
{    return includeProtocols;}
public Set<String> flume_f2732_0()
{    return excludeCipherSuites;}
public Set<String> flume_f2733_0()
{    return includeCipherSuites;}
public boolean flume_f2734_0()
{    return sslEnabled;}
protected void flume_f2735_0(Context context)
{    sslEnabled = context.getBoolean(SSL_ENABLED_KEY, SSL_ENABLED_DEFAULT_VALUE);    keystore = context.getString(KEYSTORE_KEY, SSLUtil.getGlobalKeystorePath());    keystorePassword = context.getString(KEYSTORE_PASSWORD_KEY, SSLUtil.getGlobalKeystorePassword());    keystoreType = context.getString(KEYSTORE_TYPE_KEY, SSLUtil.getGlobalKeystoreType(KEYSTORE_TYPE_DEFAULT_VALUE));    parseList(context.getString(EXCLUDE_PROTOCOLS, SSLUtil.getGlobalExcludeProtocols()), excludeProtocols);    parseList(context.getString(INCLUDE_PROTOCOLS, SSLUtil.getGlobalIncludeProtocols()), includeProtocols);    parseList(context.getString(EXCLUDE_CIPHER_SUITES, SSLUtil.getGlobalExcludeCipherSuites()), excludeCipherSuites);    parseList(context.getString(INCLUDE_CIPHER_SUITES, SSLUtil.getGlobalIncludeCipherSuites()), includeCipherSuites);    if (sslEnabled) {        Objects.requireNonNull(keystore, KEYSTORE_KEY + " must be specified when SSL is enabled");        Objects.requireNonNull(keystorePassword, KEYSTORE_PASSWORD_KEY + " must be specified when SSL is enabled");        try {            KeyStore ks = KeyStore.getInstance(keystoreType);            ks.load(new FileInputStream(keystore), keystorePassword.toCharArray());        } catch (Exception ex) {            throw new FlumeException("Source " + getName() + " configured with invalid keystore: " + keystore, ex);        }    }}
private Optional<SSLContext> flume_f2736_0()
{    if (sslEnabled) {        try {            KeyStore ks = KeyStore.getInstance(keystoreType);            ks.load(new FileInputStream(keystore), keystorePassword.toCharArray());                        String algorithm = KeyManagerFactory.getDefaultAlgorithm();                        KeyManagerFactory kmf = KeyManagerFactory.getInstance(algorithm);            kmf.init(ks, keystorePassword.toCharArray());            SSLContext serverContext = SSLContext.getInstance("TLS");            serverContext.init(kmf.getKeyManagers(), null, null);            return Optional.of(serverContext);        } catch (Exception e) {            throw new Error("Failed to initialize the server-side SSLContext", e);        }    } else {        return Optional.empty();    }}
private Optional<SSLEngine> flume_f2737_0(boolean useClientMode)
{    return getSslContext().map(sslContext -> {        SSLEngine sslEngine = sslContext.createSSLEngine();        sslEngine.setUseClientMode(useClientMode);        sslEngine.setEnabledProtocols(getFilteredProtocols(sslEngine.getEnabledProtocols()));        sslEngine.setEnabledCipherSuites(getFilteredCipherSuites(sslEngine.getEnabledCipherSuites()));        return sslEngine;    });}
protected Supplier<Optional<SSLContext>> flume_f2738_0()
{    return this::getSslContext;}
protected Supplier<Optional<SSLEngine>> flume_f2739_0(boolean useClientMode)
{    return () -> getSslEngine(useClientMode);}
protected String[] flume_f2740_0(SSLParameters sslParameters)
{    return getFilteredProtocols(sslParameters.getProtocols());}
private String[] flume_f2741_0(String[] enabledProtocols)
{    return Stream.of(enabledProtocols).filter(o -> includeProtocols.isEmpty() || includeProtocols.contains(o)).filter(o -> !excludeProtocols.contains(o)).toArray(String[]::new);}
protected String[] flume_f2742_0(SSLParameters sslParameters)
{    return getFilteredCipherSuites(sslParameters.getCipherSuites());}
private String[] flume_f2743_0(String[] enabledCipherSuites)
{    return Stream.of(enabledCipherSuites).filter(o -> includeCipherSuites.isEmpty() || includeCipherSuites.contains(o)).filter(o -> !excludeCipherSuites.contains(o)).toArray(String[]::new);}
private void flume_f2744_0(String value, Set<String> set)
{    if (Objects.nonNull(value)) {        set.addAll(Arrays.asList(value.split(" ")));    }}
protected void flume_f2745_0(Context context) throws FlumeException
{    /* Limit on the total number of events. */    maxTotalEvents = context.getLong("maxTotalEvents", -1L);    /* Limit on the total number of successful events. */    maxSuccessfulEvents = context.getLong("maxSuccessfulEvents", -1L);    /* Set max events in a batch submission */    batchSize = context.getInteger("batchSize", 1);    /* Size of events to be generated. */    int size = context.getInteger("size", 500);    int rateLimit = context.getInteger("maxEventsPerSecond", 0);    if (rateLimit > 0) {        limiter = RateLimiter.create(rateLimit);    } else {        limiter = null;    }    prepEventData(size);}
private void flume_f2746_0(int bufferSize)
{    buffer = new byte[bufferSize];    Arrays.fill(buffer, Byte.MAX_VALUE);    if (batchSize > 1) {                eventBatchList = new ArrayList<Event>();        for (int i = 0; i < batchSize; i++) {            eventBatchList.add(EventBuilder.withBody(buffer));        }    } else {                event = EventBuilder.withBody(buffer);    }}
protected Status flume_f2747_0() throws EventDeliveryException
{    long totalEventSent = counterGroup.addAndGet("events.total", lastSent);    if ((maxTotalEvents >= 0 && totalEventSent >= maxTotalEvents) || (maxSuccessfulEvents >= 0 && counterGroup.get("events.successful") >= maxSuccessfulEvents)) {        return Status.BACKOFF;    }    try {        lastSent = batchSize;        if (batchSize == 1) {            if (limiter != null) {                limiter.acquire();            }            getChannelProcessor().processEvent(event);        } else {            long eventsLeft = maxTotalEvents - totalEventSent;            if (maxTotalEvents >= 0 && eventsLeft < batchSize) {                eventBatchListToProcess = eventBatchList.subList(0, (int) eventsLeft);            } else {                eventBatchListToProcess = eventBatchList;            }            lastSent = eventBatchListToProcess.size();            if (limiter != null) {                                limiter.acquire((int) lastSent);            }            getChannelProcessor().processEventBatch(eventBatchListToProcess);        }        counterGroup.addAndGet("events.successful", lastSent);    } catch (ChannelException ex) {        counterGroup.addAndGet("events.failed", lastSent);        return Status.BACKOFF;    }    return Status.READY;}
protected void flume_f2748_1() throws FlumeException
{    }
protected void flume_f2749_1() throws FlumeException
{    }
public long flume_f2750_0()
{    return batchSize;}
public Long flume_f2751_0(String key) throws Exception
{    return timeParser.parseMillis(key);}
public Event flume_f2752_0(String msg, Charset charset, Set<String> keepFields)
{    Map<String, String> headers = Maps.newHashMap();    int msgLen = msg.length();    int curPos = 0;    Preconditions.checkArgument(msg.charAt(curPos) == '<', "Bad format: invalid priority: cannot find open bracket '<' (%s)", msg);    int endBracketPos = msg.indexOf('>');    Preconditions.checkArgument(endBracketPos > 0 && endBracketPos <= 6, "Bad format: invalid priority: cannot find end bracket '>' (%s)", msg);    String priority = msg.substring(1, endBracketPos);    int pri = Integer.parseInt(priority);    int facility = pri / 8;    int severity = pri % 8;        headers.put(SyslogUtils.SYSLOG_PRIORITY, priority);        headers.put(SyslogUtils.SYSLOG_FACILITY, String.valueOf(facility));    headers.put(SyslogUtils.SYSLOG_SEVERITY, String.valueOf(severity));    Preconditions.checkArgument(msgLen > endBracketPos + 1, "Bad format: no data except priority (%s)", msg);        curPos = endBracketPos + 1;        String version = null;    if (msgLen > curPos + 2 && "1 ".equals(msg.substring(curPos, curPos + 2))) {        version = msg.substring(curPos, curPos + 1);        headers.put(SyslogUtils.SYSLOG_VERSION, version);        curPos += 2;    }        long ts;    String tsString;    char dateStartChar = msg.charAt(curPos);    try {                if (dateStartChar == '-') {            tsString = Character.toString(dateStartChar);            ts = System.currentTimeMillis();            if (msgLen <= curPos + 2) {                throw new IllegalArgumentException("bad syslog format (missing hostname)");            }                        curPos += 2;                } else if (dateStartChar >= 'A' && dateStartChar <= 'Z') {            if (msgLen <= curPos + RFC3164_LEN) {                throw new IllegalArgumentException("bad timestamp format");            }            tsString = msg.substring(curPos, curPos + RFC3164_LEN);            ts = parseRfc3164Time(tsString);            curPos += RFC3164_LEN + 1;                } else {            int nextSpace = msg.indexOf(' ', curPos);            if (nextSpace == -1) {                throw new IllegalArgumentException("bad timestamp format");            }            tsString = msg.substring(curPos, nextSpace);            ts = parseRfc5424Date(tsString);            curPos = nextSpace + 1;        }    } catch (IllegalArgumentException ex) {        throw new IllegalArgumentException("Unable to parse message: " + msg, ex);    }    headers.put("timestamp", String.valueOf(ts));        int nextSpace = msg.indexOf(' ', curPos);    if (nextSpace == -1) {        throw new IllegalArgumentException("bad syslog format (missing hostname)");    }            String hostname = new String(msg.substring(curPos, nextSpace));    headers.put("host", hostname);        String data = "";    if (msgLen > nextSpace + 1 && !SyslogUtils.keepAllFields(keepFields)) {        curPos = nextSpace + 1;        data = msg.substring(curPos);        data = SyslogUtils.addFieldsToBody(keepFields, data, priority, version, tsString, hostname);    } else {        data = msg;    }    Event event = EventBuilder.withBody(data, charset, headers);    return event;}
protected long flume_f2753_0(String msg)
{    Long ts = null;    int curPos = 0;    int msgLen = msg.length();    Preconditions.checkArgument(msgLen > RFC5424_PREFIX_LEN, "Bad format: Not a valid RFC5424 timestamp: %s", msg);    String timestampPrefix = msg.substring(curPos, RFC5424_PREFIX_LEN);    try {        ts = timestampCache.get(timestampPrefix);    } catch (ExecutionException ex) {        throw new IllegalArgumentException("bad timestamp format", ex);    }    curPos += RFC5424_PREFIX_LEN;    Preconditions.checkArgument(ts != null, "Parsing error: timestamp is null");        if (msg.charAt(curPos) == '.') {                boolean foundEnd = false;        int endMillisPos = curPos + 1;        if (msgLen <= endMillisPos) {            throw new IllegalArgumentException("bad timestamp format (no TZ)");        }                while (!foundEnd) {            char curDigit = msg.charAt(endMillisPos);            if (curDigit >= '0' && curDigit <= '9') {                endMillisPos++;            } else {                foundEnd = true;            }        }                final int fractionalPositions = endMillisPos - (curPos + 1);        if (fractionalPositions > 0) {            long milliseconds = Long.parseLong(msg.substring(curPos + 1, endMillisPos));            if (fractionalPositions > 3) {                milliseconds /= Math.pow(10, (fractionalPositions - 3));            } else if (fractionalPositions < 3) {                milliseconds *= Math.pow(10, (3 - fractionalPositions));            }            ts += milliseconds;        } else {            throw new IllegalArgumentException("Bad format: Invalid timestamp (fractional portion): " + msg);        }        curPos = endMillisPos;    }        char tzFirst = msg.charAt(curPos);        if (tzFirst == 'Z') {        } else if (tzFirst == '+' || tzFirst == '-') {        Preconditions.checkArgument(msgLen > curPos + 5, "Bad format: Invalid timezone (%s)", msg);        int polarity;        if (tzFirst == '+') {            polarity = +1;        } else {            polarity = -1;        }        char[] h = new char[5];        for (int i = 0; i < 5; i++) {            h[i] = msg.charAt(curPos + 1 + i);        }        if (h[0] >= '0' && h[0] <= '9' && h[1] >= '0' && h[1] <= '9' && h[2] == ':' && h[3] >= '0' && h[3] <= '9' && h[4] >= '0' && h[4] <= '9') {            int hourOffset = Integer.parseInt(msg.substring(curPos + 1, curPos + 3));            int minOffset = Integer.parseInt(msg.substring(curPos + 4, curPos + 6));            ts -= polarity * ((hourOffset * 60) + minOffset) * 60000;        } else {            throw new IllegalArgumentException("Bad format: Invalid timezone: " + msg);        }    }    return ts;}
protected long flume_f2754_1(String ts)
{    DateTime now = DateTime.now();    int year = now.getYear();    ts = TWO_SPACES.matcher(ts).replaceFirst(" ");    DateTime date;    try {        date = rfc3164Format.parseDateTime(ts);    } catch (IllegalArgumentException e) {                return 0;    }    if (date != null) {        DateTime fixed = date.withYear(year);                if (fixed.isAfter(now) && fixed.minusMonths(1).isAfter(now)) {            fixed = date.minusYears(1);                } else if (fixed.isBefore(now) && fixed.plusMonths(1).isBefore(now)) {            fixed = date.plusYears(1);        }        date = fixed;    }    if (date == null) {        return 0;    }    return date.getMillis();}
public void flume_f2755_0(int eventSize)
{    syslogUtils.setEventSize(eventSize);}
public void flume_f2756_0(Set<String> keepFields)
{    syslogUtils.setKeepFields(keepFields);}
public void flume_f2757_0(Map<String, String> prop)
{    syslogUtils.addFormats(prop);}
public void flume_f2758_0(String clientIPHeader)
{    this.clientIPHeader = clientIPHeader;}
public void flume_f2759_0(String clientHostnameHeader)
{    this.clientHostnameHeader = clientHostnameHeader;}
public void flume_f2760_1(ChannelHandlerContext ctx, MessageEvent mEvent)
{    ChannelBuffer buff = (ChannelBuffer) mEvent.getMessage();    while (buff.readable()) {        Event e = syslogUtils.extractEvent(buff);        if (e == null) {                        continue;        }        if (clientIPHeader != null) {            e.getHeaders().put(clientIPHeader, SyslogUtils.getIP(ctx.getChannel().getRemoteAddress()));        }        if (clientHostnameHeader != null) {            e.getHeaders().put(clientHostnameHeader, SyslogUtils.getHostname(ctx.getChannel().getRemoteAddress()));        }        sourceCounter.incrementEventReceivedCount();        try {            getChannelProcessor().processEvent(e);            sourceCounter.incrementEventAcceptedCount();        } catch (ChannelException ex) {                        sourceCounter.incrementChannelWriteFail();        } catch (RuntimeException ex) {                        sourceCounter.incrementEventReadFail();            return;        }    }}
public void flume_f2761_1()
{    ChannelFactory factory = new NioServerSocketChannelFactory(Executors.newCachedThreadPool(), Executors.newCachedThreadPool());    ServerBootstrap serverBootstrap = new ServerBootstrap(factory);    serverBootstrap.setPipelineFactory(new PipelineFactory(eventSize, formaterProp, keepFields, clientIPHeader, clientHostnameHeader, getSslEngineSupplier(false)));        if (host == null) {        nettyChannel = serverBootstrap.bind(new InetSocketAddress(port));    } else {        nettyChannel = serverBootstrap.bind(new InetSocketAddress(host, port));    }    sourceCounter.start();    super.start();}
public void flume_f2762_1()
{            if (nettyChannel != null) {        nettyChannel.close();        try {            nettyChannel.getCloseFuture().await(60, TimeUnit.SECONDS);        } catch (InterruptedException e) {                    } finally {            nettyChannel = null;        }    }    sourceCounter.stop();    super.stop();}
public void flume_f2763_0(Context context)
{    configureSsl(context);    Configurables.ensureRequiredNonNull(context, SyslogSourceConfigurationConstants.CONFIG_PORT);    port = context.getInteger(SyslogSourceConfigurationConstants.CONFIG_PORT);    host = context.getString(SyslogSourceConfigurationConstants.CONFIG_HOST);    eventSize = context.getInteger("eventSize", SyslogUtils.DEFAULT_SIZE);    formaterProp = context.getSubProperties(SyslogSourceConfigurationConstants.CONFIG_FORMAT_PREFIX);    keepFields = SyslogUtils.chooseFieldsToKeep(context.getString(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS, SyslogSourceConfigurationConstants.DEFAULT_KEEP_FIELDS));    clientIPHeader = context.getString(SyslogSourceConfigurationConstants.CONFIG_CLIENT_IP_HEADER);    clientHostnameHeader = context.getString(SyslogSourceConfigurationConstants.CONFIG_CLIENT_HOSTNAME_HEADER);    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
 InetSocketAddress flume_f2764_0()
{    SocketAddress localAddress = nettyChannel.getLocalAddress();    if (!(localAddress instanceof InetSocketAddress)) {        throw new IllegalArgumentException("Not bound to an internet address");    }    return (InetSocketAddress) localAddress;}
 SourceCounter flume_f2765_0()
{    return sourceCounter;}
public ChannelPipeline flume_f2766_0()
{    syslogTcpHandler handler = new syslogTcpHandler();    handler.setEventSize(eventSize);    handler.setFormater(formaterProp);    handler.setKeepFields(keepFields);    handler.setClientIPHeader(clientIPHeader);    handler.setClientHostnameHeader(clientHostnameHeader);    ChannelPipeline pipeline = Channels.pipeline(handler);    sslEngineSupplier.get().ifPresent(sslEngine -> {        pipeline.addFirst("ssl", new SslHandler(sslEngine));    });    return pipeline;}
public void flume_f2767_0(Map<String, String> prop)
{    syslogUtils.addFormats(prop);}
public void flume_f2768_0(Set<String> keepFields)
{    syslogUtils.setKeepFields(keepFields);}
public void flume_f2769_0(String clientIPHeader)
{    this.clientIPHeader = clientIPHeader;}
public void flume_f2770_0(String clientHostnameHeader)
{    this.clientHostnameHeader = clientHostnameHeader;}
public void flume_f2771_1(ChannelHandlerContext ctx, MessageEvent mEvent)
{    try {        syslogUtils.setEventSize(maxsize);        Event e = syslogUtils.extractEvent((ChannelBuffer) mEvent.getMessage());        if (e == null) {            return;        }        if (clientIPHeader != null) {            e.getHeaders().put(clientIPHeader, SyslogUtils.getIP(mEvent.getRemoteAddress()));        }        if (clientHostnameHeader != null) {            e.getHeaders().put(clientHostnameHeader, SyslogUtils.getHostname(mEvent.getRemoteAddress()));        }        sourceCounter.incrementEventReceivedCount();        getChannelProcessor().processEvent(e);        sourceCounter.incrementEventAcceptedCount();    } catch (ChannelException ex) {                sourceCounter.incrementChannelWriteFail();        return;    } catch (RuntimeException ex) {                sourceCounter.incrementEventReadFail();        return;    }}
public void flume_f2772_0()
{        ConnectionlessBootstrap serverBootstrap = new ConnectionlessBootstrap(new OioDatagramChannelFactory(Executors.newCachedThreadPool()));    final syslogHandler handler = new syslogHandler();    handler.setFormater(formaterProp);    handler.setKeepFields(keepFields);    handler.setClientIPHeader(clientIPHeader);    handler.setClientHostnameHeader(clientHostnameHeader);    serverBootstrap.setOption("receiveBufferSizePredictorFactory", new AdaptiveReceiveBufferSizePredictorFactory(DEFAULT_MIN_SIZE, DEFAULT_INITIAL_SIZE, maxsize));    serverBootstrap.setPipelineFactory(new ChannelPipelineFactory() {        @Override        public ChannelPipeline getPipeline() {            return Channels.pipeline(handler);        }    });    if (host == null) {        nettyChannel = serverBootstrap.bind(new InetSocketAddress(port));    } else {        nettyChannel = serverBootstrap.bind(new InetSocketAddress(host, port));    }    sourceCounter.start();    super.start();}
public ChannelPipeline flume_f2773_0()
{    return Channels.pipeline(handler);}
public void flume_f2774_1()
{            if (nettyChannel != null) {        nettyChannel.close();        try {            nettyChannel.getCloseFuture().await(60, TimeUnit.SECONDS);        } catch (InterruptedException e) {                    } finally {            nettyChannel = null;        }    }    sourceCounter.stop();    super.stop();}
public void flume_f2775_0(Context context)
{    Configurables.ensureRequiredNonNull(context, SyslogSourceConfigurationConstants.CONFIG_PORT);    port = context.getInteger(SyslogSourceConfigurationConstants.CONFIG_PORT);    host = context.getString(SyslogSourceConfigurationConstants.CONFIG_HOST);    formaterProp = context.getSubProperties(SyslogSourceConfigurationConstants.CONFIG_FORMAT_PREFIX);    keepFields = SyslogUtils.chooseFieldsToKeep(context.getString(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS, SyslogSourceConfigurationConstants.DEFAULT_KEEP_FIELDS));    clientIPHeader = context.getString(SyslogSourceConfigurationConstants.CONFIG_CLIENT_IP_HEADER);    clientHostnameHeader = context.getString(SyslogSourceConfigurationConstants.CONFIG_CLIENT_HOSTNAME_HEADER);    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
 InetSocketAddress flume_f2776_0()
{    SocketAddress localAddress = nettyChannel.getLocalAddress();    if (!(localAddress instanceof InetSocketAddress)) {        throw new IllegalArgumentException("Not bound to an internet address");    }    return (InetSocketAddress) localAddress;}
 SourceCounter flume_f2777_0()
{    return sourceCounter;}
public static boolean flume_f2778_0(Set<String> keepFields)
{    if (keepFields == null) {        return false;    }    return keepFields.contains(KEEP_FIELDS_ALL);}
public static Set<String> flume_f2779_0(String keepFields)
{    if (keepFields == null) {        return null;    }    keepFields = keepFields.trim().toLowerCase(Locale.ENGLISH);    if (keepFields.equals("false") || keepFields.equals("none")) {        return null;    }    if (keepFields.equals("true") || keepFields.equals("all")) {        Set<String> fieldsToKeep = new HashSet<String>(1);        fieldsToKeep.add(KEEP_FIELDS_ALL);        return fieldsToKeep;    }    Set<String> fieldsToKeep = new HashSet<String>(DEFAULT_FIELDS_TO_KEEP.length);    for (String field : DEFAULT_FIELDS_TO_KEEP) {        if (keepFields.indexOf(field) != -1) {            fieldsToKeep.add(field);        }    }    return fieldsToKeep;}
public static String flume_f2780_0(Set<String> keepFields, String body, String priority, String version, String timestamp, String hostname)
{        if (keepFields != null) {        if (keepFields.contains(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS_HOSTNAME)) {            body = hostname + " " + body;        }        if (keepFields.contains(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS_TIMESTAMP)) {            body = timestamp + " " + body;        }        if (keepFields.contains(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS_VERSION)) {            if (version != null && !version.isEmpty()) {                body = version + " " + body;            }        }        if (keepFields.contains(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS_PRIORITY)) {            body = "<" + priority + ">" + body;        }    }    return body;}
public static String flume_f2781_1(SocketAddress socketAddress)
{    try {        InetSocketAddress inetSocketAddress = (InetSocketAddress) socketAddress;        String ip = inetSocketAddress.getAddress().getHostAddress();        if (ip != null) {            return ip;        } else {            throw new NullPointerException("The returned IP is null");        }    } catch (Exception e) {            }        return "";}
public static String flume_f2782_1(SocketAddress socketAddress)
{    try {        InetSocketAddress inetSocketAddress = (InetSocketAddress) socketAddress;        String hostname = inetSocketAddress.getHostName();        if (hostname != null) {            return hostname;        } else {            throw new NullPointerException("The returned hostname is null");        }    } catch (Exception e) {            }        return "";}
public void flume_f2783_0(Map<String, String> formatProp)
{    if (formatProp.isEmpty() || !formatProp.containsKey(SyslogSourceConfigurationConstants.CONFIG_REGEX)) {        return;    }    SyslogFormatter fmt1 = new SyslogFormatter();    fmt1.regexPattern = Pattern.compile(formatProp.get(SyslogSourceConfigurationConstants.CONFIG_REGEX));    if (formatProp.containsKey(SyslogSourceConfigurationConstants.CONFIG_SEARCH)) {        fmt1.searchPattern.add(formatProp.get(SyslogSourceConfigurationConstants.CONFIG_SEARCH));    }    if (formatProp.containsKey(SyslogSourceConfigurationConstants.CONFIG_REPLACE)) {        fmt1.replacePattern.add(formatProp.get(SyslogSourceConfigurationConstants.CONFIG_REPLACE));    }    if (formatProp.containsKey(SyslogSourceConfigurationConstants.CONFIG_DATEFORMAT)) {        fmt1.dateFormat.add(new SimpleDateFormat(formatProp.get(SyslogSourceConfigurationConstants.CONFIG_DATEFORMAT)));    }    formats.add(0, fmt1);}
private void flume_f2784_0()
{        SyslogFormatter fmt1 = new SyslogFormatter();    fmt1.regexPattern = Pattern.compile(SYSLOG_MSG_RFC5424_0);        fmt1.searchPattern.add("Z");    fmt1.replacePattern.add("+0000");        fmt1.searchPattern.add("([+-])(\\d{2})[:](\\d{2})");    fmt1.replacePattern.add("$1$2$3");        fmt1.searchPattern.add("(T\\d{2}:\\d{2}:\\d{2}\\.\\d{3})(\\d*)");    fmt1.replacePattern.add("$1");    fmt1.dateFormat.add(new SimpleDateFormat(SYSLOG_TIMESTAMP_FORMAT_RFC5424_1, Locale.ENGLISH));    fmt1.dateFormat.add(new SimpleDateFormat(SYSLOG_TIMESTAMP_FORMAT_RFC5424_2, Locale.ENGLISH));    fmt1.dateFormat.add(new SimpleDateFormat(SYSLOG_TIMESTAMP_FORMAT_RFC5424_3, Locale.ENGLISH));    fmt1.dateFormat.add(new SimpleDateFormat(SYSLOG_TIMESTAMP_FORMAT_RFC5424_4, Locale.ENGLISH));    fmt1.addYear = false;        SyslogFormatter fmt2 = new SyslogFormatter();    fmt2.regexPattern = Pattern.compile(SYSLOG_MSG_RFC3164_0);        fmt2.searchPattern.add("  ");    fmt2.replacePattern.add(" ");    fmt2.dateFormat.add(new SimpleDateFormat(SYSLOG_TIMESTAMP_FORMAT_RFC3164_1, Locale.ENGLISH));    fmt2.addYear = true;    formats.add(fmt1);    formats.add(fmt2);}
public String flume_f2785_0()
{    return this.syslogStatus;}
 Event flume_f2786_1()
{    try {        byte[] body;        int pri = 0;        int sev = 0;        int facility = 0;        if (!isBadEvent) {            pri = Integer.parseInt(prio.toString());            sev = pri % 8;            facility = pri / 8;            formatHeaders();        }        Map<String, String> headers = new HashMap<String, String>();        headers.put(SYSLOG_FACILITY, String.valueOf(facility));        headers.put(SYSLOG_SEVERITY, String.valueOf(sev));        if ((priority != null) && (priority.length() > 0)) {            headers.put("priority", priority);        }        if ((version != null) && (version.length() > 0)) {            headers.put("version", version);        }        if ((timeStamp != null) && timeStamp.length() > 0) {            headers.put("timestamp", timeStamp);        }        if ((hostName != null) && (hostName.length() > 0)) {            headers.put("host", hostName);        }        if (isBadEvent) {                        headers.put(EVENT_STATUS, SyslogStatus.INVALID.getSyslogStatus());        } else if (isIncompleteEvent) {                        headers.put(EVENT_STATUS, SyslogStatus.INCOMPLETE.getSyslogStatus());        }        if (!keepAllFields(keepFields)) {            if ((msgBody != null) && (msgBody.length() > 0)) {                body = msgBody.getBytes();            } else {                                body = baos.toByteArray();            }        } else {            body = baos.toByteArray();        }                return EventBuilder.withBody(body, headers);    } finally {        reset();    }}
private void flume_f2787_0()
{    String eventStr = baos.toString();    String timeStampString = null;    for (int p = 0; p < formats.size(); p++) {        SyslogFormatter fmt = formats.get(p);        Pattern pattern = fmt.regexPattern;        Matcher matcher = pattern.matcher(eventStr);        if (!matcher.matches()) {            continue;        }        MatchResult res = matcher.toMatchResult();        for (int grp = 1; grp <= res.groupCount(); grp++) {            String value = res.group(grp);            if (grp == SYSLOG_TIMESTAMP_POS) {                timeStampString = value;                                if (value != null) {                    for (int sp = 0; sp < fmt.searchPattern.size(); sp++) {                        value = value.replaceAll(fmt.searchPattern.get(sp), fmt.replacePattern.get(sp));                    }                                        if (fmt.addYear) {                        value = clock.instant().atOffset(ZoneOffset.UTC).get(ChronoField.YEAR) + value;                    }                                        for (int dt = 0; dt < fmt.dateFormat.size(); dt++) {                        try {                            Date parsedDate = fmt.dateFormat.get(dt).parse(value);                            /*                 * Some code to try and add some smarts to the year insertion.                 * Original code just added the current year which was okay-ish, but around                 * January 1st becomes pretty naïve.                 * The current year is added above. This code, if the year has been added does                 * the following:                 * 1. Compute what the computed time, but one month in the past would be.                 * 2. Compute what the computed time, but eleven months in the future would be.                 * If the computed time is more than one month in the future then roll it back a                 * year. If the computed time is more than eleven months in the past then roll it                 * forward a year. This gives us a 12 month rolling window (11 months in the past,                 * 1 month in the future) of timestamps.                 */                            if (fmt.addYear) {                                Calendar calParsed = Calendar.getInstance();                                calParsed.setTime(parsedDate);                                Calendar calMinusOneMonth = Calendar.getInstance();                                calMinusOneMonth.setTime(parsedDate);                                calMinusOneMonth.add(Calendar.MONTH, -1);                                Calendar calPlusElevenMonths = Calendar.getInstance();                                calPlusElevenMonths.setTime(parsedDate);                                calPlusElevenMonths.add(Calendar.MONTH, +11);                                long currentTimeMillis = clock.millis();                                if (calParsed.getTimeInMillis() > currentTimeMillis && calMinusOneMonth.getTimeInMillis() > currentTimeMillis) {                                                                        Calendar c1 = Calendar.getInstance();                                    c1.setTime(parsedDate);                                    c1.add(Calendar.YEAR, -1);                                    parsedDate = c1.getTime();                                } else if (calParsed.getTimeInMillis() < currentTimeMillis && calPlusElevenMonths.getTimeInMillis() < currentTimeMillis) {                                                                        Calendar c1 = Calendar.getInstance();                                    c1.setTime(parsedDate);                                    c1.add(Calendar.YEAR, +1);                                    parsedDate = c1.getTime();                                }                            }                            timeStamp = String.valueOf(parsedDate.getTime());                                                        break;                        } catch (ParseException e) {                                                        continue;                        }                    }                }            } else if (grp == SYSLOG_HOSTNAME_POS) {                hostName = value;            } else if (grp == SYSLOG_PRIORITY_POS) {                priority = value;            } else if (grp == SYSLOG_VERSION_POS) {                version = value;            } else if (grp == SYSLOG_BODY_POS) {                msgBody = addFieldsToBody(keepFields, value, priority, version, timeStampString, hostName);            }        }                break;    }}
private void flume_f2788_0()
{    baos.reset();    m = Mode.START;    prio.delete(0, prio.length());    isBadEvent = false;    isIncompleteEvent = false;    hostName = null;    timeStamp = null;    msgBody = null;}
public Event flume_f2789_1(ChannelBuffer in)
{    /* for protocol debugging    ByteBuffer bb = in.toByteBuffer();    int remaining = bb.remaining();    byte[] buf = new byte[remaining];    bb.get(buf);    HexDump.dump(buf, 0, System.out, 0);    */    byte b = 0;    Event e = null;    boolean doneReading = false;    try {        while (!doneReading && in.readable()) {            b = in.readByte();            switch(m) {                case START:                    if (b == '<') {                        baos.write(b);                        m = Mode.PRIO;                    } else if (b == '\n') {                                                                                                                                                                                            } else {                        isBadEvent = true;                        baos.write(b);                                                m = Mode.DATA;                    }                    break;                case PRIO:                    baos.write(b);                    if (b == '>') {                        if (prio.length() == 0) {                            isBadEvent = true;                        }                        m = Mode.DATA;                    } else {                        char ch = (char) b;                        prio.append(ch);                                                if (!Character.isDigit(ch) || prio.length() > 3) {                            isBadEvent = true;                                                        m = Mode.DATA;                        }                    }                    break;                case DATA:                                        if (b == '\n') {                        e = buildEvent();                        doneReading = true;                    } else {                        baos.write(b);                    }                    if (baos.size() == this.maxSize && !doneReading) {                        isIncompleteEvent = true;                        e = buildEvent();                        doneReading = true;                    }                    break;            }        }                if (e == null && isUdp) {            doneReading = true;            e = buildEvent();        }    } finally {        }    return e;}
public Integer flume_f2790_0()
{    return maxSize;}
public void flume_f2791_0(Integer eventSize)
{    this.maxSize = eventSize;}
public void flume_f2792_0(Set<String> keepFields)
{    this.keepFields = keepFields;}
public void flume_f2793_1(Context context)
{    configureSsl(context);        port = context.getInteger(CONFIG_PORT);    Preconditions.checkNotNull(port, "Port must be specified for Thrift " + "Source.");    bindAddress = context.getString(CONFIG_BIND);    Preconditions.checkNotNull(bindAddress, "Bind address must be specified " + "for Thrift Source.");    try {        maxThreads = context.getInteger(CONFIG_THREADS, 0);        maxThreads = (maxThreads <= 0) ? Integer.MAX_VALUE : maxThreads;    } catch (NumberFormatException e) {            }    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }    protocol = context.getString(CONFIG_PROTOCOL);    if (protocol == null) {                protocol = COMPACT_PROTOCOL;    }    Preconditions.checkArgument((protocol.equalsIgnoreCase(BINARY_PROTOCOL) || protocol.equalsIgnoreCase(COMPACT_PROTOCOL)), "binary or compact are the only valid Thrift protocol types to " + "choose from.");    principal = context.getString(AGENT_PRINCIPAL);    String keytab = context.getString(AGENT_KEYTAB);    enableKerberos = context.getBoolean(KERBEROS_KEY, false);    this.flumeAuth = FlumeAuthenticationUtil.getAuthenticator(principal, keytab);    if (enableKerberos) {        if (!flumeAuth.isAuthenticated()) {            throw new FlumeException("Authentication failed in Kerberos mode for " + "principal " + principal + " keytab " + keytab);        }        flumeAuth.startCredentialRefresher();    }}
public void flume_f2794_1()
{            server = getTThreadedSelectorServer();        if (server == null) {        server = getTThreadPoolServer();    }    servingExecutor = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder().setNameFormat("Flume Thrift Source I/O Boss").build());    /**     * Start serving.     */    servingExecutor.submit(new Runnable() {        @Override        public void run() {            flumeAuth.execute(new PrivilegedAction<Object>() {                @Override                public Object run() {                    server.serve();                    return null;                }            });        }    });    long timeAfterStart = System.currentTimeMillis();    while (!server.isServing()) {        try {            if (System.currentTimeMillis() - timeAfterStart >= 10000) {                throw new FlumeException("Thrift server failed to start!");            }            TimeUnit.MILLISECONDS.sleep(1000);        } catch (InterruptedException e) {            Thread.currentThread().interrupt();            throw new FlumeException("Interrupted while waiting for Thrift server" + " to start.", e);        }    }    sourceCounter.start();        super.start();}
public void flume_f2795_0()
{    flumeAuth.execute(new PrivilegedAction<Object>() {        @Override        public Object run() {            server.serve();            return null;        }    });}
public Object flume_f2796_0()
{    server.serve();    return null;}
private TServerTransport flume_f2797_0()
{    try {        TServerTransport transport;        TSSLTransportFactory.TSSLTransportParameters params = new TSSLTransportFactory.TSSLTransportParameters();        params.setKeyStore(getKeystore(), getKeystorePassword(), KeyManagerFactory.getDefaultAlgorithm(), getKeystoreType());        transport = TSSLTransportFactory.getServerSocket(port, 120000, InetAddress.getByName(bindAddress), params);        ServerSocket serverSock = ((TServerSocket) transport).getServerSocket();        if (serverSock instanceof SSLServerSocket) {            SSLServerSocket sslServerSock = (SSLServerSocket) serverSock;            SSLParameters sslParameters = sslServerSock.getSSLParameters();            sslServerSock.setEnabledCipherSuites(getFilteredCipherSuites(sslParameters));            sslServerSock.setEnabledProtocols(getFilteredProtocols(sslParameters));        }        return transport;    } catch (Throwable throwable) {        throw new FlumeException("Cannot start Thrift source.", throwable);    }}
private TServerTransport flume_f2798_0()
{    try {        return new TServerSocket(new InetSocketAddress(bindAddress, port));    } catch (Throwable throwable) {        throw new FlumeException("Cannot start Thrift source.", throwable);    }}
private TProtocolFactory flume_f2799_1()
{    if (protocol.equals(BINARY_PROTOCOL)) {                return new TBinaryProtocol.Factory();    } else {                return new TCompactProtocol.Factory();    }}
private TServer flume_f2800_0()
{    if (isSslEnabled() || enableKerberos) {        return null;    }    Class<?> serverClass;    Class<?> argsClass;    TServer.AbstractServerArgs args;    try {        serverClass = Class.forName("org.apache.thrift" + ".server.TThreadedSelectorServer");        argsClass = Class.forName("org.apache.thrift" + ".server.TThreadedSelectorServer$Args");        TServerTransport serverTransport = new TNonblockingServerSocket(new InetSocketAddress(bindAddress, port));        ExecutorService sourceService;        ThreadFactory threadFactory = new ThreadFactoryBuilder().setNameFormat("Flume Thrift IPC Thread %d").build();        if (maxThreads == 0) {            sourceService = Executors.newCachedThreadPool(threadFactory);        } else {            sourceService = Executors.newFixedThreadPool(maxThreads, threadFactory);        }        args = (TNonblockingServer.AbstractNonblockingServerArgs) argsClass.getConstructor(TNonblockingServerTransport.class).newInstance(serverTransport);        Method m = argsClass.getDeclaredMethod("executorService", ExecutorService.class);        m.invoke(args, sourceService);        populateServerParams(args);        /*       * Both THsHaServer and TThreadedSelectorServer allows us to pass in       * the executor service to use - unfortunately the "executorService"       * method does not exist in the parent abstract Args class,       * so use reflection to pass the executor in.       *       */        server = (TServer) serverClass.getConstructor(argsClass).newInstance(args);    } catch (ClassNotFoundException e) {        return null;    } catch (Throwable ex) {        throw new FlumeException("Cannot start Thrift Source.", ex);    }    return server;}
private TServer flume_f2801_0()
{    TServerTransport serverTransport;    if (isSslEnabled()) {        serverTransport = getSSLServerTransport();    } else {        serverTransport = getTServerTransport();    }    TThreadPoolServer.Args serverArgs = new TThreadPoolServer.Args(serverTransport);    serverArgs.maxWorkerThreads(maxThreads);    populateServerParams(serverArgs);    return new TThreadPoolServer(serverArgs);}
private void flume_f2802_0(TServer.AbstractServerArgs args)
{        args.protocolFactory(getProtocolFactory());        if (enableKerberos) {        args.transportFactory(getSASLTransportFactory());    } else {        args.transportFactory(new TFastFramedTransport.Factory());    }        args.processor(new ThriftSourceProtocol.Processor<ThriftSourceHandler>(new ThriftSourceHandler()));}
private TTransportFactory flume_f2803_0()
{    String[] names;    try {        names = FlumeAuthenticationUtil.splitKerberosName(principal);    } catch (IOException e) {        throw new FlumeException("Error while trying to resolve Principal name - " + principal, e);    }    Map<String, String> saslProperties = new HashMap<String, String>();    saslProperties.put(Sasl.QOP, "auth");    TSaslServerTransport.Factory saslTransportFactory = new TSaslServerTransport.Factory();    saslTransportFactory.addServerDefinition("GSSAPI", names[0], names[1], saslProperties, FlumeAuthenticationUtil.getSaslGssCallbackHandler());    return saslTransportFactory;}
public void flume_f2804_0()
{    if (server != null && server.isServing()) {        server.stop();    }    if (servingExecutor != null) {        servingExecutor.shutdown();        try {            if (!servingExecutor.awaitTermination(5, TimeUnit.SECONDS)) {                servingExecutor.shutdownNow();            }        } catch (InterruptedException e) {            throw new FlumeException("Interrupted while waiting for server to be " + "shutdown.");        }    }    sourceCounter.stop();    super.stop();}
public Status flume_f2805_1(ThriftFlumeEvent event) throws TException
{    Event flumeEvent = EventBuilder.withBody(event.getBody(), event.getHeaders());    sourceCounter.incrementAppendReceivedCount();    sourceCounter.incrementEventReceivedCount();    try {        getChannelProcessor().processEvent(flumeEvent);    } catch (ChannelException ex) {                sourceCounter.incrementChannelWriteFail();        return Status.FAILED;    }    sourceCounter.incrementAppendAcceptedCount();    sourceCounter.incrementEventAcceptedCount();    return Status.OK;}
public Status flume_f2806_1(List<ThriftFlumeEvent> events) throws TException
{    sourceCounter.incrementAppendBatchReceivedCount();    sourceCounter.addToEventReceivedCount(events.size());    List<Event> flumeEvents = Lists.newArrayList();    for (ThriftFlumeEvent event : events) {        flumeEvents.add(EventBuilder.withBody(event.getBody(), event.getHeaders()));    }    try {        getChannelProcessor().processEventBatch(flumeEvents);    } catch (ChannelException ex) {                sourceCounter.incrementChannelWriteFail();        return Status.FAILED;    }    sourceCounter.incrementAppendBatchAcceptedCount();    sourceCounter.addToEventAcceptedCount(events.size());    return Status.OK;}
public static SourceRunner flume_f2807_0(Source source)
{    SourceRunner runner = null;    if (source instanceof PollableSource) {        runner = new PollableSourceRunner();        ((PollableSourceRunner) runner).setSource((PollableSource) source);    } else if (source instanceof EventDrivenSource) {        runner = new EventDrivenSourceRunner();        ((EventDrivenSourceRunner) runner).setSource((EventDrivenSource) source);    } else {        throw new IllegalArgumentException("No known runner type for source " + source);    }    return runner;}
public Source flume_f2808_0()
{    return source;}
public void flume_f2809_0(Source source)
{    this.source = source;}
public long flume_f2810_0()
{    return System.currentTimeMillis();}
public static ByteBuffer flume_f2811_1(int size)
{    Preconditions.checkArgument(size > 0, "Size must be greater than zero");    long maxDirectMemory = getDirectMemorySize();    long allocatedCurrently = allocated.get();        try {        ByteBuffer result = ByteBuffer.allocateDirect(size);        allocated.addAndGet(size);        return result;    } catch (OutOfMemoryError error) {                throw error;    }}
public static void flume_f2812_1(ByteBuffer buffer) throws Exception
{    Preconditions.checkArgument(buffer.isDirect(), "buffer isn't direct!");    Method cleanerMethod = buffer.getClass().getMethod("cleaner");    cleanerMethod.setAccessible(true);    Object cleaner = cleanerMethod.invoke(buffer);    Method cleanMethod = cleaner.getClass().getMethod("clean");    cleanMethod.setAccessible(true);    cleanMethod.invoke(cleaner);    allocated.getAndAdd(-buffer.capacity());    long maxDirectMemory = getDirectMemorySize();    }
public static long flume_f2813_0()
{    RuntimeMXBean RuntimemxBean = ManagementFactory.getRuntimeMXBean();    List<String> arguments = Lists.reverse(RuntimemxBean.getInputArguments());        long multiplier = 1;    for (String s : arguments) {        if (s.contains(MAX_DIRECT_MEMORY_PARAM)) {            String memSize = s.toLowerCase(Locale.ENGLISH).replace(MAX_DIRECT_MEMORY_PARAM.toLowerCase(Locale.ENGLISH), "").trim();            if (memSize.contains("k")) {                multiplier = 1024;            } else if (memSize.contains("m")) {                multiplier = 1048576;            } else if (memSize.contains("g")) {                multiplier = 1073741824;            }            memSize = memSize.replaceAll("[^\\d]", "");            long retValue = Long.parseLong(memSize);            return retValue * multiplier;        }    }    return DEFAULT_SIZE;}
private static long flume_f2814_1()
{    try {        Class<?> VM = Class.forName("sun.misc.VM");        Method maxDirectMemory = VM.getDeclaredMethod("maxDirectMemory", (Class<?>) null);        Object result = maxDirectMemory.invoke(null, (Object[]) null);        if (result != null && result instanceof Long) {            return (Long) result;        }    } catch (Exception e) {            }        return Runtime.getRuntime().maxMemory();}
public static void flume_f2815_0(Object configurable, Map<String, String> properties) throws ConfigurationException
{    Class<?> clazz = configurable.getClass();    for (Method method : clazz.getMethods()) {        String methodName = method.getName();        if (methodName.startsWith("set") && method.getParameterTypes().length == 1) {            String fieldName = methodName.substring(3);            String value = properties.get(StringUtils.uncapitalize(fieldName));            if (value != null) {                Class<?> fieldType = method.getParameterTypes()[0];                ;                try {                    if (fieldType.equals(String.class)) {                        method.invoke(configurable, value);                    } else if (fieldType.equals(boolean.class)) {                        method.invoke(configurable, Boolean.parseBoolean(value));                    } else if (fieldType.equals(short.class)) {                        method.invoke(configurable, Short.parseShort(value));                    } else if (fieldType.equals(long.class)) {                        method.invoke(configurable, Long.parseLong(value));                    } else if (fieldType.equals(float.class)) {                        method.invoke(configurable, Float.parseFloat(value));                    } else if (fieldType.equals(int.class)) {                        method.invoke(configurable, Integer.parseInt(value));                    } else if (fieldType.equals(double.class)) {                        method.invoke(configurable, Double.parseDouble(value));                    } else if (fieldType.equals(char.class)) {                        method.invoke(configurable, value.charAt(0));                    } else if (fieldType.equals(byte.class)) {                        method.invoke(configurable, Byte.parseByte(value));                    } else if (fieldType.equals(String[].class)) {                        method.invoke(configurable, (Object) value.split("\\s+"));                    } else {                        throw new ConfigurationException("Unable to configure component due to an unsupported type on field: " + fieldName);                    }                } catch (Exception ex) {                    if (ex instanceof ConfigurationException) {                        throw (ConfigurationException) ex;                    } else {                        throw new ConfigurationException("Unable to configure component: ", ex);                    }                }            }        }    }}
public static void flume_f2816_0(Object configurable, Context context) throws ConfigurationException
{    Class<?> clazz = configurable.getClass();    Map<String, String> properties = context.getSubProperties(clazz.getSimpleName() + ".");    setConfigurationFields(configurable, properties);}
public static void flume_f2817_0(Object configurable, Context context, String subPropertiesPrefix) throws ConfigurationException
{    Map<String, String> properties = context.getSubProperties(subPropertiesPrefix);    setConfigurationFields(configurable, properties);}
public static void flume_f2818_0(String[] args)
{    if (args.length == 0) {        for (Object prop : System.getProperties().keySet()) {            System.out.println(prop + "=" + System.getProperty((String) prop, ""));        }    } else {        for (String prop : args) {            System.out.println(prop + "=" + System.getProperty(prop, ""));        }    }}
public static ConstraintSecurityHandler flume_f2819_0()
{    Constraint c = new Constraint();    c.setAuthenticate(true);    ConstraintMapping cmt = new ConstraintMapping();    cmt.setConstraint(c);    cmt.setMethod("TRACE");    cmt.setPathSpec("/*");    ConstraintMapping cmo = new ConstraintMapping();    cmo.setConstraint(c);    cmo.setMethod("OPTIONS");    cmo.setPathSpec("/*");    ConstraintSecurityHandler sh = new ConstraintSecurityHandler();    sh.setConstraintMappings(new ConstraintMapping[] { cmt, cmo });    return sh;}
public static boolean flume_f2820_0()
{    String os = System.getProperty("os.name");    boolean isWin = (os.toLowerCase(Locale.ENGLISH).indexOf("win") >= 0);    return isWin;}
public static long flume_f2821_0(long timestamp, int roundDownSec) throws IllegalStateException
{    return roundDownTimeStampSeconds(timestamp, roundDownSec, null);}
public static long flume_f2822_0(long timestamp, int roundDownSec, TimeZone timeZone) throws IllegalStateException
{    Preconditions.checkArgument(roundDownSec > 0 && roundDownSec <= 60, "RoundDownSec must be > 0 and <=60");    Calendar cal = roundDownField(timestamp, Calendar.SECOND, roundDownSec, timeZone);    cal.set(Calendar.MILLISECOND, 0);    return cal.getTimeInMillis();}
public static long flume_f2823_0(long timestamp, int roundDownMins) throws IllegalStateException
{    return roundDownTimeStampMinutes(timestamp, roundDownMins, null);}
public static long flume_f2824_0(long timestamp, int roundDownMins, TimeZone timeZone) throws IllegalStateException
{    Preconditions.checkArgument(roundDownMins > 0 && roundDownMins <= 60, "RoundDown must be > 0 and <=60");    Calendar cal = roundDownField(timestamp, Calendar.MINUTE, roundDownMins, timeZone);    cal.set(Calendar.SECOND, 0);    cal.set(Calendar.MILLISECOND, 0);    return cal.getTimeInMillis();}
public static long flume_f2825_0(long timestamp, int roundDownHours) throws IllegalStateException
{    return roundDownTimeStampHours(timestamp, roundDownHours, null);}
public static long flume_f2826_0(long timestamp, int roundDownHours, TimeZone timeZone) throws IllegalStateException
{    Preconditions.checkArgument(roundDownHours > 0 && roundDownHours <= 24, "RoundDown must be > 0 and <=24");    Calendar cal = roundDownField(timestamp, Calendar.HOUR_OF_DAY, roundDownHours, timeZone);    cal.set(Calendar.MINUTE, 0);    cal.set(Calendar.SECOND, 0);    cal.set(Calendar.MILLISECOND, 0);    return cal.getTimeInMillis();}
private static Calendar flume_f2827_0(long timestamp, int field, int roundDown, TimeZone timeZone)
{    Preconditions.checkArgument(timestamp > 0, "Timestamp must be positive");    Calendar cal = (timeZone == null) ? Calendar.getInstance() : Calendar.getInstance(timeZone);    cal.setTimeInMillis(timestamp);    int fieldVal = cal.get(field);    int remainder = (fieldVal % roundDown);    cal.set(field, fieldVal - remainder);    return cal;}
 static Package flume_f2828_0()
{    return myPackage;}
public static String flume_f2829_0()
{    return version != null ? version.version() : "Unknown";}
public static String flume_f2830_0()
{    if (version != null && version.revision() != null && !version.revision().isEmpty()) {        return version.revision();    }    return "Unknown";}
public static String flume_f2831_0()
{    return version != null ? version.branch() : "Unknown";}
public static String flume_f2832_0()
{    return version != null ? version.date() : "Unknown";}
public static String flume_f2833_0()
{    return version != null ? version.user() : "Unknown";}
public static String flume_f2834_0()
{    return version != null ? version.url() : "Unknown";}
public static String flume_f2835_0()
{    return version != null ? version.srcChecksum() : "Unknown";}
public static String flume_f2836_0()
{    return VersionInfo.getVersion() + " from " + VersionInfo.getRevision() + " by " + VersionInfo.getUser() + " on " + VersionInfo.getDate() + " source checksum " + VersionInfo.getSrcChecksum();}
public static void flume_f2837_0(String[] args)
{    System.out.println("Flume " + getVersion());    System.out.println("Source code repository: " + "https://git-wip-us.apache.org/repos/asf/flume.git");    System.out.println("Revision: " + getRevision());    System.out.println("Compiled by " + getUser() + " on " + getDate());    System.out.println("From source with checksum " + getSrcChecksum());}
public Mode flume_f2838_0()
{    return mode;}
public void flume_f2839_0(Mode mode)
{    this.mode = mode;}
public boolean flume_f2840_0()
{    return lastTransactionCommitted;}
public boolean flume_f2841_0()
{    return lastTransactionRolledBack;}
public boolean flume_f2842_0()
{    return lastTransactionClosed;}
protected BasicTransactionSemantics flume_f2843_0()
{    return new TestTransaction();}
protected void flume_f2844_0() throws InterruptedException
{    switch(mode) {        case THROW_ERROR:            throw new TestError();        case THROW_RUNTIME:            throw new TestRuntimeException();        case THROW_CHANNEL:            throw new ChannelException("test");        case SLEEP:            Thread.sleep(300000);            break;    }}
protected void flume_f2845_0() throws InterruptedException
{    doMode();}
protected void flume_f2846_0(Event event) throws InterruptedException
{    doMode();    synchronized (queue) {        queue.add(event);    }}
protected Event flume_f2847_0() throws InterruptedException
{    doMode();    synchronized (queue) {        return queue.poll();    }}
protected void flume_f2848_0() throws InterruptedException
{    doMode();    lastTransactionCommitted = true;}
protected void flume_f2849_0() throws InterruptedException
{    lastTransactionRolledBack = true;    doMode();}
protected void flume_f2850_0()
{    lastTransactionClosed = true;    Preconditions.checkState(mode != TestChannel.Mode.SLEEP, "doClose() can't throw InterruptedException, so why SLEEP?");    try {        doMode();    } catch (InterruptedException e) {        Assert.fail();    }}
protected void flume_f2851_0(Class<? extends Throwable> exceptionClass, Runnable test)
{    try {        test.run();        Assert.fail();    } catch (Throwable e) {        if (exceptionClass == InterruptedException.class && e instanceof ChannelException && e.getCause() instanceof InterruptedException) {            Assert.assertTrue(Thread.interrupted());        } else if (!exceptionClass.isInstance(e)) {            throw new AssertionError(e);        }    }}
protected void flume_f2852_0(Runnable test)
{    testException(IllegalArgumentException.class, test);}
protected void flume_f2853_0(Runnable test)
{    testException(IllegalStateException.class, test);}
protected void flume_f2854_0(final Runnable test) throws Exception
{    executor.submit(new Runnable() {        @Override        public void run() {            testIllegalState(test);        }    }).get();}
public void flume_f2855_0()
{    testIllegalState(test);}
protected void flume_f2856_0(TestChannel.Mode mode, Runnable test)
{    TestChannel.Mode oldMode = channel.getMode();    try {        channel.setMode(mode);        test.run();    } finally {        channel.setMode(oldMode);    }}
protected void flume_f2857_0(TestChannel.Mode mode, final Class<? extends Throwable> exceptionClass, final Runnable test)
{    testMode(mode, new Runnable() {        @Override        public void run() {            testException(exceptionClass, test);        }    });}
public void flume_f2858_0()
{    testException(exceptionClass, test);}
protected void flume_f2859_0(Runnable test)
{    testException(TestChannel.Mode.THROW_ERROR, TestError.class, test);}
protected void flume_f2860_0(Runnable test)
{    testException(TestChannel.Mode.THROW_RUNTIME, TestRuntimeException.class, test);}
protected void flume_f2861_0(Runnable test)
{    testException(TestChannel.Mode.THROW_CHANNEL, ChannelException.class, test);}
protected void flume_f2862_0(final Runnable test)
{    testMode(TestChannel.Mode.SLEEP, new Runnable() {        @Override        public void run() {            testException(InterruptedException.class, new Runnable() {                @Override                public void run() {                    interruptTest(test);                }            });        }    });}
public void flume_f2863_0()
{    testException(InterruptedException.class, new Runnable() {        @Override        public void run() {            interruptTest(test);        }    });}
public void flume_f2864_0()
{    interruptTest(test);}
protected void flume_f2865_0(final Runnable test)
{    final Thread mainThread = Thread.currentThread();    Future<?> future = executor.submit(new Runnable() {        @Override        public void run() {            try {                Thread.sleep(500);            } catch (InterruptedException e) {            }            mainThread.interrupt();        }    });    test.run();    try {        future.get();    } catch (Exception e) {        throw new AssertionError(e);    }}
public void flume_f2866_0()
{    try {        Thread.sleep(500);    } catch (InterruptedException e) {    }    mainThread.interrupt();}
protected void flume_f2867_0(Runnable test) throws Exception
{    testWrongThread(test);    testBasicExceptions(test);    testInterrupt(test);}
protected void flume_f2868_0(Runnable test) throws Exception
{    testError(test);    testRuntimeException(test);    testChannelException(test);}
public void flume_f2869_0()
{    Preconditions.checkState(channel == null, "test cleanup failed!");    Preconditions.checkState(executor == null, "test cleanup failed!");    channel = new TestChannel();    executor = Executors.newCachedThreadPool();}
public void flume_f2870_0()
{    channel = null;    executor.shutdown();    executor = null;}
public static Channel flume_f2871_0(String name)
{    Channel ch = new MockChannel();    ch.setName(name);    return ch;}
public void flume_f2872_0(Event event) throws ChannelException
{    events.add(event);}
public Event flume_f2873_0() throws ChannelException
{    return (events.size() > 0) ? events.get(0) : null;}
public Transaction flume_f2874_0()
{    return new MockTransaction();}
public void flume_f2875_0()
{}
public void flume_f2876_0()
{}
public void flume_f2877_0()
{}
public void flume_f2878_0()
{}
public Map<String, String> flume_f2879_0()
{    return headers;}
public void flume_f2880_0(Map<String, String> headers)
{    this.headers = new HashMap<String, String>();    this.headers.putAll(headers);}
public byte[] flume_f2881_0()
{    return body;}
public void flume_f2882_0(byte[] body)
{    this.body = new byte[body.length];    System.arraycopy(body, 0, this.body, 0, body.length);}
public void flume_f2883_0()
{    for (int i = 0; i < events.size(); ++i) {        Transaction transaction = channel.getTransaction();        transaction.begin();        channel.put(events.get(i));        transaction.commit();        transaction.close();    }    for (int i = 0; i < events.size(); ++i) {        Transaction transaction = channel.getTransaction();        transaction.begin();        Assert.assertSame(events.get(i), channel.take());        transaction.commit();        transaction.close();    }}
public void flume_f2884_0() throws Exception
{    final int testLength = 1000;    Future<?> producer = executor.submit(new Runnable() {        @Override        public void run() {            try {                Thread.sleep(500);                for (int i = 0; i < testLength; ++i) {                    Transaction transaction = channel.getTransaction();                    transaction.begin();                    channel.put(events.get(i % events.size()));                    transaction.commit();                    transaction.close();                    Thread.sleep(1);                }                Thread.sleep(500);            } catch (InterruptedException e) {                Assert.fail();            }        }    });    int i = 0;    while (!producer.isDone()) {        Transaction transaction = channel.getTransaction();        transaction.begin();        Event event = channel.take();        if (event != null) {            Assert.assertSame(events.get(i % events.size()), event);            ++i;        }        transaction.commit();        transaction.close();    }    Assert.assertEquals(testLength, i);    producer.get();}
public void flume_f2885_0()
{    try {        Thread.sleep(500);        for (int i = 0; i < testLength; ++i) {            Transaction transaction = channel.getTransaction();            transaction.begin();            channel.put(events.get(i % events.size()));            transaction.commit();            transaction.close();            Thread.sleep(1);        }        Thread.sleep(500);    } catch (InterruptedException e) {        Assert.fail();    }}
public void flume_f2886_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    executor.submit(new Runnable() {        @Override        public void run() {            Assert.assertNotSame(transaction, channel.getTransaction());        }    }).get();    Assert.assertSame(transaction, channel.getTransaction());    transaction.begin();    executor.submit(new Runnable() {        @Override        public void run() {            Assert.assertNotSame(transaction, channel.getTransaction());        }    }).get();    Assert.assertSame(transaction, channel.getTransaction());    transaction.commit();    executor.submit(new Runnable() {        @Override        public void run() {            Assert.assertNotSame(transaction, channel.getTransaction());        }    }).get();    Assert.assertSame(transaction, channel.getTransaction());    transaction.close();    executor.submit(new Runnable() {        @Override        public void run() {            Assert.assertNotSame(transaction, channel.getTransaction());        }    }).get();    Assert.assertNotSame(transaction, channel.getTransaction());}
public void flume_f2887_0()
{    Assert.assertNotSame(transaction, channel.getTransaction());}
public void flume_f2888_0()
{    Assert.assertNotSame(transaction, channel.getTransaction());}
public void flume_f2889_0()
{    Assert.assertNotSame(transaction, channel.getTransaction());}
public void flume_f2890_0()
{    Assert.assertNotSame(transaction, channel.getTransaction());}
public void flume_f2891_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    testExceptions(new Runnable() {        @Override        public void run() {            transaction.begin();        }    });    transaction.begin();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.begin();        }    });    transaction.commit();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.begin();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.begin();        }    });}
public void flume_f2892_0()
{    transaction.begin();}
public void flume_f2893_0()
{    transaction.begin();}
public void flume_f2894_0()
{    transaction.begin();}
public void flume_f2895_0()
{    transaction.begin();}
public void flume_f2896_0() throws Exception
{    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });    Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });    transaction.begin();    channel.put(events.get(0));    testIllegalArgument(new Runnable() {        @Override        public void run() {            channel.put(null);        }    });    testExceptions(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });    transaction.commit();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });}
public void flume_f2897_0()
{    channel.put(events.get(0));}
public void flume_f2898_0()
{    channel.put(events.get(0));}
public void flume_f2899_0()
{    channel.put(null);}
public void flume_f2900_0()
{    channel.put(events.get(0));}
public void flume_f2901_0()
{    channel.put(events.get(0));}
public void flume_f2902_0()
{    channel.put(events.get(0));}
public void flume_f2903_0() throws Exception
{    Transaction transaction = channel.getTransaction();    transaction.begin();    channel.put(events.get(0));    transaction.rollback();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });}
public void flume_f2904_0()
{    channel.put(events.get(0));}
public void flume_f2905_0()
{    channel.put(events.get(0));}
public void flume_f2906_0() throws Exception
{    Transaction transaction = channel.getTransaction();    transaction.begin();    channel.put(events.get(0));    final Transaction finalTransaction = transaction;    testChannelException(new Runnable() {        @Override        public void run() {            finalTransaction.commit();        }    });    transaction.rollback();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });}
public void flume_f2907_0()
{    finalTransaction.commit();}
public void flume_f2908_0()
{    channel.put(events.get(0));}
public void flume_f2909_0()
{    channel.put(events.get(0));}
public void flume_f2910_0() throws Exception
{    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });    Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });    transaction.begin();    Assert.assertNull(channel.take());    for (int i = 0; i < 1000; ++i) {        channel.put(events.get(i % events.size()));    }    Assert.assertNotNull(channel.take());    testWrongThread(new Runnable() {        @Override        public void run() {            channel.take();        }    });    testBasicExceptions(new Runnable() {        @Override        public void run() {            channel.take();        }    });    testMode(TestChannel.Mode.SLEEP, new Runnable() {        @Override        public void run() {            interruptTest(new Runnable() {                @Override                public void run() {                    Assert.assertNull(channel.take());                    Assert.assertTrue(Thread.interrupted());                }            });        }    });    Assert.assertNotNull(channel.take());    transaction.commit();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });}
public void flume_f2911_0()
{    channel.take();}
public void flume_f2912_0()
{    channel.take();}
public void flume_f2913_0()
{    channel.take();}
public void flume_f2914_0()
{    channel.take();}
public void flume_f2915_0()
{    interruptTest(new Runnable() {        @Override        public void run() {            Assert.assertNull(channel.take());            Assert.assertTrue(Thread.interrupted());        }    });}
public void flume_f2916_0()
{    Assert.assertNull(channel.take());    Assert.assertTrue(Thread.interrupted());}
public void flume_f2917_0()
{    channel.take();}
public void flume_f2918_0()
{    channel.take();}
public void flume_f2919_0() throws Exception
{    Transaction transaction = channel.getTransaction();    transaction.begin();    channel.take();    transaction.rollback();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });}
public void flume_f2920_0()
{    channel.take();}
public void flume_f2921_0()
{    channel.take();}
public void flume_f2922_0() throws Exception
{    Transaction transaction = channel.getTransaction();    transaction.begin();    channel.take();    final Transaction finalTransaction = transaction;    testChannelException(new Runnable() {        @Override        public void run() {            finalTransaction.commit();        }    });    transaction.rollback();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });}
public void flume_f2923_0()
{    finalTransaction.commit();}
public void flume_f2924_0()
{    channel.take();}
public void flume_f2925_0()
{    channel.take();}
public void flume_f2926_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });    transaction.begin();    testExceptions(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });    transaction.commit();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });}
public void flume_f2927_0()
{    transaction.commit();}
public void flume_f2928_0()
{    transaction.commit();}
public void flume_f2929_0()
{    transaction.commit();}
public void flume_f2930_0()
{    transaction.commit();}
public void flume_f2931_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    transaction.begin();    transaction.rollback();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });}
public void flume_f2932_0()
{    transaction.commit();}
public void flume_f2933_0()
{    transaction.commit();}
public void flume_f2934_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.begin();    testWrongThread(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.rollback();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });}
public void flume_f2935_0()
{    transaction.rollback();}
public void flume_f2936_0()
{    transaction.rollback();}
public void flume_f2937_0()
{    transaction.rollback();}
public void flume_f2938_0()
{    transaction.rollback();}
public void flume_f2939_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.begin();    testError(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });}
public void flume_f2940_0()
{    transaction.rollback();}
public void flume_f2941_0()
{    transaction.rollback();}
public void flume_f2942_0()
{    transaction.rollback();}
public void flume_f2943_0()
{    transaction.rollback();}
public void flume_f2944_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.begin();    testRuntimeException(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });}
public void flume_f2945_0()
{    transaction.rollback();}
public void flume_f2946_0()
{    transaction.rollback();}
public void flume_f2947_0()
{    transaction.rollback();}
public void flume_f2948_0()
{    transaction.rollback();}
public void flume_f2949_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.begin();    testChannelException(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });}
public void flume_f2950_0()
{    transaction.rollback();}
public void flume_f2951_0()
{    transaction.rollback();}
public void flume_f2952_0()
{    transaction.rollback();}
public void flume_f2953_0()
{    transaction.rollback();}
public void flume_f2954_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.begin();    testInterrupt(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });}
public void flume_f2955_0()
{    transaction.rollback();}
public void flume_f2956_0()
{    transaction.rollback();}
public void flume_f2957_0()
{    transaction.rollback();}
public void flume_f2958_0()
{    transaction.rollback();}
public void flume_f2959_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    transaction.begin();    transaction.commit();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });}
public void flume_f2960_0()
{    transaction.rollback();}
public void flume_f2961_0()
{    transaction.rollback();}
public void flume_f2962_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    transaction.begin();    testExceptions(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });    transaction.rollback();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });}
public void flume_f2963_0()
{    transaction.commit();}
public void flume_f2964_0()
{    transaction.rollback();}
public void flume_f2965_0()
{    transaction.rollback();}
public void flume_f2966_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    testError(new Runnable() {        @Override        public void run() {            transaction.close();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.close();        }    });}
public void flume_f2967_0()
{    transaction.close();}
public void flume_f2968_0()
{    transaction.close();}
public void flume_f2969_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    testRuntimeException(new Runnable() {        @Override        public void run() {            transaction.close();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.close();        }    });}
public void flume_f2970_0()
{    transaction.close();}
public void flume_f2971_0()
{    transaction.close();}
public void flume_f2972_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    testChannelException(new Runnable() {        @Override        public void run() {            transaction.close();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.close();        }    });}
public void flume_f2973_0()
{    transaction.close();}
public void flume_f2974_0()
{    transaction.close();}
public void flume_f2975_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    transaction.begin();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.close();        }    });}
public void flume_f2976_0()
{    transaction.close();}
public void flume_f2977_0() throws Exception
{    final Transaction transaction = channel.getTransaction();    transaction.begin();    testChannelException(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.close();        }    });}
public void flume_f2978_0()
{    transaction.commit();}
public void flume_f2979_0()
{    transaction.close();}
public void flume_f2980_0()
{        Channel ch = mock(Channel.class);    when(ch.getTransaction()).thenThrow(new ChannelException("doh!"));    ChannelSelector sel = new ReplicatingChannelSelector();    sel.setChannels(Lists.newArrayList(ch));    ChannelProcessor proc = new ChannelProcessor(sel);    List<Event> events = Lists.newArrayList();    events.add(EventBuilder.withBody("event 1", Charsets.UTF_8));    proc.processEventBatch(events);}
public void flume_f2981_0()
{        Channel ch = mock(Channel.class);    when(ch.getTransaction()).thenReturn(null);    ChannelSelector sel = new ReplicatingChannelSelector();    sel.setChannels(Lists.newArrayList(ch));    ChannelProcessor proc = new ChannelProcessor(sel);    List<Event> events = Lists.newArrayList();    events.add(EventBuilder.withBody("event 1", Charsets.UTF_8));    boolean threw = false;    try {        proc.processEventBatch(events);    } catch (NullPointerException ex) {        threw = true;        Assert.assertNotNull("NPE must be manually thrown", ex.getMessage());    }    Assert.assertTrue("Must throw NPE", threw);}
public void flume_f2982_0()
{    Context context = new Context();    ArrayList<Channel> channels = new ArrayList<Channel>();    for (int i = 0; i < 4; i++) {        Channel ch = new MemoryChannel();        ch.setName("ch" + i);        Configurables.configure(ch, context);        channels.add(ch);    }    ChannelSelector selector = new ReplicatingChannelSelector();    selector.setChannels(channels);    context = new Context();    context.put(ReplicatingChannelSelector.CONFIG_OPTIONAL, "ch2 ch3");    Configurables.configure(selector, context);    ChannelProcessor processor = new ChannelProcessor(selector);    context = new Context();    Configurables.configure(processor, context);    Event event1 = EventBuilder.withBody("event 1", Charsets.UTF_8);    processor.processEvent(event1);    try {        Thread.sleep(3000);    } catch (InterruptedException e) {    }    for (Channel channel : channels) {        Transaction transaction = channel.getTransaction();        transaction.begin();        Event event_ch = channel.take();        Assert.assertEquals(event1, event_ch);        transaction.commit();        transaction.close();    }    List<Event> events = Lists.newArrayList();    for (int i = 0; i < 100; i++) {        events.add(EventBuilder.withBody("event " + i, Charsets.UTF_8));    }    processor.processEventBatch(events);    try {        Thread.sleep(3000);    } catch (InterruptedException e) {    }    for (Channel channel : channels) {        Transaction transaction = channel.getTransaction();        transaction.begin();        for (int i = 0; i < 100; i++) {            Event event_ch = channel.take();            Assert.assertNotNull(event_ch);        }        transaction.commit();        transaction.close();    }}
public void flume_f2983_0()
{    ChannelUtils.put(channel, events.get(0));    Assert.assertTrue(channel.wasLastTransactionCommitted());    Assert.assertFalse(channel.wasLastTransactionRolledBack());    Assert.assertTrue(channel.wasLastTransactionClosed());}
public void flume_f2984_0()
{    ChannelUtils.take(channel);    Assert.assertTrue(channel.wasLastTransactionCommitted());    Assert.assertFalse(channel.wasLastTransactionRolledBack());    Assert.assertTrue(channel.wasLastTransactionClosed());}
public void flume_f2985_0()
{    ChannelUtils.put(channel, events.get(0));    Assert.assertSame(events.get(0), ChannelUtils.take(channel));}
public void flume_f2986_0()
{    for (int i = 0; i < events.size(); ++i) {        ChannelUtils.put(channel, events.get(i));    }    for (int i = 0; i < events.size(); ++i) {        Assert.assertSame(events.get(i), ChannelUtils.take(channel));    }}
public void flume_f2987_0()
{    int rounds = 10;    for (int i = 0; i < rounds; ++i) {        ChannelUtils.put(channel, events);    }    for (int i = 0; i < rounds; ++i) {        List<Event> takenEvents = ChannelUtils.take(channel, events.size());        Assert.assertTrue(takenEvents.size() == events.size());        for (int j = 0; j < events.size(); ++j) {            Assert.assertSame(events.get(j), takenEvents.get(j));        }    }}
private void flume_f2988_0(final TestChannel.Mode mode, Class<? extends Throwable> exceptionClass, final Runnable test)
{    testException(exceptionClass, new Runnable() {        @Override        public void run() {            ChannelUtils.transact(channel, new Runnable() {                @Override                public void run() {                    testMode(mode, test);                }            });        }    });    Assert.assertFalse(channel.wasLastTransactionCommitted());    Assert.assertTrue(channel.wasLastTransactionRolledBack());    Assert.assertTrue(channel.wasLastTransactionClosed());}
public void flume_f2989_0()
{    ChannelUtils.transact(channel, new Runnable() {        @Override        public void run() {            testMode(mode, test);        }    });}
public void flume_f2990_0()
{    testMode(mode, test);}
private void flume_f2991_0(TestChannel.Mode mode, Class<? extends Throwable> exceptionClass)
{    testTransact(mode, exceptionClass, new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });}
public void flume_f2992_0()
{    channel.put(events.get(0));}
public void flume_f2993_0()
{    testTransact(TestChannel.Mode.THROW_ERROR, TestError.class);}
public void flume_f2994_0()
{    testTransact(TestChannel.Mode.THROW_RUNTIME, TestRuntimeException.class);}
public void flume_f2995_0()
{    testTransact(TestChannel.Mode.THROW_CHANNEL, ChannelException.class);}
public void flume_f2996_0() throws Exception
{    testTransact(TestChannel.Mode.SLEEP, InterruptedException.class, new Runnable() {        @Override        public void run() {            interruptTest(new Runnable() {                @Override                public void run() {                    channel.put(events.get(0));                }            });        }    });}
public void flume_f2997_0()
{    interruptTest(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });}
public void flume_f2998_0()
{    channel.put(events.get(0));}
public void flume_f2999_0()
{    channel = new MemoryChannel();}
public void flume_f3000_0() throws InterruptedException, EventDeliveryException
{    Event event = EventBuilder.withBody("test event".getBytes());    Context context = new Context();    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    Assert.assertNotNull(transaction);    transaction.begin();    channel.put(event);    transaction.commit();    transaction.close();    transaction = channel.getTransaction();    Assert.assertNotNull(transaction);    transaction.begin();    Event event2 = channel.take();    Assert.assertEquals(event, event2);    transaction.commit();}
public void flume_f3001_0()
{    Configurables.configure(channel, new Context());    Event event = EventBuilder.withBody("test body".getBytes(Charsets.UTF_8), Collections.<String, String>singletonMap("test_key", null));    Transaction txPut = channel.getTransaction();    txPut.begin();    channel.put(event);    txPut.commit();    txPut.close();    Transaction txTake = channel.getTransaction();    txTake.begin();    Event eventTaken = channel.take();    Assert.assertEquals(event, eventTaken);    txTake.commit();}
public void flume_f3002_0()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("capacity", "5");    parms.put("transactionCapacity", "5");    context.putAll(parms);    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 5; i++) {        channel.put(EventBuilder.withBody(String.format("test event %d", i).getBytes()));    }    transaction.commit();    transaction.close();    /*     * Verify overflow semantics     */    transaction = channel.getTransaction();    boolean overflowed = false;    try {        transaction.begin();        channel.put(EventBuilder.withBody("overflow event".getBytes()));        transaction.commit();    } catch (ChannelException e) {        overflowed = true;        transaction.rollback();    } finally {        transaction.close();    }    Assert.assertTrue(overflowed);    /*     * Reconfigure capacity down and add another event, shouldn't result in exception     */    parms.put("capacity", "6");    context.putAll(parms);    Configurables.configure(channel, context);    transaction = channel.getTransaction();    transaction.begin();    channel.put(EventBuilder.withBody("extended capacity event".getBytes()));    transaction.commit();    transaction.close();    /*     * Attempt to reconfigure capacity to below current entry count and verify     * it wasn't carried out     */    parms.put("capacity", "2");    parms.put("transactionCapacity", "2");    context.putAll(parms);    Configurables.configure(channel, context);    for (int i = 0; i < 6; i++) {        transaction = channel.getTransaction();        transaction.begin();        Assert.assertNotNull(channel.take());        transaction.commit();        transaction.close();    }}
public void flume_f3003_0()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("capacity", "5");    parms.put("transactionCapacity", "2");    context.putAll(parms);    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    transaction.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));        channel.put(EventBuilder.withBody("test".getBytes()));    Assert.fail();}
public void flume_f3004_0()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("capacity", "5");    parms.put("transactionCapacity", "3");    context.putAll(parms);    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    transaction.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    transaction.commit();    transaction.close();    transaction = channel.getTransaction();    transaction.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));        transaction.commit();    Assert.fail();}
public void flume_f3005_0()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("capacity", "3");    parms.put("transactionCapacity", "3");    context.putAll(parms);    Configurables.configure(channel, context);    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    tx.commit();    tx.close();    tx = channel.getTransaction();    tx.begin();    channel.take();    channel.take();    tx.commit();    tx.close();    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    tx.commit();    tx.close();}
public void flume_f3006_0()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("capacity", "3");    parms.put("transactionCapacity", "3");    context.putAll(parms);    Configurables.configure(channel, context);    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    tx.rollback();    tx.close();    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    tx.commit();    tx.close();}
public void flume_f3007_0()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("byteCapacity", "2000");    parms.put("byteCapacityBufferPercentage", "20");    context.putAll(parms);    Configurables.configure(channel, context);    byte[] eventBody = new byte[405];    Transaction transaction = channel.getTransaction();    transaction.begin();    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    transaction.commit();    transaction.close();    transaction = channel.getTransaction();    transaction.begin();    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));        transaction.commit();    Assert.fail();}
public void flume_f3008_0()
{    Context ctx = new Context(ImmutableMap.of("byteCapacity", "1000"));    Configurables.configure(channel, ctx);    Assert.assertEquals(8, channel.getBytesRemainingValue());    Event e = new SimpleEvent();    Transaction t = channel.getTransaction();    t.begin();    channel.put(e);    t.rollback();    Assert.assertEquals(8, channel.getBytesRemainingValue());}
public void flume_f3009_0()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("byteCapacity", "2000");    parms.put("byteCapacityBufferPercentage", "20");    context.putAll(parms);    Configurables.configure(channel, context);    byte[] eventBody = new byte[405];    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    try {        channel.put(EventBuilder.withBody(eventBody));        throw new RuntimeException("Put was able to overflow byte capacity.");    } catch (ChannelException ce) {        }    tx.commit();    tx.close();    tx = channel.getTransaction();    tx.begin();    channel.take();    channel.take();    tx.commit();    tx.close();    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    try {        channel.put(EventBuilder.withBody(eventBody));        throw new RuntimeException("Put was able to overflow byte capacity.");    } catch (ChannelException ce) {        }    tx.commit();    tx.close();}
public void flume_f3010_0()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("byteCapacity", "2000");    parms.put("byteCapacityBufferPercentage", "20");    context.putAll(parms);    Configurables.configure(channel, context);    byte[] eventBody = new byte[405];    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    tx.rollback();    tx.close();    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    tx.commit();    tx.close();}
public void flume_f3011_0()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("byteCapacity", "2000");    parms.put("byteCapacityBufferPercentage", "20");    context.putAll(parms);    Configurables.configure(channel, context);    byte[] eventBody = new byte[405];    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(eventBody));    tx.commit();    tx.close();    channel.stop();    parms.put("byteCapacity", "1500");    context.putAll(parms);    Configurables.configure(channel, context);    channel.start();    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(eventBody));    try {        channel.put(EventBuilder.withBody(eventBody));        tx.commit();        Assert.fail();    } catch (ChannelException e) {                tx.rollback();    } finally {        tx.close();    }    channel.stop();    parms.put("byteCapacity", "250");    parms.put("byteCapacityBufferPercentage", "20");    context.putAll(parms);    Configurables.configure(channel, context);    channel.start();    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(eventBody));    tx.commit();    tx.close();    channel.stop();    parms.put("byteCapacity", "300");    context.putAll(parms);    Configurables.configure(channel, context);    channel.start();    tx = channel.getTransaction();    tx.begin();    try {        for (int i = 0; i < 2; i++) {            channel.put(EventBuilder.withBody(eventBody));        }        tx.commit();        Assert.fail();    } catch (ChannelException e) {                tx.rollback();    } finally {        tx.close();    }    channel.stop();    parms.put("byteCapacity", "3300");    context.putAll(parms);    Configurables.configure(channel, context);    channel.start();    tx = channel.getTransaction();    tx.begin();    try {        for (int i = 0; i < 15; i++) {            channel.put(EventBuilder.withBody(eventBody));        }        tx.commit();        Assert.fail();    } catch (ChannelException e) {                tx.rollback();    } finally {        tx.close();    }    channel.stop();    parms.put("byteCapacity", "4000");    context.putAll(parms);    Configurables.configure(channel, context);    channel.start();    tx = channel.getTransaction();    tx.begin();    try {        for (int i = 0; i < 25; i++) {            channel.put(EventBuilder.withBody(eventBody));        }        tx.commit();        Assert.fail();    } catch (ChannelException e) {                tx.rollback();    } finally {        tx.close();    }    channel.stop();}
public void flume_f3012_0()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("byteCapacity", "2000");    parms.put("byteCapacityBufferPercentage", "20");    context.putAll(parms);    Configurables.configure(channel, context);    Transaction tx = channel.getTransaction();    tx.begin();        channel.put(EventBuilder.withBody(null));    tx.commit();    tx.close();    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(new byte[0]));    tx.commit();    tx.close();}
public void flume_f3013_0()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("capacity", "-3");    parms.put("transactionCapacity", "-1");    context.putAll(parms);    Configurables.configure(channel, context);    Assert.assertTrue(field("queue").ofType(LinkedBlockingDeque.class).in(channel).get().remainingCapacity() > 0);    Assert.assertTrue(field("transCapacity").ofType(Integer.class).in(channel).get() > 0);}
public void flume_f3014_0()
{}
public void flume_f3015_0() throws InterruptedException
{    final Channel channel = new MemoryChannel();    barrier = new CyclicBarrier(2);    Configurables.configure(channel, new Context());    Thread t1 = new Thread(new Runnable() {        @Override        public void run() {            Transaction tx = channel.getTransaction();            tx.begin();            channel.put(EventBuilder.withBody("first event".getBytes()));            try {                barrier.await();                barrier.await();                tx.rollback();                barrier.await();                tx.close();                                barrier.await();            } catch (InterruptedException e) {                Assert.fail();            } catch (BrokenBarrierException e) {                Assert.fail();            }        }    });    Thread t2 = new Thread(new Runnable() {        @Override        public void run() {            Transaction tx = channel.getTransaction();            try {                barrier.await();                tx.begin();                channel.put(EventBuilder.withBody("second event".getBytes()));                barrier.await();                barrier.await();                tx.commit();                tx.close();                                barrier.await();            } catch (InterruptedException e) {                Assert.fail();            } catch (BrokenBarrierException e) {                Assert.fail();            }        }    });    t1.start();    t2.start();    t1.join(1000);    if (t1.isAlive()) {        Assert.fail("Thread1 failed to finish");        t1.interrupt();    }    t2.join(1000);    if (t2.isAlive()) {        Assert.fail("Thread2 failed to finish");        t2.interrupt();    }    Transaction tx = channel.getTransaction();    tx.begin();    Event e = channel.take();    Assert.assertEquals("second event", new String(e.getBody()));    Assert.assertNull(channel.take());    tx.commit();    tx.close();}
public void flume_f3016_0()
{    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("first event".getBytes()));    try {        barrier.await();        barrier.await();        tx.rollback();        barrier.await();        tx.close();                barrier.await();    } catch (InterruptedException e) {        Assert.fail();    } catch (BrokenBarrierException e) {        Assert.fail();    }}
public void flume_f3017_0()
{    Transaction tx = channel.getTransaction();    try {        barrier.await();        tx.begin();        channel.put(EventBuilder.withBody("second event".getBytes()));        barrier.await();        barrier.await();        tx.commit();        tx.close();                barrier.await();    } catch (InterruptedException e) {        Assert.fail();    } catch (BrokenBarrierException e) {        Assert.fail();    }}
public void flume_f3018_0() throws InterruptedException
{    final Channel channel = new MemoryChannel();    Context context = new Context();    context.put("keep-alive", "1");        context.put("capacity", "5000");                context.put("transactionCapacity", "5000");    Configurables.configure(channel, context);    final ConcurrentHashMap<String, AtomicInteger> committedPuts = new ConcurrentHashMap<String, AtomicInteger>();    final int threadCount = 100;    final CountDownLatch startGate = new CountDownLatch(1);    final CountDownLatch endGate = new CountDownLatch(threadCount);    for (int i = 0; i < threadCount; i++) {        Thread t = new Thread() {            @Override            public void run() {                Long tid = Thread.currentThread().getId();                String strtid = tid.toString();                Random rng = new Random(tid);                try {                    startGate.await();                } catch (InterruptedException e1) {                    Thread.currentThread().interrupt();                }                for (int j = 0; j < 10; j++) {                    int events = rng.nextInt(5) + 1;                    Transaction tx = channel.getTransaction();                    tx.begin();                    for (int k = 0; k < events; k++) {                        channel.put(EventBuilder.withBody(strtid.getBytes()));                    }                    if (rng.nextBoolean()) {                        tx.commit();                        AtomicInteger tcount = committedPuts.get(strtid);                        if (tcount == null) {                            committedPuts.put(strtid, new AtomicInteger(events));                        } else {                            tcount.addAndGet(events);                        }                    } else {                        tx.rollback();                    }                    tx.close();                }                endGate.countDown();            }        };        t.start();    }    startGate.countDown();    endGate.await();    if (committedPuts.isEmpty()) {        Assert.fail();    }        Transaction tx = channel.getTransaction();    tx.begin();    Event e;    while ((e = channel.take()) != null) {        String index = new String(e.getBody());        AtomicInteger remain = committedPuts.get(index);        int post = remain.decrementAndGet();        if (post == 0) {            committedPuts.remove(index);        }    }    tx.commit();    tx.close();    if (!committedPuts.isEmpty()) {        Assert.fail();    }}
public void flume_f3019_0()
{    Long tid = Thread.currentThread().getId();    String strtid = tid.toString();    Random rng = new Random(tid);    try {        startGate.await();    } catch (InterruptedException e1) {        Thread.currentThread().interrupt();    }    for (int j = 0; j < 10; j++) {        int events = rng.nextInt(5) + 1;        Transaction tx = channel.getTransaction();        tx.begin();        for (int k = 0; k < events; k++) {            channel.put(EventBuilder.withBody(strtid.getBytes()));        }        if (rng.nextBoolean()) {            tx.commit();            AtomicInteger tcount = committedPuts.get(strtid);            if (tcount == null) {                committedPuts.put(strtid, new AtomicInteger(events));            } else {                tcount.addAndGet(events);            }        } else {            tx.rollback();        }        tx.close();    }    endGate.countDown();}
public void flume_f3020_0() throws InterruptedException
{    final Channel channel = new MemoryChannel();    Context context = new Context();    context.put("keep-alive", "1");        context.put("capacity", "100");                context.put("transactionCapacity", "100");    Configurables.configure(channel, context);    final ConcurrentHashMap<String, AtomicInteger> committedPuts = new ConcurrentHashMap<String, AtomicInteger>();    final ConcurrentHashMap<String, AtomicInteger> committedTakes = new ConcurrentHashMap<String, AtomicInteger>();    final int threadCount = 100;    final CountDownLatch startGate = new CountDownLatch(1);    final CountDownLatch endGate = new CountDownLatch(threadCount);        for (int i = 0; i < threadCount / 2; i++) {        Thread t = new Thread() {            @Override            public void run() {                Long tid = Thread.currentThread().getId();                String strtid = tid.toString();                Random rng = new Random(tid);                try {                    startGate.await();                } catch (InterruptedException e1) {                    Thread.currentThread().interrupt();                }                for (int j = 0; j < 10; j++) {                    int events = rng.nextInt(5) + 1;                    Transaction tx = channel.getTransaction();                    tx.begin();                    for (int k = 0; k < events; k++) {                        channel.put(EventBuilder.withBody(strtid.getBytes()));                    }                    if (rng.nextBoolean()) {                        try {                            tx.commit();                            AtomicInteger tcount = committedPuts.get(strtid);                            if (tcount == null) {                                committedPuts.put(strtid, new AtomicInteger(events));                            } else {                                tcount.addAndGet(events);                            }                        } catch (ChannelException e) {                            System.out.print("puts commit failed");                            tx.rollback();                        }                    } else {                        tx.rollback();                    }                    tx.close();                }                endGate.countDown();            }        };                t.start();        final Integer takeMapLock = 0;        t = new Thread() {            @Override            public void run() {                Random rng = new Random(Thread.currentThread().getId());                try {                    startGate.await();                } catch (InterruptedException e1) {                    Thread.currentThread().interrupt();                }                for (int j = 0; j < 10; j++) {                    int events = rng.nextInt(5) + 1;                    Transaction tx = channel.getTransaction();                    tx.begin();                    Event[] taken = new Event[events];                    int k;                    for (k = 0; k < events; k++) {                        taken[k] = channel.take();                        if (taken[k] == null)                            break;                    }                    if (rng.nextBoolean()) {                        try {                            tx.commit();                            for (Event e : taken) {                                if (e == null)                                    break;                                String index = new String(e.getBody());                                synchronized (takeMapLock) {                                    AtomicInteger remain = committedTakes.get(index);                                    if (remain == null) {                                        committedTakes.put(index, new AtomicInteger(1));                                    } else {                                        remain.incrementAndGet();                                    }                                }                            }                        } catch (ChannelException e) {                            System.out.print("takes commit failed");                            tx.rollback();                        }                    } else {                        tx.rollback();                    }                    tx.close();                }                endGate.countDown();            }        };                t.start();    }    startGate.countDown();    if (!endGate.await(20, TimeUnit.SECONDS)) {        Assert.fail("Not all threads ended succesfully");    }        Transaction tx = channel.getTransaction();    tx.begin();    Event e;        while ((e = channel.take()) != null) {        String index = new String(e.getBody());        AtomicInteger remain = committedPuts.get(index);        int post = remain.decrementAndGet();        if (post == 0) {            committedPuts.remove(index);        }    }    tx.commit();    tx.close();        for (Entry<String, AtomicInteger> takes : committedTakes.entrySet()) {        AtomicInteger count = committedPuts.get(takes.getKey());        if (count == null) {            Assert.fail("Putted data doesn't exist");        }        if (count.get() != takes.getValue().get()) {            Assert.fail(String.format("Mismatched put and take counts expected %d had %d", count.get(), takes.getValue().get()));        }        committedPuts.remove(takes.getKey());    }    if (!committedPuts.isEmpty()) {        Assert.fail("Puts still has entries remaining");    }}
public void flume_f3021_0()
{    Long tid = Thread.currentThread().getId();    String strtid = tid.toString();    Random rng = new Random(tid);    try {        startGate.await();    } catch (InterruptedException e1) {        Thread.currentThread().interrupt();    }    for (int j = 0; j < 10; j++) {        int events = rng.nextInt(5) + 1;        Transaction tx = channel.getTransaction();        tx.begin();        for (int k = 0; k < events; k++) {            channel.put(EventBuilder.withBody(strtid.getBytes()));        }        if (rng.nextBoolean()) {            try {                tx.commit();                AtomicInteger tcount = committedPuts.get(strtid);                if (tcount == null) {                    committedPuts.put(strtid, new AtomicInteger(events));                } else {                    tcount.addAndGet(events);                }            } catch (ChannelException e) {                System.out.print("puts commit failed");                tx.rollback();            }        } else {            tx.rollback();        }        tx.close();    }    endGate.countDown();}
public void flume_f3022_0()
{    Random rng = new Random(Thread.currentThread().getId());    try {        startGate.await();    } catch (InterruptedException e1) {        Thread.currentThread().interrupt();    }    for (int j = 0; j < 10; j++) {        int events = rng.nextInt(5) + 1;        Transaction tx = channel.getTransaction();        tx.begin();        Event[] taken = new Event[events];        int k;        for (k = 0; k < events; k++) {            taken[k] = channel.take();            if (taken[k] == null)                break;        }        if (rng.nextBoolean()) {            try {                tx.commit();                for (Event e : taken) {                    if (e == null)                        break;                    String index = new String(e.getBody());                    synchronized (takeMapLock) {                        AtomicInteger remain = committedTakes.get(index);                        if (remain == null) {                            committedTakes.put(index, new AtomicInteger(1));                        } else {                            remain.incrementAndGet();                        }                    }                }            } catch (ChannelException e) {                System.out.print("takes commit failed");                tx.rollback();            }        } else {            tx.rollback();        }        tx.close();    }    endGate.countDown();}
public void flume_f3023_0()
{    channel = new MemoryChannel();}
public void flume_f3024_0() throws InterruptedException, EventDeliveryException
{    Event event;    Event event2;    Context context = new Context();    int putCounter = 0;    context.put("keep-alive", "1");    context.put("capacity", "100");    context.put("transactionCapacity", "50");    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    Assert.assertNotNull(transaction);    transaction.begin();    for (putCounter = 0; putCounter < 10; putCounter++) {        event = EventBuilder.withBody(("test event" + putCounter).getBytes());        channel.put(event);    }    transaction.commit();    transaction.close();    transaction = channel.getTransaction();    Assert.assertNotNull(transaction);    transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 10; i++) {        event2 = channel.take();        Assert.assertNotNull("lost an event", event2);        Assert.assertArrayEquals(event2.getBody(), ("test event" + i).getBytes());        }    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.commit();    transaction.close();}
public void flume_f3025_0() throws InterruptedException, EventDeliveryException
{    Event event;    Event event2;    Context context = new Context();    int putCounter = 0;    context.put("keep-alive", "1");    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    Assert.assertNotNull(transaction);        transaction.begin();    for (putCounter = 0; putCounter < 10; putCounter++) {        event = EventBuilder.withBody(("test event" + putCounter).getBytes());        channel.put(event);    }    transaction.rollback();    transaction.close();        transaction = channel.getTransaction();    transaction.begin();    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.commit();    transaction.close();        transaction = channel.getTransaction();    transaction.begin();    for (putCounter = 0; putCounter < 10; putCounter++) {        event = EventBuilder.withBody(("test event" + putCounter).getBytes());        channel.put(event);    }    transaction.commit();    transaction.close();    transaction = channel.getTransaction();    Assert.assertNotNull(transaction);        transaction.begin();    for (int i = 0; i < 10; i++) {        event2 = channel.take();        Assert.assertNotNull("lost an event", event2);        Assert.assertArrayEquals(event2.getBody(), ("test event" + i).getBytes());    }    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.rollback();    transaction.close();        transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 10; i++) {        event2 = channel.take();        Assert.assertNotNull("lost an event", event2);        Assert.assertArrayEquals(event2.getBody(), ("test event" + i).getBytes());    }    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.rollback();    transaction.close();}
public void flume_f3026_0() throws InterruptedException, EventDeliveryException
{    Event event;    Event event2;    Context context = new Context();    int putCounter = 0;    context.put("keep-alive", "1");    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    Assert.assertNotNull(transaction);        transaction.begin();    for (putCounter = 0; putCounter < 10; putCounter++) {                transaction.begin();        event = EventBuilder.withBody(("test event" + putCounter).getBytes());        channel.put(event);                transaction.commit();    }    transaction.commit();    transaction.close();    transaction = channel.getTransaction();    Assert.assertNotNull(transaction);    transaction.begin();    for (int i = 0; i < 10; i++) {        event2 = channel.take();        Assert.assertNotNull("lost an event", event2);        Assert.assertArrayEquals(event2.getBody(), ("test event" + i).getBytes());        }    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.commit();    transaction.close();}
public void flume_f3027_0() throws InterruptedException, EventDeliveryException
{    Event event;    Event event2;    Context context = new Context();    int putCounter = 0;    context.put("keep-alive", "1");    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    Assert.assertNotNull(transaction);        transaction.begin();    for (putCounter = 0; putCounter < 10; putCounter++) {        event = EventBuilder.withBody(("test event" + putCounter).getBytes());        channel.put(event);    }    transaction.rollback();    transaction.close();        transaction = channel.getTransaction();    transaction.begin();    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.commit();    transaction.close();        transaction = channel.getTransaction();    transaction.begin();    for (putCounter = 0; putCounter < 10; putCounter++) {        event = EventBuilder.withBody(("test event" + putCounter).getBytes());        channel.put(event);    }    transaction.commit();    transaction.close();    transaction = channel.getTransaction();    Assert.assertNotNull(transaction);        transaction.begin();    for (int i = 0; i < 10; i++) {                transaction.begin();        event2 = channel.take();        Assert.assertNotNull("lost an event", event2);        Assert.assertArrayEquals(event2.getBody(), ("test event" + i).getBytes());                transaction.commit();    }    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.rollback();    transaction.close();        transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 10; i++) {        event2 = channel.take();        Assert.assertNotNull("lost an event", event2);        Assert.assertArrayEquals(event2.getBody(), ("test event" + i).getBytes());    }    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.rollback();    transaction.close();}
public void flume_f3028_0() throws Exception
{    channels.clear();    channels.add(MockChannel.createMockChannel("ch1"));    channels.add(MockChannel.createMockChannel("ch2"));    channels.add(MockChannel.createMockChannel("ch3"));    config.put("type", "multiplexing");    config.put("header", "myheader");    config.put("optional.foo", "ch2 ch3");    config.put("optional.xyz", "ch1 ch3");    config.put("optional.zebra", "ch1 ch2");}
public void flume_f3029_0() throws Exception
{    config.put("mapping.foo", "ch1 ch2");    config.put("mapping.bar", "ch2 ch3");    config.put("mapping.xyz", "ch1 ch2 ch3");    config.put("default", "ch1 ch3");    selector = ChannelSelectorFactory.create(channels, config);    Assert.assertTrue(selector instanceof MultiplexingChannelSelector);    Event event1 = new MockEvent();    Map<String, String> header1 = new HashMap<String, String>();        header1.put("myheader", "foo");    event1.setHeaders(header1);    List<Channel> reqCh1 = selector.getRequiredChannels(event1);    Assert.assertEquals(2, reqCh1.size());    Assert.assertTrue(reqCh1.get(0).getName().equals("ch1"));    Assert.assertTrue(reqCh1.get(1).getName().equals("ch2"));    List<Channel> optCh1 = selector.getOptionalChannels(event1);    Assert.assertTrue(optCh1.size() == 1);        Assert.assertTrue(optCh1.get(0).getName().equals("ch3"));    Event event2 = new MockEvent();    Map<String, String> header2 = new HashMap<String, String>();        header2.put("myheader", "bar");    event2.setHeaders(header2);    List<Channel> reqCh2 = selector.getRequiredChannels(event2);    Assert.assertEquals(2, reqCh2.size());    Assert.assertTrue(reqCh2.get(0).getName().equals("ch2"));    Assert.assertTrue(reqCh2.get(1).getName().equals("ch3"));    List<Channel> optCh2 = selector.getOptionalChannels(event2);    Assert.assertTrue(optCh2.isEmpty());    Event event3 = new MockEvent();    Map<String, String> header3 = new HashMap<String, String>();        header3.put("myheader", "xyz");    event3.setHeaders(header3);    List<Channel> reqCh3 = selector.getRequiredChannels(event3);    Assert.assertEquals(3, reqCh3.size());    Assert.assertTrue(reqCh3.get(0).getName().equals("ch1"));    Assert.assertTrue(reqCh3.get(1).getName().equals("ch2"));    Assert.assertTrue(reqCh3.get(2).getName().equals("ch3"));    List<Channel> optCh3 = selector.getOptionalChannels(event3);        Assert.assertTrue(optCh3.size() == 0);}
public void flume_f3030_0() throws Exception
{    config.put("mapping.foo", "ch1 ch2");    config.put("mapping.bar", "ch2 ch3");    config.put("mapping.xyz", "ch1 ch2 ch3");    config.put("default", "ch1 ch3");    selector = ChannelSelectorFactory.create(channels, config);    Assert.assertTrue(selector instanceof MultiplexingChannelSelector);    Event noHeaderEvent = new MockEvent();    List<Channel> reqCh1 = selector.getRequiredChannels(noHeaderEvent);    List<Channel> optCh1 = selector.getOptionalChannels(noHeaderEvent);    Assert.assertEquals(2, reqCh1.size());    Assert.assertTrue(reqCh1.get(0).getName().equals("ch1"));    Assert.assertTrue(reqCh1.get(1).getName().equals("ch3"));    Assert.assertTrue(optCh1.isEmpty());    Map<String, String> header2 = new HashMap<String, String>();    header2.put("someheader", "foo");    Event invalidHeaderEvent = new MockEvent();    invalidHeaderEvent.setHeaders(header2);    List<Channel> reqCh2 = selector.getRequiredChannels(invalidHeaderEvent);    List<Channel> optCh2 = selector.getOptionalChannels(invalidHeaderEvent);    Assert.assertEquals(2, reqCh2.size());    Assert.assertTrue(reqCh2.get(0).getName().equals("ch1"));    Assert.assertTrue(reqCh2.get(1).getName().equals("ch3"));    Assert.assertTrue(optCh2.isEmpty());    Map<String, String> header3 = new HashMap<String, String>();    header3.put("myheader", "bar1");    Event unmatchedHeaderEvent = new MockEvent();    unmatchedHeaderEvent.setHeaders(header3);    List<Channel> reqCh3 = selector.getRequiredChannels(unmatchedHeaderEvent);    List<Channel> optCh3 = selector.getOptionalChannels(unmatchedHeaderEvent);    Assert.assertEquals(2, reqCh3.size());    Assert.assertTrue(reqCh3.get(0).getName().equals("ch1"));    Assert.assertTrue(reqCh3.get(1).getName().equals("ch3"));    Assert.assertTrue(optCh3.isEmpty());    Map<String, String> header4 = new HashMap<String, String>();    header4.put("myheader", "zebra");    Event zebraEvent = new MockEvent();    zebraEvent.setHeaders(header4);    List<Channel> reqCh4 = selector.getRequiredChannels(zebraEvent);    List<Channel> optCh4 = selector.getOptionalChannels(zebraEvent);    Assert.assertEquals(2, reqCh4.size());    Assert.assertTrue(reqCh4.get(0).getName().equals("ch1"));    Assert.assertTrue(reqCh4.get(1).getName().equals("ch3"));        Assert.assertTrue(optCh4.size() == 1);    Assert.assertTrue(optCh4.get(0).getName().equals("ch2"));    List<Channel> allChannels = selector.getAllChannels();    Assert.assertTrue(allChannels.size() == 3);    Assert.assertTrue(allChannels.get(0).getName().equals("ch1"));    Assert.assertTrue(allChannels.get(1).getName().equals("ch2"));    Assert.assertTrue(allChannels.get(2).getName().equals("ch3"));}
public void flume_f3031_0()
{    config.put("mapping.foo", "ch1 ch2");    config.put("mapping.bar", "ch2 ch3");    config.put("mapping.xyz", "ch1 ch2 ch3");    config.put("mapping.zebra", "ch2");    config.put("optional.zebra", "ch1 ch3");    selector = ChannelSelectorFactory.create(channels, config);    Assert.assertTrue(selector instanceof MultiplexingChannelSelector);    Event event1 = new MockEvent();    Map<String, String> header1 = new HashMap<String, String>();        header1.put("myheader", "foo");    event1.setHeaders(header1);    List<Channel> reqCh1 = selector.getRequiredChannels(event1);    Assert.assertEquals(2, reqCh1.size());    Assert.assertEquals("ch1", reqCh1.get(0).getName());    Assert.assertEquals("ch2", reqCh1.get(1).getName());    List<Channel> optCh1 = selector.getOptionalChannels(event1);    Assert.assertTrue(optCh1.size() == 1);        Assert.assertEquals("ch3", optCh1.get(0).getName());    Event event2 = new MockEvent();    Map<String, String> header2 = new HashMap<String, String>();        header2.put("myheader", "bar");    event2.setHeaders(header2);    List<Channel> reqCh2 = selector.getRequiredChannels(event2);    Assert.assertEquals(2, reqCh2.size());    Assert.assertEquals("ch2", reqCh2.get(0).getName());    Assert.assertEquals("ch3", reqCh2.get(1).getName());    List<Channel> optCh2 = selector.getOptionalChannels(event2);    Assert.assertTrue(optCh2.isEmpty());    Event event3 = new MockEvent();    Map<String, String> header3 = new HashMap<String, String>();        header3.put("myheader", "xyz");    event3.setHeaders(header3);    List<Channel> reqCh3 = selector.getRequiredChannels(event3);    Assert.assertEquals(3, reqCh3.size());    Assert.assertEquals("ch1", reqCh3.get(0).getName());    Assert.assertEquals("ch2", reqCh3.get(1).getName());    Assert.assertEquals("ch3", reqCh3.get(2).getName());    List<Channel> optCh3 = selector.getOptionalChannels(event3);        Assert.assertTrue(optCh3.isEmpty());    Event event4 = new MockEvent();    Map<String, String> header4 = new HashMap<String, String>();    header4.put("myheader", "zebra");    event4.setHeaders(header4);    List<Channel> reqCh4 = selector.getRequiredChannels(event4);    Assert.assertEquals(1, reqCh4.size());    Assert.assertEquals("ch2", reqCh4.get(0).getName());    List<Channel> optCh4 = selector.getOptionalChannels(event4);    Assert.assertEquals(2, optCh4.size());    Assert.assertEquals("ch1", optCh4.get(0).getName());    Assert.assertEquals("ch3", optCh4.get(1).getName());}
public void flume_f3032_0()
{    config.put("default", "ch3");    config.put("optional.foo", "ch1 ch2");    config.put("optional.zebra", "ch2 ch3");    selector = ChannelSelectorFactory.create(channels, config);    Assert.assertTrue(selector instanceof MultiplexingChannelSelector);    Event event1 = new MockEvent();    Map<String, String> header1 = new HashMap<String, String>();        header1.put("myheader", "foo");    event1.setHeaders(header1);    List<Channel> reqCh1 = selector.getRequiredChannels(event1);    Assert.assertEquals(1, reqCh1.size());    Assert.assertEquals("ch3", reqCh1.get(0).getName());    List<Channel> optCh1 = selector.getOptionalChannels(event1);    Assert.assertEquals(2, optCh1.size());        Assert.assertEquals("ch1", optCh1.get(0).getName());    Assert.assertEquals("ch2", optCh1.get(1).getName());    Event event4 = new MockEvent();    Map<String, String> header4 = new HashMap<String, String>();    header4.put("myheader", "zebra");    event4.setHeaders(header4);    List<Channel> reqCh4 = selector.getRequiredChannels(event4);    Assert.assertEquals(1, reqCh4.size());    Assert.assertTrue(reqCh4.get(0).getName().equals("ch3"));    List<Channel> optCh4 = selector.getOptionalChannels(event4);            Assert.assertEquals(1, optCh4.size());    Assert.assertEquals("ch2", optCh4.get(0).getName());}
public void flume_f3033_0()
{    config.put("optional.foo", "ch1 ch2");    config.put("optional.zebra", "ch2 ch3");    selector = ChannelSelectorFactory.create(channels, config);    Assert.assertTrue(selector instanceof MultiplexingChannelSelector);    Event event1 = new MockEvent();    Map<String, String> header1 = new HashMap<String, String>();        header1.put("myheader", "foo");    event1.setHeaders(header1);    List<Channel> reqCh1 = selector.getRequiredChannels(event1);    Assert.assertTrue(reqCh1.isEmpty());    List<Channel> optCh1 = selector.getOptionalChannels(event1);    Assert.assertEquals(2, optCh1.size());        Event event4 = new MockEvent();    Map<String, String> header4 = new HashMap<String, String>();    header4.put("myheader", "zebra");    event4.setHeaders(header4);    List<Channel> reqCh4 = selector.getRequiredChannels(event4);    Assert.assertTrue(reqCh4.isEmpty());    List<Channel> optCh4 = selector.getOptionalChannels(event4);    Assert.assertEquals(2, optCh4.size());    Assert.assertEquals("ch2", optCh4.get(0).getName());    Assert.assertEquals("ch3", optCh4.get(1).getName());}
public void flume_f3034_0() throws Exception
{    channels.clear();    channels.add(MockChannel.createMockChannel("ch1"));    channels.add(MockChannel.createMockChannel("ch2"));    channels.add(MockChannel.createMockChannel("ch3"));    channels.add(MockChannel.createMockChannel("ch4"));    selector = ChannelSelectorFactory.create(channels, new HashMap<String, String>());}
public void flume_f3035_0() throws Exception
{    selector.configure(new Context());    List<Channel> channels = selector.getRequiredChannels(new MockEvent());    Assert.assertNotNull(channels);    Assert.assertEquals(4, channels.size());    Assert.assertEquals("ch1", channels.get(0).getName());    Assert.assertEquals("ch2", channels.get(1).getName());    Assert.assertEquals("ch3", channels.get(2).getName());    Assert.assertEquals("ch4", channels.get(3).getName());    List<Channel> optCh = selector.getOptionalChannels(new MockEvent());    Assert.assertEquals(0, optCh.size());}
public void flume_f3036_0() throws Exception
{    Context context = new Context();    context.put(ReplicatingChannelSelector.CONFIG_OPTIONAL, "ch1");    Configurables.configure(selector, context);    List<Channel> channels = selector.getRequiredChannels(new MockEvent());    Assert.assertNotNull(channels);    Assert.assertEquals(3, channels.size());    Assert.assertEquals("ch2", channels.get(0).getName());    Assert.assertEquals("ch3", channels.get(1).getName());    Assert.assertEquals("ch4", channels.get(2).getName());    List<Channel> optCh = selector.getOptionalChannels(new MockEvent());    Assert.assertEquals(1, optCh.size());    Assert.assertEquals("ch1", optCh.get(0).getName());}
public void flume_f3037_0() throws Exception
{    Context context = new Context();    context.put(ReplicatingChannelSelector.CONFIG_OPTIONAL, "ch1 ch4");    Configurables.configure(selector, context);    List<Channel> channels = selector.getRequiredChannels(new MockEvent());    Assert.assertNotNull(channels);    Assert.assertEquals(2, channels.size());    Assert.assertEquals("ch2", channels.get(0).getName());    Assert.assertEquals("ch3", channels.get(1).getName());    List<Channel> optCh = selector.getOptionalChannels(new MockEvent());    Assert.assertEquals(2, optCh.size());    Assert.assertEquals("ch1", optCh.get(0).getName());    Assert.assertEquals("ch4", optCh.get(1).getName());}
public void flume_f3038_0() throws Exception
{    Context context = new Context();    context.put(ReplicatingChannelSelector.CONFIG_OPTIONAL, "ch1 ch4 ch1");    Configurables.configure(selector, context);    List<Channel> channels = selector.getRequiredChannels(new MockEvent());    Assert.assertNotNull(channels);    Assert.assertEquals(2, channels.size());    Assert.assertEquals("ch2", channels.get(0).getName());    Assert.assertEquals("ch3", channels.get(1).getName());    List<Channel> optCh = selector.getOptionalChannels(new MockEvent());    Assert.assertEquals(2, optCh.size());    Assert.assertEquals("ch1", optCh.get(0).getName());    Assert.assertEquals("ch4", optCh.get(1).getName());}
public void flume_f3039_0()
{    tmpDir = Files.createTempDir();}
public void flume_f3040_0()
{    for (File f : tmpDir.listFiles()) {        f.delete();    }    tmpDir.delete();}
public void flume_f3041_0() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    SimpleTextLineEventReader reader = new SimpleTextLineEventReader(new FileReader(f1));    assertEquals("file1line1", bodyAsString(reader.readEvent()));    assertEquals("file1line2", bodyAsString(reader.readEvent()));    assertEquals("file1line3", bodyAsString(reader.readEvent()));    assertEquals("file1line4", bodyAsString(reader.readEvent()));    assertEquals("file1line5", bodyAsString(reader.readEvent()));    assertEquals("file1line6", bodyAsString(reader.readEvent()));    assertEquals("file1line7", bodyAsString(reader.readEvent()));    assertEquals("file1line8", bodyAsString(reader.readEvent()));    assertEquals(null, reader.readEvent());}
public void flume_f3042_0() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    SimpleTextLineEventReader reader = new SimpleTextLineEventReader(new FileReader(f1));    List<String> out = bodiesAsStrings(reader.readEvents(5));        assertEquals(5, out.size());    assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file1line3"));    assertTrue(out.contains("file1line4"));    assertTrue(out.contains("file1line5"));}
public void flume_f3043_0() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    SimpleTextLineEventReader reader = new SimpleTextLineEventReader(new FileReader(f1));    List<String> out = bodiesAsStrings(reader.readEvents(10));        assertEquals(8, out.size());    assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file1line3"));    assertTrue(out.contains("file1line4"));    assertTrue(out.contains("file1line5"));    assertTrue(out.contains("file1line6"));    assertTrue(out.contains("file1line7"));    assertTrue(out.contains("file1line8"));}
public void flume_f3044_0() throws IOException, InterruptedException
{    if (!WORK_DIR.isDirectory()) {        Files.createParentDirs(new File(WORK_DIR, "dummy"));    }        for (int i = 0; i < 4; i++) {        File fileName = new File(WORK_DIR, "file" + i);        StringBuilder sb = new StringBuilder();                for (int j = 0; j < i; j++) {            sb.append("file" + i + "line" + j + "\n");        }        Files.write(sb.toString(), fileName, Charsets.UTF_8);    }        Thread.sleep(1500L);    Files.write("\n", new File(WORK_DIR, "emptylineFile"), Charsets.UTF_8);}
public void flume_f3045_0()
{    deleteDir(WORK_DIR);}
private void flume_f3046_1(File dir)
{        try {        FileUtils.deleteDirectory(dir);    } catch (IOException e) {            }}
private void flume_f3047_0(ReliableEventReader reader, int nEvents) throws IOException
{    List<Event> events;    do {        events = reader.readEvents(nEvents);        reader.commit();    } while (!events.isEmpty());}
private boolean flume_f3048_0(File dir, String[] files)
{    List<File> actualFiles = listFiles(dir);    Set<String> expectedFiles = new HashSet<String>(Arrays.asList(files));        if (actualFiles.size() != expectedFiles.size()) {        return false;    }        for (File f : actualFiles) {        expectedFiles.remove(f.getName());    }    return expectedFiles.isEmpty();}
public void flume_f3049_0() throws IOException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).includePattern("^file2$").deletePolicy(DeletePolicy.IMMEDIATE.toString()).sourceCounter(new SourceCounter("test")).build();    String[] beforeFiles = { "file0", "file1", "file2", "file3", "emptylineFile" };    Assert.assertTrue("Expected " + beforeFiles.length + " files in working dir", checkLeftFilesInDir(WORK_DIR, beforeFiles));    processEventsWithReader(reader, 10);    String[] afterFiles = { "file0", "file1", "file3", "emptylineFile" };    Assert.assertTrue("Expected " + afterFiles.length + " files left in working dir", checkLeftFilesInDir(WORK_DIR, afterFiles));    Assert.assertTrue("Expected no files left in tracker dir", checkLeftFilesInDir(TRACKER_DIR, new String[0]));}
public void flume_f3050_0() throws IOException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).ignorePattern("^file2$").deletePolicy(DeletePolicy.IMMEDIATE.toString()).sourceCounter(new SourceCounter("test")).build();    String[] beforeFiles = { "file0", "file1", "file2", "file3", "emptylineFile" };    Assert.assertTrue("Expected " + beforeFiles.length + " files in working dir", checkLeftFilesInDir(WORK_DIR, beforeFiles));    processEventsWithReader(reader, 10);    String[] files = { "file2" };    Assert.assertTrue("Expected " + files.length + " files left in working dir", checkLeftFilesInDir(WORK_DIR, files));    Assert.assertTrue("Expected no files left in tracker dir", checkLeftFilesInDir(TRACKER_DIR, new String[0]));}
public void flume_f3051_0() throws IOException
{                                ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).ignorePattern("^file[013]$").includePattern("^file2$").deletePolicy(DeletePolicy.IMMEDIATE.toString()).sourceCounter(new SourceCounter("test")).build();    String[] beforeFiles = { "file0", "file1", "file2", "file3", "emptylineFile" };    Assert.assertTrue("Expected " + beforeFiles.length + " files in working dir", checkLeftFilesInDir(WORK_DIR, beforeFiles));    processEventsWithReader(reader, 10);    String[] files = { "file0", "file1", "file3", "emptylineFile" };    Assert.assertTrue("Expected " + files.length + " files left in working dir", checkLeftFilesInDir(WORK_DIR, files));    Assert.assertTrue("Expected no files left in tracker dir", checkLeftFilesInDir(TRACKER_DIR, new String[0]));}
public void flume_f3052_0() throws IOException
{                        ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).ignorePattern("^file2$").includePattern("^file2$").deletePolicy(DeletePolicy.IMMEDIATE.toString()).sourceCounter(new SourceCounter("test")).build();    String[] beforeFiles = { "file0", "file1", "file2", "file3", "emptylineFile" };    Assert.assertTrue("Expected " + beforeFiles.length + " files in working dir", checkLeftFilesInDir(WORK_DIR, beforeFiles));    processEventsWithReader(reader, 10);    String[] files = { "file0", "file1", "file2", "file3", "emptylineFile" };    Assert.assertTrue("Expected " + files.length + " files left in working dir", checkLeftFilesInDir(WORK_DIR, files));    Assert.assertTrue("Expected no files left in tracker dir", checkLeftFilesInDir(TRACKER_DIR, new String[0]));}
public void flume_f3053_0() throws IOException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).sourceCounter(new SourceCounter("test")).build();    final int expectedLines = 1 + 1 + 2 + 3 + 1;    int seenLines = 0;    for (int i = 0; i < 10; i++) {        List<Event> events = reader.readEvents(10);        seenLines += events.size();        reader.commit();    }    Assert.assertEquals(expectedLines, seenLines);}
public void flume_f3054_0() throws IOException
{    String trackerDirPath = SpoolDirectorySourceConfigurationConstants.DEFAULT_TRACKER_DIR;    File trackerDir = new File(WORK_DIR, trackerDirPath);    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).trackerDirPath(trackerDirPath).sourceCounter(new SourceCounter("test")).build();    final int expectedLines = 1 + 1 + 2 + 3 + 1;    int seenLines = 0;    for (int i = 0; i < 10; i++) {        List<Event> events = reader.readEvents(10);        int numEvents = events.size();        if (numEvents > 0) {            seenLines += numEvents;            reader.commit();                        File[] files = trackerDir.listFiles();            Assert.assertNotNull(files);            Assert.assertTrue("Expected tracker files in tracker dir " + trackerDir.getAbsolutePath(), files.length > 0);        }    }    Assert.assertEquals(expectedLines, seenLines);}
public void flume_f3055_0() throws IOException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).deletePolicy(DeletePolicy.IMMEDIATE.name()).sourceCounter(new SourceCounter("test")).build();    List<File> before = listFiles(WORK_DIR);    Assert.assertEquals("Expected 5, not: " + before, 5, before.size());    List<Event> events;    do {        events = reader.readEvents(10);        reader.commit();    } while (!events.isEmpty());    List<File> after = listFiles(WORK_DIR);    Assert.assertEquals("Expected 0, not: " + after, 0, after.size());    List<File> trackerFiles = listFiles(new File(WORK_DIR, SpoolDirectorySourceConfigurationConstants.DEFAULT_TRACKER_DIR));    Assert.assertEquals("Expected 0, not: " + trackerFiles, 0, trackerFiles.size());}
public void flume_f3056_0() throws IOException
{    new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).consumeOrder(null).sourceCounter(new SourceCounter("test")).build();}
public void flume_f3057_0() throws IOException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).consumeOrder(ConsumeOrder.RANDOM).sourceCounter(new SourceCounter("test")).build();    File fileName = new File(WORK_DIR, "new-file");    FileUtils.write(fileName, "New file created in the end. Shoud be read randomly.\n");    Set<String> actual = Sets.newHashSet();    readEventsForFilesInDir(WORK_DIR, reader, actual);    Set<String> expected = Sets.newHashSet();    createExpectedFromFilesInSetup(expected);    expected.add("");    expected.add("New file created in the end. Shoud be read randomly.");    Assert.assertEquals(expected, actual);}
public void flume_f3058_0() throws Exception
{        if (SystemUtils.IS_OS_WINDOWS) {        return;    }    final ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).consumeOrder(ConsumeOrder.RANDOM).sourceCounter(new SourceCounter("test")).build();    File fileName = new File(WORK_DIR, "new-file");    FileUtils.write(fileName, "New file created in the end. Shoud be read randomly.\n");    Set<String> expected = Sets.newHashSet();    int totalFiles = WORK_DIR.listFiles().length;    final Set<String> actual = Sets.newHashSet();    ExecutorService executor = Executors.newSingleThreadExecutor();    final Semaphore semaphore1 = new Semaphore(0);    final Semaphore semaphore2 = new Semaphore(0);    Future<Void> wait = executor.submit(new Callable<Void>() {        @Override        public Void call() throws Exception {            readEventsForFilesInDir(WORK_DIR, reader, actual, semaphore1, semaphore2);            return null;        }    });    semaphore1.acquire();    File finalFile = new File(WORK_DIR, "t-file");    FileUtils.write(finalFile, "Last file");    semaphore2.release();    wait.get();    int listFilesCount = ((ReliableSpoolingFileEventReader) reader).getListFilesCount();    finalFile.delete();    createExpectedFromFilesInSetup(expected);    expected.add("");    expected.add("New file created in the end. Shoud be read randomly.");    expected.add("Last file");    Assert.assertTrue(listFilesCount < (totalFiles + 2));    Assert.assertEquals(expected, actual);}
public Void flume_f3059_0() throws Exception
{    readEventsForFilesInDir(WORK_DIR, reader, actual, semaphore1, semaphore2);    return null;}
public void flume_f3060_0() throws IOException, InterruptedException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).consumeOrder(ConsumeOrder.OLDEST).sourceCounter(new SourceCounter("test")).build();    File file1 = new File(WORK_DIR, "new-file1");    File file2 = new File(WORK_DIR, "new-file2");    File file3 = new File(WORK_DIR, "new-file3");    Thread.sleep(1000L);    FileUtils.write(file2, "New file2 created.\n");    Thread.sleep(1000L);    FileUtils.write(file1, "New file1 created.\n");    Thread.sleep(1000L);    FileUtils.write(file3, "New file3 created.\n");        List<String> actual = Lists.newLinkedList();    readEventsForFilesInDir(WORK_DIR, reader, actual);    List<String> expected = Lists.newLinkedList();    createExpectedFromFilesInSetup(expected);        expected.add("");    expected.add("New file2 created.");    expected.add("New file1 created.");    expected.add("New file3 created.");    Assert.assertEquals(expected, actual);}
public void flume_f3061_0() throws IOException, InterruptedException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).consumeOrder(ConsumeOrder.YOUNGEST).sourceCounter(new SourceCounter("test")).build();    File file1 = new File(WORK_DIR, "new-file1");    File file2 = new File(WORK_DIR, "new-file2");    File file3 = new File(WORK_DIR, "new-file3");    Thread.sleep(1000L);    FileUtils.write(file2, "New file2 created.\n");    Thread.sleep(1000L);    FileUtils.write(file3, "New file3 created.\n");    Thread.sleep(1000L);    FileUtils.write(file1, "New file1 created.\n");        List<String> actual = Lists.newLinkedList();    readEventsForFilesInDir(WORK_DIR, reader, actual);    List<String> expected = Lists.newLinkedList();    createExpectedFromFilesInSetup(expected);    Collections.sort(expected);        expected.add(0, "");    expected.add(0, "New file2 created.");    expected.add(0, "New file3 created.");    expected.add(0, "New file1 created.");    Assert.assertEquals(expected, actual);}
public void flume_f3062_0() throws IOException, InterruptedException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).consumeOrder(ConsumeOrder.OLDEST).sourceCounter(new SourceCounter("test")).build();    File file1 = new File(WORK_DIR, "new-file1");    File file2 = new File(WORK_DIR, "new-file2");    File file3 = new File(WORK_DIR, "new-file3");    Thread.sleep(1000L);    FileUtils.write(file3, "New file3 created.\n");    FileUtils.write(file2, "New file2 created.\n");    FileUtils.write(file1, "New file1 created.\n");    file1.setLastModified(file3.lastModified());    file1.setLastModified(file2.lastModified());            List<String> actual = Lists.newLinkedList();    readEventsForFilesInDir(WORK_DIR, reader, actual);    List<String> expected = Lists.newLinkedList();    createExpectedFromFilesInSetup(expected);        expected.add("");    expected.add("New file1 created.");    expected.add("New file2 created.");    expected.add("New file3 created.");    Assert.assertEquals(expected, actual);}
public void flume_f3063_0() throws IOException, InterruptedException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).consumeOrder(ConsumeOrder.YOUNGEST).sourceCounter(new SourceCounter("test")).build();    File file1 = new File(WORK_DIR, "new-file1");    File file2 = new File(WORK_DIR, "new-file2");    File file3 = new File(WORK_DIR, "new-file3");    Thread.sleep(1000L);    FileUtils.write(file1, "New file1 created.\n");    FileUtils.write(file2, "New file2 created.\n");    FileUtils.write(file3, "New file3 created.\n");    file1.setLastModified(file3.lastModified());    file1.setLastModified(file2.lastModified());            List<String> actual = Lists.newLinkedList();    readEventsForFilesInDir(WORK_DIR, reader, actual);    List<String> expected = Lists.newLinkedList();    createExpectedFromFilesInSetup(expected);        expected.add(0, "");    expected.add(0, "New file3 created.");    expected.add(0, "New file2 created.");    expected.add(0, "New file1 created.");    Assert.assertEquals(expected, actual);}
public void flume_f3064_0() throws IOException
{    templateTestForRecursiveDirs(ConsumeOrder.OLDEST, null, 3, 3, 37, TrackingPolicy.RENAME);}
public void flume_f3065_0() throws IOException
{    templateTestForRecursiveDirs(ConsumeOrder.YOUNGEST, Comparator.reverseOrder(), 3, 3, 37, TrackingPolicy.RENAME);}
public void flume_f3066_0() throws IOException
{    templateTestForRecursiveDirs(ConsumeOrder.RANDOM, null, 3, 3, 37, TrackingPolicy.RENAME);}
public void flume_f3067_0() throws IOException
{    templateTestForRecursiveDirs(ConsumeOrder.OLDEST, null, 3, 3, 10, TrackingPolicy.TRACKER_DIR);}
public void flume_f3068_0() throws IOException
{    templateTestForRecursiveDirs(ConsumeOrder.YOUNGEST, Comparator.reverseOrder(), 3, 3, 10, TrackingPolicy.TRACKER_DIR);}
public void flume_f3069_0() throws IOException
{    templateTestForRecursiveDirs(ConsumeOrder.RANDOM, null, 3, 3, 10, TrackingPolicy.TRACKER_DIR);}
public void flume_f3070_0() throws IOException
{    String trackerDirPath = SpoolDirectorySourceConfigurationConstants.DEFAULT_TRACKER_DIR;    File trackerDir = new File(WORK_DIR, trackerDirPath);    if (!trackerDir.exists()) {        trackerDir.mkdir();    }    File trackerFile = new File(trackerDir, ReliableSpoolingFileEventReader.metaFileName);    if (trackerFile.exists()) {        trackerFile.delete();    }    trackerFile.createNewFile();    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).trackerDirPath(trackerDirPath).sourceCounter(new SourceCounter("test")).build();    final int expectedLines = 1;    int seenLines = 0;    List<Event> events = reader.readEvents(10);    int numEvents = events.size();    if (numEvents > 0) {        seenLines += numEvents;        reader.commit();    }        Assert.assertEquals(expectedLines, seenLines);}
private void flume_f3071_0(ConsumeOrder order, Comparator<Long> comparator, int depth, int dirNum, int fileNum, TrackingPolicy trackingPolicy) throws IOException
{    File dir = null;    try {        dir = new File("target/test/work/" + this.getClass().getSimpleName() + "_large");        Files.createParentDirs(new File(dir, "dummy"));        ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(dir).consumeOrder(order).trackingPolicy(trackingPolicy.toString()).recursiveDirectorySearch(true).sourceCounter(new SourceCounter("test")).build();        Map<Long, List<String>> expected;        if (comparator == null) {            expected = new TreeMap<Long, List<String>>();        } else {            expected = new TreeMap<Long, List<String>>(comparator);        }        createMultilevelFiles(dir, 0, depth, dirNum, fileNum, expected, new MutableLong(0L));        Collection<String> expectedColl;        int index = 0;        if (order == ConsumeOrder.RANDOM) {            expectedColl = Sets.newHashSet();        } else {            expectedColl = new ArrayList<>();        }        for (Entry<Long, List<String>> entry : expected.entrySet()) {            Collections.sort(entry.getValue());            expectedColl.addAll(entry.getValue());        }        int expNum = expectedColl.size();        int actualNum = 0;        for (int i = 0; i < expNum; i++) {            List<Event> events;            events = reader.readEvents(10);            for (Event e : events) {                actualNum++;                if (order == ConsumeOrder.RANDOM) {                    Assert.assertTrue(expectedColl.remove(new String(e.getBody())));                } else {                    String exp = ((ArrayList<String>) expectedColl).get(index);                    String actual = new String(e.getBody());                    Assert.assertEquals(exp, actual);                    index++;                }            }            reader.commit();        }        Assert.assertEquals(expNum, actualNum);    } finally {        deleteDir(dir);    }}
private void flume_f3072_0(File dir, int currDepth, int maxDepth, int dirNum, int fileNum, Map<Long, List<String>> expected, MutableLong id) throws IOException
{    if (currDepth == maxDepth) {        createFiles(dir, fileNum, expected, id);    } else {        for (int i = 0; i < dirNum; i++) {            File nextDir = new File(dir, "dir-" + i);            nextDir.mkdirs();            createMultilevelFiles(nextDir, currDepth + 1, maxDepth, dirNum, fileNum, expected, id);        }    }}
private void flume_f3073_0(File dir, int fileNum, Map<Long, List<String>> expected, MutableLong id) throws IOException
{    for (int i = 0; i < fileNum; i++) {        File f = new File(dir, "file-" + id);        String data = f.getPath();        Files.write(data, f, Charsets.UTF_8);        long lastMod = id.longValue() * 10000L;        f.setLastModified(lastMod);        if (expected.containsKey(f.lastModified())) {            expected.get(f.lastModified()).add(data);        } else {            expected.put(f.lastModified(), Lists.newArrayList(data));        }        id.increment();    }}
private void flume_f3074_0(File dir, ReliableEventReader reader, Collection<String> actual) throws IOException
{    readEventsForFilesInDir(dir, reader, actual, null, null);}
private void flume_f3075_0(File dir, ReliableEventReader reader, Collection<String> actual, Semaphore semaphore1, Semaphore semaphore2) throws IOException
{    List<Event> events;    boolean executed = false;    for (int i = 0; i < listFiles(dir).size(); i++) {        events = reader.readEvents(10);        for (Event e : events) {            actual.add(new String(e.getBody()));        }        reader.commit();        try {            if (!executed) {                executed = true;                if (semaphore1 != null) {                    semaphore1.release();                }                if (semaphore2 != null) {                    semaphore2.acquire();                }            }        } catch (Exception ex) {            throw new IOException(ex);        }    }}
private void flume_f3076_0(Collection<String> expected)
{    expected.add("");    for (int i = 0; i < 4; i++) {        for (int j = 0; j < i; j++) {            expected.add("file" + i + "line" + j);        }    }}
private static List<File> flume_f3077_0(File dir)
{    List<File> files = Lists.newArrayList(dir.listFiles(new FileFilter() {        @Override        public boolean accept(File pathname) {            return !pathname.isDirectory();        }    }));    return files;}
public boolean flume_f3078_0(File pathname)
{    return !pathname.isDirectory();}
 static String flume_f3079_0(Event event)
{    return new String(event.getBody());}
 static List<String> flume_f3080_0(List<Event> events)
{    List<String> bodies = Lists.newArrayListWithCapacity(events.size());    for (Event event : events) {        bodies.add(bodyAsString(event));    }    return bodies;}
private ReliableSpoolingFileEventReader flume_f3081_0(int maxLineLength)
{    Context ctx = new Context();    ctx.put(LineDeserializer.MAXLINE_KEY, Integer.toString(maxLineLength));    ReliableSpoolingFileEventReader parser;    try {        parser = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(tmpDir).completedSuffix(completedSuffix).deserializerContext(ctx).sourceCounter(new SourceCounter("dummy")).build();    } catch (IOException ioe) {        throw Throwables.propagate(ioe);    }    return parser;}
private ReliableSpoolingFileEventReader flume_f3082_0()
{    return getParser(bufferMaxLineLength);}
private FileFilter flume_f3083_0()
{    return new FileFilter() {        public boolean accept(File candidate) {            if (candidate.isDirectory()) {                return false;            }            return true;        }    };}
public boolean flume_f3084_0(File candidate)
{    if (candidate.isDirectory()) {        return false;    }    return true;}
public void flume_f3085_0()
{    tmpDir = Files.createTempDir();}
public void flume_f3086_0()
{    for (File f : tmpDir.listFiles()) {        if (f.isDirectory()) {            for (File sdf : f.listFiles()) {                sdf.delete();            }        }        f.delete();    }    tmpDir.delete();}
public void flume_f3087_1() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    File f3 = new File(tmpDir.getAbsolutePath() + "/file3");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);    Files.write("file3line1\nfile3line2\n", f3, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out = Lists.newArrayList();    for (int i = 0; i < 6; i++) {                String body = bodyAsString(parser.readEvent());                out.add(body);        parser.commit();    }        assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file2line1"));    assertTrue(out.contains("file2line2"));    assertTrue(out.contains("file3line1"));    assertTrue(out.contains("file3line2"));    List<File> outFiles = Lists.newArrayList(tmpDir.listFiles(directoryFilter()));    assertEquals(3, outFiles.size());        assertTrue(outFiles.contains(new File(tmpDir + "/file1" + completedSuffix)));    assertTrue(outFiles.contains(new File(tmpDir + "/file2" + completedSuffix)));    assertTrue(outFiles.contains(new File(tmpDir + "/file3")));}
public void flume_f3088_0() throws IOException
{    ReliableSpoolingFileEventReader parser = getParser();    assertNull(parser.readEvent());    assertEquals(0, parser.readEvents(10).size());    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    List<String> out = bodiesAsStrings(parser.readEvents(2));    parser.commit();        assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);        parser.readEvent();    parser.commit();    List<File> outFiles = Lists.newArrayList(tmpDir.listFiles(directoryFilter()));    assertEquals(2, outFiles.size());    assertTrue(outFiles.contains(new File(tmpDir + "/file1" + completedSuffix)));    assertTrue(outFiles.contains(new File(tmpDir + "/file2")));}
public void flume_f3089_0() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser1 = getParser();    List<String> out = Lists.newArrayList();    out.addAll(bodiesAsStrings(parser1.readEvents(2)));    parser1.commit();    assertEquals(2, out.size());    assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    Files.append("file1line3\n", f1, Charsets.UTF_8);    out.add(bodyAsString(parser1.readEvent()));    parser1.commit();    out.add(bodyAsString(parser1.readEvent()));    parser1.commit();}
public void flume_f3090_0() throws IOException
{    System.setProperty("os.name", "Some version of Windows");    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    File f1Completed = new File(tmpDir.getAbsolutePath() + "/file1" + completedSuffix);    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Files.write("file1line1\nfile1line2\n", f1Completed, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out = Lists.newArrayList();    for (int i = 0; i < 2; i++) {        out.add(bodyAsString(parser.readEvent()));        parser.commit();    }    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);    for (int i = 0; i < 2; i++) {        out.add(bodyAsString(parser.readEvent()));        parser.commit();    }        assertEquals(4, out.size());    assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file2line1"));    assertTrue(out.contains("file2line2"));        List<File> outFiles = Lists.newArrayList(tmpDir.listFiles(directoryFilter()));    assertEquals(2, outFiles.size());    assertTrue(outFiles.contains(new File(tmpDir + "/file2")));    assertTrue(outFiles.contains(new File(tmpDir + "/file1" + completedSuffix)));}
public void flume_f3091_0() throws IOException
{    System.setProperty("os.name", "Some version of Linux");    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    File f1Completed = new File(tmpDir.getAbsolutePath() + "/file1" + completedSuffix);    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Files.write("file1line1\nfile1line2\n", f1Completed, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out = Lists.newArrayList();    for (int i = 0; i < 2; i++) {        out.add(bodyAsString(parser.readEvent()));        parser.commit();    }    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);    for (int i = 0; i < 2; i++) {        out.add(bodyAsString(parser.readEvent()));        parser.commit();    }}
public void flume_f3092_0() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n" + "file1line9\nfile1line10\nfile1line11\nfile1line12\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out1 = bodiesAsStrings(parser.readEvents(4));    assertTrue(out1.contains("file1line1"));    assertTrue(out1.contains("file1line2"));    assertTrue(out1.contains("file1line3"));    assertTrue(out1.contains("file1line4"));    List<String> out2 = bodiesAsStrings(parser.readEvents(4));    assertTrue(out2.contains("file1line1"));    assertTrue(out2.contains("file1line2"));    assertTrue(out2.contains("file1line3"));    assertTrue(out2.contains("file1line4"));    parser.commit();    List<String> out3 = bodiesAsStrings(parser.readEvents(4));    assertTrue(out3.contains("file1line5"));    assertTrue(out3.contains("file1line6"));    assertTrue(out3.contains("file1line7"));    assertTrue(out3.contains("file1line8"));    parser.commit();    List<String> out4 = bodiesAsStrings(parser.readEvents(4));    assertEquals(4, out4.size());    assertTrue(out4.contains("file1line9"));    assertTrue(out4.contains("file1line10"));    assertTrue(out4.contains("file1line11"));    assertTrue(out4.contains("file1line12"));}
public void flume_f3093_0() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n" + "file1line9\nfile1line10\nfile1line11\nfile1line12\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out1 = bodiesAsStrings(parser.readEvents(5));    assertTrue(out1.contains("file1line1"));    assertTrue(out1.contains("file1line2"));    assertTrue(out1.contains("file1line3"));    assertTrue(out1.contains("file1line4"));    assertTrue(out1.contains("file1line5"));    List<String> out2 = bodiesAsStrings(parser.readEvents(2));    assertTrue(out2.contains("file1line1"));    assertTrue(out2.contains("file1line2"));    parser.commit();    List<String> out3 = bodiesAsStrings(parser.readEvents(2));    assertTrue(out3.contains("file1line3"));    assertTrue(out3.contains("file1line4"));    parser.commit();    List<String> out4 = bodiesAsStrings(parser.readEvents(2));    assertTrue(out4.contains("file1line5"));    assertTrue(out4.contains("file1line6"));    parser.commit();    List<String> out5 = bodiesAsStrings(parser.readEvents(2));    assertTrue(out5.contains("file1line7"));    assertTrue(out5.contains("file1line8"));    parser.commit();    List<String> out6 = bodiesAsStrings(parser.readEvents(15));    assertTrue(out6.contains("file1line9"));    assertTrue(out6.contains("file1line10"));    assertTrue(out6.contains("file1line11"));    assertTrue(out6.contains("file1line12"));}
public void flume_f3094_0() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    File f1Completed = new File(tmpDir.getAbsolutePath() + "/file1" + completedSuffix);    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Files.write("file1line1\nfile1XXXe2\n", f1Completed, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out = Lists.newArrayList();    for (int i = 0; i < 2; i++) {        out.add(bodyAsString(parser.readEvent()));        parser.commit();    }    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);    for (int i = 0; i < 2; i++) {        out.add(bodyAsString(parser.readEvent()));        parser.commit();    }}
public void flume_f3095_0() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file0");    Files.touch(f1);    ReliableSpoolingFileEventReader parser = getParser();    File f2 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f2, Charsets.UTF_8);        Event event = parser.readEvent();    assertEquals(0, event.getBody().length);    List<String> out = bodiesAsStrings(parser.readEvents(8));    parser.commit();    assertEquals(8, out.size());    assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file1line3"));    assertTrue(out.contains("file1line4"));    assertTrue(out.contains("file1line5"));    assertTrue(out.contains("file1line6"));    assertTrue(out.contains("file1line7"));    assertTrue(out.contains("file1line8"));    assertNull(parser.readEvent());        List<File> outFiles = Lists.newArrayList(tmpDir.listFiles(directoryFilter()));    assertEquals(2, outFiles.size());    assertTrue("Outfiles should have file0 & file1: " + outFiles, outFiles.contains(new File(tmpDir + "/file0" + completedSuffix)));    assertTrue("Outfiles should have file0 & file1: " + outFiles, outFiles.contains(new File(tmpDir + "/file1" + completedSuffix)));}
public void flume_f3096_0() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out = bodiesAsStrings(parser.readEvents(5));    parser.commit();        assertEquals(5, out.size());    assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file1line3"));    assertTrue(out.contains("file1line4"));    assertTrue(out.contains("file1line5"));}
public void flume_f3097_0() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out1 = bodiesAsStrings(parser.readEvents(5));    parser.commit();    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    Files.write("file2line1\nfile2line2\nfile2line3\nfile2line4\n" + "file2line5\nfile2line6\nfile2line7\nfile2line8\n", f2, Charsets.UTF_8);    List<String> out2 = bodiesAsStrings(parser.readEvents(5));    parser.commit();    List<String> out3 = bodiesAsStrings(parser.readEvents(5));    parser.commit();        assertEquals(5, out1.size());    assertTrue(out1.contains("file1line1"));    assertTrue(out1.contains("file1line2"));    assertTrue(out1.contains("file1line3"));    assertTrue(out1.contains("file1line4"));    assertTrue(out1.contains("file1line5"));        assertEquals(3, out2.size());    assertTrue(out2.contains("file1line6"));    assertTrue(out2.contains("file1line7"));    assertTrue(out2.contains("file1line8"));        assertEquals(5, out3.size());    assertTrue(out3.contains("file2line1"));    assertTrue(out3.contains("file2line2"));    assertTrue(out3.contains("file2line3"));    assertTrue(out3.contains("file2line4"));    assertTrue(out3.contains("file2line5"));        List<File> outFiles = Lists.newArrayList(tmpDir.listFiles(directoryFilter()));    assertEquals(2, outFiles.size());    assertTrue(outFiles.contains(new File(tmpDir + "/file1" + completedSuffix)));    assertTrue(outFiles.contains(new File(tmpDir + "/file2")));}
public void flume_f3098_0() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> allLines = bodiesAsStrings(parser.readEvents(2));    assertEquals(2, allLines.size());    parser.commit();    List<String> empty = bodiesAsStrings(parser.readEvents(10));    assertEquals(0, empty.size());}
public void flume_f3099_0() throws IOException
{    final int maxLineLength = 12;    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n" + "reallyreallyreallyreallyLongLineHerefile1line9\n" + "file1line10\nfile1line11\nfile1line12\nfile1line13\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser(maxLineLength);    List<String> out1 = bodiesAsStrings(parser.readEvents(5));    assertTrue(out1.contains("file1line1"));    assertTrue(out1.contains("file1line2"));    assertTrue(out1.contains("file1line3"));    assertTrue(out1.contains("file1line4"));    assertTrue(out1.contains("file1line5"));    parser.commit();    List<String> out2 = bodiesAsStrings(parser.readEvents(4));    assertTrue(out2.contains("file1line6"));    assertTrue(out2.contains("file1line7"));    assertTrue(out2.contains("file1line8"));    assertTrue(out2.contains("reallyreally"));    parser.commit();    List<String> out3 = bodiesAsStrings(parser.readEvents(5));    assertTrue(out3.contains("reallyreally"));    assertTrue(out3.contains("LongLineHere"));    assertTrue(out3.contains("file1line9"));    assertTrue(out3.contains("file1line10"));    assertTrue(out3.contains("file1line11"));    parser.commit();    List<String> out4 = bodiesAsStrings(parser.readEvents(5));    assertTrue(out4.contains("file1line12"));    assertTrue(out4.contains("file1line13"));    assertEquals(2, out4.size());    parser.commit();    assertEquals(0, parser.readEvents(5).size());}
public void flume_f3100_0() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    parser.readEvents(5);    parser.commit();    assertNotNull(parser.getLastFileRead());    assertTrue(parser.getLastFileRead().endsWith("file1"));    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    Files.write("file2line1\nfile2line2\nfile2line3\nfile2line4\n" + "file2line5\nfile2line6\nfile2line7\nfile2line8\n", f2, Charsets.UTF_8);    parser.readEvents(5);    parser.commit();    assertNotNull(parser.getLastFileRead());    assertTrue(parser.getLastFileRead().endsWith("file1"));    parser.readEvents(5);    parser.commit();    assertNotNull(parser.getLastFileRead());    assertTrue(parser.getLastFileRead().endsWith("file2"));    parser.readEvents(5);    assertTrue(parser.getLastFileRead().endsWith("file2"));    parser.readEvents(5);    assertTrue(parser.getLastFileRead().endsWith("file2"));}
public void flume_f3101_0()
{    SimpleEvent event = new SimpleEvent();    event.setBody("Some text".getBytes());    String eventDump = EventHelper.dumpEvent(event);    System.out.println(eventDump);    Assert.assertTrue(eventDump, eventDump.contains("Some text"));}
public void flume_f3102_0()
{    SimpleEvent event = new SimpleEvent();    byte[] body = new byte[5];    event.setBody(body);    String eventDump = EventHelper.dumpEvent(event);    Assert.assertTrue(eventDump, eventDump.contains("....."));}
public void flume_f3103_0()
{    cal = createCalendar(2012, 5, 23, 13, 46, 33, 234, null);    headers = new HashMap<>();    headers.put("timestamp", String.valueOf(cal.getTimeInMillis()));    Calendar calWithTimeZone = createCalendar(2012, 5, 23, 13, 46, 33, 234, CUSTOM_TIMEZONE);    headersWithTimeZone = new HashMap<>();    headersWithTimeZone.put("timestamp", String.valueOf(calWithTimeZone.getTimeInMillis()));}
public void flume_f3104_0()
{    TimeZone utcTimeZone = TimeZone.getTimeZone("UTC");    String test = "%c";    BucketPath.escapeString(test, headers, utcTimeZone, false, Calendar.HOUR_OF_DAY, 12, false);    String escapedString = BucketPath.escapeString(test, headers, false, Calendar.HOUR_OF_DAY, 12);    System.out.println("Escaped String: " + escapedString);    SimpleDateFormat format = new SimpleDateFormat("EEE MMM d HH:mm:ss yyyy");    Date d = new Date(cal.getTimeInMillis());    String expectedString = format.format(d);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
public void flume_f3105_0()
{    String test = "%c";    String escapedString = BucketPath.escapeString(test, headers, true, Calendar.HOUR_OF_DAY, 12);    System.out.println("Escaped String: " + escapedString);    Calendar cal2 = createCalendar(2012, 5, 23, 12, 0, 0, 0, null);    SimpleDateFormat format = new SimpleDateFormat("EEE MMM d HH:mm:ss yyyy");    Date d = new Date(cal2.getTimeInMillis());    String expectedString = format.format(d);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
public void flume_f3106_0()
{    String test = "%c";    String escapedString = BucketPath.escapeString(test, headersWithTimeZone, CUSTOM_TIMEZONE, true, Calendar.HOUR_OF_DAY, 12, false);    System.out.println("Escaped String: " + escapedString);    Calendar cal2 = createCalendar(2012, 5, 23, 12, 0, 0, 0, CUSTOM_TIMEZONE);    SimpleDateFormat format = new SimpleDateFormat("EEE MMM d HH:mm:ss yyyy");    format.setTimeZone(CUSTOM_TIMEZONE);    Date d = new Date(cal2.getTimeInMillis());    String expectedString = format.format(d);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
public void flume_f3107_0()
{    String test = "%s";    String escapedString = BucketPath.escapeString(test, headers, true, Calendar.MINUTE, 5);    System.out.println("Escaped String: " + escapedString);    Calendar cal2 = createCalendar(2012, 5, 23, 13, 45, 0, 0, null);    String expectedString = String.valueOf(cal2.getTimeInMillis() / 1000);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
public void flume_f3108_0()
{    String test = "%s";    String escapedString = BucketPath.escapeString(test, headersWithTimeZone, CUSTOM_TIMEZONE, true, Calendar.MINUTE, 5, false);    System.out.println("Escaped String: " + escapedString);    Calendar cal2 = createCalendar(2012, 5, 23, 13, 45, 0, 0, CUSTOM_TIMEZONE);    String expectedString = String.valueOf(cal2.getTimeInMillis() / 1000);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
public void flume_f3109_0()
{    String test = "%s";    String escapedString = BucketPath.escapeString(test, headers, true, Calendar.SECOND, 5);    System.out.println("Escaped String: " + escapedString);    Calendar cal2 = createCalendar(2012, 5, 23, 13, 46, 30, 0, null);    String expectedString = String.valueOf(cal2.getTimeInMillis() / 1000);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
public void flume_f3110_0()
{    String test = "%s";    String escapedString = BucketPath.escapeString(test, headersWithTimeZone, CUSTOM_TIMEZONE, true, Calendar.SECOND, 5, false);    System.out.println("Escaped String: " + escapedString);    Calendar cal2 = createCalendar(2012, 5, 23, 13, 46, 30, 0, CUSTOM_TIMEZONE);    String expectedString = String.valueOf(cal2.getTimeInMillis() / 1000);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
public void flume_f3111_0()
{    String test = "%c";    String escapedString = BucketPath.escapeString(test, headers, false, Calendar.HOUR_OF_DAY, 12);    System.out.println("Escaped String: " + escapedString);    SimpleDateFormat format = new SimpleDateFormat("EEE MMM d HH:mm:ss yyyy");    Date d = new Date(cal.getTimeInMillis());    String expectedString = format.format(d);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
public void flume_f3112_0()
{    Calendar calender;    Map<String, String> calender_timestamp;    calender = Calendar.getInstance();        calender.set(2014, (5 - 1), 3, 13, 46, 33);    calender_timestamp = new HashMap<String, String>();    calender_timestamp.put("timestamp", String.valueOf(calender.getTimeInMillis()));    SimpleDateFormat format = new SimpleDateFormat("M-d");        String test = "%n-%e";    String escapedString = BucketPath.escapeString(test, calender_timestamp, false, Calendar.HOUR_OF_DAY, 12);    Date d = new Date(calender.getTimeInMillis());    String expectedString = format.format(d);        calender.set(2014, (11 - 1), 13, 13, 46, 33);    calender_timestamp.put("timestamp", String.valueOf(calender.getTimeInMillis()));    escapedString += " " + BucketPath.escapeString(test, calender_timestamp, false, Calendar.HOUR_OF_DAY, 12);    System.out.println("Escaped String: " + escapedString);    d = new Date(calender.getTimeInMillis());    expectedString += " " + format.format(d);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
public void flume_f3113_0()
{    TimeZone utcTimeZone = TimeZone.getTimeZone("UTC");    String test = "%c";    String escapedString = BucketPath.escapeString(test, headers, utcTimeZone, false, Calendar.HOUR_OF_DAY, 12, false);    System.out.println("Escaped String: " + escapedString);    SimpleDateFormat format = new SimpleDateFormat("EEE MMM d HH:mm:ss yyyy");    format.setTimeZone(utcTimeZone);    Date d = new Date(cal.getTimeInMillis());    String expectedString = format.format(d);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
public void flume_f3114_0()
{    Clock mockClock = mock(Clock.class);    DateTimeFormatter parser = ISODateTimeFormat.dateTimeParser();    long two = parser.parseMillis("2013-04-21T02:59:59-00:00");    long three = parser.parseMillis("2013-04-21T03:00:00-00:00");    when(mockClock.currentTimeMillis()).thenReturn(two, three);        Clock origClock = BucketPath.getClock();    BucketPath.setClock(mockClock);    String pat = "%H:%M";    String escaped = BucketPath.escapeString(pat, new HashMap<String, String>(), TimeZone.getTimeZone("UTC"), true, Calendar.MINUTE, 10, true);        BucketPath.setClock(origClock);    Assert.assertEquals("Race condition detected", "02:50", escaped);}
private static Calendar flume_f3115_0(int year, int month, int day, int hour, int minute, int second, int ms, @Nullable TimeZone timeZone)
{    Calendar cal = (timeZone == null) ? Calendar.getInstance() : Calendar.getInstance(timeZone);    cal.set(year, month, day, hour, minute, second);    cal.set(Calendar.MILLISECOND, ms);    return cal;}
public void flume_f3116_0()
{    Map<String, String> staticStrings;    staticStrings = new HashMap<>();    try {        InetAddress addr = InetAddress.getLocalHost();        staticStrings.put("localhost", addr.getHostName());        staticStrings.put("IP", addr.getHostAddress());        staticStrings.put("FQDN", addr.getCanonicalHostName());    } catch (UnknownHostException e) {        Assert.fail("Test failed due to UnkownHostException");    }    TimeZone utcTimeZone = TimeZone.getTimeZone("UTC");    String filePath = "%[localhost]/%[IP]/%[FQDN]";    String realPath = BucketPath.escapeString(filePath, headers, utcTimeZone, false, Calendar.HOUR_OF_DAY, 12, false);    String[] args = realPath.split("\\/");    Assert.assertEquals(args[0], staticStrings.get("localhost"));    Assert.assertEquals(args[1], staticStrings.get("IP"));    Assert.assertEquals(args[2], staticStrings.get("FQDN"));    StringBuilder s = new StringBuilder();    s.append("Expected String: ").append(staticStrings.get("localhost"));    s.append("/").append(staticStrings.get("IP")).append("/");    s.append(staticStrings.get("FQDN"));    System.out.println(s);    System.out.println("Escaped String: " + realPath);}
public void flume_f3117_0()
{    Map<String, String> staticStrings;    staticStrings = new HashMap<>();    TimeZone utcTimeZone = TimeZone.getTimeZone("UTC");    String filePath = "%[abcdefg]/%[IP]/%[FQDN]";    String realPath = BucketPath.escapeString(filePath, headers, utcTimeZone, false, Calendar.HOUR_OF_DAY, 12, false);}
private static int flume_f3118_0() throws Exception
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
public void flume_f3119_0() throws Exception
{    memChannel.setName("memChannel");    pmemChannel.setName("pmemChannel");    Context c = new Context();    Configurables.configure(memChannel, c);    Configurables.configure(pmemChannel, c);    memChannel.start();    pmemChannel.start();    Transaction txn = memChannel.getTransaction();    txn.begin();    memChannel.put(EventBuilder.withBody("blah".getBytes()));    memChannel.put(EventBuilder.withBody("blah".getBytes()));    txn.commit();    txn.close();    txn = memChannel.getTransaction();    txn.begin();    memChannel.take();    txn.commit();    txn.close();    Transaction txn2 = pmemChannel.getTransaction();    txn2.begin();    pmemChannel.put(EventBuilder.withBody("blah".getBytes()));    pmemChannel.put(EventBuilder.withBody("blah".getBytes()));    txn2.commit();    txn2.close();    txn2 = pmemChannel.getTransaction();    txn2.begin();    pmemChannel.take();    txn2.commit();    txn2.close();    testWithPort(getFreePort());    memChannel.stop();    pmemChannel.stop();}
private void flume_f3120_0(int port) throws Exception
{    MonitorService srv = new HTTPMetricsServer();    Context context = new Context();    context.put(HTTPMetricsServer.CONFIG_PORT, String.valueOf(port));    srv.configure(context);    srv.start();    Thread.sleep(1000);    URL url = new URL("http://0.0.0.0:" + String.valueOf(port) + "/metrics");    HttpURLConnection conn = (HttpURLConnection) url.openConnection();    conn.setRequestMethod("GET");    BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));    String line;    String result = "";    while ((line = reader.readLine()) != null) {        result += line;    }    reader.close();    Map<String, Map<String, String>> mbeans = gson.fromJson(result, mapType);    Assert.assertNotNull(mbeans);    Map<String, String> memBean = mbeans.get("CHANNEL.memChannel");    Assert.assertNotNull(memBean);    JMXTestUtils.checkChannelCounterParams(memBean);    Map<String, String> pmemBean = mbeans.get("CHANNEL.pmemChannel");    Assert.assertNotNull(pmemBean);    JMXTestUtils.checkChannelCounterParams(pmemBean);    srv.stop();}
public void flume_f3121_0() throws Exception
{    doTestForbiddenMethods(getFreePort(), "TRACE");}
public void flume_f3122_0() throws Exception
{    doTestForbiddenMethods(getFreePort(), "OPTIONS");}
public void flume_f3123_0(int port, String method) throws Exception
{    MonitorService srv = new HTTPMetricsServer();    Context context = new Context();    context.put(HTTPMetricsServer.CONFIG_PORT, String.valueOf(port));    srv.configure(context);    srv.start();    Thread.sleep(1000);    URL url = new URL("http://0.0.0.0:" + String.valueOf(port) + "/metrics");    HttpURLConnection conn = (HttpURLConnection) url.openConnection();    conn.setRequestMethod(method);    Assert.assertEquals(HttpServletResponse.SC_FORBIDDEN, conn.getResponseCode());    srv.stop();}
public void flume_f3124_0() throws Exception
{    counter = new KafkaSourceCounter("test");}
public void flume_f3125_0() throws Exception
{    Assert.assertEquals(1L, counter.addToKafkaEventGetTimer(1L));}
public void flume_f3126_0() throws Exception
{    Assert.assertEquals(1L, counter.addToKafkaCommitTimer(1L));}
public void flume_f3127_0() throws Exception
{    Assert.assertEquals(1L, counter.incrementKafkaEmptyCount());}
public void flume_f3128_0() throws Exception
{    Assert.assertEquals(0, counter.getKafkaCommitTimer());}
public void flume_f3129_0() throws Exception
{    Assert.assertEquals(0, counter.getKafkaEventGetTimer());}
public void flume_f3130_0() throws Exception
{    Assert.assertEquals(0, counter.getKafkaEmptyCount());}
public void flume_f3131_0()
{    mbServer = ManagementFactory.getPlatformMBeanServer();    random = new Random(System.nanoTime());}
public void flume_f3132_0() throws Exception
{    String name = getRandomName();    SinkCounter skc = new SinkCounter(name);    skc.register();    ObjectName on = new ObjectName(SINK_OBJ_NAME_PREFIX + name);    assertSkCounterState(on, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L);    skc.start();    long start1 = getStartTime(on);    Assert.assertTrue("StartTime", start1 != 0L);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    int connCreated = random.nextInt(MAX_BOUNDS);    int connClosed = random.nextInt(MAX_BOUNDS);    int connFailed = random.nextInt(MAX_BOUNDS);    int batchEmpty = random.nextInt(MAX_BOUNDS);    int batchUnderflow = random.nextInt(MAX_BOUNDS);    int batchComplete = random.nextInt(MAX_BOUNDS);    int eventDrainAttempt = random.nextInt(MAX_BOUNDS);    int eventDrainSuccess = random.nextInt(MAX_BOUNDS);    for (int i = 0; i < connCreated; i++) {        skc.incrementConnectionCreatedCount();    }    for (int i = 0; i < connClosed; i++) {        skc.incrementConnectionClosedCount();    }    for (int i = 0; i < connFailed; i++) {        skc.incrementConnectionFailedCount();    }    for (int i = 0; i < batchEmpty; i++) {        skc.incrementBatchEmptyCount();    }    for (int i = 0; i < batchUnderflow; i++) {        skc.incrementBatchUnderflowCount();    }    for (int i = 0; i < batchComplete; i++) {        skc.incrementBatchCompleteCount();    }    for (int i = 0; i < eventDrainAttempt; i++) {        skc.incrementEventDrainAttemptCount();    }    for (int i = 0; i < eventDrainSuccess; i++) {        skc.incrementEventDrainSuccessCount();    }    assertSkCounterState(on, connCreated, connClosed, connFailed, batchEmpty, batchUnderflow, batchComplete, eventDrainAttempt, eventDrainSuccess);    skc.stop();    Assert.assertTrue("StartTime", getStartTime(on) != 0L);    Assert.assertTrue("StopTime", getStopTime(on) != 0L);    assertSkCounterState(on, connCreated, connClosed, connFailed, batchEmpty, batchUnderflow, batchComplete, eventDrainAttempt, eventDrainSuccess);        Thread.sleep(5L);    skc.start();    Assert.assertTrue("StartTime", getStartTime(on) != 0L);    Assert.assertTrue("StartTime", getStartTime(on) > start1);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    assertSkCounterState(on, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L);    int eventDrainAttempt2 = random.nextInt(MAX_BOUNDS);    int eventDrainSuccess2 = random.nextInt(MAX_BOUNDS);    skc.addToEventDrainAttemptCount(eventDrainAttempt2);    skc.addToEventDrainSuccessCount(eventDrainSuccess2);    assertSkCounterState(on, 0L, 0L, 0L, 0L, 0L, 0L, eventDrainAttempt2, eventDrainSuccess2);}
public void flume_f3133_0() throws Exception
{    String name = getRandomName();    ChannelCounter chc = new ChannelCounter(name);    chc.register();    ObjectName on = new ObjectName(CHANNEL_OBJ_NAME_PREFIX + name);    assertChCounterState(on, 0L, 0L, 0L, 0L, 0L);    Assert.assertTrue("StartTime", getStartTime(on) == 0L);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    chc.start();    long start1 = getStartTime(on);    Assert.assertTrue("StartTime", start1 != 0L);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    int numChannelSize = random.nextInt(MAX_BOUNDS);    int numEventPutAttempt = random.nextInt(MAX_BOUNDS);    int numEventTakeAttempt = random.nextInt(MAX_BOUNDS);    int numEventPutSuccess = random.nextInt(MAX_BOUNDS);    int numEventTakeSuccess = random.nextInt(MAX_BOUNDS);    chc.setChannelSize(numChannelSize);    for (int i = 0; i < numEventPutAttempt; i++) {        chc.incrementEventPutAttemptCount();    }    for (int i = 0; i < numEventTakeAttempt; i++) {        chc.incrementEventTakeAttemptCount();    }    chc.addToEventPutSuccessCount(numEventPutSuccess);    chc.addToEventTakeSuccessCount(numEventTakeSuccess);    assertChCounterState(on, numChannelSize, numEventPutAttempt, numEventTakeAttempt, numEventPutSuccess, numEventTakeSuccess);    chc.stop();    Assert.assertTrue("StartTime", getStartTime(on) != 0L);    Assert.assertTrue("StopTime", getStopTime(on) != 0L);    assertChCounterState(on, numChannelSize, numEventPutAttempt, numEventTakeAttempt, numEventPutSuccess, numEventTakeSuccess);        Thread.sleep(5L);    chc.start();    Assert.assertTrue("StartTime", getStartTime(on) != 0L);    Assert.assertTrue("StartTime", getStartTime(on) > start1);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    assertChCounterState(on, 0L, 0L, 0L, 0L, 0L);}
public void flume_f3134_0() throws Exception
{    String name = getRandomName();    SourceCounter srcc = new SourceCounter(name);    srcc.register();    ObjectName on = new ObjectName(SOURCE_OBJ_NAME_PREFIX + name);    assertSrcCounterState(on, 0L, 0L, 0L, 0L, 0L, 0L);    Assert.assertTrue("StartTime", getStartTime(on) == 0L);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    srcc.start();    long start1 = getStartTime(on);    Assert.assertTrue("StartTime", start1 != 0L);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    int numEventReceived = random.nextInt(MAX_BOUNDS);    int numEventAccepted = random.nextInt(MAX_BOUNDS);    int numAppendReceived = random.nextInt(MAX_BOUNDS);    int numAppendAccepted = random.nextInt(MAX_BOUNDS);    int numAppendBatchReceived = random.nextInt(MAX_BOUNDS);    int numAppendBatchAccepted = random.nextInt(MAX_BOUNDS);    srcc.addToEventReceivedCount(numEventReceived);    srcc.addToEventAcceptedCount(numEventAccepted);    for (int i = 0; i < numAppendReceived; i++) {        srcc.incrementAppendReceivedCount();    }    for (int i = 0; i < numAppendAccepted; i++) {        srcc.incrementAppendAcceptedCount();    }    for (int i = 0; i < numAppendBatchReceived; i++) {        srcc.incrementAppendBatchReceivedCount();    }    for (int i = 0; i < numAppendBatchAccepted; i++) {        srcc.incrementAppendBatchAcceptedCount();    }    assertSrcCounterState(on, numEventReceived, numEventAccepted, numAppendReceived, numAppendAccepted, numAppendBatchReceived, numAppendBatchAccepted);    srcc.stop();    Assert.assertTrue("StartTime", getStartTime(on) != 0L);    Assert.assertTrue("StopTime", getStopTime(on) != 0L);    assertSrcCounterState(on, numEventReceived, numEventAccepted, numAppendReceived, numAppendAccepted, numAppendBatchReceived, numAppendBatchAccepted);        Thread.sleep(5L);    srcc.start();    Assert.assertTrue("StartTime", getStartTime(on) != 0L);    Assert.assertTrue("StartTime", getStartTime(on) > start1);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    assertSrcCounterState(on, 0L, 0L, 0L, 0L, 0L, 0L);    int numEventReceived2 = random.nextInt(MAX_BOUNDS);    int numEventAccepted2 = random.nextInt(MAX_BOUNDS);    for (int i = 0; i < numEventReceived2; i++) {        srcc.incrementEventReceivedCount();    }    for (int i = 0; i < numEventAccepted2; i++) {        srcc.incrementEventAcceptedCount();    }    assertSrcCounterState(on, numEventReceived2, numEventAccepted2, 0L, 0L, 0L, 0L);}
public void flume_f3135_0() throws Exception
{    String name = "re-register-" + getRandomName();    SourceCounter c1 = new SourceCounter(name);    c1.register();    ObjectName on = new ObjectName(SOURCE_OBJ_NAME_PREFIX + name);    Assert.assertEquals("StartTime", 0L, getStartTime(on));    Assert.assertEquals("StopTime", 0L, getStopTime(on));    c1.start();    c1.stop();    Assert.assertTrue("StartTime", getStartTime(on) > 0L);    Assert.assertTrue("StopTime", getStopTime(on) > 0L);    SourceCounter c2 = new SourceCounter(name);    c2.register();    Assert.assertEquals("StartTime", 0L, getStartTime(on));    Assert.assertEquals("StopTime", 0L, getStopTime(on));}
private void flume_f3136_0(ObjectName on, long eventReceivedCount, long eventAcceptedCount, long appendReceivedCount, long appendAcceptedCount, long appendBatchReceivedCount, long appendBatchAcceptedCount) throws Exception
{    Assert.assertEquals("SrcEventReceived", getSrcEventReceivedCount(on), eventReceivedCount);    Assert.assertEquals("SrcEventAccepted", getSrcEventAcceptedCount(on), eventAcceptedCount);    Assert.assertEquals("SrcAppendReceived", getSrcAppendReceivedCount(on), appendReceivedCount);    Assert.assertEquals("SrcAppendAccepted", getSrcAppendAcceptedCount(on), appendAcceptedCount);    Assert.assertEquals("SrcAppendBatchReceived", getSrcAppendBatchReceivedCount(on), appendBatchReceivedCount);    Assert.assertEquals("SrcAppendBatchAccepted", getSrcAppendBatchAcceptedCount(on), appendBatchAcceptedCount);}
private void flume_f3137_0(ObjectName on, long channelSize, long eventPutAttempt, long eventTakeAttempt, long eventPutSuccess, long eventTakeSuccess) throws Exception
{    Assert.assertEquals("ChChannelSize", getChChannelSize(on), channelSize);    Assert.assertEquals("ChEventPutAttempt", getChEventPutAttempt(on), eventPutAttempt);    Assert.assertEquals("ChEventTakeAttempt", getChEventTakeAttempt(on), eventTakeAttempt);    Assert.assertEquals("ChEventPutSuccess", getChEventPutSuccess(on), eventPutSuccess);    Assert.assertEquals("ChEventTakeSuccess", getChEventTakeSuccess(on), eventTakeSuccess);}
private void flume_f3138_0(ObjectName on, long connCreated, long connClosed, long connFailed, long batchEmpty, long batchUnderflow, long batchComplete, long eventDrainAttempt, long eventDrainSuccess) throws Exception
{    Assert.assertEquals("SkConnCreated", getSkConnectionCreated(on), connCreated);    Assert.assertEquals("SkConnClosed", getSkConnectionClosed(on), connClosed);    Assert.assertEquals("SkConnFailed", getSkConnectionFailed(on), connFailed);    Assert.assertEquals("SkBatchEmpty", getSkBatchEmpty(on), batchEmpty);    Assert.assertEquals("SkBatchUnderflow", getSkBatchUnderflow(on), batchUnderflow);    Assert.assertEquals("SkBatchComplete", getSkBatchComplete(on), batchComplete);    Assert.assertEquals("SkEventDrainAttempt", getSkEventDrainAttempt(on), eventDrainAttempt);    Assert.assertEquals("SkEventDrainSuccess", getSkEventDrainSuccess(on), eventDrainSuccess);}
private long flume_f3139_0(ObjectName on) throws Exception
{    return getLongAttribute(on, ATTR_START_TIME);}
private long flume_f3140_0(ObjectName on) throws Exception
{    return getLongAttribute(on, ATTR_STOP_TIME);}
private long flume_f3141_0(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_CONN_CREATED);}
private long flume_f3142_0(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_CONN_CLOSED);}
private long flume_f3143_0(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_CONN_FAILED);}
private long flume_f3144_0(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_BATCH_EMPTY);}
private long flume_f3145_0(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_BATCH_UNDERFLOW);}
private long flume_f3146_0(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_BATCH_COMPLETE);}
private long flume_f3147_0(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_EVENT_DRAIN_ATTEMPT);}
private long flume_f3148_0(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_EVENT_DRAIN_SUCCESS);}
private long flume_f3149_0(ObjectName on) throws Exception
{    return getLongAttribute(on, CH_ATTR_CHANNEL_SIZE);}
private long flume_f3150_0(ObjectName on) throws Exception
{    return getLongAttribute(on, CH_ATTR_EVENT_PUT_ATTEMPT);}
private long flume_f3151_0(ObjectName on) throws Exception
{    return getLongAttribute(on, CH_ATTR_EVENT_TAKE_ATTEMPT);}
private long flume_f3152_0(ObjectName on) throws Exception
{    return getLongAttribute(on, CH_ATTR_EVENT_PUT_SUCCESS);}
private long flume_f3153_0(ObjectName on) throws Exception
{    return getLongAttribute(on, CH_ATTR_EVENT_TAKE_SUCCESS);}
private long flume_f3154_0(ObjectName on) throws Exception
{    return getLongAttribute(on, SRC_ATTR_APPEND_BATCH_ACCEPTED_COUNT);}
private long flume_f3155_0(ObjectName on) throws Exception
{    return getLongAttribute(on, SRC_ATTR_APPEND_BATCH_RECEVIED_COUNT);}
private long flume_f3156_0(ObjectName on) throws Exception
{    return getLongAttribute(on, SRC_ATTR_APPEND_ACCEPTED_COUNT);}
private long flume_f3157_0(ObjectName on) throws Exception
{    return getLongAttribute(on, SRC_ATTR_APPEND_RECEVIED_COUNT);}
private long flume_f3158_0(ObjectName on) throws Exception
{    return getLongAttribute(on, SRC_ATTR_EVENT_ACCEPTED_COUNT);}
private long flume_f3159_0(ObjectName on) throws Exception
{    return getLongAttribute(on, SRC_ATTR_EVENT_RECEVIED_COUNT);}
private long flume_f3160_0(ObjectName on, String attr) throws Exception
{    Object result = getAttribute(on, attr);    return ((Long) result).longValue();}
private Object flume_f3161_0(ObjectName objName, String attrName) throws Exception
{    return mbServer.getAttribute(objName, attrName);}
private String flume_f3162_0()
{    return "random-" + System.nanoTime();}
public static void flume_f3163_0(Map<String, String> attrs)
{    Assert.assertNotNull(attrs.get("StartTime"));    Assert.assertNotNull(attrs.get("StopTime"));    Assert.assertTrue(Long.parseLong(attrs.get("ChannelSize")) != 0);    Assert.assertTrue(Long.parseLong(attrs.get("EventPutAttemptCount")) == 2);    Assert.assertTrue(Long.parseLong(attrs.get("EventTakeAttemptCount")) == 1);    Assert.assertTrue(Long.parseLong(attrs.get("EventPutSuccessCount")) == 2);    Assert.assertTrue(Long.parseLong(attrs.get("EventTakeSuccessCount")) == 1);}
public void flume_f3164_0()
{    memChannel.setName("memChannel");    pmemChannel.setName("pmemChannel");    Context c = new Context();    Configurables.configure(memChannel, c);    Configurables.configure(pmemChannel, c);    memChannel.start();    pmemChannel.start();    Transaction txn = memChannel.getTransaction();    txn.begin();    memChannel.put(EventBuilder.withBody("blah".getBytes()));    memChannel.put(EventBuilder.withBody("blah".getBytes()));    txn.commit();    txn.close();    txn = memChannel.getTransaction();    txn.begin();    memChannel.take();    txn.commit();    txn.close();    Transaction txn2 = pmemChannel.getTransaction();    txn2.begin();    pmemChannel.put(EventBuilder.withBody("blah".getBytes()));    pmemChannel.put(EventBuilder.withBody("blah".getBytes()));    txn2.commit();    txn2.close();    txn2 = pmemChannel.getTransaction();    txn2.begin();    pmemChannel.take();    txn2.commit();    txn2.close();    Map<String, Map<String, String>> mbeans = JMXPollUtil.getAllMBeans();    Assert.assertNotNull(mbeans);    Map<String, String> memBean = mbeans.get("CHANNEL.memChannel");    Assert.assertNotNull(memBean);    JMXTestUtils.checkChannelCounterParams(memBean);    Map<String, String> pmemBean = mbeans.get("CHANNEL.pmemChannel");    Assert.assertNotNull(pmemBean);    JMXTestUtils.checkChannelCounterParams(pmemBean);    memChannel.stop();    pmemChannel.stop();}
public void flume_f3165_0()
{}
public Event flume_f3166_0(Event event)
{    Map<String, String> headers = event.getHeaders();    if (headers.containsKey("Bad-Words")) {        headers.remove("Bad-Words");    }    return event;}
public List<Event> flume_f3167_0(List<Event> events)
{    for (Event e : events) {        intercept(e);    }    return events;}
public void flume_f3168_0()
{}
public Interceptor flume_f3169_0()
{    return new CensoringInterceptor();}
public void flume_f3170_0(Context context)
{}
private Event flume_f3171_0()
{    return EventBuilder.withBody("My test event".getBytes(), ImmutableMap.of(HEADER1, HEADER1, HEADER2, HEADER2, HEADER3, HEADER3, HEADER4, HEADER4, HEADER5, HEADER5));}
private Event flume_f3172_0()
{    return EventBuilder.withBody("My test event".getBytes());}
public void flume_f3173_0() throws Exception
{    new RemoveHeaderIntBuilder().fromList(HEADER1, "(").build();}
public void flume_f3174_0() throws IllegalAccessException, ClassNotFoundException, InstantiationException
{    final Interceptor removeHeaderInterceptor = new RemoveHeaderIntBuilder().withName(HEADER4).build();    final Event event1 = buildEventWithHeader();    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    removeHeaderInterceptor.intercept(event1);    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertNull(event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    final Event event2 = buildEventWithoutHeader();    Assert.assertTrue(event2.getHeaders().isEmpty());    removeHeaderInterceptor.intercept(event2);    Assert.assertTrue(event2.getHeaders().isEmpty());}
public void flume_f3175_0() throws Exception
{    final Interceptor removeHeaderInterceptor = new RemoveHeaderIntBuilder().fromList(HEADER4 + MY_SEPARATOR + HEADER2).build();    final Event event1 = buildEventWithHeader();    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    removeHeaderInterceptor.intercept(event1);    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    final Event event2 = buildEventWithoutHeader();    Assert.assertTrue(event2.getHeaders().isEmpty());    removeHeaderInterceptor.intercept(event2);    Assert.assertTrue(event2.getHeaders().isEmpty());}
public void flume_f3176_0() throws Exception
{    final Interceptor removeHeaderInterceptor = new RemoveHeaderIntBuilder().fromList(HEADER4 + DEFAULT_SEPARATOR + HEADER2).build();    final Event event1 = buildEventWithHeader();    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    removeHeaderInterceptor.intercept(event1);    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertNull(event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertNull(event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    final Event event2 = buildEventWithoutHeader();    Assert.assertTrue(event2.getHeaders().isEmpty());    removeHeaderInterceptor.intercept(event2);    Assert.assertTrue(event2.getHeaders().isEmpty());}
public void flume_f3177_0() throws Exception
{    final Interceptor removeHeaderInterceptor = new RemoveHeaderIntBuilder().fromList(HEADER4 + MY_SEPARATOR + HEADER2, MY_SEPARATOR).build();    final Event event1 = buildEventWithHeader();    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    removeHeaderInterceptor.intercept(event1);    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertNull(event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertNull(event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    final Event event2 = buildEventWithoutHeader();    Assert.assertTrue(event2.getHeaders().isEmpty());    removeHeaderInterceptor.intercept(event2);    Assert.assertTrue(event2.getHeaders().isEmpty());}
public void flume_f3178_0() throws Exception
{    final Interceptor removeHeaderInterceptor = new RemoveHeaderIntBuilder().fromList(HEADER4 + DEFAULT_SEPARATOR + HEADER2, MY_SEPARATOR).build();    final Event event1 = buildEventWithHeader();    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    removeHeaderInterceptor.intercept(event1);    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    final Event event2 = buildEventWithoutHeader();    Assert.assertTrue(event2.getHeaders().isEmpty());    removeHeaderInterceptor.intercept(event2);    Assert.assertTrue(event2.getHeaders().isEmpty());}
public void flume_f3179_0() throws Exception
{    final Interceptor removeHeaderInterceptor = new RemoveHeaderIntBuilder().matchRegex("my-header1.*").build();    final Event event1 = buildEventWithHeader();    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    removeHeaderInterceptor.intercept(event1);    Assert.assertNull(event1.getHeaders().get(HEADER1));    Assert.assertNull(event1.getHeaders().get(HEADER2));    Assert.assertNull(event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    final Event event2 = buildEventWithoutHeader();    Assert.assertTrue(event2.getHeaders().isEmpty());    removeHeaderInterceptor.intercept(event2);    Assert.assertTrue(event2.getHeaders().isEmpty());}
public void flume_f3180_0() throws Exception
{    final Interceptor removeHeaderInterceptor = new RemoveHeaderIntBuilder().matchRegex("my-header2.*").fromList(HEADER1 + MY_SEPARATOR + HEADER3, MY_SEPARATOR).withName(HEADER2).build();    final Event event1 = buildEventWithHeader();    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    removeHeaderInterceptor.intercept(event1);    Assert.assertTrue(event1.getHeaders().isEmpty());    final Event event2 = buildEventWithoutHeader();    Assert.assertTrue(event2.getHeaders().isEmpty());    removeHeaderInterceptor.intercept(event2);    Assert.assertTrue(event2.getHeaders().isEmpty());}
 RemoveHeaderIntBuilder flume_f3181_0(final String str)
{    contextMap.put(RemoveHeaderInterceptor.WITH_NAME, str);    return this;}
 RemoveHeaderIntBuilder flume_f3182_0(final String str)
{    contextMap.put(RemoveHeaderInterceptor.FROM_LIST, str);    return this;}
 RemoveHeaderIntBuilder flume_f3183_0(final String str, final String separator)
{    fromList(str);    contextMap.put(RemoveHeaderInterceptor.LIST_SEPARATOR, separator);    return this;}
 RemoveHeaderIntBuilder flume_f3184_0(final String str)
{    contextMap.put(RemoveHeaderInterceptor.MATCH_REGEX, str);    return this;}
public Interceptor flume_f3185_0() throws InstantiationException, IllegalAccessException, ClassNotFoundException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.REMOVE_HEADER.toString());    builder.configure(new Context(contextMap));    return builder.build();}
public void flume_f3186_0()
{    MemoryChannel memCh = new MemoryChannel();    memCh.configure(new Context());    memCh.start();    ChannelSelector cs = new ReplicatingChannelSelector();    cs.setChannels(Lists.<Channel>newArrayList(memCh));    ChannelProcessor cp = new ChannelProcessor(cs);        Map<String, String> cfgMap = Maps.newHashMap();    cfgMap.put("interceptors", "a");    String builderClass = CensoringInterceptor.Builder.class.getName();    cfgMap.put("interceptors.a.type", builderClass);    Context ctx = new Context(cfgMap);        cp.configure(ctx);    cp.initialize();    Map<String, String> headers = Maps.newHashMap();    String badWord = "scribe";    headers.put("Bad-Words", badWord);    Event event1 = EventBuilder.withBody("test", Charsets.UTF_8, headers);    Assert.assertEquals(badWord, event1.getHeaders().get("Bad-Words"));    cp.processEvent(event1);    Transaction tx = memCh.getTransaction();    tx.begin();    Event event1a = memCh.take();    Assert.assertNull(event1a.getHeaders().get("Bad-Words"));    tx.commit();    tx.close();        cp.close();    memCh.stop();}
public void flume_f3187_0() throws Exception
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.HOST.toString());    Interceptor interceptor = builder.build();    Event eventBeforeIntercept = EventBuilder.withBody("test event", Charsets.UTF_8);    Assert.assertNull(eventBeforeIntercept.getHeaders().get(Constants.HOST));    Event eventAfterIntercept = interceptor.intercept(eventBeforeIntercept);    String actualHost = eventAfterIntercept.getHeaders().get(Constants.HOST);    Assert.assertNotNull(actualHost);}
public void flume_f3188_0() throws Exception
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.HOST.toString());    Context ctx = new Context();    ctx.put("preserveExisting", "false");    ctx.put("hostHeader", "hostname");    builder.configure(ctx);    Interceptor interceptor = builder.build();    Event eventBeforeIntercept = EventBuilder.withBody("test event", Charsets.UTF_8);    Assert.assertNull(eventBeforeIntercept.getHeaders().get("hostname"));    Event eventAfterIntercept = interceptor.intercept(eventBeforeIntercept);    String actualHost = eventAfterIntercept.getHeaders().get("hostname");    Assert.assertNotNull(actualHost);    Assert.assertEquals(InetAddress.getLocalHost().getHostAddress(), actualHost);}
public void flume_f3189_0() throws Exception
{    Context ctx = new Context();    ctx.put("preserveExisting", "true");    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.HOST.toString());    builder.configure(ctx);    Interceptor interceptor = builder.build();    final String ORIGINAL_HOST = "originalhost";    Event eventBeforeIntercept = EventBuilder.withBody("test event", Charsets.UTF_8);    eventBeforeIntercept.getHeaders().put(Constants.HOST, ORIGINAL_HOST);    Assert.assertEquals(ORIGINAL_HOST, eventBeforeIntercept.getHeaders().get(Constants.HOST));    String expectedHost = ORIGINAL_HOST;    Event eventAfterIntercept = interceptor.intercept(eventBeforeIntercept);    String actualHost = eventAfterIntercept.getHeaders().get(Constants.HOST);    Assert.assertNotNull(actualHost);    Assert.assertEquals(expectedHost, actualHost);}
public void flume_f3190_0() throws Exception
{    Context ctx = new Context();        ctx.put("preserveExisting", "false");    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.HOST.toString());    builder.configure(ctx);    Interceptor interceptor = builder.build();    final String ORIGINAL_HOST = "originalhost";    Event eventBeforeIntercept = EventBuilder.withBody("test event", Charsets.UTF_8);    eventBeforeIntercept.getHeaders().put(Constants.HOST, ORIGINAL_HOST);    Assert.assertEquals(ORIGINAL_HOST, eventBeforeIntercept.getHeaders().get(Constants.HOST));    String expectedHost = InetAddress.getLocalHost().getHostAddress();    Event eventAfterIntercept = interceptor.intercept(eventBeforeIntercept);    String actualHost = eventAfterIntercept.getHeaders().get(Constants.HOST);    Assert.assertNotNull(actualHost);    Assert.assertEquals(expectedHost, actualHost);}
public void flume_f3191_0() throws Exception
{    Context ctx = new Context();        ctx.put("useIP", "true");    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.HOST.toString());    builder.configure(ctx);    Interceptor interceptor = builder.build();    final String ORIGINAL_HOST = "originalhost";    Event eventBeforeIntercept = EventBuilder.withBody("test event", Charsets.UTF_8);    eventBeforeIntercept.getHeaders().put(Constants.HOST, ORIGINAL_HOST);    Assert.assertEquals(ORIGINAL_HOST, eventBeforeIntercept.getHeaders().get(Constants.HOST));    String expectedHost = InetAddress.getLocalHost().getHostAddress();    Event eventAfterIntercept = interceptor.intercept(eventBeforeIntercept);    String actualHost = eventAfterIntercept.getHeaders().get(Constants.HOST);    Assert.assertNotNull(actualHost);    Assert.assertEquals(expectedHost, actualHost);}
public void flume_f3192_0() throws Exception
{    Context ctx = new Context();    ctx.put("useIP", "false");    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.HOST.toString());    builder.configure(ctx);    Interceptor interceptor = builder.build();    final String ORIGINAL_HOST = "originalhost";    Event eventBeforeIntercept = EventBuilder.withBody("test event", Charsets.UTF_8);    eventBeforeIntercept.getHeaders().put(Constants.HOST, ORIGINAL_HOST);    Assert.assertEquals(ORIGINAL_HOST, eventBeforeIntercept.getHeaders().get(Constants.HOST));    String expectedHost = InetAddress.getLocalHost().getCanonicalHostName();    Event eventAfterIntercept = interceptor.intercept(eventBeforeIntercept);    String actualHost = eventAfterIntercept.getHeaders().get(Constants.HOST);    Assert.assertNotNull(actualHost);    Assert.assertEquals(expectedHost, actualHost);}
public void flume_f3193_0() throws Exception
{    fixtureBuilder = InterceptorBuilderFactory.newInstance(InterceptorType.REGEX_EXTRACTOR.toString());}
public void flume_f3194_0() throws Exception
{    try {        fixtureBuilder.build();        Assert.fail();    } catch (IllegalArgumentException ex) {        }}
public void flume_f3195_0() throws Exception
{    try {        Context context = new Context();        context.put(RegexExtractorInterceptor.REGEX, "?&?&&&?&?&?&&&??");        fixtureBuilder.configure(context);        fixtureBuilder.build();        Assert.fail();    } catch (IllegalArgumentException ex) {        }}
public void flume_f3196_0() throws Exception
{    try {        Context context = new Context();        context.put(RegexExtractorInterceptor.REGEX, ".*");        context.put(RegexExtractorInterceptor.SERIALIZERS, "");        fixtureBuilder.configure(context);        fixtureBuilder.build();        Assert.fail();    } catch (IllegalArgumentException ex) {        }}
public void flume_f3197_0() throws Exception
{    try {        Context context = new Context();        context.put(RegexExtractorInterceptor.REGEX, "(\\d):(\\d):(\\d)");        context.put(RegexExtractorInterceptor.SERIALIZERS, ",,,");        fixtureBuilder.configure(context);        fixtureBuilder.build();        Assert.fail();    } catch (IllegalArgumentException ex) {        }}
public void flume_f3198_0() throws Exception
{    try {        String space = " ";        Context context = new Context();        context.put(RegexExtractorInterceptor.REGEX, "(\\d):(\\d):(\\d)");        context.put(RegexExtractorInterceptor.SERIALIZERS, Joiner.on(',').join(space, space, space));        fixtureBuilder.configure(context);        fixtureBuilder.build();        Assert.fail();    } catch (IllegalArgumentException ex) {        }}
public void flume_f3199_0() throws Exception
{    Context context = new Context();    context.put(RegexExtractorInterceptor.REGEX, "(\\d):(\\d):(\\d)");    context.put(RegexExtractorInterceptor.SERIALIZERS, "s1 s2 s3");    context.put(RegexExtractorInterceptor.SERIALIZERS + ".s1.name", "Num1");    context.put(RegexExtractorInterceptor.SERIALIZERS + ".s2.name", "Num2");    context.put(RegexExtractorInterceptor.SERIALIZERS + ".s3.name", "Num3");    fixtureBuilder.configure(context);    Interceptor fixture = fixtureBuilder.build();    Event event = EventBuilder.withBody("1:2:3.4foobar5", Charsets.UTF_8);    Event expected = EventBuilder.withBody("1:2:3.4foobar5", Charsets.UTF_8);    expected.getHeaders().put("Num1", "1");    expected.getHeaders().put("Num2", "2");    expected.getHeaders().put("Num3", "3");    Event actual = fixture.intercept(event);    Assert.assertArrayEquals(expected.getBody(), actual.getBody());    Assert.assertEquals(expected.getHeaders(), actual.getHeaders());}
public void flume_f3200_0() throws Exception
{    String body = "2012-10-17 14:34:44,338";    Context context = new Context();        context.put(RegexExtractorInterceptor.REGEX, "^(\\d\\d\\d\\d-\\d\\d-\\d\\d\\s\\d\\d:\\d\\d)(:\\d\\d,\\d\\d\\d)");    context.put(RegexExtractorInterceptor.SERIALIZERS, "s1");    context.put(RegexExtractorInterceptor.SERIALIZERS + ".s1.name", "timestamp");    fixtureBuilder.configure(context);    Interceptor fixture = fixtureBuilder.build();    Event event = EventBuilder.withBody(body, Charsets.UTF_8);    Event expected = EventBuilder.withBody(body, Charsets.UTF_8);    expected.getHeaders().put("timestamp", "2012-10-17 14:34");    Event actual = fixture.intercept(event);    Assert.assertArrayEquals(expected.getBody(), actual.getBody());    Assert.assertEquals(expected.getHeaders(), actual.getHeaders());}
public void flume_f3201_0() throws Exception
{    long now = (System.currentTimeMillis() / 60000L) * 60000L;    String pattern = "yyyy-MM-dd HH:mm:ss,SSS";    DateTimeFormatter formatter = DateTimeFormat.forPattern(pattern);    String body = formatter.print(now);    System.out.println(body);    Context context = new Context();        context.put(RegexExtractorInterceptor.REGEX, "^(\\d\\d\\d\\d-\\d\\d-\\d\\d\\s\\d\\d:\\d\\d)(:\\d\\d,\\d\\d\\d)");    context.put(RegexExtractorInterceptor.SERIALIZERS, "s1 s2");    String millisSerializers = RegexExtractorInterceptorMillisSerializer.class.getName();    context.put(RegexExtractorInterceptor.SERIALIZERS + ".s1.type", millisSerializers);    context.put(RegexExtractorInterceptor.SERIALIZERS + ".s1.name", "timestamp");    context.put(RegexExtractorInterceptor.SERIALIZERS + ".s1.pattern", "yyyy-MM-dd HH:mm");        context.put(RegexExtractorInterceptor.SERIALIZERS + ".s2.name", "data");    fixtureBuilder.configure(context);    Interceptor fixture = fixtureBuilder.build();    Event event = EventBuilder.withBody(body, Charsets.UTF_8);    Event expected = EventBuilder.withBody(body, Charsets.UTF_8);    expected.getHeaders().put("timestamp", String.valueOf(now));    expected.getHeaders().put("data", ":00,000");    Event actual = fixture.intercept(event);    Assert.assertArrayEquals(expected.getBody(), actual.getBody());    Assert.assertEquals(expected.getHeaders(), actual.getHeaders());}
public void flume_f3202_0()
{    try {        RegexExtractorInterceptorMillisSerializer fixture = new RegexExtractorInterceptorMillisSerializer();        fixture.configure(new Context());        Assert.fail();    } catch (IllegalArgumentException ex) {        }}
public void flume_f3203_0()
{    try {        RegexExtractorInterceptorMillisSerializer fixture = new RegexExtractorInterceptorMillisSerializer();        Context context = new Context();        context.put("pattern", "ABCDEFG");        fixture.configure(context);        Assert.fail();    } catch (IllegalArgumentException ex) {        }}
public void flume_f3204_0()
{    RegexExtractorInterceptorMillisSerializer fixture = new RegexExtractorInterceptorMillisSerializer();    Context context = new Context();    String pattern = "yyyy-MM-dd HH:mm:ss";    context.put("pattern", pattern);    fixture.configure(context);    DateTimeFormatter formatter = DateTimeFormat.forPattern(pattern);    long time = (System.currentTimeMillis() / 1000L) * 1000L;    Assert.assertEquals(String.valueOf(time), fixture.serialize(formatter.print(time)));}
public void flume_f3205_0()
{    RegexExtractorInterceptorPassThroughSerializer fixture = new RegexExtractorInterceptorPassThroughSerializer();    fixture.configure(new Context());    String input = "testing (1,2,3,4)";    Assert.assertEquals(input, fixture.serialize(input));}
public void flume_f3206_0() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.REGEX_FILTER.toString());    builder.configure(new Context());    Interceptor interceptor = builder.build();    Event event = EventBuilder.withBody("test", Charsets.UTF_8);    Event filteredEvent = interceptor.intercept(event);    Assert.assertNotNull(filteredEvent);    Assert.assertEquals(event, filteredEvent);}
public void flume_f3207_0() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.REGEX_FILTER.toString());    Context ctx = new Context();    ctx.put(Constants.REGEX, "(INFO.*)|(WARNING.*)");    ctx.put(Constants.EXCLUDE_EVENTS, "false");    builder.configure(ctx);    Interceptor interceptor = builder.build();    Event shouldPass1 = EventBuilder.withBody("INFO: some message", Charsets.UTF_8);    Assert.assertNotNull(interceptor.intercept(shouldPass1));    Event shouldPass2 = EventBuilder.withBody("WARNING: some message", Charsets.UTF_8);    Assert.assertNotNull(interceptor.intercept(shouldPass2));    Event shouldNotPass = EventBuilder.withBody("DEBUG: some message", Charsets.UTF_8);    Assert.assertNull(interceptor.intercept(shouldNotPass));    builder.configure(ctx);}
public void flume_f3208_0() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.REGEX_FILTER.toString());    Context ctx = new Context();    ctx.put(Constants.REGEX, ".*DEBUG.*");    ctx.put(Constants.EXCLUDE_EVENTS, "true");    builder.configure(ctx);    Interceptor interceptor = builder.build();    Event shouldPass1 = EventBuilder.withBody("INFO: some message", Charsets.UTF_8);    Assert.assertNotNull(interceptor.intercept(shouldPass1));    Event shouldPass2 = EventBuilder.withBody("WARNING: some message", Charsets.UTF_8);    Assert.assertNotNull(interceptor.intercept(shouldPass2));    Event shouldNotPass = EventBuilder.withBody("this message has DEBUG in it", Charsets.UTF_8);    Assert.assertNull(interceptor.intercept(shouldNotPass));    builder.configure(ctx);}
private void flume_f3209_1(Context context, String input, String output) throws Exception
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.SEARCH_REPLACE.toString());    builder.configure(context);    Interceptor interceptor = builder.build();    Event event = EventBuilder.withBody(input, Charsets.UTF_8);    event = interceptor.intercept(event);    String val = new String(event.getBody(), Charsets.UTF_8);    assertEquals(output, val);    }
public void flume_f3210_0() throws Exception
{    Context context = new Context();    context.put("searchPattern", "^prefix");    context.put("replaceString", "");    testSearchReplace(context, "prefix non-prefix suffix", " non-prefix suffix");}
public void flume_f3211_0() throws Exception
{    final String input = "<13>Feb  5 17:32:18 10.0.0.99 Use the BFG!";    final String output = "Feb  5 17:32:18 10.0.0.99 Use the BFG!";    Context context = new Context();    context.put("searchPattern", "^<[0-9]+>");    context.put("replaceString", "");    testSearchReplace(context, input, output);}
public void flume_f3212_0() throws Exception
{    final String input = "The quick brown fox jumped over the lazy dog.";    final String output = "The hungry dog ate the careless fox.";    Context context = new Context();    context.put("searchPattern", "The quick brown ([a-z]+) jumped over the lazy ([a-z]+).");    context.put("replaceString", "The hungry $2 ate the careless $1.");    testSearchReplace(context, input, output);}
public void flume_f3213_0() throws Exception
{    final String input = "Email addresses: test@test.com and foo@test.com";    final String output = "Email addresses: REDACTED and REDACTED";    Context context = new Context();    context.put("searchPattern", "[A-Za-z0-9_.]+@[A-Za-z0-9_-]+\\.com");    context.put("replaceString", "REDACTED");    testSearchReplace(context, input, output);}
public void flume_f3214_0() throws Exception
{    final String input = "Abc123@test.com";    final String output = "@test.com";    Context context = new Context();    context.put("searchPattern", "^[A-Za-z0-9_]+");    testSearchReplace(context, input, output);    context.put("replaceString", "");    testSearchReplace(context, input, output);}
public void flume_f3215_0() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.STATIC.toString());    builder.configure(new Context());    Interceptor interceptor = builder.build();    Event event = EventBuilder.withBody("test", Charsets.UTF_8);    Assert.assertNull(event.getHeaders().get(Constants.KEY));    event = interceptor.intercept(event);    String val = event.getHeaders().get(Constants.KEY);    Assert.assertNotNull(val);    Assert.assertEquals(Constants.VALUE, val);}
public void flume_f3216_0() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.STATIC.toString());    Context ctx = new Context();    ctx.put(Constants.KEY, "myKey");    ctx.put(Constants.VALUE, "myVal");    builder.configure(ctx);    Interceptor interceptor = builder.build();    Event event = EventBuilder.withBody("test", Charsets.UTF_8);    Assert.assertNull(event.getHeaders().get("myKey"));    event = interceptor.intercept(event);    String val = event.getHeaders().get("myKey");    Assert.assertNotNull(val);    Assert.assertEquals("myVal", val);}
public void flume_f3217_0() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.STATIC.toString());    Context ctx = new Context();    ctx.put(Constants.PRESERVE, "false");    ctx.put(Constants.VALUE, "replacement value");    builder.configure(ctx);    Interceptor interceptor = builder.build();    Event event = EventBuilder.withBody("test", Charsets.UTF_8);    event.getHeaders().put(Constants.KEY, "incumbent value");    Assert.assertNotNull(event.getHeaders().get(Constants.KEY));    event = interceptor.intercept(event);    String val = event.getHeaders().get(Constants.KEY);    Assert.assertNotNull(val);    Assert.assertEquals("replacement value", val);}
public void flume_f3218_0() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.STATIC.toString());    Context ctx = new Context();    ctx.put(Constants.PRESERVE, "true");    ctx.put(Constants.VALUE, "replacement value");    builder.configure(ctx);    Interceptor interceptor = builder.build();    Event event = EventBuilder.withBody("test", Charsets.UTF_8);    event.getHeaders().put(Constants.KEY, "incumbent value");    Assert.assertNotNull(event.getHeaders().get(Constants.KEY));    event = interceptor.intercept(event);    String val = event.getHeaders().get(Constants.KEY);    Assert.assertNotNull(val);    Assert.assertEquals("incumbent value", val);}
public void flume_f3219_0() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.TIMESTAMP.toString());    Interceptor interceptor = builder.build();    Event event = EventBuilder.withBody("test event", Charsets.UTF_8);    Assert.assertNull(event.getHeaders().get(Constants.DEFAULT_HEADER_NAME));    Long now = System.currentTimeMillis();    event = interceptor.intercept(event);    String timestampStr = event.getHeaders().get(Constants.DEFAULT_HEADER_NAME);    Assert.assertNotNull(timestampStr);    Assert.assertTrue(Long.parseLong(timestampStr) >= now);}
public void flume_f3220_0() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Context ctx = new Context();    ctx.put("preserveExisting", "true");    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.TIMESTAMP.toString());    builder.configure(ctx);    Interceptor interceptor = builder.build();    long originalTs = 1L;    Event event = EventBuilder.withBody("test event", Charsets.UTF_8);    event.getHeaders().put(Constants.DEFAULT_HEADER_NAME, Long.toString(originalTs));    Assert.assertEquals(Long.toString(originalTs), event.getHeaders().get(Constants.DEFAULT_HEADER_NAME));    event = interceptor.intercept(event);    String timestampStr = event.getHeaders().get(Constants.DEFAULT_HEADER_NAME);    Assert.assertNotNull(timestampStr);    Assert.assertTrue(Long.parseLong(timestampStr) == originalTs);}
public void flume_f3221_0() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Context ctx = new Context();        ctx.put("preserveExisting", "false");    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.TIMESTAMP.toString());    builder.configure(ctx);    Interceptor interceptor = builder.build();    long originalTs = 1L;    Event event = EventBuilder.withBody("test event", Charsets.UTF_8);    event.getHeaders().put(Constants.DEFAULT_HEADER_NAME, Long.toString(originalTs));    Assert.assertEquals(Long.toString(originalTs), event.getHeaders().get(Constants.DEFAULT_HEADER_NAME));    Long now = System.currentTimeMillis();    event = interceptor.intercept(event);    String timestampStr = event.getHeaders().get(Constants.DEFAULT_HEADER_NAME);    Assert.assertNotNull(timestampStr);    Assert.assertTrue(Long.parseLong(timestampStr) >= now);}
public void flume_f3222_0() throws Exception
{    Context ctx = new Context();    ctx.put(TimestampInterceptor.Constants.CONFIG_HEADER_NAME, "timestampHeader");    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.TIMESTAMP.toString());    builder.configure(ctx);    Interceptor interceptor = builder.build();    long originalTs = 1L;    Event event = EventBuilder.withBody("test event", Charsets.UTF_8);    event.getHeaders().put(Constants.DEFAULT_HEADER_NAME, Long.toString(originalTs));    Long now = System.currentTimeMillis();    event = interceptor.intercept(event);    Assert.assertEquals(Long.toString(originalTs), event.getHeaders().get(Constants.DEFAULT_HEADER_NAME));    String timestampStr = event.getHeaders().get("timestampHeader");    Assert.assertNotNull(timestampStr);    Assert.assertTrue(Long.parseLong(timestampStr) >= now);}
public void flume_f3223_0() throws LifecycleException, InterruptedException
{    LifecycleAware delegate = new SleeperLifecycleDelegate();    Assert.assertTrue(delegate.getLifecycleState().equals(LifecycleState.IDLE));    delegate.start();    boolean reached = LifecycleController.waitForState(delegate, LifecycleState.START, 2000);    Assert.assertEquals(true, reached);    Assert.assertEquals(LifecycleState.START, delegate.getLifecycleState());    delegate.stop();    reached = LifecycleController.waitForState(delegate, LifecycleState.STOP, 2000);    Assert.assertEquals(true, reached);    Assert.assertEquals(LifecycleState.STOP, delegate.getLifecycleState());    delegate.start();    reached = LifecycleController.waitForState(delegate, LifecycleState.IDLE, 500);    Assert.assertEquals(false, reached);    Assert.assertEquals(LifecycleState.START, delegate.getLifecycleState());}
public void flume_f3224_0() throws LifecycleException, InterruptedException
{    LifecycleAware delegate = new SleeperLifecycleDelegate();    Assert.assertEquals(LifecycleState.IDLE, delegate.getLifecycleState());    delegate.start();    boolean reached = LifecycleController.waitForOneOf(delegate, new LifecycleState[] { LifecycleState.STOP, LifecycleState.START }, 2000);    Assert.assertTrue("Matched a state change", reached);    Assert.assertEquals(LifecycleState.START, delegate.getLifecycleState());}
public void flume_f3225_0()
{    try {        Thread.sleep(sleepTime);    } catch (InterruptedException e) {        }    state = LifecycleState.START;}
public void flume_f3226_0()
{    try {        Thread.sleep(sleepTime);    } catch (InterruptedException e) {        }    state = LifecycleState.STOP;}
public LifecycleState flume_f3227_0()
{    return state;}
public long flume_f3228_0()
{    return sleepTime;}
public void flume_f3229_0(long sleepTime)
{    this.sleepTime = sleepTime;}
public void flume_f3230_0()
{    supervisor = new LifecycleSupervisor();}
public void flume_f3231_0() throws LifecycleException, InterruptedException
{    supervisor.start();    supervisor.stop();}
public void flume_f3232_0() throws LifecycleException, InterruptedException
{    supervisor.start();    /* Attempt to supervise a known-to-fail config. */    /*     * LogicalNode node = new LogicalNode(); SupervisorPolicy policy = new     * SupervisorPolicy.OnceOnlyPolicy(); supervisor.supervise(node, policy,     * LifecycleState.START);     */    CountingLifecycleAware node = new CountingLifecycleAware();    SupervisorPolicy policy = new SupervisorPolicy.OnceOnlyPolicy();    supervisor.supervise(node, policy, LifecycleState.START);    Thread.sleep(10000);    node = new CountingLifecycleAware();    policy = new SupervisorPolicy.OnceOnlyPolicy();    supervisor.supervise(node, policy, LifecycleState.START);    Thread.sleep(5000);    supervisor.stop();}
public void flume_f3233_0() throws LifecycleException, InterruptedException
{    supervisor.start();    /* Attempt to supervise a known-to-fail config. */    LifecycleAware node = new LifecycleAware() {        @Override        public void stop() {        }        @Override        public void start() {            throw new NullPointerException("Boom!");        }        @Override        public LifecycleState getLifecycleState() {            return LifecycleState.IDLE;        }    };    SupervisorPolicy policy = new SupervisorPolicy.OnceOnlyPolicy();    supervisor.supervise(node, policy, LifecycleState.START);    Thread.sleep(5000);    supervisor.stop();}
public void flume_f3234_0()
{}
public void flume_f3235_0()
{    throw new NullPointerException("Boom!");}
public LifecycleState flume_f3236_0()
{    return LifecycleState.IDLE;}
public void flume_f3237_0() throws LifecycleException, InterruptedException
{    supervisor.start();    LifecycleSupervisor supervisor2 = new LifecycleSupervisor();    CountingLifecycleAware node = new CountingLifecycleAware();    SupervisorPolicy policy = new SupervisorPolicy.OnceOnlyPolicy();    supervisor2.supervise(node, policy, LifecycleState.START);    supervisor.supervise(supervisor2, new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);    Thread.sleep(10000);    supervisor.stop();}
public void flume_f3238_0() throws LifecycleException, InterruptedException
{    supervisor.start();    LifecycleAware service = new CountingLifecycleAware();    SupervisorPolicy policy = new SupervisorPolicy.OnceOnlyPolicy();    supervisor.supervise(service, policy, LifecycleState.START);    supervisor.unsupervise(service);    service.stop();    supervisor.stop();}
public void flume_f3239_0() throws LifecycleException, InterruptedException
{    supervisor.start();    CountingLifecycleAware service = new CountingLifecycleAware();    SupervisorPolicy policy = new SupervisorPolicy.OnceOnlyPolicy();    Assert.assertEquals(Long.valueOf(0), service.counterGroup.get("start"));    Assert.assertEquals(Long.valueOf(0), service.counterGroup.get("stop"));    supervisor.supervise(service, policy, LifecycleState.START);    Thread.sleep(3200);    Assert.assertEquals(Long.valueOf(1), service.counterGroup.get("start"));    Assert.assertEquals(Long.valueOf(0), service.counterGroup.get("stop"));    supervisor.setDesiredState(service, LifecycleState.STOP);    Thread.sleep(3200);    Assert.assertEquals(Long.valueOf(1), service.counterGroup.get("start"));    Assert.assertEquals(Long.valueOf(1), service.counterGroup.get("stop"));    supervisor.stop();}
public void flume_f3240_0()
{    counterGroup.incrementAndGet("start");    lifecycleState = LifecycleState.START;}
public void flume_f3241_0()
{    counterGroup.incrementAndGet("stop");    lifecycleState = LifecycleState.STOP;}
public LifecycleState flume_f3242_0()
{    return lifecycleState;}
public int flume_f3243_0() throws IOException
{    if (curPos >= str.length()) {        return -1;    }    return str.charAt(curPos++);}
public void flume_f3244_0() throws IOException
{    markPos = curPos;}
public void flume_f3245_0() throws IOException
{    curPos = markPos;}
public void flume_f3246_0(long position) throws IOException
{    throw new UnsupportedOperationException("Unimplemented in test class");}
public long flume_f3247_0() throws IOException
{    throw new UnsupportedOperationException("Unimplemented in test class");}
public int flume_f3248_0() throws IOException
{    throw new UnsupportedOperationException("This test class doesn't return " + "bytes!");}
public int flume_f3249_0(byte[] b, int off, int len) throws IOException
{    throw new UnsupportedOperationException("This test class doesn't return " + "bytes!");}
public void flume_f3250_0() throws IOException
{}
protected OutputStream flume_f3251_0()
{    return out;}
protected Schema flume_f3252_0()
{    return schema;}
protected SyslogEvent flume_f3253_1(Event event)
{    SyslogEvent sle = new SyslogEvent();            String msg = new String(event.getBody(), Charsets.UTF_8);        int seek = 0;                Map<String, String> headers = event.getHeaders();    boolean fromSyslogSource = false;    if (headers.containsKey(SyslogUtils.SYSLOG_FACILITY)) {        fromSyslogSource = true;        int facility = Integer.parseInt(headers.get(SyslogUtils.SYSLOG_FACILITY));        sle.setFacility(facility);    }    if (headers.containsKey(SyslogUtils.SYSLOG_SEVERITY)) {        fromSyslogSource = true;        int severity = Integer.parseInt(headers.get(SyslogUtils.SYSLOG_SEVERITY));        sle.setSeverity(severity);    }        if (!fromSyslogSource) {        if (msg.charAt(0) == '<') {            int end = msg.indexOf(">");            if (end > -1) {                seek = end + 1;                String priStr = msg.substring(1, end);                int priority = Integer.parseInt(priStr);                int severity = priority % 8;                int facility = (priority - severity) / 8;                sle.setFacility(facility);                sle.setSeverity(severity);            }        }    }        String timestampStr = msg.substring(seek, seek + 15);    long ts = parseRfc3164Date(timestampStr);    if (ts != 0) {        sle.setTimestamp(ts);                seek += 15 + 1;    }        int nextSpace = msg.indexOf(' ', seek);    if (nextSpace > -1) {        String hostname = msg.substring(seek, nextSpace);        sle.setHostname(hostname);        seek = nextSpace + 1;    }        String actualMessage = msg.substring(seek);    sle.setMessage(actualMessage);    if (logger.isDebugEnabled() && LogPrivacyUtil.allowLogRawData()) {            }    return sle;}
private static long flume_f3254_1(String in)
{    DateTime date = null;    try {        date = dateFmt1.parseDateTime(in);    } catch (IllegalArgumentException e) {                    }    if (date == null) {        try {            date = dateFmt2.parseDateTime(in);        } catch (IllegalArgumentException e) {                                }    }        if (date != null) {        DateTime now = new DateTime();        int year = now.getYear();        DateTime corrected = date.withYear(year);                if (corrected.isAfter(now) && corrected.minusMonths(1).isAfter(now)) {            corrected = date.minusYears(1);                } else if (corrected.isBefore(now) && corrected.plusMonths(1).isBefore(now)) {            corrected = date.plusYears(1);        }        date = corrected;    }    if (date == null) {        return 0;    }    return date.getMillis();}
public EventSerializer flume_f3255_1(Context context, OutputStream out)
{    SyslogAvroEventSerializer writer = null;    try {        writer = new SyslogAvroEventSerializer(out);        writer.configure(context);    } catch (IOException e) {            }    return writer;}
public void flume_f3256_0(int f)
{    facility = f;}
public int flume_f3257_0()
{    return facility;}
public void flume_f3258_0(int s)
{    severity = s;}
public int flume_f3259_0()
{    return severity;}
public void flume_f3260_0(long t)
{    timestamp = t;}
public long flume_f3261_0()
{    return timestamp;}
public void flume_f3262_0(String h)
{    hostname = h;}
public String flume_f3263_0()
{    return hostname;}
public void flume_f3264_0(String m)
{    message = m;}
public String flume_f3265_0()
{    return message;}
public String flume_f3266_0()
{    StringBuilder builder = new StringBuilder();    builder.append("{ Facility: ").append(facility).append(", ");    builder.append(" Severity: ").append(severity).append(", ");    builder.append(" Timestamp: ").append(timestamp).append(", ");    builder.append(" Hostname: ").append(hostname).append(", ");    builder.append(" Message: \"").append(message).append("\" }");    return builder.toString();}
public void flume_f3267_1() throws IOException
{    File tempFile = newTestFile(true);    String target = tempFile.getAbsolutePath();        TransientPositionTracker tracker = new TransientPositionTracker(target);    AvroEventDeserializer.Builder desBuilder = new AvroEventDeserializer.Builder();    EventDeserializer deserializer = desBuilder.build(new Context(), new ResettableFileInputStream(tempFile, tracker));    BinaryDecoder decoder = null;    DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>(schema);    decoder = DecoderFactory.get().binaryDecoder(deserializer.readEvent().getBody(), decoder);    assertEquals("bar", reader.read(null, decoder).get("foo").toString());    deserializer.reset();    decoder = DecoderFactory.get().binaryDecoder(deserializer.readEvent().getBody(), decoder);    assertEquals("bar", reader.read(null, decoder).get("foo").toString());    deserializer.mark();    decoder = DecoderFactory.get().binaryDecoder(deserializer.readEvent().getBody(), decoder);    assertEquals("baz", reader.read(null, decoder).get("foo").toString());    deserializer.reset();    decoder = DecoderFactory.get().binaryDecoder(deserializer.readEvent().getBody(), decoder);    assertEquals("baz", reader.read(null, decoder).get("foo").toString());    assertNull(deserializer.readEvent());}
public void flume_f3268_1() throws IOException, NoSuchAlgorithmException
{    File tempFile = newTestFile(true);    String target = tempFile.getAbsolutePath();        TransientPositionTracker tracker = new TransientPositionTracker(target);    Context context = new Context();    context.put(AvroEventDeserializer.CONFIG_SCHEMA_TYPE_KEY, AvroEventDeserializer.AvroSchemaType.HASH.toString());    ResettableInputStream in = new ResettableFileInputStream(tempFile, tracker);    EventDeserializer des = new AvroEventDeserializer.Builder().build(context, in);    Event event = des.readEvent();    String eventSchemaHash = event.getHeaders().get(AvroEventDeserializer.AVRO_SCHEMA_HEADER_HASH);    String expectedSchemaHash = Hex.encodeHexString(SchemaNormalization.parsingFingerprint("CRC-64-AVRO", schema));    Assert.assertEquals(expectedSchemaHash, eventSchemaHash);}
public void flume_f3269_1() throws IOException
{    File tempFile = newTestFile(true);    String target = tempFile.getAbsolutePath();        TransientPositionTracker tracker = new TransientPositionTracker(target);    Context context = new Context();    context.put(AvroEventDeserializer.CONFIG_SCHEMA_TYPE_KEY, AvroEventDeserializer.AvroSchemaType.LITERAL.toString());    ResettableInputStream in = new ResettableFileInputStream(tempFile, tracker);    EventDeserializer des = new AvroEventDeserializer.Builder().build(context, in);    Event event = des.readEvent();    String eventSchema = event.getHeaders().get(AvroEventDeserializer.AVRO_SCHEMA_HEADER_LITERAL);    Assert.assertEquals(schema.toString(), eventSchema);}
private File flume_f3270_0(boolean deleteOnExit) throws IOException
{    File tempFile = File.createTempFile("testDirectFile", "tmp");    if (deleteOnExit) {        tempFile.deleteOnExit();    }    DataFileWriter<GenericRecord> writer = new DataFileWriter<GenericRecord>(new GenericDatumWriter<GenericRecord>(schema));    writer.create(schema, tempFile);    GenericRecordBuilder recordBuilder;    recordBuilder = new GenericRecordBuilder(schema);    recordBuilder.set("foo", "bar");    GenericRecord record = recordBuilder.build();    writer.append(record);    writer.sync();    recordBuilder = new GenericRecordBuilder(schema);    recordBuilder.set("foo", "baz");    record = recordBuilder.build();    writer.append(record);    writer.sync();    writer.flush();    writer.close();    return tempFile;}
public void flume_f3271_0() throws FileNotFoundException, IOException
{    OutputStream out = new FileOutputStream(testFile);    EventSerializer serializer = EventSerializerFactory.getInstance("text", new Context(), out);    serializer.afterCreate();    serializer.write(EventBuilder.withBody("event 1", Charsets.UTF_8));    serializer.write(EventBuilder.withBody("event 2", Charsets.UTF_8));    serializer.write(EventBuilder.withBody("event 3", Charsets.UTF_8));    serializer.flush();    serializer.beforeClose();    out.flush();    out.close();    BufferedReader reader = new BufferedReader(new FileReader(testFile));    Assert.assertEquals("event 1", reader.readLine());    Assert.assertEquals("event 2", reader.readLine());    Assert.assertEquals("event 3", reader.readLine());    Assert.assertNull(reader.readLine());    reader.close();    FileUtils.forceDelete(testFile);}
public void flume_f3272_0() throws FileNotFoundException, IOException
{    OutputStream out = new FileOutputStream(testFile);    Context context = new Context();    context.put("appendNewline", "false");    EventSerializer serializer = EventSerializerFactory.getInstance("text", context, out);    serializer.afterCreate();    serializer.write(EventBuilder.withBody("event 1\n", Charsets.UTF_8));    serializer.write(EventBuilder.withBody("event 2\n", Charsets.UTF_8));    serializer.write(EventBuilder.withBody("event 3\n", Charsets.UTF_8));    serializer.flush();    serializer.beforeClose();    out.flush();    out.close();    BufferedReader reader = new BufferedReader(new FileReader(testFile));    Assert.assertEquals("event 1", reader.readLine());    Assert.assertEquals("event 2", reader.readLine());    Assert.assertEquals("event 3", reader.readLine());    Assert.assertNull(reader.readLine());    reader.close();    FileUtils.forceDelete(testFile);}
public void flume_f3273_0() throws IOException
{    File metaFile = File.createTempFile(getClass().getName(), ".meta");    metaFile.delete();    File dataFile = File.createTempFile(getClass().getName(), ".data");    Files.write("line 1\nline2\n", dataFile, Charsets.UTF_8);    final long NEW_POS = 7;    PositionTracker tracker;    tracker = new DurablePositionTracker(metaFile, dataFile.toString());    Assert.assertEquals(0, tracker.getPosition());    tracker.storePosition(NEW_POS);    Assert.assertEquals(NEW_POS, tracker.getPosition());    tracker.close();        tracker = new DurablePositionTracker(metaFile, "foobar");    Assert.assertEquals(NEW_POS, tracker.getPosition());    Assert.assertEquals(dataFile.getAbsolutePath(), tracker.getTarget());}
public void flume_f3274_0() throws IOException, URISyntaxException
{    String fileName = "/TestResettableFileInputStream_1.avro";    File trackerFile = new File(getClass().getResource(fileName).toURI());    Assert.assertTrue(trackerFile.exists());    PositionTracker tracker;    tracker = new DurablePositionTracker(trackerFile, "foo");        Assert.assertEquals(62, tracker.getPosition());}
public void flume_f3275_0() throws IOException, URISyntaxException
{    String fileName = "/TestResettableFileInputStream_1.truncated.avro";    File trackerFile = new File(getClass().getResource(fileName).toURI());    Assert.assertTrue(trackerFile.exists());    PositionTracker tracker;    tracker = new DurablePositionTracker(trackerFile, "foo");        Assert.assertEquals(25, tracker.getPosition());}
public void flume_f3276_0() throws FileNotFoundException, IOException
{    createAvroFile(TESTFILE, null);    validateAvroFile(TESTFILE);    FileUtils.forceDelete(TESTFILE);}
public void flume_f3277_0() throws FileNotFoundException, IOException
{    createAvroFile(TESTFILE, "null");    validateAvroFile(TESTFILE);    FileUtils.forceDelete(TESTFILE);}
public void flume_f3278_0() throws FileNotFoundException, IOException
{    createAvroFile(TESTFILE, "deflate");    validateAvroFile(TESTFILE);    FileUtils.forceDelete(TESTFILE);}
public void flume_f3279_0() throws FileNotFoundException, IOException
{        Assume.assumeTrue(!"Mac OS X".equals(System.getProperty("os.name")) || !System.getProperty("java.version").startsWith("1.7."));    createAvroFile(TESTFILE, "snappy");    validateAvroFile(TESTFILE);    FileUtils.forceDelete(TESTFILE);}
public void flume_f3280_0(File file, String codec) throws FileNotFoundException, IOException
{    if (file.exists()) {        FileUtils.forceDelete(file);    }        OutputStream out = new FileOutputStream(file);    Context ctx = new Context();    if (codec != null) {        ctx.put("compressionCodec", codec);    }    EventSerializer.Builder builder = new FlumeEventAvroEventSerializer.Builder();    EventSerializer serializer = builder.build(ctx, out);    serializer.afterCreate();    serializer.write(EventBuilder.withBody("yo man!", Charsets.UTF_8));    serializer.write(EventBuilder.withBody("2nd event!", Charsets.UTF_8));    serializer.write(EventBuilder.withBody("last one!", Charsets.UTF_8));    serializer.flush();    serializer.beforeClose();    out.flush();    out.close();}
public void flume_f3281_0(File file) throws IOException
{        DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>();    DataFileReader<GenericRecord> fileReader = new DataFileReader<GenericRecord>(file, reader);    GenericRecord record = new GenericData.Record(fileReader.getSchema());    int numEvents = 0;    while (fileReader.hasNext()) {        fileReader.next(record);        ByteBuffer body = (ByteBuffer) record.get("body");        CharsetDecoder decoder = Charsets.UTF_8.newDecoder();        String bodyStr = decoder.decode(body).toString();        System.out.println(bodyStr);        numEvents++;    }    fileReader.close();    Assert.assertEquals("Should have found a total of 3 events", 3, numEvents);}
public void flume_f3282_0() throws FileNotFoundException, IOException
{    Map<String, String> headers = new HashMap<String, String>();    headers.put("header1", "value1");    headers.put("header2", "value2");    OutputStream out = new FileOutputStream(testFile);    EventSerializer serializer = EventSerializerFactory.getInstance("header_and_text", new Context(), out);    serializer.afterCreate();    serializer.write(EventBuilder.withBody("event 1", Charsets.UTF_8, headers));    serializer.write(EventBuilder.withBody("event 2", Charsets.UTF_8, headers));    serializer.write(EventBuilder.withBody("event 3", Charsets.UTF_8, headers));    serializer.flush();    serializer.beforeClose();    out.flush();    out.close();    BufferedReader reader = new BufferedReader(new FileReader(testFile));    Assert.assertEquals("{header2=value2, header1=value1} event 1", reader.readLine());    Assert.assertEquals("{header2=value2, header1=value1} event 2", reader.readLine());    Assert.assertEquals("{header2=value2, header1=value1} event 3", reader.readLine());    Assert.assertNull(reader.readLine());    reader.close();    FileUtils.forceDelete(testFile);}
public void flume_f3283_0() throws FileNotFoundException, IOException
{    Map<String, String> headers = new HashMap<String, String>();    headers.put("header1", "value1");    headers.put("header2", "value2");    OutputStream out = new FileOutputStream(testFile);    Context context = new Context();    context.put("appendNewline", "false");    EventSerializer serializer = EventSerializerFactory.getInstance("header_and_text", context, out);    serializer.afterCreate();    serializer.write(EventBuilder.withBody("event 1\n", Charsets.UTF_8, headers));    serializer.write(EventBuilder.withBody("event 2\n", Charsets.UTF_8, headers));    serializer.write(EventBuilder.withBody("event 3\n", Charsets.UTF_8, headers));    serializer.flush();    serializer.beforeClose();    out.flush();    out.close();    BufferedReader reader = new BufferedReader(new FileReader(testFile));    Assert.assertEquals("{header2=value2, header1=value1} event 1", reader.readLine());    Assert.assertEquals("{header2=value2, header1=value1} event 2", reader.readLine());    Assert.assertEquals("{header2=value2, header1=value1} event 3", reader.readLine());    Assert.assertNull(reader.readLine());    reader.close();    FileUtils.forceDelete(testFile);}
public void flume_f3284_0()
{    StringBuilder sb = new StringBuilder();    sb.append("line 1\n");    sb.append("line 2\n");    mini = sb.toString();}
public void flume_f3285_0() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer des = new LineDeserializer(new Context(), in);    validateMiniParse(des);}
public void flume_f3286_0() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer.Builder builder = new LineDeserializer.Builder();    EventDeserializer des = builder.build(new Context(), in);    validateMiniParse(des);}
public void flume_f3287_0() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer des;    des = EventDeserializerFactory.getInstance("LINE", new Context(), in);    validateMiniParse(des);}
public void flume_f3288_0() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer des = new LineDeserializer(new Context(), in);    List<Event> events;        events = des.readEvents(1);    Assert.assertEquals(1, events.size());    assertEventBodyEquals("line 1", events.get(0));        events = des.readEvents(10);    Assert.assertEquals(1, events.size());    assertEventBodyEquals("line 2", events.get(0));    des.mark();    des.close();}
public void flume_f3289_0() throws IOException
{    String longLine = "abcdefghijklmnopqrstuvwxyz\n";    Context ctx = new Context();    ctx.put(LineDeserializer.MAXLINE_KEY, "10");    ResettableInputStream in = new ResettableTestStringInputStream(longLine);    EventDeserializer des = new LineDeserializer(ctx, in);    assertEventBodyEquals("abcdefghij", des.readEvent());    assertEventBodyEquals("klmnopqrst", des.readEvent());    assertEventBodyEquals("uvwxyz", des.readEvent());    Assert.assertNull(des.readEvent());}
private void flume_f3290_0(String expected, Event event)
{    String bodyStr = new String(event.getBody(), Charsets.UTF_8);    Assert.assertEquals(expected, bodyStr);}
private void flume_f3291_0(EventDeserializer des) throws IOException
{    Event evt;    evt = des.readEvent();    Assert.assertEquals(new String(evt.getBody()), "line 1");    des.mark();    evt = des.readEvent();    Assert.assertEquals(new String(evt.getBody()), "line 2");        des.reset();    evt = des.readEvent();    Assert.assertEquals("Line 2 should be repeated, " + "because we reset() the stream", new String(evt.getBody()), "line 2");    evt = des.readEvent();    Assert.assertNull("Event should be null because there are no lines " + "left to read", evt);    des.mark();    des.close();}
public void flume_f3292_1() throws Exception
{    Files.createParentDirs(new File(WORK_DIR, "dummy"));    file = File.createTempFile(getClass().getSimpleName(), ".txt", WORK_DIR);        meta = File.createTempFile(getClass().getSimpleName(), ".avro", WORK_DIR);            meta.delete();}
public void flume_f3293_0() throws Exception
{    if (CLEANUP) {        meta.delete();        file.delete();    }}
public void flume_f3294_0() throws IOException
{    String output = singleLineFileInit(file, Charsets.UTF_8);    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker);    String result = readLine(in, output.length());    assertEquals(output, result);    String afterEOF = readLine(in, output.length());    assertNull(afterEOF);    in.close();}
public void flume_f3295_0() throws IOException
{    byte[] bytes = new byte[255];    for (int i = 0; i < 255; i++) {        bytes[i] = (byte) i;    }    Files.write(bytes, file);    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker);    for (int i = 0; i < 255; i++) {        assertEquals(i, in.read());    }    assertEquals(-1, in.read());    in.close();}
public void flume_f3296_0() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    out.write("1234567".getBytes(Charsets.UTF_8));        generateUtf83ByteSequence(out);        Files.write(out.toByteArray(), file);    ResettableInputStream in = initInputStream(8, Charsets.UTF_8, DecodeErrorPolicy.FAIL);    String result = readLine(in, 8);    assertEquals("1234567\u0A93\n", result);}
public void flume_f3297_0() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    out.write("1234567".getBytes(Charsets.UTF_8));    generateUtf8SurrogatePairSequence(out);            Files.write(out.toByteArray(), file);    ResettableInputStream in = initInputStream(8, Charsets.UTF_8, DecodeErrorPolicy.FAIL);    String result = readLine(in, 9);    assertEquals("1234567\uD83D\uDE18\n", result);}
public void flume_f3298_0() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    generateUtf16SurrogatePairSequence(out);            Files.write(out.toByteArray(), file);    ResettableInputStream in = initInputStream(8, Charsets.UTF_16, DecodeErrorPolicy.FAIL);    String result = readLine(in, 2);    assertEquals("\uD83D\uDE18\n", result);}
public void flume_f3299_0() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    out.write("1234567".getBytes(Charset.forName("Shift_JIS")));        generateShiftJis2ByteSequence(out);        Files.write(out.toByteArray(), file);    ResettableInputStream in = initInputStream(8, Charset.forName("Shift_JIS"), DecodeErrorPolicy.FAIL);    String result = readLine(in, 8);    assertEquals("1234567\u4E9C\n", result);}
public void flume_f3300_0() throws IOException
{    ResettableInputStream in = initUtf8DecodeTest(DecodeErrorPolicy.FAIL);    while (in.readChar() != -1) {        }    fail("Expected MalformedInputException!");}
public void flume_f3301_0() throws IOException
{    ResettableInputStream in = initUtf8DecodeTest(DecodeErrorPolicy.IGNORE);    int c;    StringBuilder sb = new StringBuilder();    while ((c = in.readChar()) != -1) {        sb.append((char) c);    }    assertEquals("Latin1: ()\nLong: ()\nNonUnicode: ()\n", sb.toString());}
public void flume_f3302_0() throws IOException
{    ResettableInputStream in = initUtf8DecodeTest(DecodeErrorPolicy.REPLACE);    int c;    StringBuilder sb = new StringBuilder();    while ((c = in.readChar()) != -1) {        sb.append((char) c);    }    String preJdk8ExpectedStr = "Latin1: (X)\nLong: (XXX)\nNonUnicode: (X)\n";    String expectedStr = "Latin1: (X)\nLong: (XXX)\nNonUnicode: (XXXXX)\n";    String javaVersionStr = System.getProperty("java.version");    double javaVersion = Double.parseDouble(javaVersionStr.substring(0, 3));    if (javaVersion < 1.8) {        assertTrue(preJdk8ExpectedStr.replaceAll("X", "\ufffd").equals(sb.toString()));    } else {        assertTrue(expectedStr.replaceAll("X", "\ufffd").equals(sb.toString()));    }}
public void flume_f3303_0() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    generateLatin1InvalidSequence(out);    Files.write(out.toByteArray(), file);    ResettableInputStream in = initInputStream(DecodeErrorPolicy.FAIL);    while (in.readChar() != -1) {        }    fail("Expected MalformedInputException!");}
public void flume_f3304_0() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    generateLatin1InvalidSequence(out);    Files.write(out.toByteArray(), file);    ResettableInputStream in = initInputStream(DecodeErrorPolicy.REPLACE);    int c;    StringBuilder sb = new StringBuilder();    while ((c = in.readChar()) != -1) {        sb.append((char) c);    }    assertEquals("Invalid: (X)\n".replaceAll("X", "\ufffd"), sb.toString());}
public void flume_f3305_0() throws IOException
{    String output = singleLineFileInit(file, Charsets.UTF_8);    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker);    String result1 = readLine(in, output.length());    assertEquals(output, result1);    in.reset();    String result2 = readLine(in, output.length());    assertEquals(output, result2);    String result3 = readLine(in, output.length());    assertNull("Should be null: " + result3, result3);    in.close();}
public void flume_f3306_0() throws IOException
{    List<String> expected = multiLineFileInit(file, Charsets.UTF_8);    int MAX_LEN = 100;    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker);    String result0 = readLine(in, MAX_LEN);    assertEquals(expected.get(0), result0);    in.reset();    String result0a = readLine(in, MAX_LEN);    assertEquals(expected.get(0), result0a);    in.mark();    String result1 = readLine(in, MAX_LEN);    assertEquals(expected.get(1), result1);    in.reset();    String result1a = readLine(in, MAX_LEN);    assertEquals(expected.get(1), result1a);    in.mark();    in.close();}
public void flume_f3307_0() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    out.write("foo".getBytes(Charsets.UTF_8));    generateUtf8SurrogatePairSequence(out);    out.write("bar".getBytes(Charsets.UTF_8));    Files.write(out.toByteArray(), file);    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker);    Assert.assertEquals('f', in.readChar());    Assert.assertEquals('o', in.readChar());    in.mark();    Assert.assertEquals('o', in.readChar());        Assert.assertEquals('\ud83d', in.readChar());        in.reset();            Assert.assertEquals('\ude18', in.readChar());        Assert.assertEquals('o', in.readChar());        Assert.assertEquals('\ud83d', in.readChar());            in.mark();        in.reset();            Assert.assertEquals('\ude18', in.readChar());    Assert.assertEquals('b', in.readChar());    Assert.assertEquals('a', in.readChar());        in.reset();    Assert.assertEquals('b', in.readChar());    Assert.assertEquals('a', in.readChar());    Assert.assertEquals('r', in.readChar());    Assert.assertEquals(-1, in.readChar());    in.close();        tracker.close();}
public void flume_f3308_0() throws IOException
{    List<String> expected = multiLineFileInit(file, Charsets.UTF_8);    int MAX_LEN = 100;    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker);    String result0 = readLine(in, MAX_LEN);    String result1 = readLine(in, MAX_LEN);    in.mark();    String result2 = readLine(in, MAX_LEN);    Assert.assertEquals(expected.get(2), result2);    String result3 = readLine(in, MAX_LEN);    Assert.assertEquals(expected.get(3), result3);    in.close();        tracker.close();        tracker = new DurablePositionTracker(meta, file.getPath());    in = new ResettableFileInputStream(file, tracker);    String result2a = readLine(in, MAX_LEN);    String result3a = readLine(in, MAX_LEN);    Assert.assertEquals(result2, result2a);    Assert.assertEquals(result3, result3a);}
public void flume_f3309_0() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    out.write("foo".getBytes(Charsets.UTF_8));    generateUtf8SurrogatePairSequence(out);    out.write("bar".getBytes(Charsets.UTF_8));    Files.write(out.toByteArray(), file);    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker);    Assert.assertEquals('f', in.readChar());    Assert.assertEquals('o', in.readChar());    in.mark();    Assert.assertEquals('o', in.readChar());        Assert.assertEquals('\ud83d', in.readChar());        in.reset();            in.close();        tracker.close();        tracker = new DurablePositionTracker(meta, file.getPath());    in = new ResettableFileInputStream(file, tracker);        Assert.assertEquals('o', in.readChar());        Assert.assertEquals('\ud83d', in.readChar());            in.mark();            in.close();        tracker.close();        tracker = new DurablePositionTracker(meta, file.getPath());    in = new ResettableFileInputStream(file, tracker);        Assert.assertEquals('b', in.readChar());    Assert.assertEquals('a', in.readChar());    Assert.assertEquals('r', in.readChar());    Assert.assertEquals(-1, in.readChar());    in.close();        tracker.close();}
public void flume_f3310_0() throws IOException
{    int NUM_LINES = 1000;    int LINE_LEN = 1000;    generateData(file, Charsets.UTF_8, NUM_LINES, LINE_LEN);    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker, 10 * LINE_LEN, Charsets.UTF_8, DecodeErrorPolicy.FAIL);    String line = "";    for (int i = 0; i < 9; i++) {        line = readLine(in, LINE_LEN);    }    int lineNum = Integer.parseInt(line.substring(0, 10));    assertEquals(8, lineNum);        long pos = in.tell();        in.seek(pos - 2 * LINE_LEN);    line = readLine(in, LINE_LEN);    lineNum = Integer.parseInt(line.substring(0, 10));    assertEquals(7, lineNum);        in.seek(in.tell() + LINE_LEN);    line = readLine(in, LINE_LEN);    lineNum = Integer.parseInt(line.substring(0, 10));    assertEquals(9, lineNum);        in.seek(in.tell() + 20 * LINE_LEN);    line = readLine(in, LINE_LEN);    lineNum = Integer.parseInt(line.substring(0, 10));    assertEquals(30, lineNum);        in.seek(in.tell() - 25 * LINE_LEN);    line = readLine(in, LINE_LEN);    lineNum = Integer.parseInt(line.substring(0, 10));    assertEquals(6, lineNum);        in.seek(100 * LINE_LEN);        in.seek(0);    in.seek(9 * LINE_LEN);    assertEquals(9, Integer.parseInt(readLine(in, LINE_LEN).substring(0, 10)));    assertEquals(10, Integer.parseInt(readLine(in, LINE_LEN).substring(0, 10)));    assertEquals(11, Integer.parseInt(readLine(in, LINE_LEN).substring(0, 10)));}
private ResettableInputStream flume_f3311_0(DecodeErrorPolicy policy) throws IOException
{    writeBigBadUtf8Sequence(file);    return initInputStream(policy);}
private ResettableInputStream flume_f3312_0(DecodeErrorPolicy policy) throws IOException
{    return initInputStream(2048, Charsets.UTF_8, policy);}
private ResettableInputStream flume_f3313_0(int bufferSize, Charset charset, DecodeErrorPolicy policy) throws IOException
{    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker, bufferSize, charset, policy);    return in;}
private void flume_f3314_0(File file) throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    generateUtf8Latin1Sequence(out);    generateUtf8OverlyLongSequence(out);    generateUtf8NonUnicodeSequence(out);    Files.write(out.toByteArray(), file);}
private void flume_f3315_0(OutputStream out) throws IOException
{    out.write("Long: (".getBytes(Charsets.UTF_8));        out.write(new byte[] { (byte) 0xe0, (byte) 0x80, (byte) 0xaf });    out.write(")\n".getBytes(Charsets.UTF_8));}
private void flume_f3316_0(OutputStream out) throws IOException
{    out.write("NonUnicode: (".getBytes(Charsets.UTF_8));        out.write(new byte[] { (byte) 0xf8, (byte) 0xa1, (byte) 0xa1, (byte) 0xa1, (byte) 0xa1 });    out.write(")\n".getBytes(Charsets.UTF_8));}
private void flume_f3317_0(OutputStream out) throws IOException
{    out.write("Latin1: (".getBytes(Charsets.UTF_8));        out.write(new byte[] { (byte) 0xe9 });    out.write(")\n".getBytes(Charsets.UTF_8));}
private void flume_f3318_0(OutputStream out) throws IOException
{    out.write("Invalid: (".getBytes(Charsets.UTF_8));        out.write(new byte[] { (byte) 0x81 });    out.write(")\n".getBytes(Charsets.UTF_8));}
private void flume_f3319_0(OutputStream out) throws IOException
{        out.write(new byte[] { (byte) 0xF0, (byte) 0x9F, (byte) 0x98, (byte) 0x98 });}
private void flume_f3320_0(OutputStream out) throws IOException
{        out.write(new byte[] { (byte) 0xFE, (byte) 0xFF });        out.write(new byte[] { (byte) 0xD8, (byte) 0x3D, (byte) 0xDE, (byte) 0x18 });}
private void flume_f3321_0(OutputStream out) throws IOException
{        out.write(new byte[] { (byte) 0xe0, (byte) 0xaa, (byte) 0x93 });}
private void flume_f3322_0(OutputStream out) throws IOException
{        out.write(new byte[] { (byte) 0x88, (byte) 0x9f });}
private static String flume_f3323_0(ResettableInputStream in, int maxLength) throws IOException
{    StringBuilder s = new StringBuilder();    int c;    int i = 1;    while ((c = in.readChar()) != -1) {                if (c == '\n') {            break;        }                s.append((char) c);        if (i++ > maxLength) {            System.out.println("Output: >" + s + "<");            throw new RuntimeException("Too far!");        }    }    if (s.length() > 0) {        s.append('\n');        return s.toString();    } else {        return null;    }}
private static String flume_f3324_0(File file, Charset charset) throws IOException
{    String output = "This is gonna be great!\n";    Files.write(output.getBytes(charset), file);    return output;}
private static List<String> flume_f3325_0(File file, Charset charset) throws IOException
{    List<String> lines = Lists.newArrayList();    lines.add("1. On the planet of Mars\n");    lines.add("2. They have clothes just like ours,\n");    lines.add("3. And they have the same shoes and same laces,\n");    lines.add("4. And they have the same charms and same graces...\n");    StringBuilder sb = new StringBuilder();    for (String line : lines) {        sb.append(line);    }    Files.write(sb.toString().getBytes(charset), file);    return lines;}
private static void flume_f3326_0(File file, Charset charset, int numLines, int lineLen) throws IOException
{    OutputStream out = new BufferedOutputStream(new FileOutputStream(file));    StringBuilder junk = new StringBuilder();    for (int x = 0; x < lineLen - 13; x++) {        junk.append('x');    }    String payload = junk.toString();    StringBuilder builder = new StringBuilder();    for (int i = 0; i < numLines; i++) {        builder.append(String.format("%010d: %s\n", i, payload));        if (i % 1000 == 0 && i != 0) {            out.write(builder.toString().getBytes(charset));            builder.setLength(0);        }    }    out.write(builder.toString().getBytes(charset));    out.close();    Assert.assertEquals(lineLen * numLines, file.length());}
private static List<Event> flume_f3327_0()
{    List<Event> list = Lists.newArrayList();    Event e;        e = EventBuilder.withBody("Apr  7 01:00:00 host Msg 01", Charsets.UTF_8);    e.getHeaders().put(SyslogUtils.SYSLOG_FACILITY, "1");    e.getHeaders().put(SyslogUtils.SYSLOG_SEVERITY, "2");    list.add(e);        e = EventBuilder.withBody("Apr 22 01:00:00 host Msg 02", Charsets.UTF_8);    e.getHeaders().put(SyslogUtils.SYSLOG_FACILITY, "1");    e.getHeaders().put(SyslogUtils.SYSLOG_SEVERITY, "3");    list.add(e);        e = EventBuilder.withBody("<8>Apr 22 01:00:00 host Msg 03", Charsets.UTF_8);    list.add(e);    return list;}
public void flume_f3328_0() throws FileNotFoundException, IOException
{        Assume.assumeTrue(!"Mac OS X".equals(System.getProperty("os.name")) || !System.getProperty("java.version").startsWith("1.7."));            OutputStream out = new FileOutputStream(testFile);    String builderName = SyslogAvroEventSerializer.Builder.class.getName();    Context ctx = new Context();    ctx.put("syncInterval", "4096");    ctx.put("compressionCodec", "snappy");    EventSerializer serializer = EventSerializerFactory.getInstance(builderName, ctx, out);        serializer.afterCreate();    List<Event> events = generateSyslogEvents();    for (Event e : events) {        serializer.write(e);    }    serializer.flush();    serializer.beforeClose();    out.flush();    out.close();        DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>();    DataFileReader<GenericRecord> fileReader = new DataFileReader<GenericRecord>(testFile, reader);    GenericRecord record = new GenericData.Record(fileReader.getSchema());    int numEvents = 0;    while (fileReader.hasNext()) {        fileReader.next(record);        int facility = (Integer) record.get("facility");        int severity = (Integer) record.get("severity");        long timestamp = (Long) record.get("timestamp");        String hostname = record.get("hostname").toString();        String message = record.get("message").toString();        Assert.assertEquals("Facility should be 1", 1, facility);        System.out.println(timestamp + ": " + message);        numEvents++;    }    fileReader.close();    Assert.assertEquals("Should have found a total of 3 events", 3, numEvents);    FileUtils.forceDelete(testFile);}
public void flume_f3329_0(long position) throws IOException
{    this.position = position;}
public long flume_f3330_0()
{    return position;}
public String flume_f3331_0()
{    return target;}
public void flume_f3332_0() throws IOException
{}
public Iterator<Sink> flume_f3333_0()
{    return getSinks().iterator();}
public void flume_f3334_0(Context context)
{    super.configure(context);    if (context.getString(SET_ME) == null) {        throw new RuntimeException("config key " + SET_ME + " not specified");    }}
public void flume_f3335_0()
{    Context context = new Context();    context.put("type", FailoverSinkProcessor.class.getName());    context.put("priority.sink1", "1");    context.put("priority.sink2", "2");    SinkFactory sf = new DefaultSinkFactory();    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(sf.create("sink1", "avro"));    sinks.add(sf.create("sink2", "avro"));    SinkProcessor sp = SinkProcessorFactory.getProcessor(context, sinks);    context.put("type", "failover");    SinkProcessor sp2 = SinkProcessorFactory.getProcessor(context, sinks);    Assert.assertEquals(sp.getClass(), sp2.getClass());}
public void flume_f3336_0()
{    Context context = new Context();    context.put("type", LoadBalancingSinkProcessor.class.getName());    context.put("selector", "random");    SinkFactory sf = new DefaultSinkFactory();    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(sf.create("sink1", "avro"));    sinks.add(sf.create("sink2", "avro"));    SinkProcessor sp = SinkProcessorFactory.getProcessor(context, sinks);    context.put("type", "load_balance");    SinkProcessor sp2 = SinkProcessorFactory.getProcessor(context, sinks);    Assert.assertEquals(sp.getClass(), sp2.getClass());}
public void flume_f3337_0()
{    setUp("none", 0);}
public void flume_f3338_0(String compressionType, int compressionLevel)
{    if (sink != null) {        throw new RuntimeException("double setup");    }    sink = new AvroSink();    channel = new MemoryChannel();    Context context = createBaseContext();    if (compressionType.equals("deflate")) {        context.put("compression-type", compressionType);        context.put("compression-level", Integer.toString(compressionLevel));    }    sink.setChannel(channel);    Configurables.configure(sink, context);    Configurables.configure(channel, context);}
private Context flume_f3339_0()
{    Context context = new Context();    context.put("hostname", hostname);    context.put("port", String.valueOf(port));    context.put("batch-size", String.valueOf(2));    context.put("connect-timeout", String.valueOf(2000L));    context.put("request-timeout", String.valueOf(3000L));    return context;}
private Server flume_f3340_0(AvroSourceProtocol protocol) throws IllegalAccessException, InstantiationException
{    Server server = new NettyServer(new SpecificResponder(AvroSourceProtocol.class, protocol), new InetSocketAddress(hostname, port));    return server;}
public void flume_f3341_0() throws InterruptedException, InstantiationException, IllegalAccessException
{    setUp();    Server server = createServer(new MockAvroServer());    server.start();    sink.start();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.START_OR_ERROR, 5000));    sink.stop();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.STOP_OR_ERROR, 5000));    server.close();}
public void flume_f3342_0() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    Event event = EventBuilder.withBody("test event 1", Charsets.UTF_8);    Server server = createServer(new MockAvroServer());    server.start();    sink.start();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.START_OR_ERROR, 5000));    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 10; i++) {        channel.put(event);    }    transaction.commit();    transaction.close();    for (int i = 0; i < 5; i++) {        Sink.Status status = sink.process();        Assert.assertEquals(Sink.Status.READY, status);    }    Assert.assertEquals(Sink.Status.BACKOFF, sink.process());    sink.stop();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.STOP_OR_ERROR, 5000));    server.close();}
public void flume_f3343_0() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    Server server = createServer(new MockAvroServer());    server.start();    sink.start();    Channel channel = Mockito.mock(Channel.class);    Mockito.when(channel.take()).thenThrow(new ChannelException("dummy"));    Transaction transaction = Mockito.mock(BasicTransactionSemantics.class);    Mockito.when(channel.getTransaction()).thenReturn(transaction);    sink.setChannel(channel);    Sink.Status status = sink.process();    sink.stop();    server.close();    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sinkCounter.getChannelReadFail());}
public void flume_f3344_1() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    Event event = EventBuilder.withBody("foo", Charsets.UTF_8);    AtomicLong delay = new AtomicLong();    Server server = createServer(new DelayMockAvroServer(delay));    server.start();    sink.start();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.START_OR_ERROR, 5000));    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 4; i++) {        channel.put(event);    }    txn.commit();    txn.close();            delay.set(3000L);    boolean threw = false;    try {        sink.process();    } catch (EventDeliveryException ex) {                threw = true;    }    Assert.assertTrue("Must throw due to connect timeout", threw);        delay.set(0);    sink.process();            delay.set(4000L);    threw = false;    try {        sink.process();    } catch (EventDeliveryException ex) {                threw = true;    }    Assert.assertTrue("Must throw due to request timeout", threw);    sink.stop();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.STOP_OR_ERROR, 5000));    server.close();    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(2, sinkCounter.getEventWriteFail());}
public void flume_f3345_0() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    Event event = EventBuilder.withBody("test event 1", Charset.forName("UTF8"));    Server server = createServer(new MockAvroServer());    server.start();    sink.start();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.START_OR_ERROR, 5000));        Thread.sleep(500L);    server.close();        Thread.sleep(500L);    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 10; i++) {        channel.put(event);    }    transaction.commit();    transaction.close();    for (int i = 0; i < 5; i++) {        boolean threwException = false;        try {            sink.process();        } catch (EventDeliveryException e) {            threwException = true;        }        Assert.assertTrue("Must throw EventDeliveryException if disconnected", threwException);    }    server = createServer(new MockAvroServer());    server.start();    for (int i = 0; i < 5; i++) {        Sink.Status status = sink.process();        Assert.assertEquals(Sink.Status.READY, status);    }    Assert.assertEquals(Sink.Status.BACKOFF, sink.process());    sink.stop();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.STOP_OR_ERROR, 5000));    server.close();    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(5, sinkCounter.getEventWriteFail());    Assert.assertEquals(4, sinkCounter.getConnectionFailedCount());}
public void flume_f3346_0() throws Exception
{    setUp();    Server server = createServer(new MockAvroServer());    server.start();    Context context = new Context();    context.put("hostname", hostname);    context.put("port", String.valueOf(port));    context.put("batch-size", String.valueOf(2));    context.put("connect-timeout", String.valueOf(2000L));    context.put("request-timeout", String.valueOf(3000L));    context.put("reset-connection-interval", String.valueOf("5"));    sink.setChannel(channel);    Configurables.configure(sink, context);    sink.start();    RpcClient firstClient = sink.getUnderlyingClient();    Thread.sleep(6000);    Transaction t = channel.getTransaction();    t.begin();    channel.put(EventBuilder.withBody("This is a test", Charset.defaultCharset()));    t.commit();    t.close();    sink.process();        Assert.assertFalse(firstClient == sink.getUnderlyingClient());    sink.stop();    context.put("hostname", hostname);    context.put("port", String.valueOf(port));    context.put("batch-size", String.valueOf(2));    context.put("connect-timeout", String.valueOf(2000L));    context.put("request-timeout", String.valueOf(3000L));    context.put("reset-connection-interval", String.valueOf("0"));    sink.setChannel(channel);    Configurables.configure(sink, context);    sink.start();    firstClient = sink.getUnderlyingClient();    Thread.sleep(6000);        Assert.assertTrue(firstClient == sink.getUnderlyingClient());    sink.stop();    context.clear();    context.put("hostname", hostname);    context.put("port", String.valueOf(port));    context.put("batch-size", String.valueOf(2));    context.put("connect-timeout", String.valueOf(2000L));    context.put("request-timeout", String.valueOf(3000L));    sink.setChannel(channel);    Configurables.configure(sink, context);    sink.start();    firstClient = sink.getUnderlyingClient();    Thread.sleep(6000);        Assert.assertTrue(firstClient == sink.getUnderlyingClient());    sink.stop();    server.close();}
public void flume_f3347_0() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("trust-all-certs", String.valueOf(true));    Configurables.configure(sink, context);    doTestSslProcess();}
public void flume_f3348_0() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("truststore", "src/test/resources/truststore.jks");    context.put("truststore-password", "password");    Configurables.configure(sink, context);    doTestSslProcess();}
public void flume_f3349_0() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("truststore", "src/test/resources/truststore.jks");    Configurables.configure(sink, context);    doTestSslProcess();}
public void flume_f3350_0() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    System.setProperty("javax.net.ssl.trustStore", "src/test/resources/truststore.jks");    System.setProperty("javax.net.ssl.trustStorePassword", "password");    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    Configurables.configure(sink, context);    doTestSslProcess();    System.clearProperty("javax.net.ssl.trustStore");    System.clearProperty("javax.net.ssl.trustStorePassword");}
public void flume_f3351_0() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    System.setProperty("javax.net.ssl.trustStore", "src/test/resources/truststore.jks");    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    Configurables.configure(sink, context);    doTestSslProcess();    System.clearProperty("javax.net.ssl.trustStore");}
private void flume_f3352_0() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    Server server = createSslServer(new MockAvroServer());    server.start();    sink.start();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.START_OR_ERROR, 5000));    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = EventBuilder.withBody("test event 1", Charsets.UTF_8);    for (int i = 0; i < 10; i++) {        channel.put(event);    }    transaction.commit();    transaction.close();    for (int i = 0; i < 5; i++) {        Sink.Status status = sink.process();        Assert.assertEquals(Sink.Status.READY, status);    }    Assert.assertEquals(Sink.Status.BACKOFF, sink.process());    sink.stop();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.STOP_OR_ERROR, 5000));    server.close();}
public void flume_f3353_1() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp("deflate", 6);    boolean bound = false;    AvroSource source;    Channel sourceChannel;    int selectedPort;    source = new AvroSource();    sourceChannel = new MemoryChannel();    Configurables.configure(sourceChannel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(sourceChannel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    Context context = new Context();    context.put("port", port.toString());    context.put("bind", hostname);    context.put("threads", "50");    context.put("compression-type", "deflate");    context.put("ssl", String.valueOf(true));    context.put("keystore", "src/test/resources/server.p12");    context.put("keystore-password", "password");    context.put("keystore-type", "PKCS12");    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());    Event event = EventBuilder.withBody("Hello avro", Charset.forName("UTF8"));    context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("trust-all-certs", String.valueOf(true));    context.put("compression-type", "deflate");    context.put("compression-level", Integer.toString(6));    Configurables.configure(sink, context);    sink.start();    Transaction sickTransaction = channel.getTransaction();    sickTransaction.begin();    for (int i = 0; i < 10; i++) {        channel.put(event);    }    sickTransaction.commit();    sickTransaction.close();    for (int i = 0; i < 5; i++) {        Sink.Status status = sink.process();                Assert.assertEquals(Sink.Status.READY, status);    }    sink.stop();    Transaction sourceTransaction = sourceChannel.getTransaction();    sourceTransaction.begin();    Event sourceEvent = sourceChannel.take();    Assert.assertNotNull(sourceEvent);    Assert.assertEquals("Channel contained our event", "Hello avro", new String(sourceEvent.getBody()));    sourceTransaction.commit();    sourceTransaction.close();        source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
public void flume_f3354_0() throws InterruptedException, InstantiationException, IllegalAccessException
{    setUp();    Server server = createServer(new MockAvroServer());    server.start();    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("trust-all-certs", String.valueOf(true));    Configurables.configure(sink, context);    boolean failed = doRequestWhenFailureExpected();    server.close();    if (!failed) {        Assert.fail("SSL-enabled sink successfully connected to a non-SSL-enabled server, " + "that's wrong.");    }    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sinkCounter.getEventWriteFail());}
public void flume_f3355_0() throws InterruptedException, InstantiationException, IllegalAccessException
{    setUp();    Server server = createSslServer(new MockAvroServer());    server.start();    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    Configurables.configure(sink, context);    boolean failed = doRequestWhenFailureExpected();    server.close();    if (!failed) {        Assert.fail("SSL-enabled sink successfully connected to a server with an " + "untrusted certificate when it should have failed");    }    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sinkCounter.getEventWriteFail());}
private boolean flume_f3356_1() throws InterruptedException
{    sink.start();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.START_OR_ERROR, 5000));    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = EventBuilder.withBody("test event 1", Charsets.UTF_8);    channel.put(event);    transaction.commit();    transaction.close();    boolean failed;    try {        sink.process();        failed = false;    } catch (EventDeliveryException ex) {                failed = true;    }    sink.stop();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.STOP_OR_ERROR, 5000));    return failed;}
public void flume_f3357_0() throws InterruptedException, IOException, EventDeliveryException
{    doRequest(false, false, 6);}
public void flume_f3358_0() throws InterruptedException, IOException, EventDeliveryException
{    doRequest(true, true, 0);}
public void flume_f3359_0() throws InterruptedException, IOException, EventDeliveryException
{    doRequest(true, true, 1);}
public void flume_f3360_0() throws InterruptedException, IOException, EventDeliveryException
{    doRequest(true, true, 6);}
public void flume_f3361_0() throws InterruptedException, IOException, EventDeliveryException
{    doRequest(true, true, 9);}
private void flume_f3362_1(boolean serverEnableCompression, boolean clientEnableCompression, int compressionLevel) throws InterruptedException, IOException, EventDeliveryException
{    if (clientEnableCompression) {        setUp("deflate", compressionLevel);    } else {        setUp("none", compressionLevel);    }    boolean bound = false;    AvroSource source;    Channel sourceChannel;    int selectedPort;    source = new AvroSource();    sourceChannel = new MemoryChannel();    Configurables.configure(sourceChannel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(sourceChannel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    Context context = new Context();    context.put("port", port.toString());    context.put("bind", hostname);    context.put("threads", "50");    if (serverEnableCompression) {        context.put("compression-type", "deflate");    } else {        context.put("compression-type", "none");    }    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());    Event event = EventBuilder.withBody("Hello avro", Charset.forName("UTF8"));    sink.start();    Transaction sickTransaction = channel.getTransaction();    sickTransaction.begin();    for (int i = 0; i < 10; i++) {        channel.put(event);    }    sickTransaction.commit();    sickTransaction.close();    for (int i = 0; i < 5; i++) {        Sink.Status status = sink.process();                Assert.assertEquals(Sink.Status.READY, status);    }    sink.stop();    Transaction sourceTransaction = sourceChannel.getTransaction();    sourceTransaction.begin();    Event sourceEvent = sourceChannel.take();    Assert.assertNotNull(sourceEvent);    Assert.assertEquals("Channel contained our event", "Hello avro", new String(sourceEvent.getBody()));    sourceTransaction.commit();    sourceTransaction.close();        source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
public Status flume_f3363_1(AvroFlumeEvent event) throws AvroRemoteException
{        return Status.OK;}
public Status flume_f3364_1(List<AvroFlumeEvent> events) throws AvroRemoteException
{        return Status.OK;}
private void flume_f3365_0() throws AvroRemoteException
{    try {        Thread.sleep(delay.get());    } catch (InterruptedException e) {        throw new AvroRemoteException("Interrupted while sleeping", e);    }}
private Server flume_f3368_0(AvroSourceProtocol protocol) throws IllegalAccessException, InstantiationException
{    Server server = new NettyServer(new SpecificResponder(AvroSourceProtocol.class, protocol), new InetSocketAddress(hostname, port), new NioServerSocketChannelFactory(Executors.newCachedThreadPool(), Executors.newCachedThreadPool()), new SSLChannelPipelineFactory(), null);    return server;}
private SSLContext flume_f3369_0()
{    try {        KeyStore ks = KeyStore.getInstance(keystoreType);        ks.load(new FileInputStream(keystore), keystorePassword.toCharArray());                KeyManagerFactory kmf = KeyManagerFactory.getInstance(getAlgorithm());        kmf.init(ks, keystorePassword.toCharArray());        SSLContext serverContext = SSLContext.getInstance("TLS");        serverContext.init(kmf.getKeyManagers(), null, null);        return serverContext;    } catch (Exception e) {        throw new Error("Failed to initialize the server-side SSLContext", e);    }}
private String flume_f3370_0()
{    String algorithm = Security.getProperty("ssl.KeyManagerFactory.algorithm");    if (algorithm == null) {        algorithm = "SunX509";    }    return algorithm;}
public ChannelPipeline flume_f3371_0() throws Exception
{    ChannelPipeline pipeline = Channels.pipeline();    SSLEngine sslEngine = createServerSSLContext().createSSLEngine();    sslEngine.setUseClientMode(false);    pipeline.addLast("ssl", new SslHandler(sslEngine));    return pipeline;}
public void flume_f3372_0()
{    sinkFactory = new DefaultSinkFactory();}
public void flume_f3373_0()
{    Sink avroSink1 = sinkFactory.create("avroSink1", "avro");    Sink avroSink2 = sinkFactory.create("avroSink2", "avro");    Assert.assertNotNull(avroSink1);    Assert.assertNotNull(avroSink2);    Assert.assertNotSame(avroSink1, avroSink2);    Assert.assertTrue(avroSink1 instanceof AvroSink);    Assert.assertTrue(avroSink2 instanceof AvroSink);    Sink s1 = sinkFactory.create("avroSink1", "avro");    Sink s2 = sinkFactory.create("avroSink2", "avro");    Assert.assertNotSame(avroSink1, s1);    Assert.assertNotSame(avroSink2, s2);}
private void flume_f3374_0(String name, String type, Class<?> typeClass) throws Exception
{    Sink sink = sinkFactory.create(name, type);    Assert.assertNotNull(sink);    Assert.assertTrue(typeClass.isInstance(sink));}
public void flume_f3375_0() throws Exception
{    verifySinkCreation("null-sink", "null", NullSink.class);    verifySinkCreation("logger-sink", "logger", LoggerSink.class);    verifySinkCreation("file-roll-sink", "file_roll", RollingFileSink.class);    verifySinkCreation("avro-sink", "avro", AvroSink.class);}
public void flume_f3376_0()
{    state = LifecycleState.START;}
public void flume_f3377_0()
{    state = LifecycleState.STOP;}
public LifecycleState flume_f3378_0()
{    return state;}
public void flume_f3379_0(String name)
{    this.name = name;}
public String flume_f3380_0()
{    return name;}
public void flume_f3381_0(Channel channel)
{    this.channel = channel;}
public Channel flume_f3382_0()
{    return channel;}
public synchronized void flume_f3383_0(int remaining)
{    this.remaining = remaining;}
public Status flume_f3384_0() throws EventDeliveryException
{    synchronized (this) {        if (remaining <= 0) {            throw new EventDeliveryException("can't consume more");        }    }    Transaction tx = channel.getTransaction();    tx.begin();    Event e = channel.take();    tx.commit();    tx.close();    if (e != null) {        synchronized (this) {            remaining--;        }        written++;    }    return Status.READY;}
public Integer flume_f3385_0()
{    return written;}
public void flume_f3386_0() throws InterruptedException
{    Channel ch = new MemoryChannel();    ConsumeXSink s1 = new ConsumeXSink(10);    s1.setChannel(ch);    s1.setName("s1");    ConsumeXSink s2 = new ConsumeXSink(50);    s2.setChannel(ch);    s2.setName("s2");    ConsumeXSink s3 = new ConsumeXSink(100);    s3.setChannel(ch);    s3.setName("s3");    Context context = new Context();    Configurables.configure(s1, context);    Configurables.configure(s2, context);    Configurables.configure(s3, context);    Configurables.configure(ch, context);    ch.start();    List<Sink> sinks = new LinkedList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    SinkGroup group = new SinkGroup(sinks);    Map<String, String> params = new HashMap<String, String>();    params.put("sinks", "s1 s2 s3");    params.put("processor.type", "failover");    params.put("processor.priority.s1", "3");    params.put("processor.priority.s2", "2");    params.put("processor.priority.s3", "1");    params.put("processor.maxpenalty", "10000");    context.putAll(params);    Configurables.configure(group, context);    SinkRunner runner = new SinkRunner(group.getProcessor());    runner.start();    Assert.assertEquals(LifecycleState.START, s1.getLifecycleState());    Assert.assertEquals(LifecycleState.START, s2.getLifecycleState());    Assert.assertEquals(LifecycleState.START, s3.getLifecycleState());    for (int i = 0; i < 15; i++) {        Transaction tx = ch.getTransaction();        tx.begin();        ch.put(EventBuilder.withBody("test".getBytes()));        tx.commit();        tx.close();    }    Thread.sleep(100);    Assert.assertEquals(new Integer(10), s1.getWritten());    Assert.assertEquals(new Integer(5), s2.getWritten());    for (int i = 0; i < 50; i++) {        Transaction tx = ch.getTransaction();        tx.begin();        ch.put(EventBuilder.withBody("test".getBytes()));        tx.commit();        tx.close();    }    Thread.sleep(100);    Assert.assertEquals(new Integer(50), s2.getWritten());    Assert.assertEquals(new Integer(5), s3.getWritten());        s2.setRemaining(20);        Thread.sleep(5000);    for (int i = 0; i < 100; i++) {        Transaction tx = ch.getTransaction();        tx.begin();        ch.put(EventBuilder.withBody("test".getBytes()));        tx.commit();        tx.close();    }    Thread.sleep(1000);    Assert.assertEquals(new Integer(10), s1.getWritten());    Assert.assertEquals(new Integer(70), s2.getWritten());    Assert.assertEquals(new Integer(85), s3.getWritten());    runner.stop();    ch.stop();}
private Context flume_f3387_0(String selectorType, boolean backoff)
{    Map<String, String> p = new HashMap<String, String>();    p.put("selector", selectorType);    p.put("backoff", String.valueOf(backoff));    Context ctx = new Context(p);    return ctx;}
private Context flume_f3388_0(String selectorType)
{    Map<String, String> p = new HashMap<String, String>();    p.put("selector", selectorType);    Context ctx = new Context(p);    return ctx;}
private LoadBalancingSinkProcessor flume_f3389_0(String selectorType, List<Sink> sinks, boolean backoff)
{    return getProcessor(sinks, getContext(selectorType, backoff));}
private LoadBalancingSinkProcessor flume_f3390_0(List<Sink> sinks, Context ctx)
{    LoadBalancingSinkProcessor lbsp = new LoadBalancingSinkProcessor();    lbsp.setSinks(sinks);    lbsp.configure(ctx);    lbsp.start();    return lbsp;}
public void flume_f3391_0() throws Exception
{        Channel ch = new MockChannel();    int n = 100;    int numEvents = 3 * n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor(sinks, new Context());    Status s = Status.READY;    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertTrue(s1.getEvents().size() == n);    Assert.assertTrue(s2.getEvents().size() == n);    Assert.assertTrue(s3.getEvents().size() == n);}
public void flume_f3392_0() throws Exception
{    Channel ch = new MockChannel();    int n = 10;    int numEvents = n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);        s1.setFail(true);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);        s3.setFail(true);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("random", sinks, false);    Sink.Status s = Sink.Status.READY;    while (s != Sink.Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertTrue(s1.getEvents().size() == 0);    Assert.assertTrue(s2.getEvents().size() == n);    Assert.assertTrue(s3.getEvents().size() == 0);}
public void flume_f3393_0() throws Exception
{    Channel ch = new MockChannel();    int n = 100;    int numEvents = n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);        s1.setFail(true);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);        s3.setFail(true);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("random", sinks, true);        for (int i = 0; i < 50; i++) {                lbsp.process();    }    Assert.assertEquals(50, s2.getEvents().size());    s2.setFail(true);        s1.setFail(false);    try {        lbsp.process();                Assert.fail("Expected EventDeliveryException");    } catch (EventDeliveryException e) {        }        Thread.sleep(2100);    Sink.Status s = Sink.Status.READY;    while (s != Sink.Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertEquals(50, s1.getEvents().size());    Assert.assertEquals(50, s2.getEvents().size());    Assert.assertEquals(0, s3.getEvents().size());}
public void flume_f3394_0() throws Exception
{    Channel ch = new MockChannel();    int n = 100;    int numEvents = 3 * n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);        s2.setFail(true);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("random", sinks, false);    Status s = Status.READY;    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertTrue(s2.getEvents().size() == 0);    Assert.assertTrue(s1.getEvents().size() + s3.getEvents().size() == 3 * n);}
public void flume_f3395_0() throws Exception
{    Channel ch = new MockChannel();    int n = 10000;    int numEvents = n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    MockSink s4 = new MockSink(4);    s4.setChannel(ch);    MockSink s5 = new MockSink(5);    s5.setChannel(ch);    MockSink s6 = new MockSink(6);    s6.setChannel(ch);    MockSink s7 = new MockSink(7);    s7.setChannel(ch);    MockSink s8 = new MockSink(8);    s8.setChannel(ch);    MockSink s9 = new MockSink(9);    s9.setChannel(ch);    MockSink s0 = new MockSink(0);    s0.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    sinks.add(s4);    sinks.add(s5);    sinks.add(s6);    sinks.add(s7);    sinks.add(s8);    sinks.add(s9);    sinks.add(s0);    LoadBalancingSinkProcessor lbsp = getProcessor("random", sinks, false);    Status s = Status.READY;    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Set<Integer> sizeSet = new HashSet<Integer>();    int sum = 0;    for (Sink ms : sinks) {        int count = ((MockSink) ms).getEvents().size();        sum += count;        sizeSet.add(count);    }        Assert.assertEquals(n, sum);                        Assert.assertTrue("Miraculous distribution", sizeSet.size() > 1);}
public void flume_f3396_0() throws Exception
{    Channel ch = new MockChannel();    int n = 10;    int numEvents = n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);        s1.setFail(true);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);        s3.setFail(true);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("round_robin", sinks, false);    Sink.Status s = Sink.Status.READY;    while (s != Sink.Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertTrue(s1.getEvents().size() == 0);    Assert.assertTrue(s2.getEvents().size() == n);    Assert.assertTrue(s3.getEvents().size() == 0);}
public void flume_f3397_0() throws Exception
{    Channel ch = new MockChannel();    int n = 100;    int numEvents = 3 * n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);        s2.setFail(true);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("round_robin", sinks, false);    Status s = Status.READY;    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertTrue(s1.getEvents().size() == n);    Assert.assertTrue(s2.getEvents().size() == 0);    Assert.assertTrue(s3.getEvents().size() == 2 * n);}
public void flume_f3398_0() throws EventDeliveryException
{    Channel ch = new MockChannel();    int n = 100;    int numEvents = 3 * n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("round_robin", sinks, true);    Status s = Status.READY;    for (int i = 0; i < 3 && s != Status.BACKOFF; i++) {        s = lbsp.process();    }    s2.setFail(true);    for (int i = 0; i < 3 && s != Status.BACKOFF; i++) {        s = lbsp.process();    }    s2.setFail(false);    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertEquals((3 * n) / 2, s1.getEvents().size());    Assert.assertEquals(1, s2.getEvents().size());    Assert.assertEquals((3 * n) / 2 - 1, s3.getEvents().size());}
public void flume_f3399_0() throws EventDeliveryException, InterruptedException
{    Channel ch = new MockChannel();    int n = 100;    int numEvents = 3 * n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    s2.setFail(true);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("round_robin", sinks, true);    Status s = Status.READY;    for (int i = 0; i < 3 && s != Status.BACKOFF; i++) {        s = lbsp.process();    }    Assert.assertEquals(0, s2.getEvents().size());    Thread.sleep(2100);        for (int i = 0; i < 3 && s != Status.BACKOFF; i++) {        s = lbsp.process();    }    Assert.assertEquals(0, s2.getEvents().size());    s2.setFail(false);    Thread.sleep(2100);        for (int i = 0; i < 3 && s != Status.BACKOFF; i++) {        s = lbsp.process();    }    Assert.assertEquals(0, s2.getEvents().size());        Thread.sleep(2100);    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertEquals(n + 2, s1.getEvents().size());    Assert.assertEquals(n - 3, s2.getEvents().size());    Assert.assertEquals(n + 1, s3.getEvents().size());}
public void flume_f3400_0() throws EventDeliveryException, InterruptedException
{    Channel ch = new MockChannel();    int n = 100;    int numEvents = 3 * n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    s2.setFail(true);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("round_robin", sinks, true);    Status s = Status.READY;    for (int i = 0; i < 3 && s != Status.BACKOFF; i++) {        s = lbsp.process();    }    s2.setFail(false);    Thread.sleep(2001);    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertEquals(n + 1, s1.getEvents().size());    Assert.assertEquals(n - 1, s2.getEvents().size());    Assert.assertEquals(n, s3.getEvents().size());}
public void flume_f3401_0() throws Exception
{    Channel ch = new MockChannel();    int n = 100;    int numEvents = 3 * n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("round_robin", sinks, false);    Status s = Status.READY;    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertTrue(s1.getEvents().size() == n);    Assert.assertTrue(s2.getEvents().size() == n);    Assert.assertTrue(s3.getEvents().size() == n);}
public void flume_f3402_0() throws Exception
{    Channel ch = new MockChannel();    int n = 10;    int numEvents = n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);        s1.setFail(true);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);        Context ctx = getContext(FixedOrderSelector.class.getCanonicalName());    ctx.put("selector." + FixedOrderSelector.SET_ME, "foo");    LoadBalancingSinkProcessor lbsp = getProcessor(sinks, ctx);    Sink.Status s = Sink.Status.READY;    while (s != Sink.Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertTrue(s1.getEvents().size() == 0);    Assert.assertTrue(s2.getEvents().size() == n);    Assert.assertTrue(s3.getEvents().size() == 0);}
 List<Event> flume_f3403_0()
{    return events;}
 int flume_f3404_0()
{    return id;}
 void flume_f3405_0(boolean bFail)
{    fail = bFail;}
public Status flume_f3406_0() throws EventDeliveryException
{    if (fail) {        throw new EventDeliveryException("failed");    }    Event e = this.getChannel().take();    if (e == null) {        return Status.BACKOFF;    }    events.add(e);    return Status.READY;}
public void flume_f3407_0(Event event) throws ChannelException
{    events.add(event);}
public Event flume_f3408_0() throws ChannelException
{    if (events.size() > 0) {        return events.remove(0);    }    return null;}
public Transaction flume_f3409_0()
{    return null;}
public Map<String, String> flume_f3410_0()
{    return EMPTY_HEADERS;}
public void flume_f3411_0(Map<String, String> headers)
{    throw new UnsupportedOperationException();}
public byte[] flume_f3412_0()
{    return body;}
public void flume_f3413_0(byte[] body)
{    this.body = body;}
public void flume_f3414_0()
{    sink = new LoggerSink();}
public void flume_f3415_0() throws InterruptedException, LifecycleException, EventDeliveryException
{    Channel channel = new PseudoTxnMemoryChannel();    Context context = new Context();    Configurables.configure(channel, context);    Configurables.configure(sink, context);    sink.setChannel(channel);    sink.start();    for (int i = 0; i < 10; i++) {        Event event = EventBuilder.withBody(("Test " + i).getBytes());        channel.put(event);        sink.process();    }    sink.stop();}
public void flume_f3416_0() throws InterruptedException, LifecycleException, EventDeliveryException
{    Channel channel = new PseudoTxnMemoryChannel();    Context context = new Context();    context.put(LoggerSink.MAX_BYTES_DUMP_KEY, String.valueOf(30));    Configurables.configure(channel, context);    Configurables.configure(sink, context);    sink.setChannel(channel);    sink.start();    for (int i = 0; i < 10; i++) {        Event event = EventBuilder.withBody((Strings.padStart("Test " + i, 30, 'P')).getBytes());        channel.put(event);        sink.process();    }    sink.stop();}
public void flume_f3417_0()
{    tmpDir = new File("/tmp/flume-rfs-" + System.currentTimeMillis() + "-" + Thread.currentThread().getId());    sink = new RollingFileSink();    sink.setChannel(new MemoryChannel());    tmpDir.mkdirs();}
public void flume_f3418_0()
{    tmpDir.delete();}
public void flume_f3419_0()
{    Context context = new Context();    context.put("sink.directory", tmpDir.getPath());    Configurables.configure(sink, context);    sink.start();    sink.stop();}
public void flume_f3420_0() throws InterruptedException, EventDeliveryException, IOException
{    Context context = new Context();    context.put("sink.directory", tmpDir.getPath());    context.put("sink.rollInterval", "1");    context.put("sink.batchSize", "1");    doTest(context);}
public void flume_f3421_0() throws InterruptedException, EventDeliveryException, IOException
{    Context context = new Context();    context.put("sink.directory", tmpDir.getPath());    context.put("sink.rollInterval", "0");    context.put("sink.batchSize", "1");    doTest(context);}
public void flume_f3422_0() throws InterruptedException, EventDeliveryException, IOException
{    File tmpDir = new File("target/tmpLog");    tmpDir.mkdirs();    cleanDirectory(tmpDir);    Context context = new Context();    context.put("sink.directory", "target/tmpLog");    context.put("sink.rollInterval", "0");    context.put("sink.batchSize", "1");    context.put("sink.pathManager.prefix", "test3-");    context.put("sink.pathManager.extension", "txt");    doTest(context);}
public void flume_f3423_0() throws InterruptedException, EventDeliveryException, IOException
{    File tmpDir = new File("target/tempLog");    tmpDir.mkdirs();    cleanDirectory(tmpDir);    Context context = new Context();    context.put("sink.directory", "target/tempLog/");    context.put("sink.rollInterval", "1");    context.put("sink.batchSize", "1");    context.put("sink.pathManager", "rolltime");    context.put("sink.pathManager.prefix", "test4-");    context.put("sink.pathManager.extension", "txt");    doTest(context);}
public void flume_f3424_0() throws InterruptedException, IOException
{    Context context = new Context();    context.put("sink.directory", tmpDir.getPath());    context.put("sink.rollInterval", "0");    context.put("sink.batchSize", "1");    Channel channel = Mockito.mock(Channel.class);    Mockito.when(channel.take()).thenThrow(new ChannelException("dummy"));    Transaction transaction = Mockito.mock(BasicTransactionSemantics.class);    Mockito.when(channel.getTransaction()).thenReturn(transaction);    try {        doTest(context, channel);    } catch (EventDeliveryException e) {        }    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sinkCounter.getChannelReadFail());}
private void flume_f3425_0(Context context) throws EventDeliveryException, InterruptedException, IOException
{    doTest(context, null);}
private void flume_f3426_1(Context context, Channel channel) throws EventDeliveryException, InterruptedException, IOException
{    Configurables.configure(sink, context);    if (channel == null) {        channel = new PseudoTxnMemoryChannel();        Configurables.configure(channel, context);    }    sink.setChannel(channel);    sink.start();    for (int i = 0; i < 10; i++) {        Event event = new SimpleEvent();        event.setBody(("Test event " + i).getBytes());        channel.put(event);        sink.process();        Thread.sleep(500);    }    sink.stop();    for (String file : sink.getDirectory().list()) {        BufferedReader reader = new BufferedReader(new FileReader(new File(sink.getDirectory(), file)));        String lastLine = null;        String currentLine = null;        while ((currentLine = reader.readLine()) != null) {            lastLine = currentLine;                    }        reader.close();    }}
private void flume_f3427_0(File dir)
{    File[] files = dir.listFiles();    for (File file : files) {        file.delete();    }}
public void flume_f3428_0() throws EventDeliveryException
{    Context context = new Context();    context.put("sink.directory", tmpDir.getPath());    context.put("sink.rollInterval", "0");    context.put("sink.batchSize", "1000");    Configurables.configure(sink, context);    context.put("capacity", "50");    context.put("transactionCapacity", "5");    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    try {        for (int j = 0; j < 10; j++) {            Transaction tx = channel.getTransaction();            tx.begin();            for (int i = 0; i < 5; i++) {                Event event = new SimpleEvent();                event.setBody(("Test event " + i).getBytes());                channel.put(event);            }            tx.commit();            tx.close();        }        sink.process();    } finally {        sink.stop();    }}
public void flume_f3429_0() throws Exception
{    sink = new ThriftSink();    channel = new MemoryChannel();    hostname = "0.0.0.0";    try (ServerSocket socket = new ServerSocket(0)) {        port = socket.getLocalPort();    }    Context context = createBaseContext();    context.put(ThriftRpcClient.CONFIG_PROTOCOL, ThriftRpcClient.COMPACT_PROTOCOL);    sink.setChannel(channel);    Configurables.configure(sink, context);    Configurables.configure(channel, context);}
private Context flume_f3430_0()
{    Context context = new Context();    context.put("hostname", hostname);    context.put("port", String.valueOf(port));    context.put("batch-size", String.valueOf(2));    context.put("connect-timeout", String.valueOf(2000L));    context.put("request-timeout", String.valueOf(2000L));    return context;}
public void flume_f3431_0() throws Exception
{    channel.stop();    sink.stop();    src.stop();}
public void flume_f3432_0() throws Exception
{    Event event = EventBuilder.withBody("test event 1", Charsets.UTF_8);    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.OK.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    channel.start();    sink.start();    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 11; i++) {        channel.put(event);    }    transaction.commit();    transaction.close();    for (int i = 0; i < 6; i++) {        Sink.Status status = sink.process();        Assert.assertEquals(Sink.Status.READY, status);    }    Assert.assertEquals(Sink.Status.BACKOFF, sink.process());    sink.stop();    Assert.assertEquals(11, src.flumeEvents.size());    Assert.assertEquals(6, src.batchCount);    Assert.assertEquals(0, src.individualCount);}
public void flume_f3433_0() throws Exception
{    AtomicLong delay = new AtomicLong();    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.ALTERNATE.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    src.setDelay(delay);    delay.set(2500);    Event event = EventBuilder.withBody("foo", Charsets.UTF_8);    sink.start();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 4; i++) {        channel.put(event);    }    txn.commit();    txn.close();        boolean threw = false;    try {        sink.process();    } catch (EventDeliveryException ex) {        threw = true;    }    Assert.assertTrue("Must throw due to connect timeout", threw);        delay.set(0);    sink.process();            delay.set(2500L);    threw = false;    try {        sink.process();    } catch (EventDeliveryException ex) {        threw = true;    }    Assert.assertTrue("Must throw due to request timeout", threw);    sink.stop();}
public void flume_f3434_0() throws Exception
{    Event event = EventBuilder.withBody("test event 1", Charset.forName("UTF8"));    sink.start();        Thread.sleep(500L);        Thread.sleep(500L);    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 10; i++) {        channel.put(event);    }    transaction.commit();    transaction.close();    for (int i = 0; i < 5; i++) {        boolean threwException = false;        try {            sink.process();        } catch (EventDeliveryException e) {            threwException = true;        }        Assert.assertTrue("Must throw EventDeliveryException if disconnected", threwException);    }    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.OK.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    for (int i = 0; i < 5; i++) {        Sink.Status status = sink.process();        Assert.assertEquals(Sink.Status.READY, status);    }    Assert.assertEquals(Sink.Status.BACKOFF, sink.process());    sink.stop();}
public void flume_f3435_0() throws Exception
{    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("truststore", "src/test/resources/truststorefile.jks");    context.put("truststore-password", "password");    Configurables.configure(sink, context);    doTestSslProcess();}
public void flume_f3436_0() throws Exception
{    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("truststore", "src/test/resources/truststorefile.jks");    Configurables.configure(sink, context);    doTestSslProcess();}
public void flume_f3437_0() throws Exception
{    System.setProperty("javax.net.ssl.trustStore", "src/test/resources/truststorefile.jks");    System.setProperty("javax.net.ssl.trustStorePassword", "password");    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    Configurables.configure(sink, context);    doTestSslProcess();    System.clearProperty("javax.net.ssl.trustStore");    System.clearProperty("javax.net.ssl.trustStorePassword");}
public void flume_f3438_0() throws Exception
{    System.setProperty("javax.net.ssl.trustStore", "src/test/resources/truststorefile.jks");    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    Configurables.configure(sink, context);    doTestSslProcess();    System.clearProperty("javax.net.ssl.trustStore");}
private void flume_f3439_0() throws Exception
{    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.OK.name(), port, ThriftRpcClient.COMPACT_PROTOCOL, "src/test/resources/keystorefile.jks", "password", KeyManagerFactory.getDefaultAlgorithm(), "JKS");    channel.start();    sink.start();    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = EventBuilder.withBody("test event 1", Charsets.UTF_8);    for (int i = 0; i < 11; i++) {        channel.put(event);    }    transaction.commit();    transaction.close();    for (int i = 0; i < 6; i++) {        Sink.Status status = sink.process();        Assert.assertEquals(Sink.Status.READY, status);    }    Assert.assertEquals(Sink.Status.BACKOFF, sink.process());    sink.stop();    Assert.assertEquals(11, src.flumeEvents.size());    Assert.assertEquals(6, src.batchCount);    Assert.assertEquals(0, src.individualCount);}
public void flume_f3440_0() throws Exception
{    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.OK.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("truststore", "src/test/resources/truststorefile.jks");    context.put("truststore-password", "password");    Configurables.configure(sink, context);    boolean failed = doRequestWhenFailureExpected();    if (!failed) {        Assert.fail("SSL-enabled sink successfully connected to a non-SSL-enabled server, " + "that's wrong.");    }}
public void flume_f3441_0() throws Exception
{    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.OK.name(), port, ThriftRpcClient.COMPACT_PROTOCOL, "src/test/resources/keystorefile.jks", "password", KeyManagerFactory.getDefaultAlgorithm(), "JKS");    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    Configurables.configure(sink, context);    boolean failed = doRequestWhenFailureExpected();    if (!failed) {        Assert.fail("SSL-enabled sink successfully connected to a server with an " + "untrusted certificate when it should have failed");    }}
private boolean flume_f3442_0() throws Exception
{    channel.start();    sink.start();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.START_OR_ERROR, 5000));    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = EventBuilder.withBody("test event 1", Charsets.UTF_8);    channel.put(event);    transaction.commit();    transaction.close();    boolean failed;    try {        Sink.Status status = sink.process();        failed = false;    } catch (EventDeliveryException ex) {                failed = true;    }    sink.stop();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.STOP_OR_ERROR, 5000));    return failed;}
public String flume_f3443_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public Cookie[] flume_f3444_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public long flume_f3445_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3446_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public Enumeration<String> flume_f3447_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public Enumeration<String> flume_f3448_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public int flume_f3449_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3450_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3451_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3452_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3453_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3454_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3455_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f3456_0(String role)
{    throw new UnsupportedOperationException("Not supported yet.");}
public Principal flume_f3457_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3458_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3459_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public StringBuffer flume_f3460_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3461_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public HttpSession flume_f3462_0(boolean create)
{    throw new UnsupportedOperationException("Not supported yet.");}
public HttpSession flume_f3463_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f3464_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f3465_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f3466_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f3467_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public Object flume_f3468_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public Enumeration<String> flume_f3469_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3470_0()
{    return charset;}
public void flume_f3471_0(String env) throws UnsupportedEncodingException
{    this.charset = env;}
public int flume_f3472_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3473_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public ServletInputStream flume_f3474_0() throws IOException
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3475_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public Enumeration<String> flume_f3476_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String[] flume_f3477_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public Map<String, String[]> flume_f3478_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3479_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3480_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3481_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public int flume_f3482_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public BufferedReader flume_f3483_0() throws IOException
{    return reader;}
public String flume_f3484_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3485_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public void flume_f3486_0(String name, Object o)
{    throw new UnsupportedOperationException("Not supported yet.");}
public void flume_f3487_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public Locale flume_f3488_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public Enumeration<Locale> flume_f3489_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f3490_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public RequestDispatcher flume_f3491_0(String path)
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3492_0(String path)
{    throw new UnsupportedOperationException("Not supported yet.");}
public int flume_f3493_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3494_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3495_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public int flume_f3496_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public AsyncContext flume_f3497_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public long flume_f3498_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public DispatcherType flume_f3499_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public ServletContext flume_f3500_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f3501_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f3502_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public AsyncContext flume_f3503_0() throws IllegalStateException
{    throw new UnsupportedOperationException("Not supported yet.");}
public AsyncContext flume_f3504_0(ServletRequest arg0, ServletResponse arg1) throws IllegalStateException
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f3505_0(HttpServletResponse arg0) throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f3506_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public Part flume_f3507_0(String arg0) throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
public Collection<Part> flume_f3508_0() throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
public void flume_f3509_0(String arg0, String arg1) throws ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
public void flume_f3510_0() throws ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
public T flume_f3511_0(Class<T> arg0) throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
public void flume_f3512_0()
{    handler = new BLOBHandler();}
public void flume_f3513_0() throws Exception
{    Map requestParameterMap = new HashMap();    requestParameterMap.put("param1", new String[] { "value1" });    requestParameterMap.put("param2", new String[] { "value2" });    HttpServletRequest req = mock(HttpServletRequest.class);    final String csvData = "a,b,c";    ServletInputStream servletInputStream = new DelegatingServletInputStream(new ByteArrayInputStream(csvData.getBytes()));    when(req.getInputStream()).thenReturn(servletInputStream);    when(req.getParameterMap()).thenReturn(requestParameterMap);    Context context = mock(Context.class);    when(context.getString(BLOBHandler.MANDATORY_PARAMETERS, BLOBHandler.DEFAULT_MANDATORY_PARAMETERS)).thenReturn("param1,param2");    handler.configure(context);    List<Event> deserialized = handler.getEvents(req);    assertEquals(1, deserialized.size());    Event e = deserialized.get(0);    assertEquals(new String(e.getBody()), csvData);    assertEquals(e.getHeaders().get("param1"), "value1");    assertEquals(e.getHeaders().get("param2"), "value2");}
public void flume_f3514_0() throws Exception
{    Map requestParameterMap = new HashMap();    requestParameterMap.put("param1", new String[] { "value1" });    HttpServletRequest req = mock(HttpServletRequest.class);    final String tabData = "a\tb\tc";    ServletInputStream servletInputStream = new DelegatingServletInputStream(new ByteArrayInputStream(tabData.getBytes()));    when(req.getInputStream()).thenReturn(servletInputStream);    when(req.getParameterMap()).thenReturn(requestParameterMap);    Context context = mock(Context.class);    when(context.getString(BLOBHandler.MANDATORY_PARAMETERS, BLOBHandler.DEFAULT_MANDATORY_PARAMETERS)).thenReturn("param1");    handler.configure(context);    List<Event> deserialized = handler.getEvents(req);    assertEquals(1, deserialized.size());    Event e = deserialized.get(0);    assertEquals(new String(e.getBody()), tabData);    assertEquals(e.getHeaders().get("param1"), "value1");}
public void flume_f3515_0() throws Exception
{    Map requestParameterMap = new HashMap();    HttpServletRequest req = mock(HttpServletRequest.class);    final String tabData = "a\tb\tc";    ServletInputStream servletInputStream = new DelegatingServletInputStream(new ByteArrayInputStream(tabData.getBytes()));    when(req.getInputStream()).thenReturn(servletInputStream);    when(req.getParameterMap()).thenReturn(requestParameterMap);    Context context = mock(Context.class);    when(context.getString(BLOBHandler.MANDATORY_PARAMETERS, BLOBHandler.DEFAULT_MANDATORY_PARAMETERS)).thenReturn("param1");    handler.configure(context);    handler.getEvents(req);}
public final InputStream flume_f3516_0()
{    return this.sourceStream;}
public int flume_f3517_0() throws IOException
{    return this.sourceStream.read();}
public void flume_f3518_0() throws IOException
{    super.close();    this.sourceStream.close();}
public boolean flume_f3519_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f3520_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public void flume_f3521_0(ReadListener arg0)
{    throw new UnsupportedOperationException("Not supported yet.");}
private static int flume_f3522_0() throws IOException
{    ServerSocket socket = new ServerSocket(0);    int port = socket.getLocalPort();    socket.close();    return port;}
private static Context flume_f3523_0(int port) throws IOException
{    Context ctx = new Context();    ctx.put(HTTPSourceConfigurationConstants.CONFIG_BIND, "0.0.0.0");    ctx.put(HTTPSourceConfigurationConstants.CONFIG_PORT, String.valueOf(port));    ctx.put("QueuedThreadPool.MaxThreads", "100");    return ctx;}
private static Context flume_f3524_0(int port) throws IOException
{    Context sslContext = new Context();    sslContext.put(HTTPSourceConfigurationConstants.CONFIG_PORT, String.valueOf(port));    sslContext.put(HTTPSourceConfigurationConstants.SSL_ENABLED, "true");    sslContext.put(HTTPSourceConfigurationConstants.SSL_KEYSTORE_PASSWORD, "password");    sslContext.put(HTTPSourceConfigurationConstants.SSL_KEYSTORE, "src/test/resources/jettykeystore");    return sslContext;}
private static Context flume_f3525_0(int port) throws IOException
{    System.setProperty("javax.net.ssl.keyStore", "src/test/resources/jettykeystore");    System.setProperty("javax.net.ssl.keyStorePassword", "password");    Context sslContext = new Context();    sslContext.put(HTTPSourceConfigurationConstants.CONFIG_PORT, String.valueOf(port));    sslContext.put(HTTPSourceConfigurationConstants.SSL_ENABLED, "true");    return sslContext;}
public static void flume_f3526_0() throws Exception
{    httpSource = new HTTPSource();    httpChannel = new MemoryChannel();    httpPort = findFreePort();    configureSourceAndChannel(httpSource, httpChannel, getDefaultNonSecureContext(httpPort));    httpChannel.start();    httpSource.start();    httpsSource = new HTTPSource();    httpsChannel = new MemoryChannel();    httpsPort = findFreePort();    configureSourceAndChannel(httpsSource, httpsChannel, getDefaultSecureContext(httpsPort));    httpsChannel.start();    httpsSource.start();    httpsGlobalKeystoreSource = new HTTPSource();    httpsGlobalKeystoreChannel = new MemoryChannel();    httpsGlobalKeystorePort = findFreePort();    configureSourceAndChannel(httpsGlobalKeystoreSource, httpsGlobalKeystoreChannel, getDefaultSecureContextGlobalKeystore(httpsGlobalKeystorePort));    httpsGlobalKeystoreChannel.start();    httpsGlobalKeystoreSource.start();    System.clearProperty("javax.net.ssl.keyStore");    System.clearProperty("javax.net.ssl.keyStorePassword");}
private static void flume_f3527_0(HTTPSource source, Channel channel, Context context)
{    Context channelContext = new Context();    channelContext.put("capacity", "100");    Configurables.configure(channel, channelContext);    Configurables.configure(source, context);    ChannelSelector rcs1 = new ReplicatingChannelSelector();    rcs1.setChannels(Collections.singletonList(channel));    source.setChannelProcessor(new ChannelProcessor(rcs1));}
public static void flume_f3528_0() throws Exception
{    httpSource.stop();    httpChannel.stop();    httpsSource.stop();    httpsChannel.stop();    httpsGlobalKeystoreSource.stop();    httpsGlobalKeystoreChannel.stop();}
public void flume_f3529_0()
{    HttpClientBuilder builder = HttpClientBuilder.create();    httpClient = builder.build();    postRequest = new HttpPost("http://0.0.0.0:" + httpPort);}
public void flume_f3530_0() throws IOException, InterruptedException
{    StringEntity input = new StringEntity("[{\"headers\":{\"a\": \"b\"},\"body\": \"random_body\"}," + "{\"headers\":{\"e\": \"f\"},\"body\": \"random_body2\"}]");            input.setContentType("application/json");    postRequest.setEntity(input);    HttpResponse response = httpClient.execute(postRequest);    Assert.assertEquals(HttpServletResponse.SC_OK, response.getStatusLine().getStatusCode());    Transaction tx = httpChannel.getTransaction();    tx.begin();    Event e = httpChannel.take();    Assert.assertNotNull(e);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-8"));    e = httpChannel.take();    Assert.assertNotNull(e);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-8"));    tx.commit();    tx.close();}
public void flume_f3531_0() throws Exception
{    doTestForbidden(new HttpTrace("http://0.0.0.0:" + httpPort));}
public void flume_f3532_0() throws Exception
{    doTestForbidden(new HttpOptions("http://0.0.0.0:" + httpPort));}
private void flume_f3533_0(HttpRequestBase request) throws Exception
{    HttpResponse response = httpClient.execute(request);    Assert.assertEquals(HttpServletResponse.SC_FORBIDDEN, response.getStatusLine().getStatusCode());}
public void flume_f3534_0() throws IOException, InterruptedException
{    StringEntity input = new StringEntity("[{\"headers\":{\"a\": \"b\"},\"body\": \"random_body\"}," + "{\"headers\":{\"e\": \"f\"},\"body\": \"random_body2\"}]", "UTF-16");    input.setContentType("application/json; charset=utf-16");    postRequest.setEntity(input);    HttpResponse response = httpClient.execute(postRequest);    Assert.assertEquals(HttpServletResponse.SC_OK, response.getStatusLine().getStatusCode());    Transaction tx = httpChannel.getTransaction();    tx.begin();    Event e = httpChannel.take();    Assert.assertNotNull(e);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-16"));    e = httpChannel.take();    Assert.assertNotNull(e);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-16"));    tx.commit();    tx.close();}
public void flume_f3535_0() throws Exception
{    StringEntity input = new StringEntity("[{\"a\": \"b\",[\"d\":\"e\"],\"body\": \"random_body\"}," + "{\"e\": \"f\",\"body\": \"random_body2\"}]");    input.setContentType("application/json");    postRequest.setEntity(input);    HttpResponse response = httpClient.execute(postRequest);    Assert.assertEquals(HttpServletResponse.SC_BAD_REQUEST, response.getStatusLine().getStatusCode());    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(httpSource, "sourceCounter");    Assert.assertEquals(1, sc.getEventReadFail());}
public void flume_f3536_0() throws Exception
{    testBatchWithVariousEncoding("UTF-8");}
public void flume_f3537_0() throws Exception
{    testBatchWithVariousEncoding("UTF-16");}
public void flume_f3538_0() throws Exception
{    testBatchWithVariousEncoding("UTF-32");}
public void flume_f3539_0() throws Exception
{    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(new RuntimeException("dummy")).when(cp).processEventBatch(anyListOf(Event.class));    ChannelProcessor oldCp = httpSource.getChannelProcessor();    httpSource.setChannelProcessor(cp);    testBatchWithVariousEncoding("UTF-8");    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(httpSource, "sourceCounter");    Assert.assertEquals(1, sc.getGenericProcessingFail());    httpSource.setChannelProcessor(oldCp);}
public void flume_f3540_0() throws Exception
{    StringEntity input = new StringEntity("[{\"headers\" : {\"a\": \"b\"},\"body\":" + " \"random_body\"}]");    input.setContentType("application/json");    postRequest.setEntity(input);    httpClient.execute(postRequest);    Transaction tx = httpChannel.getTransaction();    tx.begin();    Event e = httpChannel.take();    Assert.assertNotNull(e);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-8"));    tx.commit();    tx.close();}
public void flume_f3541_0() throws Exception
{    StringEntity input = new StringEntity("[{\"headers\" : {\"a\": \"b\"},\"body\":" + " \"random_body\"}]");    input.setContentType("application/json");    postRequest.setEntity(input);    HttpResponse resp = httpClient.execute(postRequest);        Assert.assertTrue(resp.getHeaders("X-Powered-By").length == 0);    Assert.assertTrue(resp.getHeaders("Server").length == 1);    Transaction tx = httpChannel.getTransaction();    tx.begin();    Event e = httpChannel.take();    Assert.assertNotNull(e);    tx.commit();    tx.close();    Assert.assertTrue(findMBeans("org.eclipse.jetty.util.thread:type=queuedthreadpool,*", "maxThreads", 123).size() == 0);    Assert.assertTrue(findMBeans("org.eclipse.jetty.server:type=serverconnector,*", "acceptQueueSize", 22).size() == 0);    int newPort = findFreePort();    Context configuredSourceContext = getDefaultNonSecureContext(newPort);    configuredSourceContext.put("HttpConfiguration.sendServerVersion", "false");    configuredSourceContext.put("HttpConfiguration.sendXPoweredBy", "true");    configuredSourceContext.put("ServerConnector.acceptQueueSize", "22");    configuredSourceContext.put("QueuedThreadPool.maxThreads", "123");    HTTPSource newSource = new HTTPSource();    Channel newChannel = new MemoryChannel();    configureSourceAndChannel(newSource, newChannel, configuredSourceContext);    newChannel.start();    newSource.start();    HttpPost newPostRequest = new HttpPost("http://0.0.0.0:" + newPort);    resp = httpClient.execute(newPostRequest);    Assert.assertTrue(resp.getHeaders("X-Powered-By").length > 0);    Assert.assertTrue(resp.getHeaders("Server").length == 0);    Assert.assertTrue(findMBeans("org.eclipse.jetty.util.thread:type=queuedthreadpool,*", "maxThreads", 123).size() == 1);    Assert.assertTrue(findMBeans("org.eclipse.jetty.server:type=serverconnector,*", "acceptQueueSize", 22).size() == 1);    newSource.stop();    newChannel.stop();        newPort = findFreePort();    configuredSourceContext = getDefaultSecureContext(newPort);    configuredSourceContext.put("SslContextFactory.IncludeProtocols", "abc def");    newSource = new HTTPSource();    newChannel = new MemoryChannel();    configureSourceAndChannel(newSource, newChannel, configuredSourceContext);    newChannel.start();    newSource.start();    newPostRequest = new HttpPost("http://0.0.0.0:" + newPort);    try {        doTestHttps(null, newPort, httpsChannel);                Assert.assertTrue(false);    } catch (AssertionError ex) {        }    newSource.stop();    newChannel.stop();}
public void flume_f3542_0() throws Exception
{    HttpResponse response = putWithEncoding("UTF-8", 150).response;    Assert.assertEquals(HttpServletResponse.SC_SERVICE_UNAVAILABLE, response.getStatusLine().getStatusCode());    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(httpSource, "sourceCounter");    Assert.assertEquals(1, sc.getChannelWriteFail());}
public void flume_f3543_0() throws Exception
{    HTTPSourceHandler handler = field("handler").ofType(HTTPSourceHandler.class).in(httpSource).get();            field("handler").ofType(HTTPSourceHandler.class).in(httpSource).set(null);    HttpResponse response = putWithEncoding("UTF-8", 1).response;    Assert.assertEquals(HttpServletResponse.SC_INTERNAL_SERVER_ERROR, response.getStatusLine().getStatusCode());        field("handler").ofType(HTTPSourceHandler.class).in(httpSource).set(handler);}
public void flume_f3544_0() throws Exception
{    MBeanServer mbeanServer = ManagementFactory.getPlatformMBeanServer();    ObjectName objectName = new ObjectName("org.eclipse.jetty.*:*");    Set<ObjectInstance> queryMBeans = mbeanServer.queryMBeans(objectName, null);    Assert.assertTrue(queryMBeans.size() > 0);}
public void flume_f3545_0() throws Exception
{            HttpResponse response = putWithEncoding("ISO-8859-1", 150).response;    Assert.assertEquals(HttpServletResponse.SC_INTERNAL_SERVER_ERROR, response.getStatusLine().getStatusCode());}
private Set<ObjectInstance> flume_f3546_0(String name, String attribute, int value) throws MalformedObjectNameException
{    MBeanServer mbeanServer = ManagementFactory.getPlatformMBeanServer();    ObjectName objectName = new ObjectName(name);    QueryExp q = Query.eq(Query.attr(attribute), Query.value(value));    return mbeanServer.queryMBeans(objectName, q);}
private ResultWrapper flume_f3547_0(String encoding, int n) throws Exception
{    Type listType = new TypeToken<List<JSONEvent>>() {    }.getType();    List<JSONEvent> events = new ArrayList<JSONEvent>();    Random rand = new Random();    for (int i = 0; i < n; i++) {        Map<String, String> input = Maps.newHashMap();        for (int j = 0; j < 10; j++) {            input.put(String.valueOf(i) + String.valueOf(j), String.valueOf(i));        }        JSONEvent e = new JSONEvent();        e.setHeaders(input);        e.setBody(String.valueOf(rand.nextGaussian()).getBytes(encoding));        events.add(e);    }    Gson gson = new Gson();    String json = gson.toJson(events, listType);    StringEntity input = new StringEntity(json);    input.setContentType("application/json; charset=" + encoding);    postRequest.setEntity(input);    HttpResponse resp = httpClient.execute(postRequest);    return new ResultWrapper(resp, events);}
public void flume_f3548_0() throws Exception
{    doTestHttps(null, httpsPort, httpsChannel);}
public void flume_f3549_0() throws Exception
{    doTestHttps("SSLv3", httpsPort, httpsChannel);}
public void flume_f3550_0() throws Exception
{    doTestHttps(null, httpsGlobalKeystorePort, httpsGlobalKeystoreChannel);}
private void flume_f3551_0(String protocol, int port, Channel channel) throws Exception
{    Type listType = new TypeToken<List<JSONEvent>>() {    }.getType();    List<JSONEvent> events = new ArrayList<JSONEvent>();    Random rand = new Random();    for (int i = 0; i < 10; i++) {        Map<String, String> input = Maps.newHashMap();        for (int j = 0; j < 10; j++) {            input.put(String.valueOf(i) + String.valueOf(j), String.valueOf(i));        }        input.put("MsgNum", String.valueOf(i));        JSONEvent e = new JSONEvent();        e.setHeaders(input);        e.setBody(String.valueOf(rand.nextGaussian()).getBytes("UTF-8"));        events.add(e);    }    Gson gson = new Gson();    String json = gson.toJson(events, listType);    HttpsURLConnection httpsURLConnection = null;    Transaction transaction = null;    try {        TrustManager[] trustAllCerts = { new X509TrustManager() {            @Override            public void checkClientTrusted(java.security.cert.X509Certificate[] x509Certificates, String s) throws CertificateException {                        }            @Override            public void checkServerTrusted(java.security.cert.X509Certificate[] x509Certificates, String s) throws CertificateException {                        }            public java.security.cert.X509Certificate[] getAcceptedIssuers() {                return null;            }        } };        SSLContext sc = null;        javax.net.ssl.SSLSocketFactory factory = null;        if (System.getProperty("java.vendor").contains("IBM")) {            sc = SSLContext.getInstance("SSL_TLS");        } else {            sc = SSLContext.getInstance("SSL");        }        HostnameVerifier hv = new HostnameVerifier() {            public boolean verify(String arg0, SSLSession arg1) {                return true;            }        };        sc.init(null, trustAllCerts, new SecureRandom());        if (protocol != null) {            factory = new DisabledProtocolsSocketFactory(sc.getSocketFactory(), protocol);        } else {            factory = sc.getSocketFactory();        }        HttpsURLConnection.setDefaultSSLSocketFactory(factory);        HttpsURLConnection.setDefaultHostnameVerifier(NoopHostnameVerifier.INSTANCE);        URL sslUrl = new URL("https://0.0.0.0:" + port);        httpsURLConnection = (HttpsURLConnection) sslUrl.openConnection();        httpsURLConnection.setDoInput(true);        httpsURLConnection.setDoOutput(true);        httpsURLConnection.setRequestMethod("POST");        httpsURLConnection.getOutputStream().write(json.getBytes());        int statusCode = httpsURLConnection.getResponseCode();        Assert.assertEquals(200, statusCode);        transaction = channel.getTransaction();        transaction.begin();        for (int i = 0; i < 10; i++) {            Event e = channel.take();            Assert.assertNotNull(e);            Assert.assertEquals(String.valueOf(i), e.getHeaders().get("MsgNum"));        }    } finally {        if (transaction != null) {            transaction.commit();            transaction.close();        }        httpsURLConnection.disconnect();    }}
public void flume_f3552_0(java.security.cert.X509Certificate[] x509Certificates, String s) throws CertificateException
{}
public void flume_f3553_0(java.security.cert.X509Certificate[] x509Certificates, String s) throws CertificateException
{}
public java.security.cert.X509Certificate[] flume_f3554_0()
{    return null;}
public boolean flume_f3555_0(String arg0, SSLSession arg1)
{    return true;}
public void flume_f3556_0() throws Exception
{    Type listType = new TypeToken<List<JSONEvent>>() {    }.getType();    List<JSONEvent> events = new ArrayList<JSONEvent>();    Random rand = new Random();    for (int i = 0; i < 10; i++) {        Map<String, String> input = Maps.newHashMap();        for (int j = 0; j < 10; j++) {            input.put(String.valueOf(i) + String.valueOf(j), String.valueOf(i));        }        input.put("MsgNum", String.valueOf(i));        JSONEvent e = new JSONEvent();        e.setHeaders(input);        e.setBody(String.valueOf(rand.nextGaussian()).getBytes("UTF-8"));        events.add(e);    }    Gson gson = new Gson();    String json = gson.toJson(events, listType);    HttpURLConnection httpURLConnection = null;    try {        URL url = new URL("http://0.0.0.0:" + httpsPort);        httpURLConnection = (HttpURLConnection) url.openConnection();        httpURLConnection.setDoInput(true);        httpURLConnection.setDoOutput(true);        httpURLConnection.setRequestMethod("POST");        httpURLConnection.getOutputStream().write(json.getBytes());        httpURLConnection.getResponseCode();        Assert.fail("HTTP Client cannot connect to HTTPS source");    } catch (Exception exception) {        Assert.assertTrue("Exception expected", true);    } finally {        httpURLConnection.disconnect();    }}
private void flume_f3557_0(String encoding, int n, List<JSONEvent> events) throws Exception
{    Transaction tx = httpChannel.getTransaction();    tx.begin();    Event e = null;    int i = 0;    while (true) {        e = httpChannel.take();        if (e == null) {            break;        }        Event current = events.get(i++);        Assert.assertEquals(new String(current.getBody(), encoding), new String(e.getBody(), encoding));        Assert.assertEquals(current.getHeaders(), e.getHeaders());    }    Assert.assertEquals(n, events.size());    tx.commit();    tx.close();}
private void flume_f3558_0(String encoding) throws Exception
{    testBatchWithVariousEncoding(encoding, 50);}
private void flume_f3559_0(String encoding, int n) throws Exception
{    List<JSONEvent> events = putWithEncoding(encoding, n).events;    takeWithEncoding(encoding, n, events);}
public String[] flume_f3560_0()
{    return socketFactory.getDefaultCipherSuites();}
public String[] flume_f3561_0()
{    return socketFactory.getSupportedCipherSuites();}
public Socket flume_f3562_0(Socket socket, String s, int i, boolean b) throws IOException
{    SSLSocket sc = (SSLSocket) socketFactory.createSocket(socket, s, i, b);    sc.setEnabledProtocols(protocols);    return sc;}
public Socket flume_f3563_0(String s, int i) throws IOException, UnknownHostException
{    SSLSocket sc = (SSLSocket) socketFactory.createSocket(s, i);    sc.setEnabledProtocols(protocols);    return sc;}
public Socket flume_f3564_0(String s, int i, InetAddress inetAddress, int i2) throws IOException, UnknownHostException
{    SSLSocket sc = (SSLSocket) socketFactory.createSocket(s, i, inetAddress, i2);    sc.setEnabledProtocols(protocols);    return sc;}
public Socket flume_f3565_0(InetAddress inetAddress, int i) throws IOException
{    SSLSocket sc = (SSLSocket) socketFactory.createSocket(inetAddress, i);    sc.setEnabledProtocols(protocols);    return sc;}
public Socket flume_f3566_0(InetAddress inetAddress, int i, InetAddress inetAddress2, int i2) throws IOException
{    SSLSocket sc = (SSLSocket) socketFactory.createSocket(inetAddress, i, inetAddress2, i2);    sc.setEnabledProtocols(protocols);    return sc;}
public void flume_f3567_0()
{    handler = new JSONHandler();}
public void flume_f3568_0() throws Exception
{    String json = "[{\"headers\":{\"a\": \"b\"},\"body\": \"random_body\"}," + "{\"headers\":{\"e\": \"f\"},\"body\": \"random_body2\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-8"));    e = deserialized.get(1);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-8"));}
public void flume_f3569_0() throws Exception
{    String json = "[{\"headers\":{\"a\": \"b\"},\"body\": \"random_body\"}," + "{\"headers\":{\"e\": \"f\"},\"body\": \"random_body2\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json, "UTF-16");    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-16"));    e = deserialized.get(1);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-16"));}
public void flume_f3570_0() throws Exception
{    String json = "[{\"headers\":{\"a\": \"b\"},\"body\": \"random_body\"}," + "{\"headers\":{\"e\": \"f\"},\"body\": \"random_body2\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json, "UTF-32");    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-32"));    e = deserialized.get(1);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-32"));}
public void flume_f3571_0() throws Exception
{    String json = "[{\"headers\":{\"a\": \"b\"},\"body\": \"random_body\"}," + "{\"headers\":{\"e\": \"f\"},\"body\": \"random_body2\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json, "UTF-8");    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-8"));    e = deserialized.get(1);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-8"));}
public void flume_f3572_0() throws Exception
{        String json = "[{\"headers\":{\"a\": \"b\"}}," + "{\"headers\":{\"e\": \"f\"},\"body\": \"rand\\\"om_body2\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertTrue(e.getBody().length == 0);    e = deserialized.get(1);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("rand\"om_body2", new String(e.getBody(), "UTF-8"));}
public void flume_f3573_0() throws Exception
{    String json = "[{\"headers\" : {\"a\": \"b\"}}," + "{\"headers\" : {\"e\": \"f\"},\"body\": \"random_body2\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertTrue(e.getBody().length == 0);    e = deserialized.get(1);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-8"));}
public void flume_f3574_0() throws Exception
{    String json = "[{\"headers\": {\"a\": \"b\"}," + "\"body\": \"<html><body>test</body></html>\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("<html><body>test</body></html>", new String(e.getBody(), "UTF-8"));}
public void flume_f3575_0() throws Exception
{    String json = "[{\"headers\" : {\"a\": \"b\"},\"body\": \"random_body\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-8"));}
public void flume_f3576_0() throws Exception
{    String json = "{[\"a\": \"b\"],\"body\": \"random_body\"}";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    handler.getEvents(req);    Assert.fail();}
public void flume_f3577_0() throws Exception
{    String json = "[{\"headers\" : {\"a\": \"b\"},\"body\": \"random_body\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json, "ISO-8859-1");    handler.getEvents(req);    Assert.fail();}
public void flume_f3578_0() throws Exception
{    String json = "[{\"headers\": {\"a\": \"b\"},\"body\": \"random_body\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-8"));}
public void flume_f3579_0() throws Exception
{    String json = "[{\"headers\" : {\"a\": \"b\", \"a2\": \"b2\"," + "\"a3\": \"b3\",\"a4\": \"b4\"},\"body\": \"random_body\"}," + "{\"headers\" :{\"e\": \"f\",\"e2\": \"f2\"," + "\"e3\": \"f3\",\"e4\": \"f4\",\"e5\": \"f5\"}," + "\"body\": \"random_body2\"}," + "{\"headers\" :{\"q1\": \"b\",\"q2\": \"b2\",\"q3\": \"b3\",\"q4\": \"b4\"}," + "\"body\": \"random_bodyq\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertNotNull(e);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("b2", e.getHeaders().get("a2"));    Assert.assertEquals("b3", e.getHeaders().get("a3"));    Assert.assertEquals("b4", e.getHeaders().get("a4"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-8"));    e = deserialized.get(1);    Assert.assertNotNull(e);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("f2", e.getHeaders().get("e2"));    Assert.assertEquals("f3", e.getHeaders().get("e3"));    Assert.assertEquals("f4", e.getHeaders().get("e4"));    Assert.assertEquals("f5", e.getHeaders().get("e5"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-8"));    e = deserialized.get(2);    Assert.assertNotNull(e);    Assert.assertEquals("b", e.getHeaders().get("q1"));    Assert.assertEquals("b2", e.getHeaders().get("q2"));    Assert.assertEquals("b3", e.getHeaders().get("q3"));    Assert.assertEquals("b4", e.getHeaders().get("q4"));    Assert.assertEquals("random_bodyq", new String(e.getBody(), "UTF-8"));}
public void flume_f3580_0() throws Exception
{    Type listType = new TypeToken<List<JSONEvent>>() {    }.getType();    List<JSONEvent> events = Lists.newArrayList();    Random rand = new Random();    for (int i = 1; i < 10; i++) {        Map<String, String> input = Maps.newHashMap();        for (int j = 1; j < 10; j++) {            input.put(String.valueOf(i) + String.valueOf(j), String.valueOf(i));        }        JSONEvent e = new JSONEvent();        e.setBody(String.valueOf(rand.nextGaussian()).getBytes("UTF-8"));        e.setHeaders(input);        events.add(e);    }    Gson gson = new Gson();    List<Event> deserialized = handler.getEvents(new FlumeHttpServletRequestWrapper(gson.toJson(events, listType)));    int i = 0;    for (Event e : deserialized) {        Event current = events.get(i++);        Assert.assertEquals(new String(current.getBody(), "UTF-8"), new String(e.getBody(), "UTF-8"));        Assert.assertEquals(current.getHeaders(), e.getHeaders());    }}
public void flume_f3581_0()
{}
public void flume_f3582_0()
{}
public LifecycleState flume_f3583_0()
{    return null;}
public void flume_f3584_0(ChannelProcessor cp)
{}
public void flume_f3585_0(String name)
{    this.name = name;}
public String flume_f3586_0()
{    return name;}
public ChannelProcessor flume_f3587_0()
{    return null;}
public void flume_f3588_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);        limiter.acquire();        limiter.acquire();        limiter.acquire();    assertEvents("R0.00", "R0.20", "R0.20");}
public void flume_f3589_0()
{    RateLimiter r = RateLimiter.create(1);    assertTrue("Unable to acquire initial permit", r.tryAcquire());    assertFalse("Capable of acquiring secondary permit", r.tryAcquire());}
public void flume_f3590_0()
{    RateLimiter limiter = RateLimiter.create(5.0, 5, SECONDS);    assertEquals(5.0, limiter.getRate());    limiter.setRate(10.0);    assertEquals(10.0, limiter.getRate());    try {        limiter.setRate(0.0);        fail();    } catch (IllegalArgumentException expected) {    }    try {        limiter.setRate(-10.0);        fail();    } catch (IllegalArgumentException expected) {    }}
public void flume_f3591_0()
{    RateLimiter limiter = RateLimiter.create(999);    try {        limiter.acquire(0);        fail();    } catch (IllegalArgumentException expected) {    }    try {        limiter.acquire(-1);        fail();    } catch (IllegalArgumentException expected) {    }    try {        limiter.tryAcquire(0);        fail();    } catch (IllegalArgumentException expected) {    }    try {        limiter.tryAcquire(-1);        fail();    } catch (IllegalArgumentException expected) {    }    try {        limiter.tryAcquire(0, 1, SECONDS);        fail();    } catch (IllegalArgumentException expected) {    }    try {        limiter.tryAcquire(-1, 1, SECONDS);        fail();    } catch (IllegalArgumentException expected) {    }}
public void flume_f3592_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);        limiter.acquire();        stopwatch.sleepMillis(200);        limiter.acquire();        limiter.acquire();    assertEvents("R0.00", "U0.20", "R0.00", "R0.20");}
public void flume_f3593_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);        assertEquals(0.0, limiter.acquire(), EPSILON);        stopwatch.sleepMillis(200);        assertEquals(0.0, limiter.acquire(), EPSILON);        assertEquals(0.2, limiter.acquire(), EPSILON);    assertEvents("R0.00", "U0.20", "R0.00", "R0.20");}
public void flume_f3594_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);    assertEquals(0.0, limiter.acquire(), EPSILON);    stopwatch.sleepMillis(400);    assertEquals(0.0, limiter.acquire(), EPSILON);    assertEquals(0.0, limiter.acquire(), EPSILON);    assertEquals(0.2, limiter.acquire(), EPSILON);}
public void flume_f3595_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);        stopwatch.sleepMillis(1000);        stopwatch.sleepMillis(1000);        limiter.acquire(1);        limiter.acquire(1);        limiter.acquire(3);        limiter.acquire(1);        limiter.acquire();    assertEvents("U1.00", "U1.00",     "R0.00",     "R0.00",     "R0.00",     "R0.00", "R0.20");}
public void flume_f3596_0()
{    RateLimiter.create(1.0, 1, NANOSECONDS);    RateLimiter.create(1.0, 0, NANOSECONDS);    try {        RateLimiter.create(0.0, 1, NANOSECONDS);        fail();    } catch (IllegalArgumentException expected) {    }    try {        RateLimiter.create(1.0, -1, NANOSECONDS);        fail();    } catch (IllegalArgumentException expected) {    }}
public void flume_f3597_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 2.0, 4000, MILLISECONDS);    for (int i = 0; i < 8; i++) {                limiter.acquire();    }        stopwatch.sleepMillis(500);        stopwatch.sleepMillis(4000);    for (int i = 0; i < 8; i++) {                limiter.acquire();    }        stopwatch.sleepMillis(500);        stopwatch.sleepMillis(2000);    for (int i = 0; i < 8; i++) {                limiter.acquire();    }    assertEvents(    "R0.00, R1.38, R1.13, R0.88, R0.63, R0.50, R0.50, R0.50",     "U0.50",     "U4.00",     "R0.00, R1.38, R1.13, R0.88, R0.63, R0.50, R0.50, R0.50",     "U0.50",     "U2.00",     "R0.00, R0.50, R0.50, R0.50, R0.50, R0.50, R0.50, R0.50");}
public void flume_f3598_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 2.0, 4000, MILLISECONDS);    for (int i = 0; i < 8; i++) {                limiter.acquire();    }        stopwatch.sleepMillis(4500);    for (int i = 0; i < 3; i++) {                        limiter.acquire();    }        limiter.setRate(4.0);        limiter.acquire();    for (int i = 0; i < 4; i++) {                limiter.acquire();    }        stopwatch.sleepMillis(4250);    for (int i = 0; i < 11; i++) {                limiter.acquire();    }        assertEvents(    "R0.00, R1.38, R1.13, R0.88, R0.63, R0.50, R0.50, R0.50",     "U4.50",     "R0.00, R1.38, R1.13",     "R0.88",     "R0.34, R0.28, R0.25, R0.25",     "U4.25",     "R0.00, R0.72, R0.66, R0.59, R0.53, R0.47, R0.41",     "R0.34, R0.28, R0.25, R0.25");}
public void flume_f3599_0()
{    RateLimiter rateLimiter = RateLimiter.create(stopwatch, 1.0);        rateLimiter.acquire(1);        rateLimiter.acquire(1);        rateLimiter.setRate(2.0);        rateLimiter.acquire(1);        rateLimiter.acquire(2);        rateLimiter.acquire(4);        rateLimiter.acquire(1);    assertEvents("R0.00", "R1.00", "R1.00", "R0.50", "R1.00", "R2.00");}
public void flume_f3600_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);    assertTrue(limiter.tryAcquire(0, SECONDS));    assertFalse(limiter.tryAcquire(0, SECONDS));    assertFalse(limiter.tryAcquire(0, SECONDS));    stopwatch.sleepMillis(100);    assertFalse(limiter.tryAcquire(0, SECONDS));}
public void flume_f3601_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);    assertTrue(limiter.tryAcquire(0, SECONDS));    assertTrue(limiter.tryAcquire(200, MILLISECONDS));    assertFalse(limiter.tryAcquire(100, MILLISECONDS));    stopwatch.sleepMillis(100);    assertTrue(limiter.tryAcquire(100, MILLISECONDS));}
public void flume_f3602_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);    assertTrue(limiter.tryAcquire(0, MICROSECONDS));    stopwatch.sleepMillis(100);    assertTrue(limiter.tryAcquire(Long.MAX_VALUE, MICROSECONDS));}
public void flume_f3603_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);    assertTrue(limiter.tryAcquire(5, 0, SECONDS));    stopwatch.sleepMillis(900);    assertFalse(limiter.tryAcquire(1, Long.MIN_VALUE, SECONDS));    stopwatch.sleepMillis(100);    assertTrue(limiter.tryAcquire(1, -1, SECONDS));}
public void flume_f3604_0()
{    RateLimiter rateLimiter = RateLimiter.create(stopwatch, 1.0);        rateLimiter.acquire(1);        rateLimiter.acquire(1);        rateLimiter.acquire(2);        rateLimiter.acquire(4);        rateLimiter.acquire(8);        rateLimiter.acquire(1);    assertEvents("R0.00", "R1.00", "R1.00", "R2.00", "R4.00", "R8.00");}
public void flume_f3605_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, Double.POSITIVE_INFINITY);    limiter.acquire(Integer.MAX_VALUE / 4);    limiter.acquire(Integer.MAX_VALUE / 2);    limiter.acquire(Integer.MAX_VALUE);        assertEvents("R0.00", "R0.00", "R0.00");    limiter.setRate(2.0);    limiter.acquire();    limiter.acquire();    limiter.acquire();    limiter.acquire();    limiter.acquire();    assertEvents(    "R0.00", "R0.00",     "R0.00",     "R0.50", "R0.50");    limiter.setRate(Double.POSITIVE_INFINITY);    limiter.acquire();    limiter.acquire();    limiter.acquire();        assertEvents("R0.50", "R0.00", "R0.00");}
public void flume_f3606_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, Double.POSITIVE_INFINITY);    stopwatch.instant += 1000000;    limiter.setRate(2.0);    for (int i = 0; i < 5; i++) {        limiter.acquire();    }    assertEvents(    "R0.00", "R0.00",     "R0.00",     "R0.50", "R0.50");}
public void flume_f3607_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, Double.POSITIVE_INFINITY, 10, SECONDS);    limiter.acquire(Integer.MAX_VALUE / 4);    limiter.acquire(Integer.MAX_VALUE / 2);    limiter.acquire(Integer.MAX_VALUE);    assertEvents("R0.00", "R0.00", "R0.00");    limiter.setRate(1.0);    limiter.acquire();    limiter.acquire();    limiter.acquire();    assertEvents("R0.00", "R1.00", "R1.00");    limiter.setRate(Double.POSITIVE_INFINITY);    limiter.acquire();    limiter.acquire();    limiter.acquire();    assertEvents("R1.00", "R0.00", "R0.00");}
public void flume_f3608_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, Double.POSITIVE_INFINITY, 10, SECONDS);    stopwatch.instant += 1000000;    limiter.setRate(1.0);    for (int i = 0; i < 5; i++) {        limiter.acquire();    }    assertEvents("R0.00", "R1.00", "R1.00", "R1.00", "R1.00");}
public void flume_f3609_0()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 1.0);    int[] rates = { 1000, 1, 10, 1000000, 10, 1 };    for (int rate : rates) {        int oneSecWorthOfWork = rate;        stopwatch.sleepMillis(rate * 1000);        limiter.setRate(rate);        long burst = measureTotalTimeMillis(limiter, oneSecWorthOfWork, new Random());                assertTrue(burst <= 1000);        long afterBurst = measureTotalTimeMillis(limiter, oneSecWorthOfWork, new Random());                assertTrue(afterBurst >= 1000);    }}
public void flume_f3610_0()
{    Random random = new Random();    int maxPermits = 10;    double[] qpsToTest = { 4.0, 2.0, 1.0, 0.5, 0.1 };    for (int trial = 0; trial < 100; trial++) {        for (double qps : qpsToTest) {                                                long warmupMillis = (long) ((2 * maxPermits / qps) * 1000.0);            RateLimiter rateLimiter = RateLimiter.create(stopwatch, qps, warmupMillis, MILLISECONDS);            assertEquals(warmupMillis, measureTotalTimeMillis(rateLimiter, maxPermits, random));        }    }}
private long flume_f3611_0(RateLimiter rateLimiter, int permits, Random random)
{    long startTime = stopwatch.instant;    while (permits > 0) {        int nextPermitsToAcquire = Math.max(1, random.nextInt(permits));        permits -= nextPermitsToAcquire;        rateLimiter.acquire(nextPermitsToAcquire);    }        rateLimiter.acquire(1);    return NANOSECONDS.toMillis(stopwatch.instant - startTime);}
private void flume_f3612_0(String... events)
{    assertEquals(Arrays.toString(events), stopwatch.readEventsAndClear());}
public long flume_f3613_0()
{    return NANOSECONDS.toMicros(instant);}
 void flume_f3614_0(int millis)
{    sleepMicros("U", MILLISECONDS.toMicros(millis));}
 void flume_f3615_0(String caption, long micros)
{    instant += MICROSECONDS.toNanos(micros);    events.add(caption + String.format("%3.2f", (micros / 1000000.0)));}
 void flume_f3616_0(long micros)
{    sleepMicros("R", micros);}
 String flume_f3617_0()
{    try {        return events.toString();    } finally {        events.clear();    }}
public String flume_f3618_0()
{    return events.toString();}
public void flume_f3619_0()
{    source = spy(new AbstractPollableSource() {        @Override        protected Status doProcess() throws EventDeliveryException {            return Status.BACKOFF;        }        @Override        protected void doConfigure(Context context) throws FlumeException {            throw new FlumeException("dummy");        }        @Override        protected void doStart() throws FlumeException {        }        @Override        protected void doStop() throws FlumeException {        }    });}
protected Status flume_f3620_0() throws EventDeliveryException
{    return Status.BACKOFF;}
protected void flume_f3621_0(Context context) throws FlumeException
{    throw new FlumeException("dummy");}
protected void flume_f3622_0() throws FlumeException
{}
protected void flume_f3623_0() throws FlumeException
{}
public void flume_f3624_0() throws Exception
{    source.configure(new Context());}
public void flume_f3625_0() throws Exception
{    source.process();}
public void flume_f3626_0()
{    source = spy(new AbstractPollableSource() {        @Override        protected Status doProcess() throws EventDeliveryException {            return Status.BACKOFF;        }        @Override        protected void doConfigure(Context context) throws FlumeException {        }        @Override        protected void doStart() throws FlumeException {        }        @Override        protected void doStop() throws FlumeException {        }    });    HashMap<String, String> inputConfigs = new HashMap<String, String>();    inputConfigs.put(PollableSourceConstants.BACKOFF_SLEEP_INCREMENT, "42");    inputConfigs.put(PollableSourceConstants.MAX_BACKOFF_SLEEP, "4242");    Context context = new Context(inputConfigs);    source.configure(context);    Assert.assertEquals("BackOffSleepIncrement should equal 42 but it equals " + source.getBackOffSleepIncrement(), 42L, source.getBackOffSleepIncrement());    Assert.assertEquals("BackOffSleepIncrement should equal 42 but it equals " + source.getMaxBackOffSleepInterval(), 4242L, source.getMaxBackOffSleepInterval());}
protected Status flume_f3627_0() throws EventDeliveryException
{    return Status.BACKOFF;}
protected void flume_f3628_0(Context context) throws FlumeException
{}
protected void flume_f3629_0() throws FlumeException
{}
protected void flume_f3630_0() throws FlumeException
{}
public void flume_f3631_0()
{    source = spy(new AbstractPollableSource() {        @Override        protected Status doProcess() throws EventDeliveryException {            return Status.BACKOFF;        }        @Override        protected void doConfigure(Context context) throws FlumeException {        }        @Override        protected void doStart() throws FlumeException {        }        @Override        protected void doStop() throws FlumeException {        }    });    HashMap<String, String> inputConfigs = new HashMap<String, String>();    Assert.assertEquals("BackOffSleepIncrement should equal " + PollableSourceConstants.DEFAULT_BACKOFF_SLEEP_INCREMENT + " but it equals " + source.getBackOffSleepIncrement(), PollableSourceConstants.DEFAULT_BACKOFF_SLEEP_INCREMENT, source.getBackOffSleepIncrement());    Assert.assertEquals("BackOffSleepIncrement should equal " + PollableSourceConstants.DEFAULT_MAX_BACKOFF_SLEEP + " but it equals " + source.getMaxBackOffSleepInterval(), PollableSourceConstants.DEFAULT_MAX_BACKOFF_SLEEP, source.getMaxBackOffSleepInterval());}
protected Status flume_f3632_0() throws EventDeliveryException
{    return Status.BACKOFF;}
protected void flume_f3633_0(Context context) throws FlumeException
{}
protected void flume_f3634_0() throws FlumeException
{}
protected void flume_f3635_0() throws FlumeException
{}
public void flume_f3636_0() throws UnknownHostException
{    localhost = InetAddress.getByName("127.0.0.1");    source = new AvroSource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));}
public void flume_f3637_0() throws InterruptedException, IOException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort = getFreePort()));    context.put("bind", "0.0.0.0");    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());    source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
public void flume_f3638_1() throws InterruptedException, IOException
{    final String loopbackIPv4 = "127.0.0.1";    final int port = 10500;        try (ServerSocketChannel dummyServerSocket = ServerSocketChannel.open()) {        dummyServerSocket.socket().setReuseAddress(true);        dummyServerSocket.socket().bind(new InetSocketAddress(loopbackIPv4, port));        Context context = new Context();        context.put("port", String.valueOf(port));        context.put("bind", loopbackIPv4);        Configurables.configure(source, context);        try {            source.start();            Assert.fail("Expected an exception during startup caused by binding on a used port");        } catch (FlumeException e) {                        Assert.assertTrue("Expected a server socket setup related root cause", e.getMessage().contains("server socket"));        }    }            Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
public void flume_f3639_1() throws InterruptedException, IOException
{    final String invalidHost = "invalid.host";    final int port = 10501;    Context context = new Context();    context.put("port", String.valueOf(port));    context.put("bind", invalidHost);    Configurables.configure(source, context);    try {        source.start();        Assert.fail("Expected an exception during startup caused by binding on a invalid host");    } catch (FlumeException e) {                Assert.assertTrue("Expected a server socket setup related root cause", e.getMessage().contains("server socket"));    }            Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
public void flume_f3640_0() throws InterruptedException, IOException
{    doRequest(false, false, 6);}
public void flume_f3641_0() throws InterruptedException, IOException
{    doRequest(true, true, 0);}
public void flume_f3642_0() throws InterruptedException, IOException
{    doRequest(true, true, 1);}
public void flume_f3643_0() throws InterruptedException, IOException
{    doRequest(true, true, 6);}
public void flume_f3644_0() throws InterruptedException, IOException
{    doRequest(true, true, 9);}
public void flume_f3645_0() throws InterruptedException, IOException
{        doRequest(true, false, 6);}
public void flume_f3646_0() throws InterruptedException, IOException
{        doRequest(false, true, 6);}
private void flume_f3647_1(boolean serverEnableCompression, boolean clientEnableCompression, int compressionLevel) throws InterruptedException, IOException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort = getFreePort()));    context.put("bind", "0.0.0.0");    context.put("threads", "50");    if (serverEnableCompression) {        context.put("compression-type", "deflate");    } else {        context.put("compression-type", "none");    }    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());    AvroSourceProtocol client;    NettyTransceiver nettyTransceiver;    if (clientEnableCompression) {        nettyTransceiver = new NettyTransceiver(new InetSocketAddress(selectedPort), new CompressionChannelFactory(compressionLevel));        client = SpecificRequestor.getClient(AvroSourceProtocol.class, nettyTransceiver);    } else {        nettyTransceiver = new NettyTransceiver(new InetSocketAddress(selectedPort));        client = SpecificRequestor.getClient(AvroSourceProtocol.class, nettyTransceiver);    }    AvroFlumeEvent avroEvent = new AvroFlumeEvent();    avroEvent.setHeaders(new HashMap<CharSequence, CharSequence>());    avroEvent.setBody(ByteBuffer.wrap("Hello avro".getBytes()));    Status status = client.append(avroEvent);    Assert.assertEquals(Status.OK, status);    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    Assert.assertEquals("Channel contained our event", "Hello avro", new String(event.getBody()));    transaction.commit();    transaction.close();        nettyTransceiver.close();    source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
private static int flume_f3648_0() throws IOException
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
public SocketChannel flume_f3649_0(ChannelPipeline pipeline)
{    try {        ZlibEncoder encoder = new ZlibEncoder(compressionLevel);        pipeline.addFirst("deflater", encoder);        pipeline.addFirst("inflater", new ZlibDecoder());        return super.newChannel(pipeline);    } catch (Exception ex) {        throw new RuntimeException("Cannot create Compression channel", ex);    }}
public void flume_f3650_0() throws InterruptedException, IOException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort = getFreePort()));    context.put("bind", "0.0.0.0");    context.put("ssl", "true");    context.put("keystore", "src/test/resources/server.p12");    context.put("keystore-password", "password");    context.put("keystore-type", "PKCS12");    Configurables.configure(source, context);    doSslRequest();}
public void flume_f3651_0() throws InterruptedException, IOException
{    System.setProperty("javax.net.ssl.keyStore", "src/test/resources/server.p12");    System.setProperty("javax.net.ssl.keyStorePassword", "password");    System.setProperty("javax.net.ssl.keyStoreType", "PKCS12");    Context context = new Context();    context.put("port", String.valueOf(selectedPort = getFreePort()));    context.put("bind", "0.0.0.0");    context.put("ssl", "true");    Configurables.configure(source, context);    doSslRequest();    System.clearProperty("javax.net.ssl.keyStore");    System.clearProperty("javax.net.ssl.keyStorePassword");}
private void flume_f3652_1() throws InterruptedException, IOException
{    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());    AvroSourceProtocol client = SpecificRequestor.getClient(AvroSourceProtocol.class, new NettyTransceiver(new InetSocketAddress(selectedPort), new SSLChannelFactory()));    AvroFlumeEvent avroEvent = new AvroFlumeEvent();    avroEvent.setHeaders(new HashMap<CharSequence, CharSequence>());    avroEvent.setBody(ByteBuffer.wrap("Hello avro ssl".getBytes()));    Status status = client.append(avroEvent);    Assert.assertEquals(Status.OK, status);    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    Assert.assertEquals("Channel contained our event", "Hello avro ssl", new String(event.getBody()));    transaction.commit();    transaction.close();        source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
public SocketChannel flume_f3653_0(ChannelPipeline pipeline)
{    try {        SSLContext sslContext = SSLContext.getInstance("TLS");        sslContext.init(null, new TrustManager[] { new PermissiveTrustManager() }, null);        SSLEngine sslEngine = sslContext.createSSLEngine();        sslEngine.setUseClientMode(true);                        pipeline.addFirst("ssl", new SslHandler(sslEngine));        return super.newChannel(pipeline);    } catch (Exception ex) {        throw new RuntimeException("Cannot create SSL channel", ex);    }}
public void flume_f3654_0(X509Certificate[] certs, String s)
{}
public void flume_f3655_0(X509Certificate[] certs, String s)
{}
public X509Certificate[] flume_f3656_0()
{    return new X509Certificate[0];}
public void flume_f3657_0() throws InterruptedException, IOException
{    doIpFilterTest(localhost, "allow:name:localhost,deny:ip:*", true, false);    doIpFilterTest(localhost, "allow:ip:" + localhost.getHostAddress() + ",deny:ip:*", true, false);    doIpFilterTest(localhost, "allow:ip:*", true, false);    doIpFilterTest(localhost, "allow:ip:" + localhost.getHostAddress().substring(0, 3) + "*,deny:ip:*", true, false);    doIpFilterTest(localhost, "allow:ip:127.0.0.2,allow:ip:" + localhost.getHostAddress().substring(0, 3) + "*,deny:ip:*", true, false);    doIpFilterTest(localhost, "allow:name:localhost,deny:ip:*", true, true);    doIpFilterTest(localhost, "allow:ip:*", true, true);}
public void flume_f3658_0() throws InterruptedException, IOException
{    doIpFilterTest(localhost, "deny:ip:*", false, false);    doIpFilterTest(localhost, "deny:name:localhost", false, false);    doIpFilterTest(localhost, "deny:ip:" + localhost.getHostAddress() + ",allow:ip:*", false, false);    doIpFilterTest(localhost, "deny:ip:*", false, false);    doIpFilterTest(localhost, "allow:ip:45.2.2.2,deny:ip:*", false, false);    doIpFilterTest(localhost, "deny:ip:" + localhost.getHostAddress().substring(0, 3) + "*,allow:ip:*", false, false);    doIpFilterTest(localhost, "deny:ip:*", false, true);}
public void flume_f3659_1() throws InterruptedException, IOException
{    doIpFilterTest(localhost, "deny:ip:*", false, false);    doIpFilterTest(localhost, "allow:name:localhost", true, false);    doIpFilterTest(localhost, "deny:ip:127.0.0.2,allow:ip:*,deny:ip:" + localhost.getHostAddress(), true, false);    doIpFilterTest(localhost, "deny:ip:" + localhost.getHostAddress().substring(0, 3) + "*,allow:ip:*", false, false);        Consumer<Exception> exceptionChecker = (Exception ex) -> {                        Assert.assertTrue("Expected an ipFilterRules related exception", ex.getMessage().contains("ipFilter"));    };    try {        doIpFilterTest(localhost, null, false, false);        Assert.fail("The null ipFilterRules config should have thrown an exception.");    } catch (FlumeException e) {        exceptionChecker.accept(e);    }    try {        doIpFilterTest(localhost, "", true, false);        Assert.fail("The empty string ipFilterRules config should have thrown " + "an exception");    } catch (FlumeException e) {        exceptionChecker.accept(e);    }    try {        doIpFilterTest(localhost, "homer:ip:45.4.23.1", true, false);        Assert.fail("Bad ipFilterRules config should have thrown an exception.");    } catch (FlumeException e) {        exceptionChecker.accept(e);    }    try {        doIpFilterTest(localhost, "allow:sleeps:45.4.23.1", true, false);        Assert.fail("Bad ipFilterRules config should have thrown an exception.");    } catch (FlumeException e) {        exceptionChecker.accept(e);    }}
public void flume_f3660_1(InetAddress dest, String ruleDefinition, boolean eventShouldBeAllowed, boolean testWithSSL) throws InterruptedException, IOException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort = getFreePort()));    context.put("bind", "0.0.0.0");    context.put("ipFilter", "true");    if (ruleDefinition != null) {        context.put("ipFilterRules", ruleDefinition);    }    if (testWithSSL) {                context.put("ssl", "true");        context.put("keystore", "src/test/resources/server.p12");        context.put("keystore-password", "password");        context.put("keystore-type", "PKCS12");    }        Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());    AvroSourceProtocol client;    NettyTransceiver nettyTransceiver = null;    try {        if (testWithSSL) {            nettyTransceiver = new NettyTransceiver(new InetSocketAddress(dest, selectedPort), new SSLChannelFactory());            client = SpecificRequestor.getClient(AvroSourceProtocol.class, nettyTransceiver);        } else {            nettyTransceiver = new NettyTransceiver(new InetSocketAddress(dest, selectedPort));            client = SpecificRequestor.getClient(AvroSourceProtocol.class, nettyTransceiver);        }        AvroFlumeEvent avroEvent = new AvroFlumeEvent();        avroEvent.setHeaders(new HashMap<CharSequence, CharSequence>());        avroEvent.setBody(ByteBuffer.wrap("Hello avro ipFilter".getBytes()));                Status status = client.append(avroEvent);                Assert.assertEquals(Status.OK, status);    } catch (IOException e) {        Assert.assertTrue("Should have been allowed: " + ruleDefinition, !eventShouldBeAllowed);        return;    } finally {        if (nettyTransceiver != null) {            nettyTransceiver.close();        }        source.stop();    }    Assert.assertTrue("Should have been denied: " + ruleDefinition, eventShouldBeAllowed);    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    Assert.assertEquals("Channel contained our event", "Hello avro ipFilter", new String(event.getBody()));    transaction.commit();    transaction.close();        Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
public void flume_f3661_0() throws Exception
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort = getFreePort()));    context.put("bind", "0.0.0.0");    source.configure(context);    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(new ChannelException("dummy")).when(cp).processEvent(any(Event.class));    doThrow(new ChannelException("dummy")).when(cp).processEventBatch(anyListOf(Event.class));    source.setChannelProcessor(cp);    source.start();    AvroFlumeEvent avroEvent = new AvroFlumeEvent();    avroEvent.setHeaders(new HashMap<CharSequence, CharSequence>());    avroEvent.setBody(ByteBuffer.wrap("Hello avro ssl".getBytes()));    source.append(avroEvent);    source.appendBatch(Arrays.asList(avroEvent));    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(source, "sourceCounter");    Assert.assertEquals(2, sc.getChannelWriteFail());    source.stop();}
public void flume_f3662_0()
{    context = new Context();    channelProcessor = mock(ChannelProcessor.class);}
public DoNothingSource flume_f3663_0(DoNothingSource source)
{    source = spy(source);    source.setChannelProcessor(channelProcessor);    source.configure(context);    return source;}
public void flume_f3664_0() throws Exception
{    source = spy(new DoNothingSource() {        @Override        protected void doConfigure(Context context) throws FlumeException {            throw new FlumeException("dummy");        }    });    source.setChannelProcessor(channelProcessor);    try {        source.configure(context);        Assert.fail();    } catch (FlumeException expected) {    }    Assert.assertFalse(source.isStarted());    Assert.assertEquals(LifecycleState.ERROR, source.getLifecycleState());    Assert.assertNotNull(source.getStartException());}
protected void flume_f3665_0(Context context) throws FlumeException
{    throw new FlumeException("dummy");}
public void flume_f3666_0() throws Exception
{    source = spyAndConfigure(new DoNothingSource() {        @Override        protected void doStart() throws FlumeException {            throw new FlumeException("dummy");        }    });    source.start();    Assert.assertFalse(source.isStarted());    Assert.assertEquals(LifecycleState.ERROR, source.getLifecycleState());    Assert.assertNotNull(source.getStartException());}
protected void flume_f3667_0() throws FlumeException
{    throw new FlumeException("dummy");}
public void flume_f3668_0() throws Exception
{    source = spyAndConfigure(new DoNothingSource() {        @Override        protected void doStop() throws FlumeException {            throw new FlumeException("dummy");        }    });    source.start();    source.stop();    Assert.assertFalse(source.isStarted());    Assert.assertEquals(LifecycleState.ERROR, source.getLifecycleState());    Assert.assertNull(source.getStartException());}
protected void flume_f3669_0() throws FlumeException
{    throw new FlumeException("dummy");}
public void flume_f3670_0() throws Exception
{    source = spyAndConfigure(new DoNothingSource());    source.start();    try {        source.configure(context);        Assert.fail();    } catch (IllegalStateException expected) {    }    Assert.assertTrue(source.isStarted());    Assert.assertNull(source.getStartException());}
protected void flume_f3671_0(Context context) throws FlumeException
{}
protected void flume_f3672_0() throws FlumeException
{}
protected void flume_f3673_0() throws FlumeException
{}
public void flume_f3674_0()
{    sourceFactory = new DefaultSourceFactory();}
public void flume_f3675_0()
{    Source avroSource1 = sourceFactory.create("avroSource1", "avro");    Source avroSource2 = sourceFactory.create("avroSource2", "avro");    Assert.assertNotNull(avroSource1);    Assert.assertNotNull(avroSource2);    Assert.assertNotSame(avroSource1, avroSource2);    Assert.assertTrue(avroSource1 instanceof AvroSource);    Assert.assertTrue(avroSource2 instanceof AvroSource);    Source s1 = sourceFactory.create("avroSource1", "avro");    Source s2 = sourceFactory.create("avroSource2", "avro");    Assert.assertNotSame(avroSource1, s1);    Assert.assertNotSame(avroSource2, s2);}
private void flume_f3676_0(String name, String type, Class<?> typeClass) throws Exception
{    Source src = sourceFactory.create(name, type);    Assert.assertNotNull(src);    Assert.assertTrue(typeClass.isInstance(src));}
public void flume_f3677_0() throws Exception
{    verifySourceCreation("seq-src", "seq", SequenceGeneratorSource.class);    verifySourceCreation("netcat-src", "netcat", NetcatSource.class);    verifySourceCreation("netcat-udp-src", "netcatudp", NetcatUdpSource.class);    verifySourceCreation("exec-src", "exec", ExecSource.class);    verifySourceCreation("avro-src", "avro", AvroSource.class);    verifySourceCreation("syslogtcp-src", "syslogtcp", SyslogTcpSource.class);    verifySourceCreation("multiport_syslogtcp-src", "multiport_syslogtcp", MultiportSyslogTCPSource.class);    verifySourceCreation("syslogudp-src", "syslogudp", SyslogUDPSource.class);    verifySourceCreation("spooldir-src", "spooldir", SpoolDirectorySource.class);    verifySourceCreation("http-src", "http", HTTPSource.class);    verifySourceCreation("thrift-src", "thrift", ThriftSource.class);    verifySourceCreation("custom-src", MockSource.class.getCanonicalName(), MockSource.class);}
public void flume_f3678_0()
{    context.put("keep-alive", "1");    context.put("capacity", "1000");    context.put("transactionCapacity", "1000");    Configurables.configure(channel, context);    rcs.setChannels(Lists.newArrayList(channel));    source = new ExecSource();    source.setChannelProcessor(new ChannelProcessor(rcs));}
public void flume_f3679_0()
{    source.stop();        ObjectName objName = null;    try {        objName = new ObjectName("org.apache.flume.source" + ":type=" + source.getName());        ManagementFactory.getPlatformMBeanServer().unregisterMBean(objName);    } catch (Exception ex) {        System.out.println("Failed to unregister the monitored counter: " + objName + ex.getMessage());    }}
public void flume_f3680_0() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        File inputFile = File.createTempFile("input", null);    File ouputFile = File.createTempFile("ouput", null);    FileUtils.forceDeleteOnExit(inputFile);    FileUtils.forceDeleteOnExit(ouputFile);        FileOutputStream outputStream1 = new FileOutputStream(inputFile);    for (int i = 0; i < 10; i++) {        outputStream1.write(RandomStringUtils.randomAlphanumeric(200).getBytes());        outputStream1.write('\n');    }    outputStream1.close();    String command = SystemUtils.IS_OS_WINDOWS ? String.format("cmd /c type %s", inputFile.getAbsolutePath()) : String.format("cat %s", inputFile.getAbsolutePath());    context.put("command", command);    context.put("keep-alive", "1");    context.put("capacity", "1000");    context.put("transactionCapacity", "1000");    Configurables.configure(source, context);    source.start();    Thread.sleep(2000);    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event;    FileOutputStream outputStream = new FileOutputStream(ouputFile);    while ((event = channel.take()) != null) {        outputStream.write(event.getBody());        outputStream.write('\n');    }    outputStream.close();    transaction.commit();    transaction.close();    Assert.assertEquals(FileUtils.checksumCRC32(inputFile), FileUtils.checksumCRC32(ouputFile));}
public void flume_f3681_0() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{    if (SystemUtils.IS_OS_WINDOWS) {        runTestShellCmdHelper("powershell -ExecutionPolicy Unrestricted -command", "1..5", new String[] { "1", "2", "3", "4", "5" });    } else {        runTestShellCmdHelper("/bin/bash -c", "seq 5", new String[] { "1", "2", "3", "4", "5" });    }}
public void flume_f3682_0() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        if (SystemUtils.IS_OS_WINDOWS) {        runTestShellCmdHelper("powershell -ExecutionPolicy Unrestricted -command", "$(1..5)", new String[] { "1", "2", "3", "4", "5" });    } else {        runTestShellCmdHelper("/bin/bash -c", "echo `seq 5`", new String[] { "1 2 3 4 5" });        runTestShellCmdHelper("/bin/bash -c", "echo $(seq 5)", new String[] { "1 2 3 4 5" });    }}
public void flume_f3683_0() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        String[] expected = { "1234", "abcd", "ijk", "xyz", "zzz" };        if (SystemUtils.IS_OS_WINDOWS) {        runTestShellCmdHelper("powershell -ExecutionPolicy Unrestricted -command", "'zzz','1234','xyz','abcd','ijk' | sort", expected);    } else {        runTestShellCmdHelper("/bin/bash -c", "echo zzz 1234 xyz abcd ijk | xargs -n1 echo | sort -f", expected);    }}
public void flume_f3684_0() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        if (SystemUtils.IS_OS_WINDOWS) {        runTestShellCmdHelper("powershell -ExecutionPolicy Unrestricted -command", "foreach ($i in 1..5) { $i }", new String[] { "1", "2", "3", "4", "5" });                runTestShellCmdHelper("powershell -ExecutionPolicy Unrestricted -command", "if(2+2 -gt 3) { 'good' } else { 'not good' } ", new String[] { "good" });    } else {        runTestShellCmdHelper("/bin/bash -c", "for i in {1..5}; do echo $i;done", new String[] { "1", "2", "3", "4", "5" });                runTestShellCmdHelper("/bin/bash -c", "if ((2+2>3)); " + "then  echo good; else echo not good; fi", new String[] { "good" });    }}
public void flume_f3685_0() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        String fileName = SystemUtils.IS_OS_WINDOWS ? "src\\test\\resources\\test_command.ps1" : "src/test/resources/test_command.txt";    BufferedReader reader = new BufferedReader(new FileReader(fileName));    try {        String shell = SystemUtils.IS_OS_WINDOWS ? "powershell -ExecutionPolicy Unrestricted -command" : "/bin/bash -c";        String command1 = reader.readLine();        Assert.assertNotNull(command1);        String[] output1 = new String[] { "'1'", "\"2\"", "\\3", "\\4" };        runTestShellCmdHelper(shell, command1, output1);        String command2 = reader.readLine();        Assert.assertNotNull(command2);        String[] output2 = new String[] { "1", "2", "3", "4", "5" };        runTestShellCmdHelper(shell, command2, output2);        String command3 = reader.readLine();        Assert.assertNotNull(command3);        String[] output3 = new String[] { "2", "3", "4", "5", "6" };        runTestShellCmdHelper(shell, command3, output3);    } finally {        reader.close();    }}
public void flume_f3686_0() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        if (SystemUtils.IS_OS_WINDOWS) {        runTestShellCmdHelper("powershell -ExecutionPolicy Unrestricted -command", "foreach ($i in 1..5) { $i }", new String[] { "1", "2", "3", "4", "5" });    } else {        runTestShellCmdHelper("/bin/bash -c", "for i in {1..5}; do echo $i;done", new String[] { "1", "2", "3", "4", "5" });    }    ObjectName objName = null;    try {        objName = new ObjectName("org.apache.flume.source" + ":type=" + source.getName());        MBeanServer mbeanServer = ManagementFactory.getPlatformMBeanServer();        String[] strAtts = { "Type", "EventReceivedCount", "EventAcceptedCount" };        AttributeList attrList = mbeanServer.getAttributes(objName, strAtts);        Assert.assertNotNull(attrList.get(0));        Assert.assertEquals("Expected Value: Type", "Type", ((Attribute) attrList.get(0)).getName());        Assert.assertEquals("Expected Value: SOURCE", "SOURCE", ((Attribute) attrList.get(0)).getValue());        Assert.assertNotNull(attrList.get(1));        Assert.assertEquals("Expected Value: EventReceivedCount", "EventReceivedCount", ((Attribute) attrList.get(1)).getName());        Assert.assertEquals("Expected Value: 5", "5", ((Attribute) attrList.get(1)).getValue().toString());        Assert.assertNotNull(attrList.get(2));        Assert.assertEquals("Expected Value: EventAcceptedCount", "EventAcceptedCount", ((Attribute) attrList.get(2)).getName());        Assert.assertEquals("Expected Value: 5", "5", ((Attribute) attrList.get(2)).getValue().toString());    } catch (Exception ex) {        System.out.println("Unable to retreive the monitored counter: " + objName + ex.getMessage());    }}
public void flume_f3687_0() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{    String filePath = "/tmp/flume-execsource." + Thread.currentThread().getId();    String eventBody = "TestMessage";    FileOutputStream outputStream = new FileOutputStream(filePath);    context.put(ExecSourceConfigurationConstants.CONFIG_BATCH_SIZE, "50000");    context.put(ExecSourceConfigurationConstants.CONFIG_BATCH_TIME_OUT, "750");    context.put("shell", SystemUtils.IS_OS_WINDOWS ? "powershell -ExecutionPolicy Unrestricted -command" : "/bin/bash -c");    context.put("command", SystemUtils.IS_OS_WINDOWS ? "Get-Content " + filePath + " | Select-Object -Last 10" : ("tail -f " + filePath));    Configurables.configure(source, context);    source.start();    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int lineNumber = 0; lineNumber < 3; lineNumber++) {        outputStream.write((eventBody).getBytes());        outputStream.write(String.valueOf(lineNumber).getBytes());        outputStream.write('\n');        outputStream.flush();    }    outputStream.close();    Thread.sleep(1500);    for (int i = 0; i < 3; i++) {        Event event = channel.take();        assertNotNull(event);        assertNotNull(event.getBody());        assertEquals(eventBody + String.valueOf(i), new String(event.getBody()));    }    transaction.commit();    transaction.close();    source.stop();    File file = new File(filePath);    FileUtils.forceDelete(file);}
private void flume_f3688_0(String shell, String command, String[] expectedOutput) throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{    context.put("shell", shell);    context.put("command", command);    Configurables.configure(source, context);    source.start();            Thread.sleep(2500);    Transaction transaction = channel.getTransaction();    transaction.begin();    try {        List<String> output = Lists.newArrayList();        Event event;        while ((event = channel.take()) != null) {            output.add(new String(event.getBody(), Charset.defaultCharset()));        }        transaction.commit();        Assert.assertArrayEquals(expectedOutput, output.toArray(new String[] {}));    } finally {        transaction.close();        source.stop();    }}
public void flume_f3689_0() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{    context.put(ExecSourceConfigurationConstants.CONFIG_RESTART_THROTTLE, "10");    context.put(ExecSourceConfigurationConstants.CONFIG_RESTART, "true");    context.put("command", SystemUtils.IS_OS_WINDOWS ? "cmd /c echo flume" : "echo flume");    Configurables.configure(source, context);    source.start();    Transaction transaction = channel.getTransaction();    transaction.begin();    long start = System.currentTimeMillis();    for (int i = 0; i < 5; i++) {        Event event = channel.take();        assertNotNull(event);        assertNotNull(event.getBody());        assertEquals("flume", new String(event.getBody(), Charsets.UTF_8));    }        assertTrue(System.currentTimeMillis() - start < 10000L);    transaction.commit();    transaction.close();    source.stop();}
public void flume_f3690_0() throws Exception
{        int seconds = 272;        boolean searchForCommand = true;    while (searchForCommand) {        searchForCommand = false;        String command = SystemUtils.IS_OS_WINDOWS ? "cmd /c sleep " + seconds : "sleep " + seconds;        String searchTxt = SystemUtils.IS_OS_WINDOWS ? "sleep.exe" : "\b" + command + "\b";        Pattern pattern = Pattern.compile(searchTxt);        for (String line : exec(SystemUtils.IS_OS_WINDOWS ? "cmd /c tasklist /FI \"SESSIONNAME eq Console\"" : "ps -ef")) {            if (pattern.matcher(line).find()) {                seconds++;                searchForCommand = true;                break;            }        }    }            String command = "sleep " + seconds;    Pattern pattern = Pattern.compile("\b" + command + "\b");    context.put(ExecSourceConfigurationConstants.CONFIG_RESTART, "false");    context.put("command", command);    Configurables.configure(source, context);    source.start();    Thread.sleep(1000L);    source.stop();    Thread.sleep(1000L);    for (String line : exec(SystemUtils.IS_OS_WINDOWS ? "cmd /c tasklist /FI \"SESSIONNAME eq Console\"" : "ps -ef")) {        if (pattern.matcher(line).find()) {            Assert.fail("Found [" + line + "]");        }    }}
private static List<String> flume_f3691_0(String command) throws Exception
{    String[] commandArgs = command.split("\\s+");    Process process = new ProcessBuilder(commandArgs).start();    BufferedReader reader = null;    try {        reader = new BufferedReader(new InputStreamReader(process.getInputStream()));        List<String> result = Lists.newArrayList();        String line;        while ((line = reader.readLine()) != null) {            result.add(line);        }        return result;    } finally {        process.destroy();        if (reader != null) {            reader.close();        }        int exit = process.waitFor();        if (exit != 0) {            throw new IllegalStateException("Command [" + command + "] exited with " + exit);        }    }}
private static final int flume_f3692_0() throws IOException
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
private byte[] flume_f3693_0(int counter)
{        String msg1 = "<10>" + stamp1 + " " + host1 + " " + data1 + " " + String.valueOf(counter) + "\n";    return msg1.getBytes();}
private List<Integer> flume_f3694_0(MultiportSyslogTCPSource source, Channel channel, List<Event> channelEvents, int numPorts, ChannelProcessor channelProcessor, BiConsumer<Integer, byte[]> eventSenderFuncton, Context additionalContext) throws IOException
{    Context channelContext = new Context();    channelContext.put("capacity", String.valueOf(2000));    channelContext.put("transactionCapacity", String.valueOf(2000));    Configurables.configure(channel, channelContext);    if (channelProcessor == null) {        List<Channel> channels = Lists.newArrayList();        channels.add(channel);        ChannelSelector rcs = new ReplicatingChannelSelector();        rcs.setChannels(channels);        source.setChannelProcessor(new ChannelProcessor(rcs));    } else {        source.setChannelProcessor(channelProcessor);    }    List<Integer> portList = new ArrayList<>(numPorts);    while (portList.size() < numPorts) {        int port = getFreePort();        if (!portList.contains(port)) {            portList.add(port);        }    }    StringBuilder ports = new StringBuilder();    for (int i = 0; i < numPorts; i++) {        ports.append(String.valueOf(portList.get(i))).append(" ");    }    Context context = new Context();    context.put(SyslogSourceConfigurationConstants.CONFIG_PORTS, ports.toString().trim());    context.put("portHeader", "port");    context.putAll(additionalContext.getParameters());    source.configure(context);    source.start();    for (int i = 0; i < numPorts; i++) {        eventSenderFuncton.accept(portList.get(i), getEvent(i));    }    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < numPorts; i++) {        Event e = channel.take();        if (e == null) {            throw new NullPointerException("Event is null");        }        channelEvents.add(e);    }    try {        txn.commit();    } catch (Throwable t) {        txn.rollback();    } finally {        txn.close();    }    return portList;}
public void flume_f3695_0() throws IOException, ParseException
{    MultiportSyslogTCPSource source = new MultiportSyslogTCPSource();    Channel channel = new MemoryChannel();    List<Event> channelEvents = new ArrayList<>();    int numPorts = 1000;    List<Integer> portList = testNPorts(source, channel, channelEvents, numPorts, null, getSimpleEventSender(), new Context());        processEvents(channelEvents, numPorts, portList);    source.stop();}
public void flume_f3696_0() throws Exception
{    SSLContext sslContext = SSLContext.getInstance("TLS");    sslContext.init(null, new TrustManager[] { new X509TrustManager() {        @Override        public void checkClientTrusted(X509Certificate[] certs, String s) {                }        @Override        public void checkServerTrusted(X509Certificate[] certs, String s) {                }        @Override        public X509Certificate[] getAcceptedIssuers() {            return new X509Certificate[0];        }    } }, null);    SocketFactory socketFactory = sslContext.getSocketFactory();    Context context = new Context();    context.put("ssl", "true");    context.put("keystore", "src/test/resources/server.p12");    context.put("keystore-password", "password");    context.put("keystore-type", "PKCS12");    MultiportSyslogTCPSource source = new MultiportSyslogTCPSource();    Channel channel = new MemoryChannel();    List<Event> channelEvents = new ArrayList<>();    int numPorts = 10;    List<Integer> portList = testNPorts(source, channel, channelEvents, numPorts, null, getSSLEventSender(socketFactory), context);        processEvents(channelEvents, numPorts, portList);    source.stop();}
public void flume_f3697_0(X509Certificate[] certs, String s)
{}
public void flume_f3698_0(X509Certificate[] certs, String s)
{}
public X509Certificate[] flume_f3699_0()
{    return new X509Certificate[0];}
private BiConsumer<Integer, byte[]> flume_f3700_0(SocketFactory socketFactory)
{    return (port, event) -> {        try {            Socket syslogSocket = socketFactory.createSocket(InetAddress.getLocalHost(), port);            syslogSocket.getOutputStream().write(event);            syslogSocket.close();        } catch (Exception e) {            e.printStackTrace();        }    };}
private BiConsumer<Integer, byte[]> flume_f3701_0()
{    return (Integer port, byte[] event) -> {        try {            Socket syslogSocket = new Socket(InetAddress.getLocalHost(), port);            syslogSocket.getOutputStream().write(event);            syslogSocket.close();        } catch (IOException e) {            e.printStackTrace();        }    };}
private void flume_f3702_0(List<Event> channelEvents, int numPorts, List<Integer> portList)
{    for (int i = 0; i < numPorts; i++) {        Iterator<Event> iter = channelEvents.iterator();        while (iter.hasNext()) {            Event e = iter.next();            Map<String, String> headers = e.getHeaders();                        Integer port = null;            if (headers.containsKey("port")) {                port = Integer.parseInt(headers.get("port"));            }            iter.remove();            Assert.assertEquals("Timestamps must match", String.valueOf(time.getMillis()), headers.get("timestamp"));            String host2 = headers.get("host");            Assert.assertEquals(host1, host2);            if (port != null) {                int num = portList.indexOf(port);                Assert.assertEquals(data1 + " " + String.valueOf(num), new String(e.getBody()));            }        }    }}
public void flume_f3703_0() throws CharacterCodingException
{    final int maxLen = 100;    IoBuffer savedBuf = IoBuffer.allocate(maxLen);    String origMsg = "<1>- - blah blam foo\n";    IoBuffer buf1 = IoBuffer.wrap(origMsg.substring(0, 11).getBytes(Charsets.UTF_8));    IoBuffer buf2 = IoBuffer.wrap(origMsg.substring(11, 16).getBytes(Charsets.UTF_8));    IoBuffer buf3 = IoBuffer.wrap(origMsg.substring(16, 21).getBytes(Charsets.UTF_8));    LineSplitter lineSplitter = new LineSplitter(maxLen);    ParsedBuffer parsedLine = new ParsedBuffer();    Assert.assertFalse("Incomplete line should not be parsed", lineSplitter.parseLine(buf1, savedBuf, parsedLine));    Assert.assertFalse("Incomplete line should not be parsed", lineSplitter.parseLine(buf2, savedBuf, parsedLine));    Assert.assertTrue("Completed line should be parsed", lineSplitter.parseLine(buf3, savedBuf, parsedLine));        Assert.assertEquals(origMsg.trim(), parsedLine.buffer.getString(Charsets.UTF_8.newDecoder()));    parsedLine.buffer.rewind();    MultiportSyslogHandler handler = new MultiportSyslogHandler(maxLen, 100, null, null, null, null, null, new ThreadSafeDecoder(Charsets.UTF_8), new ConcurrentHashMap<Integer, ThreadSafeDecoder>(), null);    Event event = handler.parseEvent(parsedLine, Charsets.UTF_8.newDecoder());    String body = new String(event.getBody(), Charsets.UTF_8);    Assert.assertEquals("Event body incorrect", origMsg.trim().substring(7), body);}
public void flume_f3704_0() throws FileNotFoundException, IOException
{    String header = "<10>2012-08-11T01:01:01Z localhost ";    String enBody = "Yarf yarf yarf";    String enMsg = header + enBody;    String frBody = "Comment " + "\u00EA" + "tes-vous?";    String frMsg = header + frBody;    String esBody = "¿Cómo estás?";    String esMsg = header + esBody;        MultiportSyslogHandler handler = new MultiportSyslogHandler(1000, 10, new ChannelProcessor(new ReplicatingChannelSelector()), new SourceCounter("test"), null, null, null, new ThreadSafeDecoder(Charsets.UTF_8), new ConcurrentHashMap<Integer, ThreadSafeDecoder>(), null);    ParsedBuffer parsedBuf = new ParsedBuffer();    parsedBuf.incomplete = false;        String[] bodies = { enBody, esBody, frBody };    String[] msgs = { enMsg, esMsg, frMsg };    Charset[] charsets = { Charsets.UTF_8, Charsets.ISO_8859_1 };    for (Charset charset : charsets) {        for (int i = 0; i < msgs.length; i++) {            String msg = msgs[i];            String body = bodies[i];            parsedBuf.buffer = IoBuffer.wrap(msg.getBytes(charset));            Event evt = handler.parseEvent(parsedBuf, charset.newDecoder());            String result = new String(evt.getBody(), charset);                        Assert.assertEquals(charset + " parse error: " + msg, body, result);            Assert.assertNull(evt.getHeaders().get(SyslogUtils.EVENT_STATUS));        }    }            byte[] badUtf8Seq = enMsg.getBytes(Charsets.ISO_8859_1);    int badMsgLen = badUtf8Seq.length;        badUtf8Seq[badMsgLen - 2] = (byte) 0xFE;        badUtf8Seq[badMsgLen - 1] = (byte) 0xFF;    parsedBuf.buffer = IoBuffer.wrap(badUtf8Seq);    Event evt = handler.parseEvent(parsedBuf, Charsets.UTF_8.newDecoder());    Assert.assertEquals("event body: " + new String(evt.getBody(), Charsets.ISO_8859_1) + " and my default charset = " + Charset.defaultCharset() + " with event = " + evt, SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), evt.getHeaders().get(SyslogUtils.EVENT_STATUS));    Assert.assertArrayEquals("Raw message data should be kept in body of event", badUtf8Seq, evt.getBody());    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(handler, "sourceCounter");    Assert.assertEquals(1, sc.getEventReadFail());}
public void flume_f3705_0() throws Exception
{        MultiportSyslogHandler handler = new MultiportSyslogHandler(1000, 10, new ChannelProcessor(new ReplicatingChannelSelector()), new SourceCounter("test"), null, null, null, new ThreadSafeDecoder(Charsets.UTF_8), new ConcurrentHashMap<Integer, ThreadSafeDecoder>(), null);    handler.exceptionCaught(null, new RuntimeException("dummy"));    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(handler, "sourceCounter");    Assert.assertEquals(1, sc.getGenericProcessingFail());}
private static Event flume_f3706_0(Channel channel)
{    Transaction txn = channel.getTransaction();    txn.begin();    Event evt = channel.take();    txn.commit();    txn.close();    return evt;}
public void flume_f3707_0() throws UnknownHostException, Exception
{            InetAddress localAddr = InetAddress.getLocalHost();    DefaultIoSessionDataStructureFactory dsFactory = new DefaultIoSessionDataStructureFactory();        int port1 = 10001;    NioSession session1 = mock(NioSession.class);    session1.setAttributeMap(dsFactory.getAttributeMap(session1));    SocketAddress sockAddr1 = new InetSocketAddress(localAddr, port1);    when(session1.getLocalAddress()).thenReturn(sockAddr1);        int port2 = 10002;    NioSession session2 = mock(NioSession.class);    session2.setAttributeMap(dsFactory.getAttributeMap(session2));    SocketAddress sockAddr2 = new InetSocketAddress(localAddr, port2);    when(session2.getLocalAddress()).thenReturn(sockAddr2);        ConcurrentMap<Integer, ThreadSafeDecoder> portCharsets = new ConcurrentHashMap<Integer, ThreadSafeDecoder>();    portCharsets.put(port1, new ThreadSafeDecoder(Charsets.ISO_8859_1));    portCharsets.put(port2, new ThreadSafeDecoder(Charsets.UTF_8));                MemoryChannel chan = new MemoryChannel();    chan.configure(new Context());    chan.start();    ReplicatingChannelSelector sel = new ReplicatingChannelSelector();    sel.setChannels(Lists.<Channel>newArrayList(chan));    ChannelProcessor chanProc = new ChannelProcessor(sel);        MultiportSyslogHandler handler = new MultiportSyslogHandler(1000, 10, chanProc, new SourceCounter("test"), null, null, null, new ThreadSafeDecoder(Charsets.UTF_8), portCharsets, null);        handler.sessionCreated(session1);    handler.sessionCreated(session2);                String header = "<10>2012-08-17T02:14:00-07:00 192.168.1.110 ";        String dangerousChars = "þÿÀÁ";            String msg;    IoBuffer buf;    Event evt;        msg = header + dangerousChars + "\n";    buf = IoBuffer.wrap(msg.getBytes(Charsets.ISO_8859_1));    handler.messageReceived(session1, buf);    evt = takeEvent(chan);    Assert.assertNotNull("Event vanished!", evt);    Assert.assertNull(evt.getHeaders().get(SyslogUtils.EVENT_STATUS));        msg = header + dangerousChars + "\n";    buf = IoBuffer.wrap(msg.getBytes(Charsets.ISO_8859_1));    handler.messageReceived(session2, buf);    evt = takeEvent(chan);    Assert.assertNotNull("Event vanished!", evt);    Assert.assertEquals("Expected invalid event due to character encoding", SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), evt.getHeaders().get(SyslogUtils.EVENT_STATUS));        msg = header + dangerousChars + "\n";    buf = IoBuffer.wrap(msg.getBytes(Charsets.UTF_8));    handler.messageReceived(session2, buf);    evt = takeEvent(chan);    Assert.assertNotNull("Event vanished!", evt);    Assert.assertNull(evt.getHeaders().get(SyslogUtils.EVENT_STATUS));    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(handler, "sourceCounter");    Assert.assertEquals(1, sc.getEventReadFail());}
public void flume_f3708_0() throws Exception
{    MultiportSyslogTCPSource source = new MultiportSyslogTCPSource();    Channel channel = new MemoryChannel();    List<Event> channelEvents = new ArrayList<>();    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(new ChannelException("dummy")).doNothing().when(cp).processEventBatch(anyListOf(Event.class));    try {        testNPorts(source, channel, channelEvents, 1, cp, getSimpleEventSender(), new Context());    } catch (Exception e) {        }    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(source, "sourceCounter");    Assert.assertEquals(1, sc.getChannelWriteFail());    source.stop();}
public void flume_f3709_0() throws IOException
{    String testClientIPHeader = "testClientIPHeader";    String testClientHostnameHeader = "testClientHostnameHeader";    MultiportSyslogTCPSource source = new MultiportSyslogTCPSource();    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = Lists.newArrayList();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    int port = getFreePort();    Context context = new Context();    context.put("host", InetAddress.getLoopbackAddress().getHostAddress());    context.put("ports", String.valueOf(port));    context.put("clientIPHeader", testClientIPHeader);    context.put("clientHostnameHeader", testClientHostnameHeader);    source.configure(context);    source.start();        Socket syslogSocket = new Socket(InetAddress.getLoopbackAddress().getHostAddress(), port);    syslogSocket.getOutputStream().write(getEvent(0));    Event e = takeEvent(channel);    source.stop();    Map<String, String> headers = e.getHeaders();    checkHeader(headers, testClientIPHeader, InetAddress.getLoopbackAddress().getHostAddress());    checkHeader(headers, testClientHostnameHeader, InetAddress.getLoopbackAddress().getHostName());}
private static void flume_f3710_0(Map<String, String> headers, String headerName, String expectedValue)
{    assertTrue("Missing event header: " + headerName, headers.containsKey(headerName));    assertEquals("Event header value does not match: " + headerName, expectedValue, headers.get(headerName));}
private static int flume_f3711_0()
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    } catch (IOException e) {        throw new AssertionError("Can not find free port.", e);    }}
public void flume_f3712_0() throws UnknownHostException
{    localhost = InetAddress.getByName("127.0.0.1");    source = new NetcatSource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));}
public void flume_f3713_0() throws InterruptedException, IOException
{    String encoding = "UTF-16BE";    startSource(encoding, "false", "1", "512");    Socket netcatSocket = new Socket(localhost, selectedPort);    try {                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, english, encoding);            Assert.assertArrayEquals("Channel contained our event", english.getBytes(defaultCharset), getFlumeEvent());        }                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, french, encoding);            Assert.assertArrayEquals("Channel contained our event", french.getBytes(defaultCharset), getFlumeEvent());        }    } finally {        netcatSocket.close();        stopSource();    }}
public void flume_f3714_0() throws InterruptedException, IOException
{    String encoding = "UTF-16LE";    startSource(encoding, "false", "1", "512");    Socket netcatSocket = new Socket(localhost, selectedPort);    try {                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, english, encoding);            Assert.assertArrayEquals("Channel contained our event", english.getBytes(defaultCharset), getFlumeEvent());        }                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, french, encoding);            Assert.assertArrayEquals("Channel contained our event", french.getBytes(defaultCharset), getFlumeEvent());        }    } finally {        netcatSocket.close();        stopSource();    }}
public void flume_f3715_0() throws InterruptedException, IOException
{    String encoding = "UTF-8";    startSource(encoding, "false", "1", "512");    Socket netcatSocket = new Socket(localhost, selectedPort);    try {                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, english, encoding);            Assert.assertArrayEquals("Channel contained our event", english.getBytes(defaultCharset), getFlumeEvent());        }                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, french, encoding);            Assert.assertArrayEquals("Channel contained our event", french.getBytes(defaultCharset), getFlumeEvent());        }    } finally {        netcatSocket.close();        stopSource();    }}
public void flume_f3716_0() throws InterruptedException, IOException
{    String encoding = "ISO-8859-1";    startSource(encoding, "false", "1", "512");    Socket netcatSocket = new Socket(localhost, selectedPort);    try {                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, english, encoding);            Assert.assertArrayEquals("Channel contained our event", english.getBytes(defaultCharset), getFlumeEvent());        }                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, french, encoding);            Assert.assertArrayEquals("Channel contained our event", french.getBytes(defaultCharset), getFlumeEvent());        }    } finally {        netcatSocket.close();        stopSource();    }}
public void flume_f3717_0() throws InterruptedException, IOException
{    String encoding = "UTF-8";    String ackEvent = "OK";    startSource(encoding, "true", "1", "512");    Socket netcatSocket = new Socket(localhost, selectedPort);    LineIterator inputLineIterator = IOUtils.lineIterator(netcatSocket.getInputStream(), encoding);    try {                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, english, encoding);            Assert.assertArrayEquals("Channel contained our event", english.getBytes(defaultCharset), getFlumeEvent());            Assert.assertEquals("Socket contained the Ack", ackEvent, inputLineIterator.nextLine());        }                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, french, encoding);            Assert.assertArrayEquals("Channel contained our event", french.getBytes(defaultCharset), getFlumeEvent());            Assert.assertEquals("Socket contained the Ack", ackEvent, inputLineIterator.nextLine());        }    } finally {        netcatSocket.close();        stopSource();    }}
public void flume_f3718_0() throws InterruptedException, IOException
{    String encoding = "UTF-8";    startSource(encoding, "false", "1", "10");    Socket netcatSocket = new Socket(localhost, selectedPort);    try {        sendEvent(netcatSocket, "123456789", encoding);        Assert.assertArrayEquals("Channel contained our event", "123456789".getBytes(defaultCharset), getFlumeEvent());        sendEvent(netcatSocket, english, encoding);        Assert.assertEquals("Channel does not contain an event", null, getRawFlumeEvent());    } finally {        netcatSocket.close();        stopSource();    }}
public void flume_f3719_0() throws InterruptedException, IOException
{    String encoding = "UTF-8";    String ackEvent = "OK";    String ackErrorEvent = "FAILED: Event exceeds the maximum length (10 chars, including newline)";    startSource(encoding, "true", "1", "10");    Socket netcatSocket = new Socket(localhost, selectedPort);    LineIterator inputLineIterator = IOUtils.lineIterator(netcatSocket.getInputStream(), encoding);    try {        sendEvent(netcatSocket, "123456789", encoding);        Assert.assertArrayEquals("Channel contained our event", "123456789".getBytes(defaultCharset), getFlumeEvent());        Assert.assertEquals("Socket contained the Ack", ackEvent, inputLineIterator.nextLine());        sendEvent(netcatSocket, english, encoding);        Assert.assertEquals("Channel does not contain an event", null, getRawFlumeEvent());        Assert.assertEquals("Socket contained the Error Ack", ackErrorEvent, inputLineIterator.nextLine());    } finally {        netcatSocket.close();        stopSource();    }}
public void flume_f3720_0() throws InterruptedException, IOException
{    boolean isFlumeExceptionThrown = false;        try (ServerSocketChannel dummyServerSocket = ServerSocketChannel.open()) {        dummyServerSocket.socket().setReuseAddress(true);        dummyServerSocket.socket().bind(new InetSocketAddress("0.0.0.0", 10500));        Context context = new Context();        context.put("port", String.valueOf(10500));        context.put("bind", "0.0.0.0");        context.put("ack-every-event", "false");        Configurables.configure(source, context);        source.start();    } catch (FlumeException fe) {        isFlumeExceptionThrown = true;    }            Assert.assertTrue("Flume exception is thrown as port already in use", isFlumeExceptionThrown);    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
private void flume_f3721_0(String encoding, String ack, String batchSize, String maxLineLength) throws InterruptedException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort = getFreePort()));    context.put("bind", "0.0.0.0");    context.put("ack-every-event", ack);    context.put("encoding", encoding);    context.put("batch-size", batchSize);    context.put("max-line-length", maxLineLength);    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());}
private void flume_f3722_0(Socket socket, String content, String encoding) throws IOException
{    OutputStream output = socket.getOutputStream();    IOUtils.write(content + IOUtils.LINE_SEPARATOR_UNIX, output, encoding);    output.flush();}
private byte[] flume_f3723_1()
{    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    try {        transaction.commit();    } catch (Throwable t) {        transaction.rollback();    } finally {        transaction.close();    }        return event.getBody();}
private Event flume_f3724_1()
{    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    try {        transaction.commit();    } catch (Throwable t) {        transaction.rollback();    } finally {        transaction.close();    }        return event;}
private void flume_f3725_1() throws InterruptedException
{    source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());    }
private void flume_f3726_0()
{    source = new NetcatUdpSource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    Context context = new Context();    context.put("port", String.valueOf(TEST_NETCAT_PORT));    source.configure(context);}
private void flume_f3727_1(String data1) throws IOException
{    init();    source.start();        DatagramSocket socket;    DatagramPacket datagramPacket;    datagramPacket = new DatagramPacket(data1.getBytes(), data1.getBytes().length, InetAddress.getLocalHost(), source.getSourcePort());    for (int i = 0; i < 10; i++) {        socket = new DatagramSocket();        socket.send(datagramPacket);        socket.close();    }    List<Event> channelEvents = new ArrayList<Event>();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 10; i++) {        Event e = channel.take();        Assert.assertNotNull(e);        channelEvents.add(e);    }    try {        txn.commit();    } catch (Throwable t) {        txn.rollback();    } finally {        txn.close();    }    source.stop();    for (Event e : channelEvents) {        Assert.assertNotNull(e);        String str = new String(e.getBody(), Charsets.UTF_8);                Assert.assertArrayEquals(data1.getBytes(), e.getBody());    }}
public void flume_f3728_0() throws Exception
{    init();    source.start();        byte[] largePayload = getPayload(1000).getBytes();    DatagramSocket socket;    DatagramPacket datagramPacket;    datagramPacket = new DatagramPacket(largePayload, 1000, InetAddress.getLocalHost(), source.getSourcePort());    for (int i = 0; i < 10; i++) {        socket = new DatagramSocket();        socket.send(datagramPacket);        socket.close();    }    List<Event> channelEvents = new ArrayList<Event>();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 10; i++) {        Event e = channel.take();        Assert.assertNotNull(e);        channelEvents.add(e);    }    try {        txn.commit();    } catch (Throwable t) {        txn.rollback();    } finally {        txn.close();    }    source.stop();    for (Event e : channelEvents) {        Assert.assertNotNull(e);        Assert.assertArrayEquals(largePayload, e.getBody());    }}
public void flume_f3729_0() throws IOException
{    runUdpTest(shortString);}
public void flume_f3730_0() throws IOException
{    runUdpTest(mediumString);}
private String flume_f3731_0(int length)
{    StringBuilder payload = new StringBuilder(length);    for (int n = 0; n < length; ++n) {        payload.append("x");    }    return payload.toString();}
public void flume_f3732_0()
{    sourceRunner = new PollableSourceRunner();}
public void flume_f3733_1() throws InterruptedException
{    final Channel channel = new MemoryChannel();    final CountDownLatch latch = new CountDownLatch(50);    Configurables.configure(channel, new Context());    final ChannelSelector cs = new ReplicatingChannelSelector();    cs.setChannels(Lists.newArrayList(channel));    PollableSource source = new PollableSource() {        private String name;        private ChannelProcessor cp = new ChannelProcessor(cs);        @Override        public Status process() throws EventDeliveryException {            Transaction transaction = channel.getTransaction();            try {                transaction.begin();                Event event = EventBuilder.withBody(String.valueOf("Event " + latch.getCount()).getBytes());                latch.countDown();                if (latch.getCount() % 20 == 0) {                    throw new EventDeliveryException("I don't like event:" + event);                }                channel.put(event);                transaction.commit();                return Status.READY;            } catch (EventDeliveryException e) {                                transaction.rollback();                return Status.BACKOFF;            } finally {                transaction.close();            }        }        @Override        public long getBackOffSleepIncrement() {            return PollableSourceConstants.DEFAULT_BACKOFF_SLEEP_INCREMENT;        }        @Override        public long getMaxBackOffSleepInterval() {            return PollableSourceConstants.DEFAULT_MAX_BACKOFF_SLEEP;        }        @Override        public void start() {                }        @Override        public void stop() {                }        @Override        public LifecycleState getLifecycleState() {                        return null;        }        @Override        public void setName(String name) {            this.name = name;        }        @Override        public String getName() {            return name;        }        @Override        public void setChannelProcessor(ChannelProcessor channelProcessor) {            cp = channelProcessor;        }        @Override        public ChannelProcessor getChannelProcessor() {            return cp;        }    };    sourceRunner.setSource(source);    sourceRunner.start();    latch.await();    sourceRunner.stop();}
public Status flume_f3734_1() throws EventDeliveryException
{    Transaction transaction = channel.getTransaction();    try {        transaction.begin();        Event event = EventBuilder.withBody(String.valueOf("Event " + latch.getCount()).getBytes());        latch.countDown();        if (latch.getCount() % 20 == 0) {            throw new EventDeliveryException("I don't like event:" + event);        }        channel.put(event);        transaction.commit();        return Status.READY;    } catch (EventDeliveryException e) {                transaction.rollback();        return Status.BACKOFF;    } finally {        transaction.close();    }}
public long flume_f3735_0()
{    return PollableSourceConstants.DEFAULT_BACKOFF_SLEEP_INCREMENT;}
public long flume_f3736_0()
{    return PollableSourceConstants.DEFAULT_MAX_BACKOFF_SLEEP;}
public void flume_f3737_0()
{}
public void flume_f3738_0()
{}
public LifecycleState flume_f3739_0()
{        return null;}
public void flume_f3740_0(String name)
{    this.name = name;}
public String flume_f3741_0()
{    return name;}
public void flume_f3742_0(ChannelProcessor channelProcessor)
{    cp = channelProcessor;}
public ChannelProcessor flume_f3743_0()
{    return cp;}
public void flume_f3744_0()
{    source = new SequenceGeneratorSource();    source.setName(TestSequenceGeneratorSource.class.getCanonicalName());}
public void flume_f3745_0() throws org.apache.flume.EventDeliveryException
{    final int DOPROCESS_LOOPS = 5;    Context context = new Context();    Configurables.configure(source, context);    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    source.setChannelProcessor(cp);    source.start();    for (int i = 0; i < DOPROCESS_LOOPS; i++) {        source.process();    }    source.stop();}
public void flume_f3746_0() throws EventDeliveryException
{    final int BATCH_SIZE = 1;    final int TOTAL_EVENTS = 5;    final int DOPROCESS_LOOPS = 10;    Context context = new Context();    context.put("batchSize", Integer.toString(BATCH_SIZE));    context.put("totalEvents", Integer.toString(TOTAL_EVENTS));    Configurables.configure(source, context);    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    Mockito.doNothing().doThrow(    ChannelException.class).doNothing().when(cp).processEvent(Mockito.any(Event.class));    source.setChannelProcessor(cp);    source.start();    for (int i = 0; i < DOPROCESS_LOOPS; i++) {        source.process();    }    ArgumentCaptor<Event> argumentCaptor = ArgumentCaptor.forClass(Event.class);    Mockito.verify(cp, Mockito.times(6)).processEvent(argumentCaptor.capture());    Mockito.verify(cp, Mockito.never()).processEventBatch(Mockito.anyListOf(Event.class));    verifyEventSequence(TOTAL_EVENTS, argumentCaptor.getAllValues());}
public void flume_f3747_0() throws EventDeliveryException
{    final int BATCH_SIZE = 3;    final int TOTAL_EVENTS = 5;    final int DOPROCESS_LOOPS = 10;    Context context = new Context();    context.put("batchSize", Integer.toString(BATCH_SIZE));    context.put("totalEvents", Integer.toString(TOTAL_EVENTS));    Configurables.configure(source, context);    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    Mockito.doNothing().doThrow(    ChannelException.class).doNothing().when(cp).processEventBatch(Mockito.anyListOf(Event.class));    source.setChannelProcessor(cp);    source.start();    for (int i = 0; i < DOPROCESS_LOOPS; i++) {        source.process();    }    ArgumentCaptor<List<Event>> argumentCaptor = ArgumentCaptor.forClass((Class) List.class);    Mockito.verify(cp, Mockito.never()).processEvent(Mockito.any(Event.class));    Mockito.verify(cp, Mockito.times(3)).processEventBatch(argumentCaptor.capture());    List<List<Event>> eventBatches = argumentCaptor.getAllValues();    verifyEventSequence(TOTAL_EVENTS, flatOutBatches(eventBatches));}
private static void flume_f3748_0(int expectedTotalEvents, List<Event> actualEvents)
{    Set<Integer> uniqueEvents = new LinkedHashSet<>();    for (Event e : actualEvents) {        uniqueEvents.add(Integer.parseInt(new String(e.getBody())));    }    List<Integer> sortedFilteredEvents = new ArrayList<>(uniqueEvents);    Collections.sort(sortedFilteredEvents);    Assert.assertEquals("mismatching number of events", expectedTotalEvents, sortedFilteredEvents.size());    for (int i = 0; i < sortedFilteredEvents.size(); ++i) {        Assert.assertEquals("missing or unexpected event body", i, (int) sortedFilteredEvents.get(i));    }}
private static List<Event> flume_f3749_0(List<List<Event>> eventBatches)
{    List<Event> events = new ArrayList<>();    for (List<Event> le : eventBatches) {        for (Event e : le) {            events.add(e);        }    }    return events;}
public void flume_f3750_0()
{    source = new SpoolDirectorySource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    tmpDir = Files.createTempDir();}
public void flume_f3751_0()
{    deleteFiles(tmpDir);    tmpDir.delete();}
private void flume_f3752_0(File directory)
{    for (File f : directory.listFiles()) {        if (f.isDirectory()) {            deleteFiles(f);            f.delete();        } else {            f.delete();        }    }}
public void flume_f3753_0()
{    Context context = new Context();    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.CONSUME_ORDER, "undefined");    Configurables.configure(source, context);}
public void flume_f3754_0()
{    Context context = new Context();    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.CONSUME_ORDER, "oLdESt");    Configurables.configure(source, context);    context.put(SpoolDirectorySourceConfigurationConstants.CONSUME_ORDER, "yoUnGest");    Configurables.configure(source, context);    context.put(SpoolDirectorySourceConfigurationConstants.CONSUME_ORDER, "rAnDom");    Configurables.configure(source, context);}
public void flume_f3755_0() throws IOException, InterruptedException
{    Context context = new Context();    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.FILENAME_HEADER, "true");    context.put(SpoolDirectorySourceConfigurationConstants.FILENAME_HEADER_KEY, "fileHeaderKeyTest");    Configurables.configure(source, context);    source.start();    while (source.getSourceCounter().getEventAcceptedCount() < 8) {        Thread.sleep(10);    }    Transaction txn = channel.getTransaction();    txn.begin();    Event e = channel.take();    Assert.assertNotNull("Event must not be null", e);    Assert.assertNotNull("Event headers must not be null", e.getHeaders());    Assert.assertNotNull(e.getHeaders().get("fileHeaderKeyTest"));    Assert.assertEquals(f1.getAbsolutePath(), e.getHeaders().get("fileHeaderKeyTest"));    txn.commit();    txn.close();}
public void flume_f3756_0() throws IOException, InterruptedException
{    Context context = new Context();    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.BASENAME_HEADER, "true");    context.put(SpoolDirectorySourceConfigurationConstants.BASENAME_HEADER_KEY, "basenameHeaderKeyTest");    Configurables.configure(source, context);    source.start();    while (source.getSourceCounter().getEventAcceptedCount() < 8) {        Thread.sleep(10);    }    Transaction txn = channel.getTransaction();    txn.begin();    Event e = channel.take();    Assert.assertNotNull("Event must not be null", e);    Assert.assertNotNull("Event headers must not be null", e.getHeaders());    Assert.assertNotNull(e.getHeaders().get("basenameHeaderKeyTest"));    Assert.assertEquals(f1.getName(), e.getHeaders().get("basenameHeaderKeyTest"));    txn.commit();    txn.close();}
public void flume_f3757_0() throws IOException, InterruptedException
{    File subDir = new File(tmpDir, "directorya/directoryb/directoryc");    boolean directoriesCreated = subDir.mkdirs();    Assert.assertTrue("source directories must be created", directoriesCreated);    final String FILE_NAME = "recursion_file.txt";    File f1 = new File(subDir, FILE_NAME);    String origBody = "file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n";    Files.write(origBody, f1, Charsets.UTF_8);    Context context = new Context();    context.put(SpoolDirectorySourceConfigurationConstants.RECURSIVE_DIRECTORY_SEARCH,     "true");    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY,     tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.FILENAME_HEADER,     "true");    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Recursion setting in source is correct", source.getRecursiveDirectorySearch());    Transaction txn = channel.getTransaction();    txn.begin();    long startTime = System.currentTimeMillis();    Event e = null;    while (System.currentTimeMillis() - startTime < 300 && e == null) {        e = channel.take();        Thread.sleep(10);    }    Assert.assertNotNull("Event must not be null", e);    Assert.assertNotNull("Event headers must not be null", e.getHeaders());    Assert.assertTrue("File header value did not end with expected filename", e.getHeaders().get("file").endsWith(FILE_NAME));    ByteArrayOutputStream baos = new ByteArrayOutputStream();    do {                baos.write(e.getBody());                baos.write('\n');        e = channel.take();    } while (e != null);    Assert.assertEquals("Event body is correct", Arrays.toString(origBody.getBytes()), Arrays.toString(baos.toByteArray()));    txn.commit();    txn.close();}
public void flume_f3758_0() throws IOException, InterruptedException
{    Context context = new Context();    File subDir = new File(tmpDir, "directory");    boolean directoriesCreated = subDir.mkdirs();    Assert.assertTrue("source directories must be created", directoriesCreated);    File f1 = new File(subDir.getAbsolutePath() + "/file1.txt");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    context.put(SpoolDirectorySourceConfigurationConstants.RECURSIVE_DIRECTORY_SEARCH, "false");    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.FILENAME_HEADER, "true");    context.put(SpoolDirectorySourceConfigurationConstants.FILENAME_HEADER_KEY, "fileHeaderKeyTest");    Configurables.configure(source, context);    source.start();        Assert.assertFalse("Recursion setting in source is not set to false (this" + "test does not want recursion enabled)", source.getRecursiveDirectorySearch());    Transaction txn = channel.getTransaction();    txn.begin();    long startTime = System.currentTimeMillis();    Event e = null;    while (System.currentTimeMillis() - startTime < 300 && e == null) {        e = channel.take();        Thread.sleep(10);    }    Assert.assertNull("Event must be null", e);    txn.commit();    txn.close();}
public void flume_f3759_0() throws IOException, InterruptedException
{    Context context = new Context();    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    Configurables.configure(source, context);    for (int i = 0; i < 10; i++) {        source.start();        Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));        Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());        source.stop();        Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));        Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());    }}
public void flume_f3760_0() throws InterruptedException, IOException
{    final int NUM_RECONFIGS = 20;    for (int i = 0; i < NUM_RECONFIGS; i++) {        Context context = new Context();        File file = new File(tmpDir.getAbsolutePath() + "/file-" + i);        Files.write("File " + i, file, Charsets.UTF_8);        context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());        Configurables.configure(source, context);        source.start();        Thread.sleep(TimeUnit.SECONDS.toMillis(1));        Transaction txn = channel.getTransaction();        txn.begin();        try {            Event event = channel.take();            String content = new String(event.getBody(), Charsets.UTF_8);            Assert.assertEquals("File " + i, content);            txn.commit();        } catch (Throwable t) {            txn.rollback();        } finally {            txn.close();        }        source.stop();        Assert.assertFalse("Fatal error on iteration " + i, source.hasFatalError());    }}
public void flume_f3761_0() throws Exception
{    Context chContext = new Context();    chContext.put("capacity", "2");    chContext.put("transactionCapacity", "2");    chContext.put("keep-alive", "0");    channel.stop();    Configurables.configure(channel, chContext);    channel.start();    Context context = new Context();    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.BATCH_SIZE, "2");    Configurables.configure(source, context);    source.setBackOff(false);    source.start();        long startTime = System.currentTimeMillis();    while (System.currentTimeMillis() - startTime < 5000 && !source.didHitChannelFullException()) {        Thread.sleep(10);    }    Assert.assertTrue("Expected to hit ChannelFullException, but did not!", source.didHitChannelFullException());    List<String> dataOut = Lists.newArrayList();    for (int i = 0; i < 8; ) {        Transaction tx = channel.getTransaction();        tx.begin();        Event e = channel.take();        if (e != null) {            dataOut.add(new String(e.getBody(), "UTF-8"));            i++;        }        e = channel.take();        if (e != null) {            dataOut.add(new String(e.getBody(), "UTF-8"));            i++;        }        tx.commit();        tx.close();    }    Assert.assertEquals(8, dataOut.size());    source.stop();}
public void flume_f3762_0() throws IOException, InterruptedException
{    Context context = new Context();    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\n", f1, Charsets.UTF_8);    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    File f3 = new File(tmpDir.getAbsolutePath() + "/file3");    File f4 = new File(tmpDir.getAbsolutePath() + "/file4");    Files.touch(f2);    Files.touch(f3);    Files.touch(f4);    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    Configurables.configure(source, context);    source.start();        Thread.sleep(5000);    Assert.assertFalse("Server did not error", source.hasFatalError());    Assert.assertEquals("Four messages were read", 4, source.getSourceCounter().getEventAcceptedCount());    source.stop();}
public void flume_f3763_0() throws InterruptedException, IOException
{    Context context = new Context();    File[] f = new File[10];    for (int i = 0; i < 10; i++) {        f[i] = new File(tmpDir.getAbsolutePath() + "/file" + i);        Files.write(new byte[0], f[i]);    }    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.FILENAME_HEADER, "true");    context.put(SpoolDirectorySourceConfigurationConstants.FILENAME_HEADER_KEY, "fileHeaderKeyTest");    Configurables.configure(source, context);    source.start();    Thread.sleep(10);    for (int i = 0; i < 10; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        Event e = channel.take();        Assert.assertNotNull("Event must not be null", e);        Assert.assertNotNull("Event headers must not be null", e.getHeaders());        Assert.assertNotNull(e.getHeaders().get("fileHeaderKeyTest"));        Assert.assertEquals(f[i].getAbsolutePath(), e.getHeaders().get("fileHeaderKeyTest"));        Assert.assertArrayEquals(new byte[0], e.getBody());        txn.commit();        txn.close();    }    source.stop();}
public void flume_f3764_0() throws InterruptedException, IOException
{    Context context = new Context();    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("some data".getBytes(), f1);    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    Files.write(new byte[0], f2);    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    Configurables.configure(source, context);    source.start();    Thread.sleep(10);    for (int i = 0; i < 2; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        Event e = channel.take();        txn.commit();        txn.close();    }    Transaction txn = channel.getTransaction();    txn.begin();    Assert.assertNull(channel.take());    txn.commit();    txn.close();    source.stop();}
private SourceCounter flume_f3765_0()
{    SourceCounter sc = new SourceCounter("dummy");    sc.start();    Context context = new Context();    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    Configurables.configure(source, context);    return sc;}
public void flume_f3766_0() throws Exception
{    SourceCounter sc = errorCounterCommonInit();    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    Mockito.doThrow(new ChannelException("dummy")).doThrow(new ChannelFullException("dummy")).doThrow(new RuntimeException("runtime")).when(cp).processEventBatch(Matchers.anyListOf(Event.class));    source.setChannelProcessor(cp);    ReliableSpoolingFileEventReader reader = Mockito.mock(ReliableSpoolingFileEventReader.class);    List<Event> events = new ArrayList<>();    events.add(Mockito.mock(Event.class));    Mockito.doReturn(events).doReturn(events).doReturn(events).doThrow(new IOException("dummy")).when(reader).readEvents(Mockito.anyInt());    Runnable runner = source.new SpoolDirectoryRunnable(reader, sc);    try {        runner.run();    } catch (Exception ex) {        }    Assert.assertEquals(2, sc.getChannelWriteFail());    Assert.assertEquals(1, sc.getGenericProcessingFail());}
public void flume_f3767_0() throws Exception
{    SourceCounter sc = errorCounterCommonInit();    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    source.setChannelProcessor(cp);    ReliableSpoolingFileEventReader reader = Mockito.mock(ReliableSpoolingFileEventReader.class);    List<Event> events = new ArrayList<>();    events.add(Mockito.mock(Event.class));    Mockito.doReturn(events).doThrow(new IOException("dummy")).when(reader).readEvents(Mockito.anyInt());    Runnable runner = source.new SpoolDirectoryRunnable(reader, sc);    try {        runner.run();    } catch (Exception ex) {        }    Assert.assertEquals(1, sc.getEventReadFail());}
public void flume_f3768_0()
{    mockProcessor = mock(ChannelProcessor.class);}
private Event flume_f3769_0(StressSource source)
{    return field("event").ofType(Event.class).in(source).get();}
private List<Event> flume_f3770_0(StressSource source)
{    return field("eventBatchListToProcess").ofType(List.class).in(source).get();}
private CounterGroup flume_f3771_0(StressSource source)
{    return field("counterGroup").ofType(CounterGroup.class).in(source).get();}
public void flume_f3772_0() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();    context.put("maxTotalEvents", "35");    source.configure(context);    source.start();    for (int i = 0; i < 50; i++) {        source.process();    }    verify(mockProcessor, times(35)).processEvent(getEvent(source));}
public void flume_f3773_0() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();    context.put("maxTotalEvents", "20");    context.put("maxEventsPerSecond", "20");    source.configure(context);    long startTime = System.currentTimeMillis();    source.start();    for (int i = 0; i < 20; i++) {        source.process();    }    long finishTime = System.currentTimeMillis();        Assert.assertTrue(finishTime - startTime < 1300);    Assert.assertTrue(finishTime - startTime > 700);    source.stop();}
public void flume_f3774_0() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();        context = new Context();    context.put("maxTotalEvents", "20");    context.put("maxEventsPerSecond", "0");    source.configure(context);    long startTime = System.currentTimeMillis();    source.start();    for (int i = 0; i <= 20; i++) {        source.process();    }    long finishTime = System.currentTimeMillis();    Assert.assertTrue(finishTime - startTime < 70);}
public void flume_f3775_0() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();    context.put("maxTotalEvents", "20");    context.put("maxEventsPerSecond", "20");    context.put("batchSize", "3");    source.configure(context);    long startTime = System.currentTimeMillis();    source.start();    for (int i = 0; i < 20; i++) {        source.process();    }    long finishTime = System.currentTimeMillis();        Assert.assertTrue(finishTime - startTime < 1300);    Assert.assertTrue(finishTime - startTime > 700);    source.stop();}
public void flume_f3776_0() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();        context.put("maxTotalEvents", "20");    context.put("maxEventsPerSecond", "0");    source.configure(context);    long startTime = System.currentTimeMillis();    source.start();    for (int i = 0; i <= 20; i++) {        source.process();    }    long finishTime = System.currentTimeMillis();    Assert.assertTrue(finishTime - startTime < 70);}
public void flume_f3777_0() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();    context.put("maxTotalEvents", "35");    context.put("batchSize", "10");    source.configure(context);    source.start();    for (int i = 0; i < 50; i++) {        if (source.process() == Status.BACKOFF) {            TestCase.assertTrue("Source should have sent all events in 4 batches", i == 4);            break;        }        if (i < 3) {            verify(mockProcessor, times(i + 1)).processEventBatch(getLastProcessedEventList(source));        } else {            verify(mockProcessor, times(1)).processEventBatch(getLastProcessedEventList(source));        }    }    long successfulEvents = getCounterGroup(source).get("events.successful");    TestCase.assertTrue("Number of successful events should be 35 but was " + successfulEvents, successfulEvents == 35);    long failedEvents = getCounterGroup(source).get("events.failed");    TestCase.assertTrue("Number of failure events should be 0 but was " + failedEvents, failedEvents == 0);}
public void flume_f3778_0() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();    context.put("batchSize", "10");    source.configure(context);    source.start();    for (int i = 0; i < 10; i++) {        Assert.assertFalse("StressSource with no maxTotalEvents should not return " + Status.BACKOFF, source.process() == Status.BACKOFF);    }    verify(mockProcessor, times(10)).processEventBatch(getLastProcessedEventList(source));    long successfulEvents = getCounterGroup(source).get("events.successful");    TestCase.assertTrue("Number of successful events should be 100 but was " + successfulEvents, successfulEvents == 100);    long failedEvents = getCounterGroup(source).get("events.failed");    TestCase.assertTrue("Number of failure events should be 0 but was " + failedEvents, failedEvents == 0);}
public void flume_f3779_0() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();    context.put("maxSuccessfulEvents", "35");    source.configure(context);    source.start();    for (int i = 0; i < 10; i++) {        source.process();    }        doThrow(new ChannelException("stub")).when(mockProcessor).processEvent(getEvent(source));    source.process();    doNothing().when(mockProcessor).processEvent(getEvent(source));    for (int i = 0; i < 10; i++) {        source.process();    }        doThrow(new ChannelException("stub")).when(mockProcessor).processEvent(getEvent(source));    source.process();    doNothing().when(mockProcessor).processEvent(getEvent(source));    for (int i = 0; i < 50; i++) {        source.process();    }            verify(mockProcessor, times(37)).processEvent(getEvent(source));}
public void flume_f3780_0()
{    final String[] examples = { "1985-04-12T23:20:50.52Z", "1985-04-12T19:20:50.52-04:00", "2003-10-11T22:14:15.003Z", "2003-08-24T05:14:15.000003-07:00", "2012-04-13T11:11:11-08:00", "2012-04-13T08:08:08.0001+00:00", "2012-04-13T08:08:08.251+00:00" };    SyslogParser parser = new SyslogParser();    DateTimeFormatter jodaParser = ISODateTimeFormat.dateTimeParser();    for (String ex : examples) {        Assert.assertEquals("Problem parsing date string: " + ex, jodaParser.parseMillis(ex), parser.parseRfc5424Date(ex));    }}
public void flume_f3781_0()
{    SyslogParser parser = new SyslogParser();    Charset charset = Charsets.UTF_8;    List<String> messages = Lists.newArrayList();        messages.add("<34>Oct 11 22:14:15 mymachine su: 'su root' failed for " + "lonvick on /dev/pts/8");    messages.add("<13>Feb  5 17:32:18 10.0.0.99 Use the BFG!");    messages.add("<165>Aug 24 05:34:00 CST 1987 mymachine myproc[10]: %% " + "It's time to make the do-nuts.  %%  Ingredients: Mix=OK, Jelly=OK # " + "Devices: Mixer=OK, Jelly_Injector=OK, Frier=OK # Transport: " + "Conveyer1=OK, Conveyer2=OK # %%");    messages.add("<0>Oct 22 10:52:12 scapegoat 1990 Oct 22 10:52:01 TZ-6 " + "scapegoat.dmz.example.org 10.1.2.3 sched[0]: That's All Folks!");        messages.add("<34>1 2003-10-11T22:14:15.003Z mymachine.example.com su - " + "ID47 - BOM'su root' failed for lonvick on /dev/pts/8");    messages.add("<165>1 2003-08-24T05:14:15.000003-07:00 192.0.2.1 myproc " + "8710 - - %% It's time to make the do-nuts.");        messages.add("<13>2003-08-24T05:14:15Z localhost snarf?");    messages.add("<13>2012-08-16T14:34:03-08:00 127.0.0.1 test shnap!");        for (String msg : messages) {        Set<String> keepFields = new HashSet<String>();        Event event = parser.parseMessage(msg, charset, keepFields);        Assert.assertNull("Failure to parse known-good syslog message", event.getHeaders().get(SyslogUtils.EVENT_STATUS));    }        for (String msg : messages) {        Set<String> keepFields = new HashSet<String>();        keepFields.add(SyslogUtils.KEEP_FIELDS_ALL);        Event event = parser.parseMessage(msg, charset, keepFields);        Assert.assertArrayEquals(event.getBody(), msg.getBytes());        Assert.assertNull("Failure to parse known-good syslog message", event.getHeaders().get(SyslogUtils.EVENT_STATUS));    }        for (String msg : messages) {        Set<String> keepFields = new HashSet<String>();        keepFields.add(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS_HOSTNAME);        Event event = parser.parseMessage(msg, charset, keepFields);        Assert.assertTrue("Failure to persist hostname", new String(event.getBody()).contains(event.getHeaders().get("host")));        Assert.assertNull("Failure to parse known-good syslog message", event.getHeaders().get(SyslogUtils.EVENT_STATUS));    }}
private void flume_f3782_0(String keepFields)
{    init(keepFields, new Context());}
private void flume_f3783_0(String keepFields, Context context)
{    source = new SyslogTcpSource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    context.put("host", InetAddress.getLoopbackAddress().getHostAddress());    context.put("port", String.valueOf(TEST_SYSLOG_PORT));    context.put("keepFields", keepFields);    source.configure(context);}
private void flume_f3784_0()
{    Context context = new Context();    context.put("ssl", "true");    context.put("keystore", "src/test/resources/server.p12");    context.put("keystore-password", "password");    context.put("keystore-type", "PKCS12");    init("none", context);}
private void flume_f3785_1(String keepFields) throws IOException
{    init(keepFields);    source.start();        InetSocketAddress addr = source.getBoundAddress();    for (int i = 0; i < 10; i++) {        try (Socket syslogSocket = new Socket(addr.getAddress(), addr.getPort())) {            syslogSocket.getOutputStream().write(bodyWithTandH.getBytes());        }    }    List<Event> channelEvents = new ArrayList<>();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 10; i++) {        Event e = channel.take();        if (e == null) {            throw new NullPointerException("Event is null");        }        channelEvents.add(e);    }    try {        txn.commit();    } catch (Throwable t) {        txn.rollback();    } finally {        txn.close();    }    source.stop();    for (Event e : channelEvents) {        Assert.assertNotNull(e);        String str = new String(e.getBody(), Charsets.UTF_8);                if (keepFields.equals("true") || keepFields.equals("all")) {            Assert.assertArrayEquals(bodyWithTandH.trim().getBytes(), e.getBody());        } else if (keepFields.equals("false") || keepFields.equals("none")) {            Assert.assertArrayEquals(data1.getBytes(), e.getBody());        } else if (keepFields.equals("hostname")) {            Assert.assertArrayEquals(bodyWithHostname.getBytes(), e.getBody());        } else if (keepFields.equals("timestamp")) {            Assert.assertArrayEquals(bodyWithTimestamp.getBytes(), e.getBody());        }    }}
public void flume_f3786_0() throws IOException
{    runKeepFieldsTest("all");        runKeepFieldsTest("true");}
public void flume_f3787_0() throws IOException
{    runKeepFieldsTest("none");        runKeepFieldsTest("false");}
public void flume_f3788_0() throws IOException
{    runKeepFieldsTest("hostname");}
public void flume_f3789_0() throws IOException
{    runKeepFieldsTest("timestamp");}
public void flume_f3790_0() throws IOException
{    runKeepFieldsTest("all");    assertEquals(10, source.getSourceCounter().getEventAcceptedCount());    assertEquals(10, source.getSourceCounter().getEventReceivedCount());}
public void flume_f3791_0() throws Exception
{    init("true");    errorCounterCommon(new ChannelException("dummy"));    for (int i = 0; i < 10 && source.getSourceCounter().getChannelWriteFail() == 0; i++) {        Thread.sleep(100);    }    assertEquals(1, source.getSourceCounter().getChannelWriteFail());}
public void flume_f3792_0() throws Exception
{    init("true");    errorCounterCommon(new RuntimeException("dummy"));    for (int i = 0; i < 10 && source.getSourceCounter().getEventReadFail() == 0; i++) {        Thread.sleep(100);    }    assertEquals(1, source.getSourceCounter().getEventReadFail());}
private void flume_f3793_0(Exception e) throws IOException
{    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(e).when(cp).processEvent(any(Event.class));    source.setChannelProcessor(cp);    source.start();        InetSocketAddress addr = source.getBoundAddress();    try (Socket syslogSocket = new Socket(addr.getAddress(), addr.getPort())) {        syslogSocket.getOutputStream().write(bodyWithTandH.getBytes());    }}
public void flume_f3794_0() throws Exception
{    initSsl();    source.start();    InetSocketAddress address = source.getBoundAddress();    SSLContext sslContext = SSLContext.getInstance("TLS");    sslContext.init(null, new TrustManager[] { new X509TrustManager() {        @Override        public void checkClientTrusted(X509Certificate[] certs, String s) {                }        @Override        public void checkServerTrusted(X509Certificate[] certs, String s) {                }        @Override        public X509Certificate[] getAcceptedIssuers() {            return new X509Certificate[0];        }    } }, null);    SocketFactory socketFactory = sslContext.getSocketFactory();    Socket socket = socketFactory.createSocket();    socket.connect(address);    OutputStream outputStream = socket.getOutputStream();    outputStream.write(bodyWithTandH.getBytes());    socket.close();        Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    assertEquals(new String(event.getBody()), data1);    transaction.commit();    transaction.close();}
public void flume_f3795_0(X509Certificate[] certs, String s)
{}
public void flume_f3796_0(X509Certificate[] certs, String s)
{}
public X509Certificate[] flume_f3797_0()
{    return new X509Certificate[0];}
public void flume_f3798_0() throws IOException
{    String testClientIPHeader = "testClientIPHeader";    String testClientHostnameHeader = "testClientHostnameHeader";    Context context = new Context();    context.put("clientIPHeader", testClientIPHeader);    context.put("clientHostnameHeader", testClientHostnameHeader);    init("none", context);    source.start();        InetSocketAddress addr = source.getBoundAddress();    Socket syslogSocket = new Socket(addr.getAddress(), addr.getPort());    syslogSocket.getOutputStream().write(bodyWithTandH.getBytes());    Transaction txn = channel.getTransaction();    txn.begin();    Event e = channel.take();    try {        txn.commit();    } catch (Throwable t) {        txn.rollback();    } finally {        txn.close();    }    source.stop();    Map<String, String> headers = e.getHeaders();    checkHeader(headers, testClientIPHeader, InetAddress.getLoopbackAddress().getHostAddress());    checkHeader(headers, testClientHostnameHeader, InetAddress.getLoopbackAddress().getHostName());}
private static void flume_f3799_0(Map<String, String> headers, String headerName, String expectedValue)
{    assertTrue("Missing event header: " + headerName, headers.containsKey(headerName));    assertEquals("Event header value does not match: " + headerName, expectedValue, headers.get(headerName));}
private void flume_f3800_0(String keepFields)
{    init(keepFields, new Context());}
private void flume_f3801_0(String keepFields, Context context)
{    source = new SyslogUDPSource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    context.put("host", InetAddress.getLoopbackAddress().getHostAddress());    context.put("port", String.valueOf(TEST_SYSLOG_PORT));    context.put("keepFields", keepFields);    source.configure(context);}
private void flume_f3802_1(String keepFields) throws IOException
{    init(keepFields);    source.start();        DatagramPacket datagramPacket = createDatagramPacket(bodyWithTandH.getBytes());    for (int i = 0; i < 10; i++) {        sendDatagramPacket(datagramPacket);    }    List<Event> channelEvents = new ArrayList<>();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 10; i++) {        Event e = channel.take();        Assert.assertNotNull(e);        channelEvents.add(e);    }    commitAndCloseTransaction(txn);    source.stop();    for (Event e : channelEvents) {        Assert.assertNotNull(e);        String str = new String(e.getBody(), Charsets.UTF_8);                if (keepFields.equals("true") || keepFields.equals("all")) {            Assert.assertArrayEquals(bodyWithTandH.trim().getBytes(), e.getBody());        } else if (keepFields.equals("false") || keepFields.equals("none")) {            Assert.assertArrayEquals(data1.getBytes(), e.getBody());        } else if (keepFields.equals("hostname")) {            Assert.assertArrayEquals(bodyWithHostname.getBytes(), e.getBody());        } else if (keepFields.equals("timestamp")) {            Assert.assertArrayEquals(bodyWithTimestamp.getBytes(), e.getBody());        }    }}
public void flume_f3803_0() throws Exception
{    init("true");    source.start();        byte[] largePayload = getPayload(1000).getBytes();    DatagramPacket datagramPacket = createDatagramPacket(largePayload);    for (int i = 0; i < 10; i++) {        sendDatagramPacket(datagramPacket);    }    List<Event> channelEvents = new ArrayList<>();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 10; i++) {        Event e = channel.take();        Assert.assertNotNull(e);        channelEvents.add(e);    }    commitAndCloseTransaction(txn);    source.stop();    for (Event e : channelEvents) {        Assert.assertNotNull(e);        Assert.assertArrayEquals(largePayload, e.getBody());    }}
public void flume_f3804_0() throws IOException
{    runKeepFieldsTest("all");        runKeepFieldsTest("true");}
public void flume_f3805_0() throws IOException
{    runKeepFieldsTest("none");        runKeepFieldsTest("false");}
public void flume_f3806_0() throws IOException
{    runKeepFieldsTest("hostname");}
public void flume_f3807_0() throws IOException
{    runKeepFieldsTest("timestamp");}
public void flume_f3808_0() throws Exception
{    init("true");    doCounterCommon();        for (int i = 0; i < 10 && source.getSourceCounter().getEventAcceptedCount() == 0; i++) {        Thread.sleep(100);    }    Assert.assertEquals(1, source.getSourceCounter().getEventAcceptedCount());    Assert.assertEquals(1, source.getSourceCounter().getEventReceivedCount());}
private void flume_f3809_0() throws IOException, InterruptedException
{    source.start();    DatagramPacket datagramPacket = createDatagramPacket("test".getBytes());    sendDatagramPacket(datagramPacket);}
public void flume_f3810_0() throws Exception
{    init("true");    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(new ChannelException("dummy")).when(cp).processEvent(any(Event.class));    source.setChannelProcessor(cp);    doCounterCommon();    for (int i = 0; i < 10 && source.getSourceCounter().getChannelWriteFail() == 0; i++) {        Thread.sleep(100);    }    Assert.assertEquals(1, source.getSourceCounter().getChannelWriteFail());}
public void flume_f3811_0() throws Exception
{    init("true");    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(new RuntimeException("dummy")).when(cp).processEvent(any(Event.class));    source.setChannelProcessor(cp);    doCounterCommon();    for (int i = 0; i < 10 && source.getSourceCounter().getEventReadFail() == 0; i++) {        Thread.sleep(100);    }    Assert.assertEquals(1, source.getSourceCounter().getEventReadFail());}
private DatagramPacket flume_f3812_0(byte[] payload)
{    InetSocketAddress addr = source.getBoundAddress();    return new DatagramPacket(payload, payload.length, addr.getAddress(), addr.getPort());}
private void flume_f3813_0(DatagramPacket datagramPacket) throws IOException
{    try (DatagramSocket syslogSocket = new DatagramSocket()) {        syslogSocket.send(datagramPacket);    }}
private void flume_f3814_1(Transaction txn)
{    try {        txn.commit();    } catch (Throwable t) {                txn.rollback();    } finally {        txn.close();    }}
private String flume_f3815_0(int length)
{    StringBuilder payload = new StringBuilder(length);    for (int n = 0; n < length; ++n) {        payload.append("x");    }    return payload.toString();}
public void flume_f3816_0() throws IOException
{    String testClientIPHeader = "testClientIPHeader";    String testClientHostnameHeader = "testClientHostnameHeader";    Context context = new Context();    context.put("clientIPHeader", testClientIPHeader);    context.put("clientHostnameHeader", testClientHostnameHeader);    init("none", context);    source.start();    DatagramPacket datagramPacket = createDatagramPacket(bodyWithTandH.getBytes());    sendDatagramPacket(datagramPacket);    Transaction txn = channel.getTransaction();    txn.begin();    Event e = channel.take();    commitAndCloseTransaction(txn);    source.stop();    Map<String, String> headers = e.getHeaders();    checkHeader(headers, testClientIPHeader, InetAddress.getLoopbackAddress().getHostAddress());    checkHeader(headers, testClientHostnameHeader, InetAddress.getLoopbackAddress().getHostName());}
private static void flume_f3817_0(Map<String, String> headers, String headerName, String expectedValue)
{    assertTrue("Missing event header: " + headerName, headers.containsKey(headerName));    assertEquals("Event header value does not match: " + headerName, expectedValue, headers.get(headerName));}
public void flume_f3818_0() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ssZ";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";        String msg1 = "<10>" + stamp1 + "+08:00" + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1 + "+0800", format1, host1, data1);}
public void flume_f3819_0() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ss";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";    String msg1 = "<10>1 " + stamp1 + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1, format1, host1, data1);}
public void flume_f3820_0() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ssZ";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";        String msg1 = "<10>1 " + stamp1 + "Z" + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1 + "+0000", format1, host1, data1);}
public void flume_f3821_0() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ssZ";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";        String msg1 = "<10>1 " + stamp1 + "+08:00" + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1 + "+0800", format1, host1, data1);}
public void flume_f3822_0() throws ParseException
{    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";        String msg1 = "<10>1 " + "-" + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, null, null, host1, data1);}
public void flume_f3823_0() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ss";    String host1 = "-";    String data1 = "some msg";        String msg1 = "<10>1 " + stamp1 + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1, format1, null, data1);}
public void flume_f3824_0() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ssZ";    String host1 = "-";    String data1 = "some msg";        String msg1 = "<10>1 " + stamp1 + "Z" + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1 + "+0000", format1, null, data1);}
public void flume_f3825_0() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ssZ";    String host1 = "-";    String data1 = "some msg";        String msg1 = "<10>1 " + stamp1 + "+08:00" + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1 + "+0800", format1, null, data1);}
public void flume_f3826_0() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11.999";    String format1 = "yyyy-MM-dd'T'HH:mm:ss.S";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";    String msg1 = "<10>1 " + stamp1 + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1, format1, host1, data1);}
public void flume_f3827_0() throws ParseException
{    SimpleDateFormat sdf = new SimpleDateFormat("MMM  d hh:MM:ss", Locale.ENGLISH);    Calendar cal = Calendar.getInstance();    String year = String.valueOf(cal.get(Calendar.YEAR));    String stamp1 = sdf.format(cal.getTime());    String format1 = "yyyyMMM d HH:mm:ss";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";        String msg1 = "<10>" + stamp1 + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, year + stamp1, format1, host1, data1);}
public void flume_f3828_0() throws ParseException
{    SimpleDateFormat sdf = new SimpleDateFormat("MMM  d hh:MM:ss", Locale.ENGLISH);    Calendar cal = Calendar.getInstance();    String year = String.valueOf(cal.get(Calendar.YEAR));    String stamp1 = sdf.format(cal.getTime());    String format1 = "yyyyMMM d HH:mm:ss";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";        String msg1 = "<10>" + stamp1 + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, year + stamp1, format1, host1, data1);}
public void flume_f3829_0() throws ParseException
{            String inputStamp = "2014-10-03T17:20:01.123456-07:00";    String outputStamp = "2014-10-03T17:20:01.123-07:00";    String format1 = "yyyy-MM-dd'T'HH:mm:ss.S";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";    String msg1 = "<10>" + inputStamp + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, outputStamp, format1, host1, data1);}
public void flume_f3830_0() throws ParseException
{    SimpleDateFormat sdf = new SimpleDateFormat("MMM  d hh:MM:ss", Locale.ENGLISH);    Calendar cal = Calendar.getInstance();    String year = String.valueOf(cal.get(Calendar.YEAR));    String stamp1 = sdf.format(cal.getTime());    String format1 = "yyyyMMM d HH:mm:ss";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "- hyphen_null_breaks_5424_pattern [07/Jun/2012:14:46:44 -0600]";    String msg1 = "<10>" + stamp1 + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, year + stamp1, format1, host1, data1);}
public void flume_f3831_0() throws ParseException
{        for (int monthOffset = 0; monthOffset <= 13; monthOffset++) {        Clock mockClock = Clock.fixed(LocalDateTime.now().plusMonths(monthOffset).toInstant(ZoneOffset.UTC), Clock.systemDefaultZone().getZone());                for (int i = -10; i <= 1; i++) {            SimpleDateFormat sdf = new SimpleDateFormat("MMM  d hh:MM:ss", Locale.ENGLISH);            Date date = new Date(mockClock.millis());            Calendar cal = Calendar.getInstance();            cal.setTime(date);            cal.add(Calendar.MONTH, i);                        if (i == 1) {                cal.add(Calendar.DAY_OF_MONTH, -1);            }            String stamp1 = sdf.format(cal.getTime());            String year = String.valueOf(cal.get(Calendar.YEAR));            String format1 = "yyyyMMM d HH:mm:ss";            String host1 = "ubuntu-11.cloudera.com";            String data1 = "some msg";                        String msg1 = "<10>" + stamp1 + " " + host1 + " " + data1 + "\n";            checkHeader(msg1, year + stamp1, format1, host1, data1, mockClock);        }    }}
public static void flume_f3832_0(String keepFields, String msg1, String stamp1, String format1, String host1, String data1, Clock clock) throws ParseException
{    SyslogUtils util;    if (keepFields == null || keepFields.isEmpty()) {        util = new SyslogUtils(SyslogUtils.DEFAULT_SIZE, new HashSet<String>(), false, clock);    } else {        util = new SyslogUtils(SyslogUtils.DEFAULT_SIZE, SyslogUtils.chooseFieldsToKeep(keepFields), false, clock);    }    ChannelBuffer buff = ChannelBuffers.buffer(200);    buff.writeBytes(msg1.getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers2 = e.getHeaders();    if (stamp1 == null) {        Assert.assertFalse(headers2.containsKey("timestamp"));    } else {        SimpleDateFormat formater = new SimpleDateFormat(format1, Locale.ENGLISH);        Assert.assertEquals(String.valueOf(formater.parse(stamp1).getTime()), headers2.get("timestamp"));    }    if (host1 == null) {        Assert.assertFalse(headers2.containsKey("host"));    } else {        String host2 = headers2.get("host");        Assert.assertEquals(host2, host1);    }    Assert.assertEquals(data1, new String(e.getBody()));}
public static void flume_f3833_0(String keepFields, String msg1, String stamp1, String format1, String host1, String data1) throws ParseException
{    checkHeader(keepFields, msg1, stamp1, format1, host1, data1, Clock.system(Clock.systemDefaultZone().getZone()));}
public static void flume_f3834_0(String msg1, String stamp1, String format1, String host1, String data1, Clock clock) throws ParseException
{    checkHeader("none", msg1, stamp1, format1, host1, data1, clock);}
public static void flume_f3835_0(String msg1, String stamp1, String format1, String host1, String data1) throws ParseException
{    checkHeader("none", msg1, stamp1, format1, host1, data1, Clock.system(Clock.systemDefaultZone().getZone()));}
public void flume_f3836_0()
{    String badData1 = "<10F> bad bad data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes(badData1.getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData1.trim(), new String(e.getBody()).trim());}
public void flume_f3837_0()
{    String badData1 = "hi guys! <10> bad bad data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes(badData1.getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData1.trim(), new String(e.getBody()).trim());}
public void flume_f3838_0()
{    String badData1 = "<> bad bad data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes(badData1.getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData1.trim(), new String(e.getBody()).trim());}
public void flume_f3839_0()
{    String badData1 = "<123123123123123123123123123123> bad bad data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes(badData1.getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData1.trim(), new String(e.getBody()).trim());}
public void flume_f3840_0()
{    String priority = "<10>";    String goodData1 = "Good good good data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes((priority + goodData1).getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("1", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("2", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(null, headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(priority + goodData1.trim(), new String(e.getBody()).trim());}
public void flume_f3841_0()
{    String badData1 = "hi guys! <10F> bad bad data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes(badData1.getBytes());    String priority = "<10>";    String goodData1 = "Good good good data\n";    buff.writeBytes((priority + goodData1).getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData1.trim(), new String(e.getBody()).trim());    Event e2 = util.extractEvent(buff);    if (e2 == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers2 = e2.getHeaders();    Assert.assertEquals("1", headers2.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("2", headers2.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(null, headers2.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(priority + goodData1.trim(), new String(e2.getBody()).trim());}
public void flume_f3842_0()
{    String badData1 = "hi guys! <10F> bad bad data\n";    String priority = "<10>";    String goodData1 = "Good good good data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes((priority + goodData1).getBytes());    buff.writeBytes(badData1.getBytes());    Event e2 = util.extractEvent(buff);    if (e2 == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers2 = e2.getHeaders();    Assert.assertEquals("1", headers2.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("2", headers2.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(null, headers2.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(priority + goodData1.trim(), new String(e2.getBody()).trim());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData1.trim(), new String(e.getBody()).trim());}
public void flume_f3843_0()
{    String badData1 = "hi guys! <10F> bad bad data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes(badData1.getBytes());    String badData2 = "hi guys! <20> bad bad data\n";    buff.writeBytes((badData2).getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData1.trim(), new String(e.getBody()).trim());    Event e2 = util.extractEvent(buff);    if (e2 == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers2 = e2.getHeaders();    Assert.assertEquals("0", headers2.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers2.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers2.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData2.trim(), new String(e2.getBody()).trim());}
public void flume_f3844_0()
{    String priority = "<10>";    String goodData1 = "Good good good data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes((priority + goodData1).getBytes());    String priority2 = "<20>";    String goodData2 = "Good really good data\n";    buff.writeBytes((priority2 + goodData2).getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("1", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("2", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(null, headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(priority + goodData1.trim(), new String(e.getBody()).trim());    Event e2 = util.extractEvent(buff);    if (e2 == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers2 = e2.getHeaders();    Assert.assertEquals("2", headers2.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("4", headers2.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(null, headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(priority2 + goodData2.trim(), new String(e2.getBody()).trim());}
public void flume_f3845_0()
{    String badData1 = "<10> bad bad data bad bad\n";        SyslogUtils util = new SyslogUtils(5, null, false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes(badData1.getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("1", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("2", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INCOMPLETE.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals("<10> bad b".trim(), new String(e.getBody()).trim());    Event e2 = util.extractEvent(buff);    if (e2 == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers2 = e2.getHeaders();    Assert.assertEquals("0", headers2.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers2.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers2.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals("ad data ba".trim(), new String(e2.getBody()).trim());}
public void flume_f3846_0() throws Exception
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ssZ";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";        String msg1 = "<10>1 " + stamp1 + "+08:00" + " " + host1 + " " + data1 + "\n";    checkHeader("none", msg1, stamp1 + "+0800", format1, host1, data1);    checkHeader("false", msg1, stamp1 + "+0800", format1, host1, data1);    String data2 = "ubuntu-11.cloudera.com some msg";    checkHeader("hostname", msg1, stamp1 + "+0800", format1, host1, data2);    String data3 = "2012-04-13T11:11:11+08:00 ubuntu-11.cloudera.com some msg";    checkHeader("timestamp hostname", msg1, stamp1 + "+0800", format1, host1, data3);    String data4 = "<10>2012-04-13T11:11:11+08:00 ubuntu-11.cloudera.com some msg";    checkHeader("priority timestamp hostname", msg1, stamp1 + "+0800", format1, host1, data4);    String data5 = "<10>1 2012-04-13T11:11:11+08:00 ubuntu-11.cloudera.com some msg";    checkHeader("priority version timestamp hostname", msg1, stamp1 + "+0800", format1, host1, data5);    checkHeader("all", msg1, stamp1 + "+0800", format1, host1, data5);    checkHeader("true", msg1, stamp1 + "+0800", format1, host1, data5);}
public void flume_f3847_0()
{    SocketAddress socketAddress = new InetSocketAddress("localhost", 2000);    String ip = SyslogUtils.getIP(socketAddress);    assertEquals("127.0.0.1", ip);}
public void flume_f3848_0()
{    SocketAddress socketAddress = null;    String ip = SyslogUtils.getIP(socketAddress);    assertEquals("", ip);}
public void flume_f3849_0()
{    SocketAddress socketAddress = new SocketAddress() {    };    String ip = SyslogUtils.getIP(socketAddress);    assertEquals("", ip);}
public void flume_f3850_0()
{    SocketAddress socketAddress = new InetSocketAddress("127.0.0.1", 2000);    String hostname = SyslogUtils.getHostname(socketAddress);    assertEquals("localhost", hostname);}
public void flume_f3851_0()
{    SocketAddress socketAddress = null;    String hostname = SyslogUtils.getHostname(socketAddress);    assertEquals("", hostname);}
public void flume_f3852_0()
{    SocketAddress socketAddress = new SocketAddress() {    };    String hostname = SyslogUtils.getHostname(socketAddress);    assertEquals("", hostname);}
public void flume_f3853_0() throws IOException
{    try (ServerSocket socket = new ServerSocket(0)) {        port = socket.getLocalPort();    }    props.clear();    props.setProperty("hosts", "h1");    props.setProperty("hosts.h1", "0.0.0.0:" + String.valueOf(port));    props.setProperty(RpcClientConfigurationConstants.CONFIG_BATCH_SIZE, "10");    props.setProperty(RpcClientConfigurationConstants.CONFIG_REQUEST_TIMEOUT, "2000");    channel = new MemoryChannel();    source = new ThriftSource();}
public void flume_f3854_0() throws Exception
{    source.stop();}
private void flume_f3855_0()
{    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));}
public void flume_f3856_0() throws Exception
{    Context context = new Context();    channel.configure(context);    configureSource();    context.put(ThriftSource.CONFIG_BIND, "0.0.0.0");    context.put(ThriftSource.CONFIG_PORT, String.valueOf(port));    context.put("ssl", "true");    context.put("keystore", "src/test/resources/keystorefile.jks");    context.put("keystore-password", "password");    context.put("keystore-type", "JKS");    Configurables.configure(source, context);    doAppendSSL();}
public void flume_f3857_0() throws Exception
{    System.setProperty("javax.net.ssl.keyStore", "src/test/resources/keystorefile.jks");    System.setProperty("javax.net.ssl.keyStorePassword", "password");    System.setProperty("javax.net.ssl.keyStoreType", "JKS");    Context context = new Context();    channel.configure(context);    configureSource();    context.put(ThriftSource.CONFIG_BIND, "0.0.0.0");    context.put(ThriftSource.CONFIG_PORT, String.valueOf(port));    context.put("ssl", "true");    Configurables.configure(source, context);    doAppendSSL();    System.clearProperty("javax.net.ssl.keyStore");    System.clearProperty("javax.net.ssl.keyStorePassword");    System.clearProperty("javax.net.ssl.keyStoreType");}
private void flume_f3858_0() throws EventDeliveryException
{    Properties sslprops = (Properties) props.clone();    sslprops.put("ssl", "true");    sslprops.put("truststore", "src/test/resources/truststorefile.jks");    sslprops.put("truststore-password", "password");    client = RpcClientFactory.getThriftInstance(sslprops);    source.start();    for (int i = 0; i < 30; i++) {        client.append(EventBuilder.withBody(String.valueOf(i).getBytes()));    }    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 30; i++) {        Event event = channel.take();        Assert.assertNotNull(event);        Assert.assertEquals(String.valueOf(i), new String(event.getBody()));    }    transaction.commit();    transaction.close();}
public void flume_f3859_0() throws Exception
{    client = RpcClientFactory.getThriftInstance(props);    Context context = new Context();    channel.configure(context);    configureSource();    context.put(ThriftSource.CONFIG_BIND, "0.0.0.0");    context.put(ThriftSource.CONFIG_PORT, String.valueOf(port));    Configurables.configure(source, context);    source.start();    for (int i = 0; i < 30; i++) {        client.append(EventBuilder.withBody(String.valueOf(i).getBytes()));    }    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 30; i++) {        Event event = channel.take();        Assert.assertNotNull(event);        Assert.assertEquals(String.valueOf(i), new String(event.getBody()));    }    transaction.commit();    transaction.close();}
public void flume_f3860_0() throws Exception
{    client = RpcClientFactory.getThriftInstance(props);    Context context = new Context();    context.put("capacity", "1000");    context.put("transactionCapacity", "1000");    channel.configure(context);    configureSource();    context.put(ThriftSource.CONFIG_BIND, "0.0.0.0");    context.put(ThriftSource.CONFIG_PORT, String.valueOf(port));    Configurables.configure(source, context);    source.start();    for (int i = 0; i < 30; i++) {        List<Event> events = Lists.newArrayList();        for (int j = 0; j < 10; j++) {            Map<String, String> hdrs = Maps.newHashMap();            hdrs.put("time", String.valueOf(System.currentTimeMillis()));            events.add(EventBuilder.withBody(String.valueOf(i).getBytes(), hdrs));        }        client.appendBatch(events);    }    Transaction transaction = channel.getTransaction();    transaction.begin();    long after = System.currentTimeMillis();    List<Integer> events = Lists.newArrayList();    for (int i = 0; i < 300; i++) {        Event event = channel.take();        Assert.assertNotNull(event);        Assert.assertTrue(Long.valueOf(event.getHeaders().get("time")) <= after);        events.add(Integer.parseInt(new String(event.getBody())));    }    transaction.commit();    transaction.close();    Collections.sort(events);    int index = 0;        for (int i = 0; i < 30; i++) {        for (int j = 0; j < 10; j++) {            Assert.assertEquals(i, events.get(index++).intValue());        }    }}
public void flume_f3861_0() throws Exception
{    client = RpcClientFactory.getThriftInstance(props);    Context context = new Context();    context.put("capacity", "3000");    context.put("transactionCapacity", "3000");    channel.configure(context);    configureSource();    context.put(ThriftSource.CONFIG_BIND, "0.0.0.0");    context.put(ThriftSource.CONFIG_PORT, String.valueOf(port));    Configurables.configure(source, context);    source.start();    for (int i = 0; i < 5; i++) {        List<Event> events = Lists.newArrayList();        for (int j = 0; j < 500; j++) {            Map<String, String> hdrs = Maps.newHashMap();            hdrs.put("time", String.valueOf(System.currentTimeMillis()));            events.add(EventBuilder.withBody(String.valueOf(i).getBytes(), hdrs));        }        client.appendBatch(events);    }    Transaction transaction = channel.getTransaction();    transaction.begin();    long after = System.currentTimeMillis();    List<Integer> events = Lists.newArrayList();    for (int i = 0; i < 2500; i++) {        Event event = channel.take();        Assert.assertNotNull(event);        Assert.assertTrue(Long.valueOf(event.getHeaders().get("time")) < after);        events.add(Integer.parseInt(new String(event.getBody())));    }    transaction.commit();    transaction.close();    Collections.sort(events);    int index = 0;        for (int i = 0; i < 5; i++) {        for (int j = 0; j < 500; j++) {            Assert.assertEquals(i, events.get(index++).intValue());        }    }}
public void flume_f3862_0() throws Exception
{    ExecutorService submitter = Executors.newCachedThreadPool();    client = RpcClientFactory.getThriftInstance(props);    Context context = new Context();    context.put("capacity", "1000");    context.put("transactionCapacity", "1000");    channel.configure(context);    configureSource();    context.put(ThriftSource.CONFIG_BIND, "0.0.0.0");    context.put(ThriftSource.CONFIG_PORT, String.valueOf(port));    Configurables.configure(source, context);    source.start();    ExecutorCompletionService<Void> completionService = new ExecutorCompletionService<>(submitter);    for (int i = 0; i < 30; i++) {        completionService.submit(new SubmitHelper(i), null);    }    for (int i = 0; i < 30; i++) {        completionService.take();    }    Transaction transaction = channel.getTransaction();    transaction.begin();    long after = System.currentTimeMillis();    List<Integer> events = Lists.newArrayList();    for (int i = 0; i < 300; i++) {        Event event = channel.take();        Assert.assertNotNull(event);        Assert.assertTrue(Long.valueOf(event.getHeaders().get("time")) < after);        events.add(Integer.parseInt(new String(event.getBody())));    }    transaction.commit();    transaction.close();    Collections.sort(events);    int index = 0;        for (int i = 0; i < 30; i++) {        for (int j = 0; j < 10; j++) {            Assert.assertEquals(i, events.get(index++).intValue());        }    }}
public void flume_f3863_0() throws Exception
{    client = RpcClientFactory.getThriftInstance(props);    Context context = new Context();    context.put(ThriftSource.CONFIG_BIND, "0.0.0.0");    context.put(ThriftSource.CONFIG_PORT, String.valueOf(port));    source.configure(context);    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(new ChannelException("dummy")).when(cp).processEvent(any(Event.class));    doThrow(new ChannelException("dummy")).when(cp).processEventBatch(anyListOf(Event.class));    source.setChannelProcessor(cp);    source.start();    Event event = EventBuilder.withBody("hello".getBytes());    try {        client.append(event);    } catch (EventDeliveryException e) {        }    try {        client.appendBatch(Arrays.asList(event));    } catch (EventDeliveryException e) {        }    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(source, "sourceCounter");    Assert.assertEquals(2, sc.getChannelWriteFail());    source.stop();}
public void flume_f3864_0()
{    List<Event> events = Lists.newArrayList();    for (int j = 0; j < 10; j++) {        Map<String, String> hdrs = Maps.newHashMap();        hdrs.put("time", String.valueOf(System.currentTimeMillis()));        events.add(EventBuilder.withBody(String.valueOf(i).getBytes(), hdrs));    }    try {        client.appendBatch(events);    } catch (EventDeliveryException e) {        throw new FlumeException(e);    }}
public void flume_f3865_0()
{    context = new Context();}
public void flume_f3866_0()
{    assertEquals("Context is empty", 0, context.getParameters().size());    context.put("test", "value");    assertEquals("value", context.getString("test"));    context.clear();    assertNull(context.getString("test"));    assertEquals("value", context.getString("test", "value"));    context.put("test", "true");    assertEquals(new Boolean(true), context.getBoolean("test"));    context.clear();    assertNull(context.getBoolean("test"));    assertEquals(new Boolean(true), context.getBoolean("test", true));    context.put("test", "1");    assertEquals(new Integer(1), context.getInteger("test"));    context.clear();    assertNull(context.getInteger("test"));    assertEquals(new Integer(1), context.getInteger("test", 1));    context.put("test", String.valueOf(Long.MAX_VALUE));    assertEquals(new Long(Long.MAX_VALUE), context.getLong("test"));    context.clear();    assertNull(context.getLong("test"));    assertEquals(new Long(Long.MAX_VALUE), context.getLong("test", Long.MAX_VALUE));    context.put("test", "0.1");    assertEquals(new Float(0.1), context.getFloat("test"));    context.clear();    assertNull(context.getFloat("test"));    assertEquals(new Float(1.1), context.getFloat("test", 1.1F));    context.put("test", "0.1");    assertEquals(new Double(0.1), context.getDouble("test"));    context.clear();    assertNull(context.getDouble("test"));    assertEquals(new Double(1.1), context.getDouble("test", 1.1));}
public void flume_f3867_0()
{    context.put("my.key", "1");    context.put("otherKey", "otherValue");    assertEquals(ImmutableMap.of("key", "1"), context.getSubProperties("my."));}
public void flume_f3868_0()
{    context.put("test", "1");    context.clear();    assertNull(context.getInteger("test"));}
public void flume_f3869_0()
{    context.putAll(ImmutableMap.of("test", "1"));    assertEquals("1", context.getString("test"));}
public void flume_f3870_0()
{    counterGroup = new CounterGroup();}
public void flume_f3871_0()
{    AtomicLong counter = counterGroup.getCounter("test");    Assert.assertNotNull(counter);    Assert.assertEquals(0, counter.get());}
public void flume_f3872_0()
{    long value = counterGroup.get("test");    Assert.assertEquals(0, value);}
public void flume_f3873_0()
{    long value = counterGroup.incrementAndGet("test");    Assert.assertEquals(1, value);}
public void flume_f3874_0()
{    long value = counterGroup.addAndGet("test", 13L);    Assert.assertEquals(13, value);}
public void flume_f3875_0()
{    Map<String, String> props = new HashMap<String, String>();    Random random = new Random();    int intValue = random.nextInt(Integer.MAX_VALUE - 1) + 1;    props.put(testPrefix + "testInt", Integer.toString(intValue));    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(0, bean.getTestInt());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(intValue, bean.getTestInt());}
public void flume_f3876_0()
{    Map<String, String> props = new HashMap<String, String>();    Random random = new Random();    short shortValue = (short) (random.nextInt(Short.MAX_VALUE - 1) + 1);    props.put(testPrefix + "testShort", Short.toString(shortValue));    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(0, bean.getTestShort());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(shortValue, bean.getTestShort());}
public void flume_f3877_0()
{    Map<String, String> props = new HashMap<String, String>();    long longValue = ThreadLocalRandom.current().nextLong(Integer.MAX_VALUE, Long.MAX_VALUE);    props.put(testPrefix + "testLong", Long.toString(longValue));    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(0, bean.getTestLong());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(longValue, bean.getTestLong());}
public void flume_f3878_0()
{    Map<String, String> props = new HashMap<String, String>();    Random random = new Random();    byte byteValue = (byte) (random.nextInt(Byte.MAX_VALUE - 1) + 1);    props.put(testPrefix + "testByte", Byte.toString(byteValue));    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(0, bean.getTestByte());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(byteValue, bean.getTestByte());}
public void flume_f3879_0()
{    Map<String, String> props = new HashMap<String, String>();    props.put(testPrefix + "testBoolean", "true");    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(false, bean.getTestBoolean());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(true, bean.getTestBoolean());}
public void flume_f3880_0()
{    Map<String, String> props = new HashMap<String, String>();    Random random = new Random();    double doubleValue = random.nextDouble();    props.put(testPrefix + "testDouble", Double.toString(doubleValue));    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(0.0d, bean.getTestDouble());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(doubleValue, bean.getTestDouble());}
public void flume_f3881_0()
{    Map<String, String> props = new HashMap<String, String>();    Random random = new Random();    float floatValue = random.nextFloat();    props.put(testPrefix + "testFloat", Float.toString(floatValue));    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(0.0f, bean.getTestFloat());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(floatValue, bean.getTestFloat());}
public void flume_f3882_0()
{    Map<String, String> props = new HashMap<String, String>();    String stringValue = UUID.randomUUID().toString();    props.put(testPrefix + "testString", stringValue);    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals("", bean.getTestString());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(stringValue, bean.getTestString());}
public void flume_f3883_0()
{    Map<String, String> props = new HashMap<String, String>();    Random random = new Random();    int intValue = random.nextInt(Integer.MAX_VALUE - 1) + 1;    props.put(testPrefix + "privateInt", Integer.toString(intValue));    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(0, bean.getPrivateInt());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertTrue(bean.getPrivateInt() != intValue);}
public int flume_f3884_0()
{    return testInt;}
public void flume_f3885_0(int testInt)
{    this.testInt = testInt;}
public short flume_f3886_0()
{    return testShort;}
public void flume_f3887_0(short testShort)
{    this.testShort = testShort;}
public long flume_f3888_0()
{    return testLong;}
public void flume_f3889_0(long testLong)
{    this.testLong = testLong;}
public byte flume_f3890_0()
{    return testByte;}
public void flume_f3891_0(byte testByte)
{    this.testByte = testByte;}
public boolean flume_f3892_0()
{    return testBoolean;}
public void flume_f3893_0(boolean testBoolean)
{    this.testBoolean = testBoolean;}
public float flume_f3894_0()
{    return testFloat;}
public void flume_f3895_0(float testFloat)
{    this.testFloat = testFloat;}
public double flume_f3896_0()
{    return testDouble;}
public void flume_f3897_0(double testDouble)
{    this.testDouble = testDouble;}
public String flume_f3898_0()
{    return testString;}
public void flume_f3899_0(String testString)
{    this.testString = testString;}
private int flume_f3900_0()
{    return privateInt;}
private void flume_f3901_0(int privateInt)
{    this.privateInt = privateInt;}
public void flume_f3902_0()
{    Calendar cal = BASE_CALENDAR_WITH_DEFAULT_TIMEZONE;    Calendar cal2 = createCalendar(2012, 5, 15, 15, 12, 0, 0, null);    long timeToVerify = cal2.getTimeInMillis();    long ret = TimestampRoundDownUtil.roundDownTimeStampSeconds(cal.getTimeInMillis(), 60);    System.out.println("Cal 1: " + cal.toString());    System.out.println("Cal 2: " + cal2.toString());    Assert.assertEquals(timeToVerify, ret);}
public void flume_f3903_0()
{    Calendar cal = BASE_CALENDAR_WITH_CUSTOM_TIMEZONE;    Calendar cal2 = createCalendar(2012, 5, 15, 15, 12, 0, 0, CUSTOM_TIMEZONE);    long timeToVerify = cal2.getTimeInMillis();    long withoutTimeZone = TimestampRoundDownUtil.roundDownTimeStampSeconds(cal.getTimeInMillis(), 60);    long withTimeZone = TimestampRoundDownUtil.roundDownTimeStampSeconds(cal.getTimeInMillis(), 60, CUSTOM_TIMEZONE);    assertThat(withoutTimeZone, not(equalTo(timeToVerify)));    Assert.assertEquals(withTimeZone, timeToVerify);}
public void flume_f3904_0()
{    Calendar cal = BASE_CALENDAR_WITH_DEFAULT_TIMEZONE;    Calendar cal2 = createCalendar(2012, 5, 15, 15, 10, 0, 0, null);    long timeToVerify = cal2.getTimeInMillis();    long ret = TimestampRoundDownUtil.roundDownTimeStampMinutes(cal.getTimeInMillis(), 5);    System.out.println("Cal 1: " + cal.toString());    System.out.println("Cal 2: " + cal2.toString());    Assert.assertEquals(timeToVerify, ret);}
public void flume_f3905_0()
{    Calendar cal = BASE_CALENDAR_WITH_CUSTOM_TIMEZONE;    Calendar cal2 = createCalendar(2012, 5, 15, 15, 10, 0, 0, CUSTOM_TIMEZONE);    long timeToVerify = cal2.getTimeInMillis();    long withoutTimeZone = TimestampRoundDownUtil.roundDownTimeStampMinutes(cal.getTimeInMillis(), 5);    long withTimeZone = TimestampRoundDownUtil.roundDownTimeStampMinutes(cal.getTimeInMillis(), 5, CUSTOM_TIMEZONE);    assertThat(withoutTimeZone, not(equalTo(timeToVerify)));    Assert.assertEquals(withTimeZone, timeToVerify);}
public void flume_f3906_0()
{    Calendar cal = BASE_CALENDAR_WITH_DEFAULT_TIMEZONE;    Calendar cal2 = createCalendar(2012, 5, 15, 14, 0, 0, 0, null);    long timeToVerify = cal2.getTimeInMillis();    long ret = TimestampRoundDownUtil.roundDownTimeStampHours(cal.getTimeInMillis(), 2);    System.out.println("Cal 1: " + ret);    System.out.println("Cal 2: " + cal2.toString());    Assert.assertEquals(timeToVerify, ret);}
public void flume_f3907_0()
{    Calendar cal = BASE_CALENDAR_WITH_CUSTOM_TIMEZONE;    Calendar cal2 = createCalendar(2012, 5, 15, 14, 0, 0, 0, CUSTOM_TIMEZONE);    long timeToVerify = cal2.getTimeInMillis();    long withoutTimeZone = TimestampRoundDownUtil.roundDownTimeStampHours(cal.getTimeInMillis(), 2);    long withTimeZone = TimestampRoundDownUtil.roundDownTimeStampHours(cal.getTimeInMillis(), 2, CUSTOM_TIMEZONE);    assertThat(withoutTimeZone, not(equalTo(timeToVerify)));    Assert.assertEquals(withTimeZone, timeToVerify);}
private static Calendar flume_f3908_0(int year, int month, int day, int hour, int minute, int second, int ms, @Nullable TimeZone timeZone)
{    Calendar cal = (timeZone == null) ? Calendar.getInstance() : Calendar.getInstance(timeZone);    cal.set(year, month, day, hour, minute, second);    cal.set(Calendar.MILLISECOND, ms);    return cal;}
public void flume_f3909_1()
{                        assertTrue("getVersion returned Unknown", !VersionInfo.getVersion().equals("Unknown"));    assertTrue("getUser returned Unknown", !VersionInfo.getUser().equals("Unknown"));    assertTrue("getUrl returned Unknown", !VersionInfo.getUrl().equals("Unknown"));    assertTrue("getSrcChecksum returned Unknown", !VersionInfo.getSrcChecksum().equals("Unknown"));        assertTrue("getBuildVersion returned unexpected format", VersionInfo.getBuildVersion().matches(".+from.+by.+on.+source checksum.+"));        assertNotNull("getRevision returned null", VersionInfo.getRevision());    assertNotNull("getBranch returned null", VersionInfo.getBranch());}
public void flume_f3910_0(Map<String, String> properties) throws FlumeException
{    if (state == State.STARTED) {        throw new IllegalStateException("Cannot be configured while started");    }    doConfigure(properties);    state = State.STOPPED;}
public void flume_f3911_0() throws FlumeException
{    if (state == State.STARTED) {        throw new IllegalStateException("Cannot be started while started");    } else if (state == State.NEW) {        throw new IllegalStateException("Cannot be started before being " + "configured");    }            Source source = Preconditions.checkNotNull(sourceRunner.getSource(), "Source runner returned null source");    if (source instanceof EmbeddedSource) {        embeddedSource = (EmbeddedSource) source;    } else {        throw new IllegalStateException("Unknown source type: " + source.getClass().getName());    }    doStart();    state = State.STARTED;}
public void flume_f3912_0() throws FlumeException
{    if (state != State.STARTED) {        throw new IllegalStateException("Cannot be stopped unless started");    }    supervisor.stop();    embeddedSource = null;    state = State.STOPPED;}
private void flume_f3913_1(Map<String, String> properties)
{    properties = EmbeddedAgentConfiguration.configure(name, properties);    if (LOGGER.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {                for (String key : new TreeSet<String>(properties.keySet())) {                    }    }    MaterializedConfiguration conf = configurationProvider.get(name, properties);    Map<String, SourceRunner> sources = conf.getSourceRunners();    if (sources.size() != 1) {        throw new FlumeException("Expected one source and got " + sources.size());    }    Map<String, Channel> channels = conf.getChannels();    if (channels.size() != 1) {        throw new FlumeException("Expected one channel and got " + channels.size());    }    Map<String, SinkRunner> sinks = conf.getSinkRunners();    if (sinks.size() != 1) {        throw new FlumeException("Expected one sink group and got " + sinks.size());    }    this.sourceRunner = sources.values().iterator().next();    this.channel = channels.values().iterator().next();    this.sinkRunner = sinks.values().iterator().next();}
public void flume_f3914_0(Event event) throws EventDeliveryException
{    if (state != State.STARTED) {        throw new IllegalStateException("Cannot put events unless started");    }    try {        embeddedSource.put(event);    } catch (ChannelException ex) {        throw new EventDeliveryException("Embedded agent " + name + ": Unable to process event: " + ex.getMessage(), ex);    }}
public void flume_f3915_0(List<Event> events) throws EventDeliveryException
{    if (state != State.STARTED) {        throw new IllegalStateException("Cannot put events unless started");    }    try {        embeddedSource.putAll(events);    } catch (ChannelException ex) {        throw new EventDeliveryException("Embedded agent " + name + ": Unable to process event: " + ex.getMessage(), ex);    }}
private void flume_f3916_0()
{    boolean error = true;    try {        channel.start();        sinkRunner.start();        sourceRunner.start();        supervisor.supervise(channel, new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);        supervisor.supervise(sinkRunner, new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);        supervisor.supervise(sourceRunner, new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);        error = false;    } finally {        if (error) {            stopLogError(sourceRunner);            stopLogError(channel);            stopLogError(sinkRunner);            supervisor.stop();        }    }}
private void flume_f3917_1(LifecycleAware lifeCycleAware)
{    try {        if (LifecycleState.START.equals(lifeCycleAware.getLifecycleState())) {            lifeCycleAware.stop();        }    } catch (Exception e) {            }}
private static void flume_f3918_0(String name, Map<String, String> properties) throws FlumeException
{    if (properties.containsKey(SOURCE_TYPE)) {        checkAllowed(ALLOWED_SOURCES, properties.get(SOURCE_TYPE));    }    checkRequired(properties, CHANNEL_TYPE);    checkAllowed(ALLOWED_CHANNELS, properties.get(CHANNEL_TYPE));    checkRequired(properties, SINKS);    String sinkNames = properties.get(SINKS);    for (String sink : sinkNames.split("\\s+")) {        if (DISALLOWED_SINK_NAMES.contains(sink.toLowerCase(Locale.ENGLISH))) {            throw new FlumeException("Sink name " + sink + " is one of the" + " disallowed sink names: " + DISALLOWED_SINK_NAMES);        }        String key = join(sink, TYPE);        checkRequired(properties, key);        checkAllowed(ALLOWED_SINKS, properties.get(key));    }    checkRequired(properties, SINK_PROCESSOR_TYPE);    checkAllowed(ALLOWED_SINK_PROCESSORS, properties.get(SINK_PROCESSOR_TYPE));}
 static Map<String, String> flume_f3919_0(String name, Map<String, String> properties) throws FlumeException
{    validate(name, properties);        properties = new HashMap<String, String>(properties);    if (!properties.containsKey(SOURCE_TYPE) || SOURCE_TYPE_EMBEDDED_ALIAS.equalsIgnoreCase(properties.get(SOURCE_TYPE))) {        properties.put(SOURCE_TYPE, SOURCE_TYPE_EMBEDDED);    }    String sinkNames = properties.remove(SINKS);    String strippedName = name.replaceAll("\\s+", "");    String sourceName = "source-" + strippedName;    String channelName = "channel-" + strippedName;    String sinkGroupName = "sink-group-" + strippedName;    /*     * Now we are going to process the user supplied configuration     * and generate an agent configuration. This is only to supply     * a simpler client api than passing in an entire agent configuration.     */        Map<String, String> result = Maps.newHashMap();    /*     * First we are going to setup all the root level pointers. I.E     * point the agent at the components, sink group at sinks, and     * source at the channel.     */        result.put(join(name, BasicConfigurationConstants.CONFIG_SOURCES), sourceName);        result.put(join(name, BasicConfigurationConstants.CONFIG_CHANNELS), channelName);        result.put(join(name, BasicConfigurationConstants.CONFIG_SINKS), sinkNames);        result.put(join(name, BasicConfigurationConstants.CONFIG_SINKGROUPS), sinkGroupName);        result.put(join(name, BasicConfigurationConstants.CONFIG_SINKGROUPS, sinkGroupName, SINKS), sinkNames);        result.put(join(name, BasicConfigurationConstants.CONFIG_SOURCES, sourceName, BasicConfigurationConstants.CONFIG_CHANNELS), channelName);            Set<String> userProvidedKeys = new HashSet<String>(properties.keySet());    /*     * Second process the sink configuration and point the sinks     * at the channel.     */    for (String sink : sinkNames.split("\\s+")) {        for (String key : userProvidedKeys) {            String value = properties.get(key);            if (key.startsWith(sink + SEPERATOR)) {                properties.remove(key);                result.put(join(name, BasicConfigurationConstants.CONFIG_SINKS, key), value);            }        }                result.put(join(name, BasicConfigurationConstants.CONFIG_SINKS, sink, BasicConfigurationConstants.CONFIG_CHANNEL), channelName);    }    /*     * Third, process all remaining configuration items, prefixing them     * correctly and then passing them on to the agent.     */    userProvidedKeys = new HashSet<String>(properties.keySet());    for (String key : userProvidedKeys) {        String value = properties.get(key);        if (key.startsWith(SOURCE_PREFIX)) {                        key = key.replaceFirst(SOURCE, sourceName);            result.put(join(name, BasicConfigurationConstants.CONFIG_SOURCES, key), value);        } else if (key.startsWith(CHANNEL_PREFIX)) {                        key = key.replaceFirst(CHANNEL, channelName);            result.put(join(name, BasicConfigurationConstants.CONFIG_CHANNELS, key), value);        } else if (key.startsWith(SINK_PROCESSOR_PREFIX)) {                        result.put(join(name, BasicConfigurationConstants.CONFIG_SINKGROUPS, sinkGroupName, key), value);        } else {                        throw new FlumeException("Unknown configuration " + key);        }    }    return result;}
private static void flume_f3920_0(String[] allowedTypes, String type)
{    boolean isAllowed = false;    type = type.trim();    for (String allowedType : allowedTypes) {        if (allowedType.equalsIgnoreCase(type)) {            isAllowed = true;            break;        }    }    if (!isAllowed) {        throw new FlumeException("Component type of " + type + " is not in " + "allowed types of " + Arrays.toString(allowedTypes));    }}
private static void flume_f3921_0(Map<String, String> properties, String name)
{    if (!properties.containsKey(name)) {        throw new FlumeException("Required parameter not found " + name);    }}
private static String flume_f3922_0(String... parts)
{    return JOINER.join(parts);}
public void flume_f3923_0(Context context)
{}
public void flume_f3924_0(Event event) throws ChannelException
{    getChannelProcessor().processEvent(event);}
public void flume_f3925_0(List<Event> events) throws ChannelException
{    getChannelProcessor().processEventBatch(events);}
 MaterializedConfiguration flume_f3926_0(String name, Map<String, String> properties)
{    MemoryConfigurationProvider confProvider = new MemoryConfigurationProvider(name, properties);    return confProvider.getConfiguration();}
protected FlumeConfiguration flume_f3927_0()
{    return new FlumeConfiguration(properties);}
public void flume_f3928_0() throws Exception
{    headers = Maps.newHashMap();    headers.put("key1", "value1");    body = "body".getBytes(Charsets.UTF_8);    int port = findFreePort();    eventCollector = new EventCollector();    Responder responder = new SpecificResponder(AvroSourceProtocol.class, eventCollector);    nettyServer = new NettyServer(responder, new InetSocketAddress(HOSTNAME, port));    nettyServer.start();        Thread.sleep(1000L);    properties = Maps.newHashMap();    properties.put("channel.type", "memory");    properties.put("channel.capacity", "200");    properties.put("sinks", "sink1 sink2");    properties.put("sink1.type", "avro");    properties.put("sink2.type", "avro");    properties.put("sink1.hostname", HOSTNAME);    properties.put("sink1.port", String.valueOf(port));    properties.put("sink2.hostname", HOSTNAME);    properties.put("sink2.port", String.valueOf(port));    properties.put("processor.type", "load_balance");    agent = new EmbeddedAgent("test-" + serialNumber.incrementAndGet());}
public void flume_f3929_1() throws Exception
{    if (agent != null) {        try {            agent.stop();        } catch (Exception e) {                    }    }    if (nettyServer != null) {        try {            nettyServer.close();        } catch (Exception e) {                    }    }}
public void flume_f3930_0() throws Exception
{    agent.configure(properties);    agent.start();    agent.put(EventBuilder.withBody(body, headers));    Event event;    while ((event = eventCollector.poll()) == null) {        Thread.sleep(500L);    }    Assert.assertNotNull(event);    Assert.assertArrayEquals(body, event.getBody());    Assert.assertEquals(headers, event.getHeaders());}
public void flume_f3931_0() throws Exception
{    List<Event> events = Lists.newArrayList();    events.add(EventBuilder.withBody(body, headers));    agent.configure(properties);    agent.start();    agent.putAll(events);    Event event;    while ((event = eventCollector.poll()) == null) {        Thread.sleep(500L);    }    Assert.assertNotNull(event);    Assert.assertArrayEquals(body, event.getBody());    Assert.assertEquals(headers, event.getHeaders());}
public void flume_f3932_0() throws Exception
{    properties.put("source.interceptors", "i1");    properties.put("source.interceptors.i1.type", "static");    properties.put("source.interceptors.i1.key", "key2");    properties.put("source.interceptors.i1.value", "value2");    agent.configure(properties);    agent.start();    agent.put(EventBuilder.withBody(body, headers));    Event event;    while ((event = eventCollector.poll()) == null) {        Thread.sleep(500L);    }    Assert.assertNotNull(event);    Assert.assertArrayEquals(body, event.getBody());    Map<String, String> newHeaders = new HashMap<String, String>(headers);    newHeaders.put("key2", "value2");    Assert.assertEquals(newHeaders, event.getHeaders());}
public void flume_f3933_1() throws Exception
{    EmbeddedAgent embedAgent = new EmbeddedAgent("test 1 2" + serialNumber.incrementAndGet());    List<Event> events = Lists.newArrayList();    events.add(EventBuilder.withBody(body, headers));    embedAgent.configure(properties);    embedAgent.start();    embedAgent.putAll(events);    Event event;    while ((event = eventCollector.poll()) == null) {        Thread.sleep(500L);    }    Assert.assertNotNull(event);    Assert.assertArrayEquals(body, event.getBody());    Assert.assertEquals(headers, event.getHeaders());    if (embedAgent != null) {        try {            embedAgent.stop();        } catch (Exception e) {                    }    }}
public Event flume_f3934_0()
{    AvroFlumeEvent avroEvent = eventQueue.poll();    if (avroEvent != null) {        return EventBuilder.withBody(avroEvent.getBody().array(), toStringMap(avroEvent.getHeaders()));    }    return null;}
public Status flume_f3935_0(AvroFlumeEvent event) throws AvroRemoteException
{    eventQueue.add(event);    return Status.OK;}
public Status flume_f3936_0(List<AvroFlumeEvent> events) throws AvroRemoteException
{    Preconditions.checkState(eventQueue.addAll(events));    return Status.OK;}
private static Map<String, String> flume_f3937_0(Map<CharSequence, CharSequence> charSeqMap)
{    Map<String, String> stringMap = new HashMap<String, String>();    for (Map.Entry<CharSequence, CharSequence> entry : charSeqMap.entrySet()) {        stringMap.put(entry.getKey().toString(), entry.getValue().toString());    }    return stringMap;}
private static int flume_f3938_0() throws IOException
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
public void flume_f3939_0() throws Exception
{    properties = Maps.newHashMap();    properties.put("source.type", EmbeddedAgentConfiguration.SOURCE_TYPE_EMBEDDED);    properties.put("channel.type", "memory");    properties.put("channel.capacity", "200");    properties.put("sinks", "sink1 sink2");    properties.put("sink1.type", "avro");    properties.put("sink2.type", "avro");    properties.put("sink1.hostname", "sink1.host");    properties.put("sink1.port", "2");    properties.put("sink2.hostname", "sink2.host");    properties.put("sink2.port", "2");    properties.put("processor.type", "load_balance");    properties.put("source.interceptors", "i1");    properties.put("source.interceptors.i1.type", "timestamp");}
public void flume_f3940_0() throws Exception
{    doTestExcepted(EmbeddedAgentConfiguration.configure("test1", properties));}
public void flume_f3941_0() throws Exception
{    Assert.assertNotNull(properties.remove("source.type"));    doTestExcepted(EmbeddedAgentConfiguration.configure("test1", properties));}
public void flume_f3942_0() throws Exception
{    properties.put("source.type", "EMBEDDED");    doTestExcepted(EmbeddedAgentConfiguration.configure("test1", properties));}
public void flume_f3943_0(Map<String, String> actual) throws Exception
{    Map<String, String> expected = Maps.newHashMap();    expected.put("test1.channels", "channel-test1");    expected.put("test1.channels.channel-test1.capacity", "200");    expected.put("test1.channels.channel-test1.type", "memory");    expected.put("test1.sinkgroups", "sink-group-test1");    expected.put("test1.sinkgroups.sink-group-test1.processor.type", "load_balance");    expected.put("test1.sinkgroups.sink-group-test1.sinks", "sink1 sink2");    expected.put("test1.sinks", "sink1 sink2");    expected.put("test1.sinks.sink1.channel", "channel-test1");    expected.put("test1.sinks.sink1.hostname", "sink1.host");    expected.put("test1.sinks.sink1.port", "2");    expected.put("test1.sinks.sink1.type", "avro");    expected.put("test1.sinks.sink2.channel", "channel-test1");    expected.put("test1.sinks.sink2.hostname", "sink2.host");    expected.put("test1.sinks.sink2.port", "2");    expected.put("test1.sinks.sink2.type", "avro");    expected.put("test1.sources", "source-test1");    expected.put("test1.sources.source-test1.channels", "channel-test1");    expected.put("test1.sources.source-test1.type", EmbeddedAgentConfiguration.SOURCE_TYPE_EMBEDDED);    expected.put("test1.sources.source-test1.interceptors", "i1");    expected.put("test1.sources.source-test1.interceptors.i1.type", "timestamp");    Assert.assertEquals(expected, actual);}
public void flume_f3944_0() throws Exception
{    properties.put("source.type", "exec");    EmbeddedAgentConfiguration.configure("test1", properties);}
public void flume_f3945_0() throws Exception
{    properties.put("channel.type", "jdbc");    EmbeddedAgentConfiguration.configure("test1", properties);}
public void flume_f3946_0() throws Exception
{    properties.put("sink1.type", "hbase");    EmbeddedAgentConfiguration.configure("test1", properties);}
public void flume_f3947_0() throws Exception
{    properties.put("processor.type", "bad");    EmbeddedAgentConfiguration.configure("test1", properties);}
public void flume_f3948_0() throws Exception
{    properties.remove("channel.type");    EmbeddedAgentConfiguration.configure("test1", properties);}
public void flume_f3949_0() throws Exception
{    properties.remove("sink2.type");    EmbeddedAgentConfiguration.configure("test1", properties);}
public void flume_f3950_0() throws Exception
{    properties.remove("processor.type");    EmbeddedAgentConfiguration.configure("test1", properties);}
public void flume_f3951_0() throws Exception
{    properties.put("bad.key.name", "bad");    EmbeddedAgentConfiguration.configure("test1", properties);}
public void flume_f3952_0() throws Exception
{    properties.put("sinks", "source");    EmbeddedAgentConfiguration.configure("test1", properties);}
public void flume_f3953_0() throws Exception
{    properties.put("sinks", "channel");    EmbeddedAgentConfiguration.configure("test1", properties);}
public void flume_f3954_0() throws Exception
{    properties.put("sinks", "processor");    EmbeddedAgentConfiguration.configure("test1", properties);}
public void flume_f3955_0() throws Exception
{    properties = Maps.newHashMap();    properties.put("source.type", EmbeddedAgentConfiguration.SOURCE_TYPE_EMBEDDED);    properties.put("channel.type", "memory");    properties.put("sinks", "sink1 sink2");    properties.put("sink1.type", "avro");    properties.put("sink2.type", "avro");    properties.put("processor.type", "load_balance");    sourceRunner = mock(SourceRunner.class);    channel = mock(Channel.class);    sinkRunner = mock(SinkRunner.class);    source = mock(EmbeddedSource.class);    when(sourceRunner.getSource()).thenReturn(source);    when(sourceRunner.getLifecycleState()).thenReturn(LifecycleState.START);    when(channel.getLifecycleState()).thenReturn(LifecycleState.START);    when(sinkRunner.getLifecycleState()).thenReturn(LifecycleState.START);    config = new MaterializedConfiguration() {        @Override        public Map<String, SourceRunner> getSourceRunners() {            Map<String, SourceRunner> result = Maps.newHashMap();            result.put("source", sourceRunner);            return ImmutableMap.copyOf(result);        }        @Override        public Map<String, SinkRunner> getSinkRunners() {            Map<String, SinkRunner> result = Maps.newHashMap();            result.put("sink", sinkRunner);            return ImmutableMap.copyOf(result);        }        @Override        public Map<String, Channel> getChannels() {            Map<String, Channel> result = Maps.newHashMap();            result.put("channel", channel);            return ImmutableMap.copyOf(result);        }        @Override        public void addSourceRunner(String name, SourceRunner sourceRunner) {            throw new UnsupportedOperationException();        }        @Override        public void addSinkRunner(String name, SinkRunner sinkRunner) {            throw new UnsupportedOperationException();        }        @Override        public void addChannel(String name, Channel channel) {            throw new UnsupportedOperationException();        }    };    agent = new EmbeddedAgent(new MaterializedConfigurationProvider() {        public MaterializedConfiguration get(String name, Map<String, String> properties) {            return config;        }    }, "dummy");}
public Map<String, SourceRunner> flume_f3956_0()
{    Map<String, SourceRunner> result = Maps.newHashMap();    result.put("source", sourceRunner);    return ImmutableMap.copyOf(result);}
public Map<String, SinkRunner> flume_f3957_0()
{    Map<String, SinkRunner> result = Maps.newHashMap();    result.put("sink", sinkRunner);    return ImmutableMap.copyOf(result);}
public Map<String, Channel> flume_f3958_0()
{    Map<String, Channel> result = Maps.newHashMap();    result.put("channel", channel);    return ImmutableMap.copyOf(result);}
public void flume_f3959_0(String name, SourceRunner sourceRunner)
{    throw new UnsupportedOperationException();}
public void flume_f3960_0(String name, SinkRunner sinkRunner)
{    throw new UnsupportedOperationException();}
public void flume_f3961_0(String name, Channel channel)
{    throw new UnsupportedOperationException();}
public MaterializedConfiguration flume_f3962_0(String name, Map<String, String> properties)
{    return config;}
public void flume_f3963_0()
{    agent.configure(properties);    agent.start();    verify(sourceRunner, times(1)).start();    verify(channel, times(1)).start();    verify(sinkRunner, times(1)).start();}
public void flume_f3964_0()
{    agent.configure(properties);    agent.start();    agent.stop();    verify(sourceRunner, times(1)).stop();    verify(channel, times(1)).stop();    verify(sinkRunner, times(1)).stop();}
public void flume_f3965_0()
{    doThrow(new LocalRuntimeException()).when(sourceRunner).start();    startExpectingLocalRuntimeException();}
public void flume_f3966_0()
{    doThrow(new LocalRuntimeException()).when(channel).start();    startExpectingLocalRuntimeException();}
public void flume_f3967_0()
{    doThrow(new LocalRuntimeException()).when(sinkRunner).start();    startExpectingLocalRuntimeException();}
private void flume_f3968_0()
{    agent.configure(properties);    try {        agent.start();        Assert.fail();    } catch (LocalRuntimeException e) {        }    verify(sourceRunner, times(1)).stop();    verify(channel, times(1)).stop();    verify(sinkRunner, times(1)).stop();}
public void flume_f3969_0() throws EventDeliveryException
{    Event event = new SimpleEvent();    agent.configure(properties);    agent.start();    agent.put(event);    verify(source, times(1)).put(event);}
public void flume_f3970_0() throws EventDeliveryException
{    Event event = new SimpleEvent();    List<Event> events = Lists.newArrayList();    events.add(event);    agent.configure(properties);    agent.start();    agent.putAll(events);    verify(source, times(1)).putAll(events);}
public void flume_f3971_0() throws EventDeliveryException
{    Event event = new SimpleEvent();    agent.configure(properties);    agent.put(event);}
public void flume_f3972_0() throws EventDeliveryException
{    Event event = new SimpleEvent();    List<Event> events = Lists.newArrayList();    events.add(event);    agent.configure(properties);    agent.putAll(events);}
public void flume_f3973_0() throws Exception
{    agent = new EmbeddedAgent("dummy");    properties = Maps.newHashMap();    properties.put("source.type", EmbeddedAgentConfiguration.SOURCE_TYPE_EMBEDDED);    properties.put("channel.type", "memory");    properties.put("sinks", "sink1 sink2");    properties.put("sink1.type", "avro");    properties.put("sink2.type", "avro");    properties.put("sink1.hostname", HOSTNAME);    properties.put("sink1.port", "0");    properties.put("sink2.hostname", HOSTNAME);    properties.put("sink2.port", "0");    properties.put("processor.type", "load_balance");}
public void flume_f3974_0()
{    properties.put(EmbeddedAgentConfiguration.SOURCE_TYPE, "bad");    agent.configure(properties);}
public void flume_f3975_0()
{    try {        agent.configure(properties);        agent.start();    } catch (Exception e) {        Throwables.propagate(e);    }    agent.configure(properties);}
public void flume_f3976_0()
{    agent.configure(properties);    agent.configure(properties);}
public void flume_f3977_0()
{    try {        agent.configure(properties);        agent.start();    } catch (Exception e) {        Throwables.propagate(e);    }    agent.start();}
public void flume_f3978_0()
{    agent.start();}
public void flume_f3979_0()
{    agent.stop();}
public void flume_f3980_0()
{    try {        agent.configure(properties);    } catch (Exception e) {        Throwables.propagate(e);    }    agent.stop();}
public void flume_f3981_0()
{    try {        agent.configure(properties);        agent.start();        agent.stop();    } catch (Exception e) {        Throwables.propagate(e);    }    agent.stop();}
public void flume_f3982_0()
{    try {        agent.configure(properties);    } catch (Exception e) {        Throwables.propagate(e);    }    agent.stop();}
public void flume_f3983_1()
{        res = new SpecificResponder(FlumeOGEventAvroServer.class, this);    try {        http = new HttpServer(res, host, port);    } catch (IOException eI) {                return;    }    http.start();    super.start();}
public void flume_f3984_0()
{    http.close();    super.stop();}
public Void flume_f3985_0(AvroFlumeOGEvent evt) throws AvroRemoteException
{    counterGroup.incrementAndGet("rpc.received");    Map<String, String> headers = new HashMap<String, String>();        headers.put(HOST, evt.getHost().toString());    headers.put(TIMESTAMP, evt.getTimestamp().toString());    headers.put(PRIORITY, evt.getPriority().toString());    headers.put(NANOS, evt.getNanos().toString());    for (Entry<CharSequence, ByteBuffer> entry : evt.getFields().entrySet()) {        headers.put(entry.getKey().toString(), entry.getValue().toString());    }    headers.put(OG_EVENT, "yes");    Event event = EventBuilder.withBody(evt.getBody().array(), headers);    try {        getChannelProcessor().processEvent(event);        counterGroup.incrementAndGet("rpc.events");    } catch (ChannelException ex) {        return null;    }    counterGroup.incrementAndGet("rpc.successful");    return null;}
public void flume_f3986_0(Context context)
{    port = Integer.parseInt(context.getString("port"));    host = context.getString("host");}
public void flume_f3987_0() throws Exception
{    source = new AvroLegacySource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    try (ServerSocket socket = new ServerSocket(0)) {        selectedPort = socket.getLocalPort();    }}
public void flume_f3988_0() throws InterruptedException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort));    context.put("host", "0.0.0.0");    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());    source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
public void flume_f3989_0() throws InterruptedException, IOException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort));    context.put("host", "0.0.0.0");    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());        URL url = new URL("http", "0.0.0.0", selectedPort, "/");    Transceiver http = new HttpTransceiver(url);    FlumeOGEventAvroServer client = SpecificRequestor.getClient(FlumeOGEventAvroServer.class, http);    AvroFlumeOGEvent avroEvent = AvroFlumeOGEvent.newBuilder().setHost("foo").setPriority(Priority.INFO).setNanos(0).setTimestamp(1).setFields(new HashMap<CharSequence, ByteBuffer>()).setBody(ByteBuffer.wrap("foo".getBytes())).build();    client.append(avroEvent);        Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    Assert.assertEquals("Channel contained our event", "foo", new String(event.getBody()));    transaction.commit();    transaction.close();    source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
public int flume_f3990_0()
{    return value;}
public static EventStatus flume_f3991_0(int value)
{    switch(value) {        case 0:            return ACK;        case 1:            return COMMITED;        case 2:            return ERR;        default:            return null;    }}
public int flume_f3992_0()
{    return value;}
public static Priority flume_f3993_0(int value)
{    switch(value) {        case 0:            return FATAL;        case 1:            return ERROR;        case 2:            return WARN;        case 3:            return INFO;        case 4:            return DEBUG;        case 5:            return TRACE;        default:            return null;    }}
public static _Fields flume_f3994_0(int fieldId)
{    switch(fieldId) {        case         1:            return TIMESTAMP;        case         2:            return PRIORITY;        case         3:            return BODY;        case         4:            return NANOS;        case         5:            return HOST;        case         6:            return FIELDS;        default:            return null;    }}
public static _Fields flume_f3995_0(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
public static _Fields flume_f3996_0(String name)
{    return byName.get(name);}
public short flume_f3997_0()
{    return _thriftId;}
public String flume_f3998_0()
{    return _fieldName;}
public ThriftFlumeEvent flume_f3999_0()
{    return new ThriftFlumeEvent(this);}
public void flume_f4000_0()
{    setTimestampIsSet(false);    this.timestamp = 0;    this.priority = null;    this.body = null;    setNanosIsSet(false);    this.nanos = 0;    this.host = null;    this.fields = null;}
public long flume_f4001_0()
{    return this.timestamp;}
public ThriftFlumeEvent flume_f4002_0(long timestamp)
{    this.timestamp = timestamp;    setTimestampIsSet(true);    return this;}
public void flume_f4003_0()
{    __isset_bitfield = EncodingUtils.clearBit(__isset_bitfield, __TIMESTAMP_ISSET_ID);}
public boolean flume_f4004_0()
{    return EncodingUtils.testBit(__isset_bitfield, __TIMESTAMP_ISSET_ID);}
public void flume_f4005_0(boolean value)
{    __isset_bitfield = EncodingUtils.setBit(__isset_bitfield, __TIMESTAMP_ISSET_ID, value);}
public Priority flume_f4006_0()
{    return this.priority;}
public ThriftFlumeEvent flume_f4007_0(Priority priority)
{    this.priority = priority;    return this;}
public void flume_f4008_0()
{    this.priority = null;}
public boolean flume_f4009_0()
{    return this.priority != null;}
public void flume_f4010_0(boolean value)
{    if (!value) {        this.priority = null;    }}
public byte[] flume_f4011_0()
{    setBody(org.apache.thrift.TBaseHelper.rightSize(body));    return body == null ? null : body.array();}
public ByteBuffer flume_f4012_0()
{    return org.apache.thrift.TBaseHelper.copyBinary(body);}
public ThriftFlumeEvent flume_f4013_0(byte[] body)
{    this.body = body == null ? (ByteBuffer) null : ByteBuffer.wrap(Arrays.copyOf(body, body.length));    return this;}
public ThriftFlumeEvent flume_f4014_0(ByteBuffer body)
{    this.body = org.apache.thrift.TBaseHelper.copyBinary(body);    return this;}
public void flume_f4015_0()
{    this.body = null;}
public boolean flume_f4016_0()
{    return this.body != null;}
public void flume_f4017_0(boolean value)
{    if (!value) {        this.body = null;    }}
public long flume_f4018_0()
{    return this.nanos;}
public ThriftFlumeEvent flume_f4019_0(long nanos)
{    this.nanos = nanos;    setNanosIsSet(true);    return this;}
public void flume_f4020_0()
{    __isset_bitfield = EncodingUtils.clearBit(__isset_bitfield, __NANOS_ISSET_ID);}
public boolean flume_f4021_0()
{    return EncodingUtils.testBit(__isset_bitfield, __NANOS_ISSET_ID);}
public void flume_f4022_0(boolean value)
{    __isset_bitfield = EncodingUtils.setBit(__isset_bitfield, __NANOS_ISSET_ID, value);}
public String flume_f4023_0()
{    return this.host;}
public ThriftFlumeEvent flume_f4024_0(String host)
{    this.host = host;    return this;}
public void flume_f4025_0()
{    this.host = null;}
public boolean flume_f4026_0()
{    return this.host != null;}
public void flume_f4027_0(boolean value)
{    if (!value) {        this.host = null;    }}
public int flume_f4028_0()
{    return (this.fields == null) ? 0 : this.fields.size();}
public void flume_f4029_0(String key, ByteBuffer val)
{    if (this.fields == null) {        this.fields = new HashMap<String, ByteBuffer>();    }    this.fields.put(key, val);}
public Map<String, ByteBuffer> flume_f4030_0()
{    return this.fields;}
public ThriftFlumeEvent flume_f4031_0(Map<String, ByteBuffer> fields)
{    this.fields = fields;    return this;}
public void flume_f4032_0()
{    this.fields = null;}
public boolean flume_f4033_0()
{    return this.fields != null;}
public void flume_f4034_0(boolean value)
{    if (!value) {        this.fields = null;    }}
public void flume_f4035_0(_Fields field, Object value)
{    switch(field) {        case TIMESTAMP:            if (value == null) {                unsetTimestamp();            } else {                setTimestamp((Long) value);            }            break;        case PRIORITY:            if (value == null) {                unsetPriority();            } else {                setPriority((Priority) value);            }            break;        case BODY:            if (value == null) {                unsetBody();            } else {                setBody((ByteBuffer) value);            }            break;        case NANOS:            if (value == null) {                unsetNanos();            } else {                setNanos((Long) value);            }            break;        case HOST:            if (value == null) {                unsetHost();            } else {                setHost((String) value);            }            break;        case FIELDS:            if (value == null) {                unsetFields();            } else {                setFields((Map<String, ByteBuffer>) value);            }            break;    }}
public Object flume_f4036_0(_Fields field)
{    switch(field) {        case TIMESTAMP:            return getTimestamp();        case PRIORITY:            return getPriority();        case BODY:            return getBody();        case NANOS:            return getNanos();        case HOST:            return getHost();        case FIELDS:            return getFields();    }    throw new IllegalStateException();}
public boolean flume_f4037_0(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case TIMESTAMP:            return isSetTimestamp();        case PRIORITY:            return isSetPriority();        case BODY:            return isSetBody();        case NANOS:            return isSetNanos();        case HOST:            return isSetHost();        case FIELDS:            return isSetFields();    }    throw new IllegalStateException();}
public boolean flume_f4038_0(Object that)
{    if (that == null)        return false;    if (that instanceof ThriftFlumeEvent)        return this.equals((ThriftFlumeEvent) that);    return false;}
public boolean flume_f4039_0(ThriftFlumeEvent that)
{    if (that == null)        return false;    boolean this_present_timestamp = true;    boolean that_present_timestamp = true;    if (this_present_timestamp || that_present_timestamp) {        if (!(this_present_timestamp && that_present_timestamp))            return false;        if (this.timestamp != that.timestamp)            return false;    }    boolean this_present_priority = true && this.isSetPriority();    boolean that_present_priority = true && that.isSetPriority();    if (this_present_priority || that_present_priority) {        if (!(this_present_priority && that_present_priority))            return false;        if (!this.priority.equals(that.priority))            return false;    }    boolean this_present_body = true && this.isSetBody();    boolean that_present_body = true && that.isSetBody();    if (this_present_body || that_present_body) {        if (!(this_present_body && that_present_body))            return false;        if (!this.body.equals(that.body))            return false;    }    boolean this_present_nanos = true;    boolean that_present_nanos = true;    if (this_present_nanos || that_present_nanos) {        if (!(this_present_nanos && that_present_nanos))            return false;        if (this.nanos != that.nanos)            return false;    }    boolean this_present_host = true && this.isSetHost();    boolean that_present_host = true && that.isSetHost();    if (this_present_host || that_present_host) {        if (!(this_present_host && that_present_host))            return false;        if (!this.host.equals(that.host))            return false;    }    boolean this_present_fields = true && this.isSetFields();    boolean that_present_fields = true && that.isSetFields();    if (this_present_fields || that_present_fields) {        if (!(this_present_fields && that_present_fields))            return false;        if (!this.fields.equals(that.fields))            return false;    }    return true;}
public int flume_f4040_0()
{    List<Object> list = new ArrayList<Object>();    boolean present_timestamp = true;    list.add(present_timestamp);    if (present_timestamp)        list.add(timestamp);    boolean present_priority = true && (isSetPriority());    list.add(present_priority);    if (present_priority)        list.add(priority.getValue());    boolean present_body = true && (isSetBody());    list.add(present_body);    if (present_body)        list.add(body);    boolean present_nanos = true;    list.add(present_nanos);    if (present_nanos)        list.add(nanos);    boolean present_host = true && (isSetHost());    list.add(present_host);    if (present_host)        list.add(host);    boolean present_fields = true && (isSetFields());    list.add(present_fields);    if (present_fields)        list.add(fields);    return list.hashCode();}
public int flume_f4041_0(ThriftFlumeEvent other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetTimestamp()).compareTo(other.isSetTimestamp());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetTimestamp()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.timestamp, other.timestamp);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetPriority()).compareTo(other.isSetPriority());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetPriority()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.priority, other.priority);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetBody()).compareTo(other.isSetBody());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetBody()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.body, other.body);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetNanos()).compareTo(other.isSetNanos());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetNanos()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.nanos, other.nanos);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetHost()).compareTo(other.isSetHost());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetHost()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.host, other.host);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetFields()).compareTo(other.isSetFields());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetFields()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.fields, other.fields);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
public _Fields flume_f4042_0(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
public void flume_f4043_0(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
public void flume_f4044_0(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
public String flume_f4045_0()
{    StringBuilder sb = new StringBuilder("ThriftFlumeEvent(");    boolean first = true;    sb.append("timestamp:");    sb.append(this.timestamp);    first = false;    if (!first)        sb.append(", ");    sb.append("priority:");    if (this.priority == null) {        sb.append("null");    } else {        sb.append(this.priority);    }    first = false;    if (!first)        sb.append(", ");    sb.append("body:");    if (this.body == null) {        sb.append("null");    } else {        org.apache.thrift.TBaseHelper.toString(this.body, sb);    }    first = false;    if (!first)        sb.append(", ");    sb.append("nanos:");    sb.append(this.nanos);    first = false;    if (!first)        sb.append(", ");    sb.append("host:");    if (this.host == null) {        sb.append("null");    } else {        sb.append(this.host);    }    first = false;    if (!first)        sb.append(", ");    sb.append("fields:");    if (this.fields == null) {        sb.append("null");    } else {        sb.append(this.fields);    }    first = false;    sb.append(")");    return sb.toString();}
public void flume_f4046_0() throws org.apache.thrift.TException
{}
private void flume_f4047_0(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
private void flume_f4048_0(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {                __isset_bitfield = 0;        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
public ThriftFlumeEventStandardScheme flume_f4049_0()
{    return new ThriftFlumeEventStandardScheme();}
public void flume_f4050_0(org.apache.thrift.protocol.TProtocol iprot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.thrift.protocol.TType.I64) {                    struct.timestamp = iprot.readI64();                    struct.setTimestampIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             2:                if (schemeField.type == org.apache.thrift.protocol.TType.I32) {                    struct.priority = com.cloudera.flume.handlers.thrift.Priority.findByValue(iprot.readI32());                    struct.setPriorityIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             3:                if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {                    struct.body = iprot.readBinary();                    struct.setBodyIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             4:                if (schemeField.type == org.apache.thrift.protocol.TType.I64) {                    struct.nanos = iprot.readI64();                    struct.setNanosIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             5:                if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {                    struct.host = iprot.readString();                    struct.setHostIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             6:                if (schemeField.type == org.apache.thrift.protocol.TType.MAP) {                    {                        org.apache.thrift.protocol.TMap _map0 = iprot.readMapBegin();                        struct.fields = new HashMap<String, ByteBuffer>(2 * _map0.size);                        String _key1;                        ByteBuffer _val2;                        for (int _i3 = 0; _i3 < _map0.size; ++_i3) {                            _key1 = iprot.readString();                            _val2 = iprot.readBinary();                            struct.fields.put(_key1, _val2);                        }                        iprot.readMapEnd();                    }                    struct.setFieldsIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
public void flume_f4051_0(org.apache.thrift.protocol.TProtocol oprot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    oprot.writeFieldBegin(TIMESTAMP_FIELD_DESC);    oprot.writeI64(struct.timestamp);    oprot.writeFieldEnd();    if (struct.priority != null) {        oprot.writeFieldBegin(PRIORITY_FIELD_DESC);        oprot.writeI32(struct.priority.getValue());        oprot.writeFieldEnd();    }    if (struct.body != null) {        oprot.writeFieldBegin(BODY_FIELD_DESC);        oprot.writeBinary(struct.body);        oprot.writeFieldEnd();    }    oprot.writeFieldBegin(NANOS_FIELD_DESC);    oprot.writeI64(struct.nanos);    oprot.writeFieldEnd();    if (struct.host != null) {        oprot.writeFieldBegin(HOST_FIELD_DESC);        oprot.writeString(struct.host);        oprot.writeFieldEnd();    }    if (struct.fields != null) {        oprot.writeFieldBegin(FIELDS_FIELD_DESC);        {            oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, struct.fields.size()));            for (Map.Entry<String, ByteBuffer> _iter4 : struct.fields.entrySet()) {                oprot.writeString(_iter4.getKey());                oprot.writeBinary(_iter4.getValue());            }            oprot.writeMapEnd();        }        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
public ThriftFlumeEventTupleScheme flume_f4052_0()
{    return new ThriftFlumeEventTupleScheme();}
public void flume_f4053_0(org.apache.thrift.protocol.TProtocol prot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetTimestamp()) {        optionals.set(0);    }    if (struct.isSetPriority()) {        optionals.set(1);    }    if (struct.isSetBody()) {        optionals.set(2);    }    if (struct.isSetNanos()) {        optionals.set(3);    }    if (struct.isSetHost()) {        optionals.set(4);    }    if (struct.isSetFields()) {        optionals.set(5);    }    oprot.writeBitSet(optionals, 6);    if (struct.isSetTimestamp()) {        oprot.writeI64(struct.timestamp);    }    if (struct.isSetPriority()) {        oprot.writeI32(struct.priority.getValue());    }    if (struct.isSetBody()) {        oprot.writeBinary(struct.body);    }    if (struct.isSetNanos()) {        oprot.writeI64(struct.nanos);    }    if (struct.isSetHost()) {        oprot.writeString(struct.host);    }    if (struct.isSetFields()) {        {            oprot.writeI32(struct.fields.size());            for (Map.Entry<String, ByteBuffer> _iter5 : struct.fields.entrySet()) {                oprot.writeString(_iter5.getKey());                oprot.writeBinary(_iter5.getValue());            }        }    }}
public void flume_f4054_0(org.apache.thrift.protocol.TProtocol prot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(6);    if (incoming.get(0)) {        struct.timestamp = iprot.readI64();        struct.setTimestampIsSet(true);    }    if (incoming.get(1)) {        struct.priority = com.cloudera.flume.handlers.thrift.Priority.findByValue(iprot.readI32());        struct.setPriorityIsSet(true);    }    if (incoming.get(2)) {        struct.body = iprot.readBinary();        struct.setBodyIsSet(true);    }    if (incoming.get(3)) {        struct.nanos = iprot.readI64();        struct.setNanosIsSet(true);    }    if (incoming.get(4)) {        struct.host = iprot.readString();        struct.setHostIsSet(true);    }    if (incoming.get(5)) {        {            org.apache.thrift.protocol.TMap _map6 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());            struct.fields = new HashMap<String, ByteBuffer>(2 * _map6.size);            String _key7;            ByteBuffer _val8;            for (int _i9 = 0; _i9 < _map6.size; ++_i9) {                _key7 = iprot.readString();                _val8 = iprot.readBinary();                struct.fields.put(_key7, _val8);            }        }        struct.setFieldsIsSet(true);    }}
public Client flume_f4055_0(org.apache.thrift.protocol.TProtocol prot)
{    return new Client(prot);}
public Client flume_f4056_0(org.apache.thrift.protocol.TProtocol iprot, org.apache.thrift.protocol.TProtocol oprot)
{    return new Client(iprot, oprot);}
public void flume_f4057_0(ThriftFlumeEvent evt) throws org.apache.thrift.TException
{    send_append(evt);}
public void flume_f4058_0(ThriftFlumeEvent evt) throws org.apache.thrift.TException
{    append_args args = new append_args();    args.setEvt(evt);    sendBaseOneway("append", args);}
public void flume_f4059_0() throws org.apache.thrift.TException
{    send_close();    recv_close();}
public void flume_f4060_0() throws org.apache.thrift.TException
{    close_args args = new close_args();    sendBase("close", args);}
public void flume_f4061_0() throws org.apache.thrift.TException
{    close_result result = new close_result();    receiveBase(result, "close");    return;}
public AsyncClient flume_f4062_0(org.apache.thrift.transport.TNonblockingTransport transport)
{    return new AsyncClient(protocolFactory, clientManager, transport);}
public void flume_f4063_0(ThriftFlumeEvent evt, org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException
{    checkReady();    append_call method_call = new append_call(evt, resultHandler, this, ___protocolFactory, ___transport);    this.___currentMethod = method_call;    ___manager.call(method_call);}
public void flume_f4064_0(org.apache.thrift.protocol.TProtocol prot) throws org.apache.thrift.TException
{    prot.writeMessageBegin(new org.apache.thrift.protocol.TMessage("append", org.apache.thrift.protocol.TMessageType.ONEWAY, 0));    append_args args = new append_args();    args.setEvt(evt);    args.write(prot);    prot.writeMessageEnd();}
public void flume_f4065_0() throws org.apache.thrift.TException
{    if (getState() != org.apache.thrift.async.TAsyncMethodCall.State.RESPONSE_READ) {        throw new IllegalStateException("Method call not finished!");    }    org.apache.thrift.transport.TMemoryInputTransport memoryTransport = new org.apache.thrift.transport.TMemoryInputTransport(getFrameBuffer().array());    org.apache.thrift.protocol.TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);}
public void flume_f4066_0(org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException
{    checkReady();    close_call method_call = new close_call(resultHandler, this, ___protocolFactory, ___transport);    this.___currentMethod = method_call;    ___manager.call(method_call);}
public void flume_f4067_0(org.apache.thrift.protocol.TProtocol prot) throws org.apache.thrift.TException
{    prot.writeMessageBegin(new org.apache.thrift.protocol.TMessage("close", org.apache.thrift.protocol.TMessageType.CALL, 0));    close_args args = new close_args();    args.write(prot);    prot.writeMessageEnd();}
public void flume_f4068_0() throws org.apache.thrift.TException
{    if (getState() != org.apache.thrift.async.TAsyncMethodCall.State.RESPONSE_READ) {        throw new IllegalStateException("Method call not finished!");    }    org.apache.thrift.transport.TMemoryInputTransport memoryTransport = new org.apache.thrift.transport.TMemoryInputTransport(getFrameBuffer().array());    org.apache.thrift.protocol.TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);    (new Client(prot)).recv_close();}
private static Map<String, org.apache.thrift.ProcessFunction<I, ? extends org.apache.thrift.TBase>> flume_f4069_0(Map<String, org.apache.thrift.ProcessFunction<I, ? extends org.apache.thrift.TBase>> processMap)
{    processMap.put("append", new append());    processMap.put("close", new close());    return processMap;}
public append_args flume_f4070_0()
{    return new append_args();}
protected boolean flume_f4071_0()
{    return true;}
public org.apache.thrift.TBase flume_f4072_0(I iface, append_args args) throws org.apache.thrift.TException
{    iface.append(args.evt);    return null;}
public close_args flume_f4073_0()
{    return new close_args();}
protected boolean flume_f4074_0()
{    return false;}
public close_result flume_f4075_0(I iface, close_args args) throws org.apache.thrift.TException
{    close_result result = new close_result();    iface.close();    return result;}
private static Map<String, org.apache.thrift.AsyncProcessFunction<I, ? extends org.apache.thrift.TBase, ?>> flume_f4076_0(Map<String, org.apache.thrift.AsyncProcessFunction<I, ? extends org.apache.thrift.TBase, ?>> processMap)
{    processMap.put("append", new append());    processMap.put("close", new close());    return processMap;}
public append_args flume_f4077_0()
{    return new append_args();}
public AsyncMethodCallback<Void> flume_f4078_0(final AsyncFrameBuffer fb, final int seqid)
{    final org.apache.thrift.AsyncProcessFunction fcall = this;    return new AsyncMethodCallback<Void>() {        public void onComplete(Void o) {        }        public void onError(Exception e) {        }    };}
public void flume_f4079_0(Void o)
{}
public void flume_f4080_0(Exception e)
{}
protected boolean flume_f4081_0()
{    return true;}
public void flume_f4082_0(I iface, append_args args, org.apache.thrift.async.AsyncMethodCallback<Void> resultHandler) throws TException
{    iface.append(args.evt, resultHandler);}
public close_args flume_f4083_0()
{    return new close_args();}
public AsyncMethodCallback<Void> flume_f4084_1(final AsyncFrameBuffer fb, final int seqid)
{    final org.apache.thrift.AsyncProcessFunction fcall = this;    return new AsyncMethodCallback<Void>() {        public void onComplete(Void o) {            close_result result = new close_result();            try {                fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);                return;            } catch (Exception e) {                            }            fb.close();        }        public void onError(Exception e) {            byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;            org.apache.thrift.TBase msg;            close_result result = new close_result();            {                msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;                msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());            }            try {                fcall.sendResponse(fb, msg, msgType, seqid);                return;            } catch (Exception ex) {                            }            fb.close();        }    };}
public void flume_f4085_1(Void o)
{    close_result result = new close_result();    try {        fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);        return;    } catch (Exception e) {            }    fb.close();}
public void flume_f4086_1(Exception e)
{    byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;    org.apache.thrift.TBase msg;    close_result result = new close_result();    {        msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;        msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());    }    try {        fcall.sendResponse(fb, msg, msgType, seqid);        return;    } catch (Exception ex) {            }    fb.close();}
protected boolean flume_f4087_0()
{    return false;}
public void flume_f4088_0(I iface, close_args args, org.apache.thrift.async.AsyncMethodCallback<Void> resultHandler) throws TException
{    iface.close(resultHandler);}
public static _Fields flume_f4089_0(int fieldId)
{    switch(fieldId) {        case         1:            return EVT;        default:            return null;    }}
public static _Fields flume_f4090_0(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
public static _Fields flume_f4091_0(String name)
{    return byName.get(name);}
public short flume_f4092_0()
{    return _thriftId;}
public String flume_f4093_0()
{    return _fieldName;}
public append_args flume_f4094_0()
{    return new append_args(this);}
public void flume_f4095_0()
{    this.evt = null;}
public ThriftFlumeEvent flume_f4096_0()
{    return this.evt;}
public append_args flume_f4097_0(ThriftFlumeEvent evt)
{    this.evt = evt;    return this;}
public void flume_f4098_0()
{    this.evt = null;}
public boolean flume_f4099_0()
{    return this.evt != null;}
public void flume_f4100_0(boolean value)
{    if (!value) {        this.evt = null;    }}
public void flume_f4101_0(_Fields field, Object value)
{    switch(field) {        case EVT:            if (value == null) {                unsetEvt();            } else {                setEvt((ThriftFlumeEvent) value);            }            break;    }}
public Object flume_f4102_0(_Fields field)
{    switch(field) {        case EVT:            return getEvt();    }    throw new IllegalStateException();}
public boolean flume_f4103_0(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case EVT:            return isSetEvt();    }    throw new IllegalStateException();}
public boolean flume_f4104_0(Object that)
{    if (that == null)        return false;    if (that instanceof append_args)        return this.equals((append_args) that);    return false;}
public boolean flume_f4105_0(append_args that)
{    if (that == null)        return false;    boolean this_present_evt = true && this.isSetEvt();    boolean that_present_evt = true && that.isSetEvt();    if (this_present_evt || that_present_evt) {        if (!(this_present_evt && that_present_evt))            return false;        if (!this.evt.equals(that.evt))            return false;    }    return true;}
public int flume_f4106_0()
{    List<Object> list = new ArrayList<Object>();    boolean present_evt = true && (isSetEvt());    list.add(present_evt);    if (present_evt)        list.add(evt);    return list.hashCode();}
public int flume_f4107_0(append_args other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetEvt()).compareTo(other.isSetEvt());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetEvt()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.evt, other.evt);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
public _Fields flume_f4108_0(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
public void flume_f4109_0(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
public void flume_f4110_0(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
public String flume_f4111_0()
{    StringBuilder sb = new StringBuilder("append_args(");    boolean first = true;    sb.append("evt:");    if (this.evt == null) {        sb.append("null");    } else {        sb.append(this.evt);    }    first = false;    sb.append(")");    return sb.toString();}
public void flume_f4112_0() throws org.apache.thrift.TException
{        if (evt != null) {        evt.validate();    }}
private void flume_f4113_0(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
private void flume_f4114_0(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
public append_argsStandardScheme flume_f4115_0()
{    return new append_argsStandardScheme();}
public void flume_f4116_0(org.apache.thrift.protocol.TProtocol iprot, append_args struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.thrift.protocol.TType.STRUCT) {                    struct.evt = new ThriftFlumeEvent();                    struct.evt.read(iprot);                    struct.setEvtIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
public void flume_f4117_0(org.apache.thrift.protocol.TProtocol oprot, append_args struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.evt != null) {        oprot.writeFieldBegin(EVT_FIELD_DESC);        struct.evt.write(oprot);        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
public append_argsTupleScheme flume_f4118_0()
{    return new append_argsTupleScheme();}
public void flume_f4119_0(org.apache.thrift.protocol.TProtocol prot, append_args struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetEvt()) {        optionals.set(0);    }    oprot.writeBitSet(optionals, 1);    if (struct.isSetEvt()) {        struct.evt.write(oprot);    }}
public void flume_f4120_0(org.apache.thrift.protocol.TProtocol prot, append_args struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(1);    if (incoming.get(0)) {        struct.evt = new ThriftFlumeEvent();        struct.evt.read(iprot);        struct.setEvtIsSet(true);    }}
public static _Fields flume_f4121_0(int fieldId)
{    switch(fieldId) {        default:            return null;    }}
public static _Fields flume_f4122_0(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
public static _Fields flume_f4123_0(String name)
{    return byName.get(name);}
public short flume_f4124_0()
{    return _thriftId;}
public String flume_f4125_0()
{    return _fieldName;}
public close_args flume_f4126_0()
{    return new close_args(this);}
public void flume_f4127_0()
{}
public void flume_f4128_0(_Fields field, Object value)
{    switch(field) {    }}
public Object flume_f4129_0(_Fields field)
{    switch(field) {    }    throw new IllegalStateException();}
public boolean flume_f4130_0(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {    }    throw new IllegalStateException();}
public boolean flume_f4131_0(Object that)
{    if (that == null)        return false;    if (that instanceof close_args)        return this.equals((close_args) that);    return false;}
public boolean flume_f4132_0(close_args that)
{    if (that == null)        return false;    return true;}
public int flume_f4133_0()
{    List<Object> list = new ArrayList<Object>();    return list.hashCode();}
public int flume_f4134_0(close_args other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    return 0;}
public _Fields flume_f4135_0(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
public void flume_f4136_0(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
public void flume_f4137_0(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
public String flume_f4138_0()
{    StringBuilder sb = new StringBuilder("close_args(");    boolean first = true;    sb.append(")");    return sb.toString();}
public void flume_f4139_0() throws org.apache.thrift.TException
{}
private void flume_f4140_0(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
private void flume_f4141_0(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
public close_argsStandardScheme flume_f4142_0()
{    return new close_argsStandardScheme();}
public void flume_f4143_0(org.apache.thrift.protocol.TProtocol iprot, close_args struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
public void flume_f4144_0(org.apache.thrift.protocol.TProtocol oprot, close_args struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    oprot.writeFieldStop();    oprot.writeStructEnd();}
public close_argsTupleScheme flume_f4145_0()
{    return new close_argsTupleScheme();}
public void flume_f4146_0(org.apache.thrift.protocol.TProtocol prot, close_args struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;}
public void flume_f4147_0(org.apache.thrift.protocol.TProtocol prot, close_args struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;}
public static _Fields flume_f4148_0(int fieldId)
{    switch(fieldId) {        default:            return null;    }}
public static _Fields flume_f4149_0(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
public static _Fields flume_f4150_0(String name)
{    return byName.get(name);}
public short flume_f4151_0()
{    return _thriftId;}
public String flume_f4152_0()
{    return _fieldName;}
public close_result flume_f4153_0()
{    return new close_result(this);}
public void flume_f4154_0()
{}
public void flume_f4155_0(_Fields field, Object value)
{    switch(field) {    }}
public Object flume_f4156_0(_Fields field)
{    switch(field) {    }    throw new IllegalStateException();}
public boolean flume_f4157_0(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {    }    throw new IllegalStateException();}
public boolean flume_f4158_0(Object that)
{    if (that == null)        return false;    if (that instanceof close_result)        return this.equals((close_result) that);    return false;}
public boolean flume_f4159_0(close_result that)
{    if (that == null)        return false;    return true;}
public int flume_f4160_0()
{    List<Object> list = new ArrayList<Object>();    return list.hashCode();}
public int flume_f4161_0(close_result other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    return 0;}
public _Fields flume_f4162_0(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
public void flume_f4163_0(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
public void flume_f4164_0(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
public String flume_f4165_0()
{    StringBuilder sb = new StringBuilder("close_result(");    boolean first = true;    sb.append(")");    return sb.toString();}
public void flume_f4166_0() throws org.apache.thrift.TException
{}
private void flume_f4167_0(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
private void flume_f4168_0(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
public close_resultStandardScheme flume_f4169_0()
{    return new close_resultStandardScheme();}
public void flume_f4170_0(org.apache.thrift.protocol.TProtocol iprot, close_result struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
public void flume_f4171_0(org.apache.thrift.protocol.TProtocol oprot, close_result struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    oprot.writeFieldStop();    oprot.writeStructEnd();}
public close_resultTupleScheme flume_f4172_0()
{    return new close_resultTupleScheme();}
public void flume_f4173_0(org.apache.thrift.protocol.TProtocol prot, close_result struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;}
public void flume_f4174_0(org.apache.thrift.protocol.TProtocol prot, close_result struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;}
public void flume_f4175_1(ThriftFlumeEvent evt)
{    if (evt == null) {        return;    }    Map<String, String> headers = new HashMap<String, String>();        headers.put(HOST, evt.getHost());    headers.put(TIMESTAMP, Long.toString(evt.getTimestamp()));    headers.put(PRIORITY, evt.getPriority().toString());    headers.put(NANOS, Long.toString(evt.getNanos()));    for (Entry<String, ByteBuffer> entry : evt.getFields().entrySet()) {        headers.put(entry.getKey().toString(), UTF_8.decode(entry.getValue()).toString());    }    headers.put(OG_EVENT, "yes");    Event event = EventBuilder.withBody(evt.getBody(), headers);    counterGroup.incrementAndGet("rpc.events");    try {        getChannelProcessor().processEvent(event);    } catch (ChannelException ex) {                return;    }    counterGroup.incrementAndGet("rpc.successful");    return;}
public void flume_f4176_0()
{}
public void flume_f4177_0()
{    server.serve();}
public void flume_f4178_0(Context context)
{    port = Integer.parseInt(context.getString("port"));    host = context.getString("host");}
public void flume_f4179_0()
{    try {        InetSocketAddress bindAddr = new InetSocketAddress(host, port);        serverTransport = new TServerSocket(bindAddr);        ThriftFlumeEventServer.Processor processor = new ThriftFlumeEventServer.Processor(new ThriftFlumeEventServerImpl());        server = new TThreadPoolServer(new TThreadPoolServer.Args(serverTransport).processor(processor));    } catch (TTransportException e) {        throw new FlumeException("Failed starting source", e);    }    ThriftHandler thriftHandler = new ThriftHandler(server);    thriftHandlerThread = new Thread(thriftHandler);    thriftHandlerThread.start();    super.start();}
public void flume_f4180_1()
{    server.stop();    serverTransport.close();    try {        thriftHandlerThread.join();    } catch (InterruptedException eI) {                return;    }    super.stop();}
public void flume_f4181_0(ThriftFlumeEvent evt)
{    TTransport transport;    try {        transport = new TSocket(host, port);        TProtocol protocol = new TBinaryProtocol(transport);        Client client = new Client(protocol);        transport.open();        client.append(evt);        transport.close();    } catch (TTransportException e) {        e.printStackTrace();    } catch (TException e) {        e.printStackTrace();    }}
public void flume_f4182_0() throws Exception
{    source = new ThriftLegacySource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    try (ServerSocket socket = new ServerSocket(0)) {        selectedPort = socket.getLocalPort();    }}
private void flume_f4183_0() throws InterruptedException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort));    context.put("host", "0.0.0.0");    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());}
private void flume_f4184_0() throws InterruptedException
{    source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
public void flume_f4185_0() throws InterruptedException
{    bind();    stop();}
public void flume_f4186_0() throws InterruptedException, IOException
{    bind();    Map<String, ByteBuffer> flumeMap = new HashMap<>();    ThriftFlumeEvent thriftEvent = new ThriftFlumeEvent(1, Priority.INFO, ByteBuffer.wrap("foo".getBytes()), 0, "fooHost", flumeMap);    FlumeClient fClient = new FlumeClient("0.0.0.0", selectedPort);    fClient.append(thriftEvent);        Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    Assert.assertEquals("Channel contained our event", "foo", new String(event.getBody()));    transaction.commit();    transaction.close();    stop();}
public void flume_f4187_0() throws InterruptedException, IOException
{    bind();    Map<String, ByteBuffer> flumeHeaders = new HashMap<>();    flumeHeaders.put("hello", ByteBuffer.wrap("world".getBytes("UTF-8")));    ThriftFlumeEvent thriftEvent = new ThriftFlumeEvent(1, Priority.INFO, ByteBuffer.wrap("foo".getBytes()), 0, "fooHost", flumeHeaders);    FlumeClient fClient = new FlumeClient("0.0.0.0", selectedPort);    fClient.append(thriftEvent);        Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    Assert.assertEquals("Event in channel has our header", "world", event.getHeaders().get("hello"));    transaction.commit();    transaction.close();    stop();}
public MaterializedConfiguration flume_f4188_1()
{    MaterializedConfiguration conf = new SimpleMaterializedConfiguration();    FlumeConfiguration fconfig = getFlumeConfiguration();    AgentConfiguration agentConf = fconfig.getConfigurationFor(getAgentName());    if (agentConf != null) {        Map<String, ChannelComponent> channelComponentMap = Maps.newHashMap();        Map<String, SourceRunner> sourceRunnerMap = Maps.newHashMap();        Map<String, SinkRunner> sinkRunnerMap = Maps.newHashMap();        try {            loadChannels(agentConf, channelComponentMap);            loadSources(agentConf, channelComponentMap, sourceRunnerMap);            loadSinks(agentConf, channelComponentMap, sinkRunnerMap);            Set<String> channelNames = new HashSet<String>(channelComponentMap.keySet());            for (String channelName : channelNames) {                ChannelComponent channelComponent = channelComponentMap.get(channelName);                if (channelComponent.components.isEmpty()) {                                        channelComponentMap.remove(channelName);                    Map<String, Channel> nameChannelMap = channelCache.get(channelComponent.channel.getClass());                    if (nameChannelMap != null) {                        nameChannelMap.remove(channelName);                    }                } else {                                        conf.addChannel(channelName, channelComponent.channel);                }            }            for (Map.Entry<String, SourceRunner> entry : sourceRunnerMap.entrySet()) {                conf.addSourceRunner(entry.getKey(), entry.getValue());            }            for (Map.Entry<String, SinkRunner> entry : sinkRunnerMap.entrySet()) {                conf.addSinkRunner(entry.getKey(), entry.getValue());            }        } catch (InstantiationException ex) {                    } finally {            channelComponentMap.clear();            sourceRunnerMap.clear();            sinkRunnerMap.clear();        }    } else {            }    return conf;}
public String flume_f4189_0()
{    return agentName;}
private void flume_f4190_1(AgentConfiguration agentConf, Map<String, ChannelComponent> channelComponentMap) throws InstantiationException
{        /*     * Some channels will be reused across re-configurations. To handle this,     * we store all the names of current channels, perform the reconfiguration,     * and then if a channel was not used, we delete our reference to it.     * This supports the scenario where you enable channel "ch0" then remove it     * and add it back. Without this, channels like memory channel would cause     * the first instances data to show up in the seconds.     */    ListMultimap<Class<? extends Channel>, String> channelsNotReused = ArrayListMultimap.create();        for (Map.Entry<Class<? extends Channel>, Map<String, Channel>> entry : channelCache.entrySet()) {        Class<? extends Channel> channelKlass = entry.getKey();        Set<String> channelNames = entry.getValue().keySet();        channelsNotReused.get(channelKlass).addAll(channelNames);    }    Set<String> channelNames = agentConf.getChannelSet();    Map<String, ComponentConfiguration> compMap = agentConf.getChannelConfigMap();    /*     * Components which have a ComponentConfiguration object     */    for (String chName : channelNames) {        ComponentConfiguration comp = compMap.get(chName);        if (comp != null) {            Channel channel = getOrCreateChannel(channelsNotReused, comp.getComponentName(), comp.getType());            try {                Configurables.configure(channel, comp);                channelComponentMap.put(comp.getComponentName(), new ChannelComponent(channel));                            } catch (Exception e) {                String msg = String.format("Channel %s has been removed due to an " + "error during configuration", chName);                            }        }    }    /*     * Components which DO NOT have a ComponentConfiguration object     * and use only Context     */    for (String chName : channelNames) {        Context context = agentConf.getChannelContext().get(chName);        if (context != null) {            Channel channel = getOrCreateChannel(channelsNotReused, chName, context.getString(BasicConfigurationConstants.CONFIG_TYPE));            try {                Configurables.configure(channel, context);                channelComponentMap.put(chName, new ChannelComponent(channel));                            } catch (Exception e) {                String msg = String.format("Channel %s has been removed due to an " + "error during configuration", chName);                            }        }    }    /*     * Any channel which was not re-used, will have it's reference removed     */    for (Class<? extends Channel> channelKlass : channelsNotReused.keySet()) {        Map<String, Channel> channelMap = channelCache.get(channelKlass);        if (channelMap != null) {            for (String channelName : channelsNotReused.get(channelKlass)) {                if (channelMap.remove(channelName) != null) {                                    }            }            if (channelMap.isEmpty()) {                channelCache.remove(channelKlass);            }        }    }}
private Channel flume_f4191_0(ListMultimap<Class<? extends Channel>, String> channelsNotReused, String name, String type) throws FlumeException
{    Class<? extends Channel> channelClass = channelFactory.getClass(type);    /*     * Channel has requested a new instance on each re-configuration     */    if (channelClass.isAnnotationPresent(Disposable.class)) {        Channel channel = channelFactory.create(name, type);        channel.setName(name);        return channel;    }    Map<String, Channel> channelMap = channelCache.get(channelClass);    if (channelMap == null) {        channelMap = new HashMap<String, Channel>();        channelCache.put(channelClass, channelMap);    }    Channel channel = channelMap.get(name);    if (channel == null) {        channel = channelFactory.create(name, type);        channel.setName(name);        channelMap.put(name, channel);    }    channelsNotReused.get(channelClass).remove(name);    return channel;}
private void flume_f4192_1(AgentConfiguration agentConf, Map<String, ChannelComponent> channelComponentMap, Map<String, SourceRunner> sourceRunnerMap) throws InstantiationException
{    Set<String> sourceNames = agentConf.getSourceSet();    Map<String, ComponentConfiguration> compMap = agentConf.getSourceConfigMap();    /*     * Components which have a ComponentConfiguration object     */    for (String sourceName : sourceNames) {        ComponentConfiguration comp = compMap.get(sourceName);        if (comp != null) {            SourceConfiguration config = (SourceConfiguration) comp;            Source source = sourceFactory.create(comp.getComponentName(), comp.getType());            try {                Configurables.configure(source, config);                Set<String> channelNames = config.getChannels();                List<Channel> sourceChannels = getSourceChannels(channelComponentMap, source, channelNames);                if (sourceChannels.isEmpty()) {                    String msg = String.format("Source %s is not connected to a " + "channel", sourceName);                    throw new IllegalStateException(msg);                }                ChannelSelectorConfiguration selectorConfig = config.getSelectorConfiguration();                ChannelSelector selector = ChannelSelectorFactory.create(sourceChannels, selectorConfig);                ChannelProcessor channelProcessor = new ChannelProcessor(selector);                Configurables.configure(channelProcessor, config);                source.setChannelProcessor(channelProcessor);                sourceRunnerMap.put(comp.getComponentName(), SourceRunner.forSource(source));                for (Channel channel : sourceChannels) {                    ChannelComponent channelComponent = Preconditions.checkNotNull(channelComponentMap.get(channel.getName()), String.format("Channel %s", channel.getName()));                    channelComponent.components.add(sourceName);                }            } catch (Exception e) {                String msg = String.format("Source %s has been removed due to an " + "error during configuration", sourceName);                            }        }    }    /*     * Components which DO NOT have a ComponentConfiguration object     * and use only Context     */    Map<String, Context> sourceContexts = agentConf.getSourceContext();    for (String sourceName : sourceNames) {        Context context = sourceContexts.get(sourceName);        if (context != null) {            Source source = sourceFactory.create(sourceName, context.getString(BasicConfigurationConstants.CONFIG_TYPE));            try {                Configurables.configure(source, context);                String[] channelNames = context.getString(BasicConfigurationConstants.CONFIG_CHANNELS).split("\\s+");                List<Channel> sourceChannels = getSourceChannels(channelComponentMap, source, Arrays.asList(channelNames));                if (sourceChannels.isEmpty()) {                    String msg = String.format("Source %s is not connected to a " + "channel", sourceName);                    throw new IllegalStateException(msg);                }                Map<String, String> selectorConfig = context.getSubProperties(BasicConfigurationConstants.CONFIG_SOURCE_CHANNELSELECTOR_PREFIX);                ChannelSelector selector = ChannelSelectorFactory.create(sourceChannels, selectorConfig);                ChannelProcessor channelProcessor = new ChannelProcessor(selector);                Configurables.configure(channelProcessor, context);                source.setChannelProcessor(channelProcessor);                sourceRunnerMap.put(sourceName, SourceRunner.forSource(source));                for (Channel channel : sourceChannels) {                    ChannelComponent channelComponent = Preconditions.checkNotNull(channelComponentMap.get(channel.getName()), String.format("Channel %s", channel.getName()));                    channelComponent.components.add(sourceName);                }            } catch (Exception e) {                String msg = String.format("Source %s has been removed due to an " + "error during configuration", sourceName);                            }        }    }}
private List<Channel> flume_f4193_0(Map<String, ChannelComponent> channelComponentMap, Source source, Collection<String> channelNames) throws InstantiationException
{    List<Channel> sourceChannels = new ArrayList<Channel>();    for (String chName : channelNames) {        ChannelComponent channelComponent = channelComponentMap.get(chName);        if (channelComponent != null) {            checkSourceChannelCompatibility(source, channelComponent.channel);            sourceChannels.add(channelComponent.channel);        }    }    return sourceChannels;}
private void flume_f4194_0(Source source, Channel channel) throws InstantiationException
{    if (source instanceof BatchSizeSupported && channel instanceof TransactionCapacitySupported) {        long transCap = ((TransactionCapacitySupported) channel).getTransactionCapacity();        long batchSize = ((BatchSizeSupported) source).getBatchSize();        if (transCap < batchSize) {            String msg = String.format("Incompatible source and channel settings defined. " + "source's batch size is greater than the channels transaction capacity. " + "Source: %s, batch size = %d, channel %s, transaction capacity = %d", source.getName(), batchSize, channel.getName(), transCap);            throw new InstantiationException(msg);        }    }}
private void flume_f4195_0(Sink sink, Channel channel) throws InstantiationException
{    if (sink instanceof BatchSizeSupported && channel instanceof TransactionCapacitySupported) {        long transCap = ((TransactionCapacitySupported) channel).getTransactionCapacity();        long batchSize = ((BatchSizeSupported) sink).getBatchSize();        if (transCap < batchSize) {            String msg = String.format("Incompatible sink and channel settings defined. " + "sink's batch size is greater than the channels transaction capacity. " + "Sink: %s, batch size = %d, channel %s, transaction capacity = %d", sink.getName(), batchSize, channel.getName(), transCap);            throw new InstantiationException(msg);        }    }}
private void flume_f4196_1(AgentConfiguration agentConf, Map<String, ChannelComponent> channelComponentMap, Map<String, SinkRunner> sinkRunnerMap) throws InstantiationException
{    Set<String> sinkNames = agentConf.getSinkSet();    Map<String, ComponentConfiguration> compMap = agentConf.getSinkConfigMap();    Map<String, Sink> sinks = new HashMap<String, Sink>();    /*     * Components which have a ComponentConfiguration object     */    for (String sinkName : sinkNames) {        ComponentConfiguration comp = compMap.get(sinkName);        if (comp != null) {            SinkConfiguration config = (SinkConfiguration) comp;            Sink sink = sinkFactory.create(comp.getComponentName(), comp.getType());            try {                Configurables.configure(sink, config);                ChannelComponent channelComponent = channelComponentMap.get(config.getChannel());                if (channelComponent == null) {                    String msg = String.format("Sink %s is not connected to a " + "channel", sinkName);                    throw new IllegalStateException(msg);                }                checkSinkChannelCompatibility(sink, channelComponent.channel);                sink.setChannel(channelComponent.channel);                sinks.put(comp.getComponentName(), sink);                channelComponent.components.add(sinkName);            } catch (Exception e) {                String msg = String.format("Sink %s has been removed due to an " + "error during configuration", sinkName);                            }        }    }    /*     * Components which DO NOT have a ComponentConfiguration object     * and use only Context     */    Map<String, Context> sinkContexts = agentConf.getSinkContext();    for (String sinkName : sinkNames) {        Context context = sinkContexts.get(sinkName);        if (context != null) {            Sink sink = sinkFactory.create(sinkName, context.getString(BasicConfigurationConstants.CONFIG_TYPE));            try {                Configurables.configure(sink, context);                ChannelComponent channelComponent = channelComponentMap.get(context.getString(BasicConfigurationConstants.CONFIG_CHANNEL));                if (channelComponent == null) {                    String msg = String.format("Sink %s is not connected to a " + "channel", sinkName);                    throw new IllegalStateException(msg);                }                checkSinkChannelCompatibility(sink, channelComponent.channel);                sink.setChannel(channelComponent.channel);                sinks.put(sinkName, sink);                channelComponent.components.add(sinkName);            } catch (Exception e) {                String msg = String.format("Sink %s has been removed due to an " + "error during configuration", sinkName);                            }        }    }    loadSinkGroups(agentConf, sinks, sinkRunnerMap);}
private void flume_f4197_1(AgentConfiguration agentConf, Map<String, Sink> sinks, Map<String, SinkRunner> sinkRunnerMap) throws InstantiationException
{    Set<String> sinkGroupNames = agentConf.getSinkgroupSet();    Map<String, ComponentConfiguration> compMap = agentConf.getSinkGroupConfigMap();    Map<String, String> usedSinks = new HashMap<String, String>();    for (String groupName : sinkGroupNames) {        ComponentConfiguration comp = compMap.get(groupName);        if (comp != null) {            SinkGroupConfiguration groupConf = (SinkGroupConfiguration) comp;            List<Sink> groupSinks = new ArrayList<Sink>();            for (String sink : groupConf.getSinks()) {                Sink s = sinks.remove(sink);                if (s == null) {                    String sinkUser = usedSinks.get(sink);                    if (sinkUser != null) {                        throw new InstantiationException(String.format("Sink %s of group %s already " + "in use by group %s", sink, groupName, sinkUser));                    } else {                        throw new InstantiationException(String.format("Sink %s of group %s does " + "not exist or is not properly configured", sink, groupName));                    }                }                groupSinks.add(s);                usedSinks.put(sink, groupName);            }            try {                SinkGroup group = new SinkGroup(groupSinks);                Configurables.configure(group, groupConf);                sinkRunnerMap.put(comp.getComponentName(), new SinkRunner(group.getProcessor()));            } catch (Exception e) {                String msg = String.format("SinkGroup %s has been removed due to " + "an error during configuration", groupName);                            }        }    }        for (Entry<String, Sink> entry : sinks.entrySet()) {        if (!usedSinks.containsValue(entry.getKey())) {            try {                SinkProcessor pr = new DefaultSinkProcessor();                List<Sink> sinkMap = new ArrayList<Sink>();                sinkMap.add(entry.getValue());                pr.setSinks(sinkMap);                Configurables.configure(pr, new Context());                sinkRunnerMap.put(entry.getKey(), new SinkRunner(pr));            } catch (Exception e) {                String msg = String.format("SinkGroup %s has been removed due to " + "an error during configuration", entry.getKey());                            }        }    }}
protected Map<String, String> flume_f4198_0(Properties properties)
{    Map<String, String> result = Maps.newHashMap();    Enumeration<?> propertyNames = properties.propertyNames();    while (propertyNames.hasMoreElements()) {        String name = (String) propertyNames.nextElement();        String value = properties.getProperty(name);        result.put(name, value);    }    return result;}
protected CuratorFramework flume_f4199_0()
{    return CuratorFrameworkFactory.newClient(zkConnString, new ExponentialBackoffRetry(1000, 1));}
protected FlumeConfiguration flume_f4200_0(byte[] configData) throws IOException
{    Map<String, String> configMap;    if (configData == null || configData.length == 0) {        configMap = Collections.emptyMap();    } else {        String fileContent = new String(configData, Charsets.UTF_8);        Properties properties = new Properties();        properties.load(new StringReader(fileContent));        configMap = toMap(properties);    }    return new FlumeConfiguration(configMap);}
public void flume_f4201_0()
{    lifecycleLock.lock();    try {        for (LifecycleAware component : components) {            supervisor.supervise(component, new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);        }    } finally {        lifecycleLock.unlock();    }}
public void flume_f4202_1(MaterializedConfiguration conf)
{    try {        lifecycleLock.lockInterruptibly();        stopAllComponents();        startAllComponents(conf);    } catch (InterruptedException e) {                return;    } finally {                if (lifecycleLock.isHeldByCurrentThread()) {            lifecycleLock.unlock();        }    }}
public void flume_f4203_0()
{    lifecycleLock.lock();    stopAllComponents();    try {        supervisor.stop();        if (monitorServer != null) {            monitorServer.stop();        }    } finally {        lifecycleLock.unlock();    }}
private void flume_f4204_1()
{    if (this.materializedConfiguration != null) {                for (Entry<String, SourceRunner> entry : this.materializedConfiguration.getSourceRunners().entrySet()) {            try {                                supervisor.unsupervise(entry.getValue());            } catch (Exception e) {                            }        }        for (Entry<String, SinkRunner> entry : this.materializedConfiguration.getSinkRunners().entrySet()) {            try {                                supervisor.unsupervise(entry.getValue());            } catch (Exception e) {                            }        }        for (Entry<String, Channel> entry : this.materializedConfiguration.getChannels().entrySet()) {            try {                                supervisor.unsupervise(entry.getValue());            } catch (Exception e) {                            }        }    }    if (monitorServer != null) {        monitorServer.stop();    }}
private void flume_f4205_1(MaterializedConfiguration materializedConfiguration)
{        this.materializedConfiguration = materializedConfiguration;    for (Entry<String, Channel> entry : materializedConfiguration.getChannels().entrySet()) {        try {                        supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);        } catch (Exception e) {                    }    }    /*     * Wait for all channels to start.     */    for (Channel ch : materializedConfiguration.getChannels().values()) {        while (ch.getLifecycleState() != LifecycleState.START && !supervisor.isComponentInErrorState(ch)) {            try {                                Thread.sleep(500);            } catch (InterruptedException e) {                                Throwables.propagate(e);            }        }    }    for (Entry<String, SinkRunner> entry : materializedConfiguration.getSinkRunners().entrySet()) {        try {                        supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);        } catch (Exception e) {                    }    }    for (Entry<String, SourceRunner> entry : materializedConfiguration.getSourceRunners().entrySet()) {        try {                        supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);        } catch (Exception e) {                    }    }    this.loadMonitoring();}
private void flume_f4206_1()
{    Properties systemProps = System.getProperties();    Set<String> keys = systemProps.stringPropertyNames();    try {        if (keys.contains(CONF_MONITOR_CLASS)) {            String monitorType = systemProps.getProperty(CONF_MONITOR_CLASS);            Class<? extends MonitorService> klass;            try {                                klass = MonitoringType.valueOf(monitorType.toUpperCase(Locale.ENGLISH)).getMonitorClass();            } catch (Exception e) {                                klass = (Class<? extends MonitorService>) Class.forName(monitorType);            }            this.monitorServer = klass.newInstance();            Context context = new Context();            for (String key : keys) {                if (key.startsWith(CONF_MONITOR_PREFIX)) {                    context.put(key.substring(CONF_MONITOR_PREFIX.length()), systemProps.getProperty(key));                }            }            monitorServer.configure(context);            monitorServer.start();        }    } catch (Exception e) {            }}
public static void flume_f4207_1(String[] args)
{    try {        SSLUtil.initGlobalSSLParameters();        Options options = new Options();        Option option = new Option("n", "name", true, "the name of this agent");        option.setRequired(true);        options.addOption(option);        option = new Option("f", "conf-file", true, "specify a config file (required if -z missing)");        option.setRequired(false);        options.addOption(option);        option = new Option(null, "no-reload-conf", false, "do not reload config file if changed");        options.addOption(option);                option = new Option("z", "zkConnString", true, "specify the ZooKeeper connection to use (required if -f missing)");        option.setRequired(false);        options.addOption(option);        option = new Option("p", "zkBasePath", true, "specify the base path in ZooKeeper for agent configs");        option.setRequired(false);        options.addOption(option);        option = new Option("h", "help", false, "display help text");        options.addOption(option);        CommandLineParser parser = new GnuParser();        CommandLine commandLine = parser.parse(options, args);        if (commandLine.hasOption('h')) {            new HelpFormatter().printHelp("flume-ng agent", options, true);            return;        }        String agentName = commandLine.getOptionValue('n');        boolean reload = !commandLine.hasOption("no-reload-conf");        boolean isZkConfigured = false;        if (commandLine.hasOption('z') || commandLine.hasOption("zkConnString")) {            isZkConfigured = true;        }        Application application;        if (isZkConfigured) {                        String zkConnectionStr = commandLine.getOptionValue('z');            String baseZkPath = commandLine.getOptionValue('p');            if (reload) {                EventBus eventBus = new EventBus(agentName + "-event-bus");                List<LifecycleAware> components = Lists.newArrayList();                PollingZooKeeperConfigurationProvider zookeeperConfigurationProvider = new PollingZooKeeperConfigurationProvider(agentName, zkConnectionStr, baseZkPath, eventBus);                components.add(zookeeperConfigurationProvider);                application = new Application(components);                eventBus.register(application);            } else {                StaticZooKeeperConfigurationProvider zookeeperConfigurationProvider = new StaticZooKeeperConfigurationProvider(agentName, zkConnectionStr, baseZkPath);                application = new Application();                application.handleConfigurationEvent(zookeeperConfigurationProvider.getConfiguration());            }        } else {            File configurationFile = new File(commandLine.getOptionValue('f'));            /*         * The following is to ensure that by default the agent will fail on         * startup if the file does not exist.         */            if (!configurationFile.exists()) {                                if (System.getProperty(Constants.SYSPROP_CALLED_FROM_SERVICE) == null) {                    String path = configurationFile.getPath();                    try {                        path = configurationFile.getCanonicalPath();                    } catch (IOException ex) {                                            }                    throw new ParseException("The specified configuration file does not exist: " + path);                }            }            List<LifecycleAware> components = Lists.newArrayList();            if (reload) {                EventBus eventBus = new EventBus(agentName + "-event-bus");                PollingPropertiesFileConfigurationProvider configurationProvider = new PollingPropertiesFileConfigurationProvider(agentName, configurationFile, eventBus, 30);                components.add(configurationProvider);                application = new Application(components);                eventBus.register(application);            } else {                PropertiesFileConfigurationProvider configurationProvider = new PropertiesFileConfigurationProvider(agentName, configurationFile);                application = new Application();                application.handleConfigurationEvent(configurationProvider.getConfiguration());            }        }        application.start();        final Application appReference = application;        Runtime.getRuntime().addShutdownHook(new Thread("agent-shutdown-hook") {            @Override            public void run() {                appReference.stop();            }        });    } catch (Exception e) {            }}
public void flume_f4208_0()
{    appReference.stop();}
protected static String flume_f4209_0(String input)
{    Preconditions.checkNotNull(input);        Pattern p = Pattern.compile("\\$\\{(\\w+)\\}");    Matcher m = p.matcher(input);    StringBuffer sb = new StringBuffer();    while (m.find()) {        String envVarName = m.group(1);        String envVarValue = System.getenv(envVarName);        m.appendReplacement(sb, null == envVarValue ? "" : envVarValue);    }    m.appendTail(sb);    return sb.toString();}
public String flume_f4210_0(String key)
{    return resolveEnvVars(super.getProperty(key));}
public void flume_f4211_1()
{        Preconditions.checkState(file != null, "The parameter file must not be null");    executorService = Executors.newSingleThreadScheduledExecutor(new ThreadFactoryBuilder().setNameFormat("conf-file-poller-%d").build());    FileWatcherRunnable fileWatcherRunnable = new FileWatcherRunnable(file, counterGroup);    executorService.scheduleWithFixedDelay(fileWatcherRunnable, 0, interval, TimeUnit.SECONDS);    lifecycleState = LifecycleState.START;    }
public void flume_f4212_1()
{        executorService.shutdown();    try {        if (!executorService.awaitTermination(500, TimeUnit.MILLISECONDS)) {                        executorService.shutdownNow();            while (!executorService.awaitTermination(500, TimeUnit.MILLISECONDS)) {                            }        }    } catch (InterruptedException e) {                Thread.currentThread().interrupt();    }    lifecycleState = LifecycleState.STOP;    }
public synchronized LifecycleState flume_f4213_0()
{    return lifecycleState;}
public String flume_f4214_0()
{    return "{ file:" + file + " counterGroup:" + counterGroup + "  provider:" + getClass().getCanonicalName() + " agentName:" + getAgentName() + " }";}
public void flume_f4215_1()
{        counterGroup.incrementAndGet("file.checks");    long lastModified = file.lastModified();    if (lastModified > lastChange) {                counterGroup.incrementAndGet("file.loads");        lastChange = lastModified;        try {            eventBus.post(getConfiguration());        } catch (Exception e) {                    } catch (NoClassDefFoundError e) {                    } catch (Throwable t) {                                }    }}
protected FlumeConfiguration flume_f4216_0()
{    return flumeConfiguration;}
public void flume_f4217_1()
{        try {        client.start();        try {            agentNodeCache = new NodeCache(client, basePath + "/" + getAgentName());            agentNodeCache.start();            agentNodeCache.getListenable().addListener(new NodeCacheListener() {                @Override                public void nodeChanged() throws Exception {                    refreshConfiguration();                }            });        } catch (Exception e) {            client.close();            throw e;        }    } catch (Exception e) {        lifecycleState = LifecycleState.ERROR;        if (e instanceof RuntimeException) {            throw (RuntimeException) e;        } else {            throw new FlumeException(e);        }    }    lifecycleState = LifecycleState.START;}
public void flume_f4218_0() throws Exception
{    refreshConfiguration();}
private void flume_f4219_1() throws IOException
{        byte[] data = null;    ChildData childData = agentNodeCache.getCurrentData();    if (childData != null) {        data = childData.getData();    }    flumeConfiguration = configFromBytes(data);    eventBus.post(getConfiguration());}
public void flume_f4220_1()
{        if (agentNodeCache != null) {        try {            agentNodeCache.close();        } catch (IOException e) {                        lifecycleState = LifecycleState.ERROR;        }    }    try {        client.close();    } catch (Exception e) {                lifecycleState = LifecycleState.ERROR;    }    if (lifecycleState != LifecycleState.ERROR) {        lifecycleState = LifecycleState.STOP;    }}
public LifecycleState flume_f4221_0()
{    return lifecycleState;}
public FlumeConfiguration flume_f4222_1()
{    BufferedReader reader = null;    try {        reader = new BufferedReader(new FileReader(file));        String resolverClassName = System.getProperty("propertiesImplementation", DEFAULT_PROPERTIES_IMPLEMENTATION);        Class<? extends Properties> propsclass = Class.forName(resolverClassName).asSubclass(Properties.class);        Properties properties = propsclass.newInstance();        properties.load(reader);        return new FlumeConfiguration(toMap(properties));    } catch (IOException ex) {            } catch (ClassNotFoundException e) {            } catch (InstantiationException e) {            } catch (IllegalAccessException e) {            } finally {        if (reader != null) {            try {                reader.close();            } catch (IOException ex) {                            }        }    }    return new FlumeConfiguration(new HashMap<String, String>());}
public String flume_f4223_0()
{    return "{ sourceRunners:" + sourceRunners + " sinkRunners:" + sinkRunners + " channels:" + channels + " }";}
public void flume_f4224_0(String name, SourceRunner sourceRunner)
{    sourceRunners.put(name, sourceRunner);}
public void flume_f4225_0(String name, SinkRunner sinkRunner)
{    sinkRunners.put(name, sinkRunner);}
public void flume_f4226_0(String name, Channel channel)
{    channels.put(name, channel);}
public Map<String, Channel> flume_f4227_0()
{    return ImmutableMap.copyOf(channels);}
public Map<String, SourceRunner> flume_f4228_0()
{    return ImmutableMap.copyOf(sourceRunners);}
public Map<String, SinkRunner> flume_f4229_0()
{    return ImmutableMap.copyOf(sinkRunners);}
protected FlumeConfiguration flume_f4230_1()
{    try {        CuratorFramework cf = createClient();        cf.start();        try {            byte[] data = cf.getData().forPath(basePath + "/" + getAgentName());            return configFromBytes(data);        } finally {            cf.close();        }    } catch (Exception e) {                throw new FlumeException(e);    }}
public void flume_f4231_0() throws Exception
{    String agentName = "agent1";    Map<String, String> properties = getPropertiesForChannel(agentName, DisposableChannel.class.getName());    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config1 = provider.getConfiguration();    Channel channel1 = config1.getChannels().values().iterator().next();    Assert.assertTrue(channel1 instanceof DisposableChannel);    MaterializedConfiguration config2 = provider.getConfiguration();    Channel channel2 = config2.getChannels().values().iterator().next();    Assert.assertTrue(channel2 instanceof DisposableChannel);    Assert.assertNotSame(channel1, channel2);}
public void flume_f4232_0() throws Exception
{    String agentName = "agent1";    Map<String, String> properties = getPropertiesForChannel(agentName, RecyclableChannel.class.getName());    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config1 = provider.getConfiguration();    Channel channel1 = config1.getChannels().values().iterator().next();    Assert.assertTrue(channel1 instanceof RecyclableChannel);    MaterializedConfiguration config2 = provider.getConfiguration();    Channel channel2 = config2.getChannels().values().iterator().next();    Assert.assertTrue(channel2 instanceof RecyclableChannel);    Assert.assertSame(channel1, channel2);}
public void flume_f4233_0() throws Exception
{    String agentName = "agent1";    Map<String, String> properties = getPropertiesForChannel(agentName, UnspecifiedChannel.class.getName());    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config1 = provider.getConfiguration();    Channel channel1 = config1.getChannels().values().iterator().next();    Assert.assertTrue(channel1 instanceof UnspecifiedChannel);    MaterializedConfiguration config2 = provider.getConfiguration();    Channel channel2 = config2.getChannels().values().iterator().next();    Assert.assertTrue(channel2 instanceof UnspecifiedChannel);    Assert.assertSame(channel1, channel2);}
public void flume_f4234_0() throws Exception
{    String agentName = "agent1";    Map<String, String> propertiesReusable = getPropertiesForChannel(agentName, RecyclableChannel.class.getName());    Map<String, String> propertiesDispoable = getPropertiesForChannel(agentName, DisposableChannel.class.getName());    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, propertiesReusable);    MaterializedConfiguration config1 = provider.getConfiguration();    Channel channel1 = config1.getChannels().values().iterator().next();    Assert.assertTrue(channel1 instanceof RecyclableChannel);    provider.setProperties(propertiesDispoable);    MaterializedConfiguration config2 = provider.getConfiguration();    Channel channel2 = config2.getChannels().values().iterator().next();    Assert.assertTrue(channel2 instanceof DisposableChannel);    provider.setProperties(propertiesReusable);    MaterializedConfiguration config3 = provider.getConfiguration();    Channel channel3 = config3.getChannels().values().iterator().next();    Assert.assertTrue(channel3 instanceof RecyclableChannel);    Assert.assertNotSame(channel1, channel3);}
public void flume_f4235_0() throws Exception
{    String agentName = "agent1";    String sourceType = UnconfigurableSource.class.getName();    String channelType = "memory";    String sinkType = "null";    Map<String, String> properties = getProperties(agentName, sourceType, channelType, sinkType);    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 0);    Assert.assertTrue(config.getChannels().size() == 1);    Assert.assertTrue(config.getSinkRunners().size() == 1);}
public void flume_f4236_0() throws Exception
{    String agentName = "agent1";    String sourceType = "seq";    String channelType = UnconfigurableChannel.class.getName();    String sinkType = "null";    Map<String, String> properties = getProperties(agentName, sourceType, channelType, sinkType);    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 0);    Assert.assertTrue(config.getChannels().size() == 0);    Assert.assertTrue(config.getSinkRunners().size() == 0);}
public void flume_f4237_0() throws Exception
{    String agentName = "agent1";    String sourceType = "seq";    String channelType = "memory";    String sinkType = UnconfigurableSink.class.getName();    Map<String, String> properties = getProperties(agentName, sourceType, channelType, sinkType);    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 1);    Assert.assertTrue(config.getChannels().size() == 1);    Assert.assertTrue(config.getSinkRunners().size() == 0);}
public void flume_f4238_0() throws Exception
{    String agentName = "agent1";    String sourceType = UnconfigurableSource.class.getName();    String channelType = "memory";    String sinkType = UnconfigurableSink.class.getName();    Map<String, String> properties = getProperties(agentName, sourceType, channelType, sinkType);    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 0);    Assert.assertTrue(config.getChannels().size() == 0);    Assert.assertTrue(config.getSinkRunners().size() == 0);}
public void flume_f4239_0() throws Exception
{    String agentName = "agent1";    String sourceType = "seq";    String channelType = "memory";    String sinkType = "avro";    Map<String, String> properties = getProperties(agentName, sourceType, channelType, sinkType);    properties.put(agentName + ".channels.channel1.capacity", "1000");    properties.put(agentName + ".channels.channel1.transactionCapacity", "1000");    properties.put(agentName + ".sources.source1.batchSize", "1000");    properties.put(agentName + ".sinks.sink1.batch-size", "1000");    properties.put(agentName + ".sinks.sink1.hostname", "10.10.10.10");    properties.put(agentName + ".sinks.sink1.port", "1010");    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 1);    Assert.assertTrue(config.getChannels().size() == 1);    Assert.assertTrue(config.getSinkRunners().size() == 1);    properties.put(agentName + ".sources.source1.batchSize", "1001");    properties.put(agentName + ".sinks.sink1.batch-size", "1000");    provider = new MemoryConfigurationProvider(agentName, properties);    config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 0);    Assert.assertTrue(config.getChannels().size() == 1);    Assert.assertTrue(config.getSinkRunners().size() == 1);    properties.put(agentName + ".sources.source1.batchSize", "1000");    properties.put(agentName + ".sinks.sink1.batch-size", "1001");    provider = new MemoryConfigurationProvider(agentName, properties);    config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 1);    Assert.assertTrue(config.getChannels().size() == 1);    Assert.assertTrue(config.getSinkRunners().size() == 0);    properties.put(agentName + ".sources.source1.batchSize", "1001");    properties.put(agentName + ".sinks.sink1.batch-size", "1001");    provider = new MemoryConfigurationProvider(agentName, properties);    config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 0);    Assert.assertTrue(config.getChannels().size() == 0);    Assert.assertTrue(config.getSinkRunners().size() == 0);}
private Map<String, String> flume_f4240_0(String agentName, String sourceType, String channelType, String sinkType)
{    Map<String, String> properties = Maps.newHashMap();    properties.put(agentName + ".sources", "source1");    properties.put(agentName + ".channels", "channel1");    properties.put(agentName + ".sinks", "sink1");    properties.put(agentName + ".sources.source1.type", sourceType);    properties.put(agentName + ".sources.source1.channels", "channel1");    properties.put(agentName + ".channels.channel1.type", channelType);    properties.put(agentName + ".channels.channel1.capacity", "100");    properties.put(agentName + ".sinks.sink1.type", sinkType);    properties.put(agentName + ".sinks.sink1.channel", "channel1");    return properties;}
private Map<String, String> flume_f4241_0(String agentName, String channelType)
{    return getProperties(agentName, "seq", channelType, "null");}
public void flume_f4242_0(Map<String, String> properties)
{    this.properties = properties;}
protected FlumeConfiguration flume_f4243_0()
{    return new FlumeConfiguration(properties);}
public void flume_f4244_0(Event event) throws ChannelException
{    throw new UnsupportedOperationException();}
public Event flume_f4245_0() throws ChannelException
{    throw new UnsupportedOperationException();}
public Transaction flume_f4246_0()
{    throw new UnsupportedOperationException();}
public void flume_f4247_0(Event event) throws ChannelException
{    throw new UnsupportedOperationException();}
public Event flume_f4248_0() throws ChannelException
{    throw new UnsupportedOperationException();}
public Transaction flume_f4249_0()
{    throw new UnsupportedOperationException();}
public void flume_f4250_0(Event event) throws ChannelException
{    throw new UnsupportedOperationException();}
public Event flume_f4251_0() throws ChannelException
{    throw new UnsupportedOperationException();}
public Transaction flume_f4252_0()
{    throw new UnsupportedOperationException();}
public void flume_f4253_0(Context context)
{    throw new RuntimeException("expected");}
public void flume_f4254_0(Event event) throws ChannelException
{    throw new UnsupportedOperationException();}
public Event flume_f4255_0() throws ChannelException
{    throw new UnsupportedOperationException();}
public Transaction flume_f4256_0()
{    throw new UnsupportedOperationException();}
public void flume_f4257_0(Context context)
{    throw new RuntimeException("expected");}
public void flume_f4258_0(Context context)
{    throw new RuntimeException("expected");}
public Status flume_f4259_0() throws EventDeliveryException
{    throw new UnsupportedOperationException();}
public void flume_f4260_0() throws Exception
{    zkServer = new TestingServer();    client = CuratorFrameworkFactory.newClient("localhost:" + zkServer.getPort(), new ExponentialBackoffRetry(1000, 3));    client.start();    EnsurePath ensurePath = new EnsurePath(AGENT_PATH);    ensurePath.ensure(client.getZookeeperClient());    doSetUp();}
public void flume_f4261_0() throws Exception
{    doTearDown();    zkServer.close();    client.close();}
protected void flume_f4262_0() throws Exception
{    Reader in = new InputStreamReader(getClass().getClassLoader().getResourceAsStream(FLUME_CONF_FILE), Charsets.UTF_8);    try {        String config = IOUtils.toString(in);        client.setData().forPath(AGENT_PATH, config.getBytes());    } finally {        in.close();    }}
protected void flume_f4263_0(AbstractConfigurationProvider cp)
{    FlumeConfiguration configuration = cp.getFlumeConfiguration();    Assert.assertNotNull(configuration);    /*     * Test the known errors in the file     */    List<String> expected = Lists.newArrayList();    expected.add("host5 CONFIG_ERROR");    expected.add("host5 INVALID_PROPERTY");    expected.add("host4 CONFIG_ERROR");    expected.add("host4 CONFIG_ERROR");    expected.add("host4 PROPERTY_VALUE_NULL");    expected.add("host4 PROPERTY_VALUE_NULL");    expected.add("host4 PROPERTY_VALUE_NULL");    expected.add("host4 AGENT_CONFIGURATION_INVALID");    expected.add("ch2 ATTRS_MISSING");    expected.add("host3 CONFIG_ERROR");    expected.add("host3 PROPERTY_VALUE_NULL");    expected.add("host3 AGENT_CONFIGURATION_INVALID");    expected.add("host2 PROPERTY_VALUE_NULL");    expected.add("host2 AGENT_CONFIGURATION_INVALID");    List<String> actual = Lists.newArrayList();    for (FlumeConfigurationError error : configuration.getConfigurationErrors()) {        actual.add(error.getComponentName() + " " + error.getErrorType().toString());    }    Collections.sort(expected);    Collections.sort(actual);    Assert.assertEquals(expected, actual);    FlumeConfiguration.AgentConfiguration agentConfiguration = configuration.getConfigurationFor("host1");    Assert.assertNotNull(agentConfiguration);    Set<String> sources = Sets.newHashSet("source1");    Set<String> sinks = Sets.newHashSet("sink1");    Set<String> channels = Sets.newHashSet("channel1");    Assert.assertEquals(sources, agentConfiguration.getSourceSet());    Assert.assertEquals(sinks, agentConfiguration.getSinkSet());    Assert.assertEquals(channels, agentConfiguration.getChannelSet());}
public void flume_f4264_0() throws Exception
{    baseDir = Files.createTempDir();}
public void flume_f4265_0() throws Exception
{    FileUtils.deleteDirectory(baseDir);}
private T flume_f4266_0(Class<T> klass)
{    T lifeCycleAware = mock(klass);    final AtomicReference<LifecycleState> state = new AtomicReference<LifecycleState>();    state.set(LifecycleState.IDLE);    when(lifeCycleAware.getLifecycleState()).then(new Answer<LifecycleState>() {        @Override        public LifecycleState answer(InvocationOnMock invocation) throws Throwable {            return state.get();        }    });    doAnswer(new Answer<Void>() {        @Override        public Void answer(InvocationOnMock invocation) throws Throwable {            state.set(LifecycleState.START);            return null;        }    }).when(lifeCycleAware).start();    doAnswer(new Answer<Void>() {        @Override        public Void answer(InvocationOnMock invocation) throws Throwable {            state.set(LifecycleState.STOP);            return null;        }    }).when(lifeCycleAware).stop();    return lifeCycleAware;}
public LifecycleState flume_f4267_0(InvocationOnMock invocation) throws Throwable
{    return state.get();}
public Void flume_f4268_0(InvocationOnMock invocation) throws Throwable
{    state.set(LifecycleState.START);    return null;}
public Void flume_f4269_0(InvocationOnMock invocation) throws Throwable
{    state.set(LifecycleState.STOP);    return null;}
public void flume_f4270_0() throws Exception
{    EventBus eventBus = new EventBus("test-event-bus");    MaterializedConfiguration materializedConfiguration = new SimpleMaterializedConfiguration();    SourceRunner sourceRunner = mockLifeCycle(SourceRunner.class);    materializedConfiguration.addSourceRunner("test", sourceRunner);    SinkRunner sinkRunner = mockLifeCycle(SinkRunner.class);    materializedConfiguration.addSinkRunner("test", sinkRunner);    Channel channel = mockLifeCycle(Channel.class);    materializedConfiguration.addChannel("test", channel);    ConfigurationProvider configurationProvider = mock(ConfigurationProvider.class);    when(configurationProvider.getConfiguration()).thenReturn(materializedConfiguration);    Application application = new Application();    eventBus.register(application);    eventBus.post(materializedConfiguration);    application.start();    Thread.sleep(1000L);    verify(sourceRunner).start();    verify(sinkRunner).start();    verify(channel).start();    application.stop();    Thread.sleep(1000L);    verify(sourceRunner).stop();    verify(sinkRunner).stop();    verify(channel).stop();}
public void flume_f4271_0() throws Exception
{    File configFile = new File(baseDir, "flume-conf.properties");    Files.copy(new File(getClass().getClassLoader().getResource("flume-conf.properties").getFile()), configFile);    Random random = new Random();    for (int i = 0; i < 3; i++) {        EventBus eventBus = new EventBus("test-event-bus");        PollingPropertiesFileConfigurationProvider configurationProvider = new PollingPropertiesFileConfigurationProvider("host1", configFile, eventBus, 1);        List<LifecycleAware> components = Lists.newArrayList();        components.add(configurationProvider);        Application application = new Application(components);        eventBus.register(application);        application.start();        Thread.sleep(random.nextInt(10000));        application.stop();    }}
public void flume_f4272_0() throws Exception
{    final String agentName = "test";    final int interval = 1;    final long intervalMs = 1000L;    File configFile = new File(baseDir, "flume-conf.properties");    Files.copy(new File(getClass().getClassLoader().getResource("flume-conf.properties.2786").getFile()), configFile);    File mockConfigFile = spy(configFile);    when(mockConfigFile.lastModified()).then(new Answer<Long>() {        @Override        public Long answer(InvocationOnMock invocation) throws Throwable {            Thread.sleep(intervalMs);            return System.currentTimeMillis();        }    });    EventBus eventBus = new EventBus(agentName + "-event-bus");    PollingPropertiesFileConfigurationProvider configurationProvider = new PollingPropertiesFileConfigurationProvider(agentName, mockConfigFile, eventBus, interval);    PollingPropertiesFileConfigurationProvider mockConfigurationProvider = spy(configurationProvider);    doAnswer(new Answer<Void>() {        @Override        public Void answer(InvocationOnMock invocation) throws Throwable {            Thread.sleep(intervalMs);            invocation.callRealMethod();            return null;        }    }).when(mockConfigurationProvider).stop();    List<LifecycleAware> components = Lists.newArrayList();    components.add(mockConfigurationProvider);    Application application = new Application(components);    eventBus.register(application);    application.start();    Thread.sleep(1500L);    application.stop();}
public Long flume_f4273_0(InvocationOnMock invocation) throws Throwable
{    Thread.sleep(intervalMs);    return System.currentTimeMillis();}
public Void flume_f4274_0(InvocationOnMock invocation) throws Throwable
{    Thread.sleep(intervalMs);    invocation.callRealMethod();    return null;}
public void flume_f4275_0() throws Exception
{    provider = new PropertiesFileConfigurationProvider("a1", TESTFILE);}
public void flume_f4276_0() throws Exception
{    environmentVariables.set("VARNAME", "varvalue");    String resolved = EnvVarResolverProperties.resolveEnvVars("padding ${VARNAME} padding");    Assert.assertEquals("padding varvalue padding", resolved);}
public void flume_f4277_0() throws Exception
{    environmentVariables.set("VARNAME1", "varvalue1");    environmentVariables.set("VARNAME2", "varvalue2");    String resolved = EnvVarResolverProperties.resolveEnvVars("padding ${VARNAME1} ${VARNAME2} padding");    Assert.assertEquals("padding varvalue1 varvalue2 padding", resolved);}
public void flume_f4278_0() throws Exception
{    String NC_PORT = "6667";    environmentVariables.set("NC_PORT", NC_PORT);    System.setProperty("propertiesImplementation", "org.apache.flume.node.EnvVarResolverProperties");    Assert.assertEquals(NC_PORT, provider.getFlumeConfiguration().getConfigurationFor("a1").getSourceContext().get("r1").getParameters().get("port"));}
public void flume_f4279_0() throws Exception
{    baseDir = Files.createTempDir();    configFile = new File(baseDir, TESTFILE.getName());    Files.copy(TESTFILE, configFile);    eventBus = new EventBus("test");    provider = new PollingPropertiesFileConfigurationProvider("host1", configFile, eventBus, 1);    provider.start();    LifecycleController.waitForOneOf(provider, LifecycleState.START_OR_ERROR);}
public void flume_f4280_0() throws Exception
{    FileUtils.deleteDirectory(baseDir);    provider.stop();}
public void flume_f4281_0() throws Exception
{        Thread.sleep(2000L);    final List<MaterializedConfiguration> events = Lists.newArrayList();    Object eventHandler = new Object() {        @Subscribe        public synchronized void handleConfigurationEvent(MaterializedConfiguration event) {            events.add(event);        }    };    eventBus.register(eventHandler);    configFile.setLastModified(System.currentTimeMillis());        Thread.sleep(2000L);    Assert.assertEquals(String.valueOf(events), 1, events.size());    MaterializedConfiguration materializedConfiguration = events.remove(0);    Assert.assertEquals(1, materializedConfiguration.getSourceRunners().size());    Assert.assertEquals(1, materializedConfiguration.getSinkRunners().size());    Assert.assertEquals(1, materializedConfiguration.getChannels().size());}
public synchronized void flume_f4282_0(MaterializedConfiguration event)
{    events.add(event);}
public synchronized void flume_f4283_0(MaterializedConfiguration mConfig)
{    notified = true;    notifyAll();}
public synchronized void flume_f4284_0() throws InterruptedException
{    while (!notified) {        wait();    }}
public synchronized void flume_f4285_0()
{    notified = false;}
protected void flume_f4286_0() throws Exception
{    eb = new EventBus("test");    es = new EventSync();    es.reset();    eb.register(es);    cp = new PollingZooKeeperConfigurationProvider(AGENT_NAME, "localhost:" + zkServer.getPort(), null, eb);    cp.start();    LifecycleController.waitForOneOf(cp, LifecycleState.START_OR_ERROR);}
protected void flume_f4287_0() throws Exception
{}
public void flume_f4288_0() throws Exception
{    es.awaitEvent();    es.reset();    FlumeConfiguration fc = cp.getFlumeConfiguration();    Assert.assertTrue(fc.getConfigurationErrors().isEmpty());    AgentConfiguration ac = fc.getConfigurationFor(AGENT_NAME);    Assert.assertNull(ac);    addData();    es.awaitEvent();    es.reset();    verifyProperties(cp);}
public void flume_f4289_0() throws Exception
{    provider = new PropertiesFileConfigurationProvider("test", TESTFILE);}
public void flume_f4290_0() throws Exception
{}
public void flume_f4291_1() throws Exception
{    FlumeConfiguration configuration = provider.getFlumeConfiguration();    Assert.assertNotNull(configuration);    /*     * Test the known errors in the file     */    List<String> expected = Lists.newArrayList();    expected.add("host5 CONFIG_ERROR");    expected.add("host5 INVALID_PROPERTY");    expected.add("host4 CONFIG_ERROR");    expected.add("host4 CONFIG_ERROR");    expected.add("host4 PROPERTY_VALUE_NULL");    expected.add("host4 PROPERTY_VALUE_NULL");    expected.add("host4 PROPERTY_VALUE_NULL");    expected.add("host4 AGENT_CONFIGURATION_INVALID");    expected.add("ch2 ATTRS_MISSING");    expected.add("host3 CONFIG_ERROR");    expected.add("host3 PROPERTY_VALUE_NULL");    expected.add("host3 AGENT_CONFIGURATION_INVALID");    expected.add("host2 PROPERTY_VALUE_NULL");    expected.add("host2 AGENT_CONFIGURATION_INVALID");    List<String> actual = Lists.newArrayList();    for (FlumeConfigurationError error : configuration.getConfigurationErrors()) {        actual.add(error.getComponentName() + " " + error.getErrorType().toString());    }    Collections.sort(expected);    Collections.sort(actual);    Assert.assertEquals(expected, actual);    AgentConfiguration agentConfiguration = configuration.getConfigurationFor("host1");    Assert.assertNotNull(agentConfiguration);            Set<String> sources = Sets.newHashSet("source1");    Set<String> sinks = Sets.newHashSet("sink1");    Set<String> channels = Sets.newHashSet("channel1");    Assert.assertEquals(sources, agentConfiguration.getSourceSet());    Assert.assertEquals(sinks, agentConfiguration.getSinkSet());    Assert.assertEquals(channels, agentConfiguration.getChannelSet());}
protected void flume_f4292_0() throws Exception
{    addData();    configurationProvider = new StaticZooKeeperConfigurationProvider(AGENT_NAME, "localhost:" + zkServer.getPort(), null);}
protected void flume_f4293_0() throws Exception
{}
public void flume_f4294_0() throws Exception
{    verifyProperties(configurationProvider);}
public static Collection<?> flume_f4295_0()
{    Object[][] data = new Object[][] { { true }, { false } };    return Arrays.asList(data);}
private static int flume_f4296_0()
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    } catch (IOException e) {        throw new AssertionError("Can not open socket", e);    }}
public void flume_f4297_1()
{        channel = new MemoryChannel();    source = new NetcatSource();    Context context = new Context();    Configurables.configure(channel, context);    List<Channel> channels = Lists.newArrayList(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));}
public void flume_f4298_1() throws InterruptedException, LifecycleException, EventDeliveryException
{    final int port = getFreePort();    ExecutorService executor = Executors.newFixedThreadPool(3);    Context context = new Context();    context.put("bind", "0.0.0.0");    context.put("port", String.valueOf(port));    context.put("ack-every-event", String.valueOf(ackEveryEvent));    Configurables.configure(source, context);    source.start();    Runnable clientRequestRunnable = new Runnable() {        @Override        public void run() {            try {                SocketChannel clientChannel = SocketChannel.open(new InetSocketAddress(port));                Writer writer = Channels.newWriter(clientChannel, "utf-8");                BufferedReader reader = new BufferedReader(Channels.newReader(clientChannel, "utf-8"));                writer.write("Test message\n");                writer.flush();                if (ackEveryEvent) {                    String response = reader.readLine();                    Assert.assertEquals("Server should return OK", "OK", response);                } else {                    Assert.assertFalse("Server should not return anything", reader.ready());                }                clientChannel.close();            } catch (IOException e) {                            }        }    };    ChannelSelector selector = source.getChannelProcessor().getSelector();    Transaction tx = selector.getAllChannels().get(0).getTransaction();    tx.begin();    for (int i = 0; i < 100; i++) {                executor.submit(clientRequestRunnable);        Event event = channel.take();        Assert.assertNotNull(event);        Assert.assertArrayEquals("Test message".getBytes(), event.getBody());    }    tx.commit();    tx.close();    executor.shutdown();    while (!executor.isTerminated()) {        executor.awaitTermination(500, TimeUnit.MILLISECONDS);    }    source.stop();}
public void flume_f4299_1()
{    try {        SocketChannel clientChannel = SocketChannel.open(new InetSocketAddress(port));        Writer writer = Channels.newWriter(clientChannel, "utf-8");        BufferedReader reader = new BufferedReader(Channels.newReader(clientChannel, "utf-8"));        writer.write("Test message\n");        writer.flush();        if (ackEveryEvent) {            String response = reader.readLine();            Assert.assertEquals("Server should return OK", "OK", response);        } else {            Assert.assertFalse("Server should not return anything", reader.ready());        }        clientChannel.close();    } catch (IOException e) {            }}
public int flume_f4300_0()
{    return batchSize;}
private synchronized void flume_f4302_1(Properties properties) throws FlumeException
{    if (isActive) {                throw new FlumeException("This client was already configured, " + "cannot reconfigure.");    }    hosts = HostInfo.getHostInfoList(properties);    String tries = properties.getProperty(RpcClientConfigurationConstants.CONFIG_MAX_ATTEMPTS);    if (tries == null || tries.isEmpty()) {        maxTries = hosts.size();    } else {        try {            maxTries = Integer.parseInt(tries);        } catch (NumberFormatException e) {            maxTries = hosts.size();        }    }    batchSize = parseBatchSize(properties);    isActive = true;}
protected Integer flume_f4303_0()
{    return maxTries;}
private synchronized RpcClient flume_f4304_0()
{    if (client == null || !this.client.isActive()) {        client = getNextClient();        return client;    } else {        return client;    }}
public void flume_f4305_1(Event event) throws EventDeliveryException
{                    RpcClient localClient = null;    synchronized (this) {        if (!isActive) {                        throw new EventDeliveryException("Attempting to append to an already closed client.");        }    }        int tries = 0;    while (tries < maxTries) {        try {            tries++;            localClient = getClient();            localClient.append(event);            return;        } catch (EventDeliveryException e) {                                    localClient.close();            localClient = null;        } catch (Exception e2) {                        throw new EventDeliveryException("Failed to send event. Exception follows: ", e2);        }    }        throw new EventDeliveryException("Failed to send the event!");}
public void flume_f4306_1(List<Event> events) throws EventDeliveryException
{    RpcClient localClient = null;    synchronized (this) {        if (!isActive) {                        throw new EventDeliveryException("Attempting to append to an already closed client!");        }    }    int tries = 0;    while (tries < maxTries) {        try {            tries++;            localClient = getClient();            localClient.appendBatch(events);            return;        } catch (EventDeliveryException e) {                                    localClient.close();            localClient = null;        } catch (Exception e1) {                        throw new EventDeliveryException("No clients currently active. " + "Exception follows: ", e1);        }    }        throw new EventDeliveryException("Failed to send the event!");}
public synchronized boolean flume_f4307_0()
{    return isActive;}
public synchronized void flume_f4308_0() throws FlumeException
{    if (client != null) {        client.close();        isActive = false;    }}
protected InetSocketAddress flume_f4309_0()
{    HostInfo hostInfo = hosts.get(lastCheckedhost);    return new InetSocketAddress(hostInfo.getHostName(), hostInfo.getPortNumber());}
private RpcClient flume_f4310_1() throws FlumeException
{    lastCheckedhost = (lastCheckedhost == (hosts.size() - 1)) ? -1 : lastCheckedhost;    RpcClient localClient = null;    int limit = hosts.size();    Properties props = new Properties();    props.putAll(configurationProperties);    props.put(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, RpcClientConfigurationConstants.DEFAULT_CLIENT_TYPE);        for (int count = lastCheckedhost + 1; count < limit; count++) {        HostInfo hostInfo = hosts.get(count);        try {            setDefaultProperties(hostInfo, props);            localClient = RpcClientFactory.getInstance(props);            lastCheckedhost = count;            return localClient;        } catch (FlumeException e) {                        continue;        }    }    for (int count = 0; count <= lastCheckedhost; count++) {        HostInfo hostInfo = hosts.get(count);        try {            setDefaultProperties(hostInfo, props);            localClient = RpcClientFactory.getInstance(props);            lastCheckedhost = count;            return localClient;        } catch (FlumeException e) {                        continue;        }    }    if (localClient == null) {        lastCheckedhost = -1;                throw new FlumeException("No active client.");    }        return localClient;}
private void flume_f4311_0(HostInfo hostInfo, Properties props)
{    props.put(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, RpcClientFactory.ClientType.DEFAULT.name());    props.put(RpcClientConfigurationConstants.CONFIG_HOSTS, hostInfo.getReferenceName());}
public void flume_f4312_0(Properties properties) throws FlumeException
{    configurationProperties = new Properties();    configurationProperties.putAll(properties);    configureHosts(configurationProperties);}
public String flume_f4313_0()
{    return referenceName;}
public String flume_f4314_0()
{    return hostName;}
public int flume_f4315_0()
{    return portNumber;}
public String flume_f4316_0()
{    return referenceName + "{" + hostName + ":" + portNumber + "}";}
public static List<HostInfo> flume_f4317_1(Properties properties)
{    List<HostInfo> hosts = new ArrayList<HostInfo>();    String hostNames = properties.getProperty(RpcClientConfigurationConstants.CONFIG_HOSTS);    String[] hostList;    if (hostNames != null && !hostNames.isEmpty()) {        hostList = hostNames.split("\\s+");        for (int i = 0; i < hostList.length; i++) {            String hostAndPortStr = properties.getProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + hostList[i]);                        if (hostAndPortStr != null) {                String[] hostAndPort = hostAndPortStr.split(":");                if (hostAndPort.length != 2) {                                        throw new FlumeException("Invalid host address" + hostAndPortStr);                }                Integer port = null;                try {                    port = Integer.parseInt(hostAndPort[1]);                } catch (NumberFormatException e) {                                        throw new FlumeException("Invalid port number" + hostAndPortStr);                }                HostInfo info = new HostInfo(hostList[i], hostAndPort[0].trim(), port);                hosts.add(info);            }        }    }    return hosts;}
public void flume_f4318_1(Event event) throws EventDeliveryException
{    throwIfClosed();    boolean eventSent = false;    Iterator<HostInfo> it = selector.createHostIterator();    while (it.hasNext()) {        HostInfo host = it.next();        try {            RpcClient client = getClient(host);            client.append(event);            eventSent = true;            break;        } catch (Exception ex) {            selector.informFailure(host);                    }    }    if (!eventSent) {        throw new EventDeliveryException("Unable to send event to any host");    }}
public void flume_f4319_1(List<Event> events) throws EventDeliveryException
{    throwIfClosed();    boolean batchSent = false;    Iterator<HostInfo> it = selector.createHostIterator();    while (it.hasNext()) {        HostInfo host = it.next();        try {            RpcClient client = getClient(host);            client.appendBatch(events);            batchSent = true;            break;        } catch (Exception ex) {            selector.informFailure(host);                    }    }    if (!batchSent) {        throw new EventDeliveryException("Unable to send batch to any host");    }}
public boolean flume_f4320_0()
{    return isOpen;}
private void flume_f4321_0() throws EventDeliveryException
{    if (!isOpen) {        throw new EventDeliveryException("Rpc Client is closed");    }}
public void flume_f4322_1() throws FlumeException
{    isOpen = false;    synchronized (this) {        Iterator<String> it = clientMap.keySet().iterator();        while (it.hasNext()) {            String name = it.next();            RpcClient client = clientMap.get(name);            if (client != null) {                try {                    client.close();                } catch (Exception ex) {                                    }            }            it.remove();        }    }}
protected void flume_f4323_0(Properties properties) throws FlumeException
{    clientMap = new HashMap<String, RpcClient>();    configurationProperties = new Properties();    configurationProperties.putAll(properties);    hosts = HostInfo.getHostInfoList(properties);    if (hosts.size() < 2) {        throw new FlumeException("At least two hosts are required to use the " + "load balancing RPC client.");    }    String lbTypeName = properties.getProperty(RpcClientConfigurationConstants.CONFIG_HOST_SELECTOR, RpcClientConfigurationConstants.HOST_SELECTOR_ROUND_ROBIN);    boolean backoff = Boolean.valueOf(properties.getProperty(RpcClientConfigurationConstants.CONFIG_BACKOFF, String.valueOf(false)));    String maxBackoffStr = properties.getProperty(RpcClientConfigurationConstants.CONFIG_MAX_BACKOFF);    long maxBackoff = 0;    if (maxBackoffStr != null) {        maxBackoff = Long.parseLong(maxBackoffStr);    }    if (lbTypeName.equalsIgnoreCase(RpcClientConfigurationConstants.HOST_SELECTOR_ROUND_ROBIN)) {        selector = new RoundRobinHostSelector(backoff, maxBackoff);    } else if (lbTypeName.equalsIgnoreCase(RpcClientConfigurationConstants.HOST_SELECTOR_RANDOM)) {        selector = new RandomOrderHostSelector(backoff, maxBackoff);    } else {        try {            @SuppressWarnings("unchecked")            Class<? extends HostSelector> klass = (Class<? extends HostSelector>) Class.forName(lbTypeName);            selector = klass.newInstance();        } catch (Exception ex) {            throw new FlumeException("Unable to instantiate host selector: " + lbTypeName, ex);        }    }    selector.setHosts(hosts);    batchSize = parseBatchSize(properties);    isOpen = true;}
private synchronized RpcClient flume_f4324_1(HostInfo info) throws FlumeException, EventDeliveryException
{    throwIfClosed();    String name = info.getReferenceName();    RpcClient client = clientMap.get(name);    if (client == null) {        client = createClient(name);        clientMap.put(name, client);    } else if (!client.isActive()) {        try {            client.close();        } catch (Exception ex) {                    }        client = createClient(name);        clientMap.put(name, client);    }    return client;}
private RpcClient flume_f4325_0(String referenceName) throws FlumeException
{    Properties props = getClientConfigurationProperties(referenceName);    return RpcClientFactory.getInstance(props);}
private Properties flume_f4326_0(String referenceName)
{    Properties props = new Properties();    props.putAll(configurationProperties);    props.put(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, RpcClientFactory.ClientType.DEFAULT);    props.put(RpcClientConfigurationConstants.CONFIG_HOSTS, referenceName);    return props;}
public synchronized Iterator<HostInfo> flume_f4327_0()
{    return selector.createIterator();}
public synchronized void flume_f4328_0(List<HostInfo> hosts)
{    selector.setObjects(hosts);}
public synchronized void flume_f4329_0(HostInfo failedHost)
{    selector.informFailure(failedHost);}
public synchronized Iterator<HostInfo> flume_f4330_0()
{    return selector.createIterator();}
public synchronized void flume_f4331_0(List<HostInfo> hosts)
{    selector.setObjects(hosts);}
public void flume_f4332_0(HostInfo failedHost)
{    selector.informFailure(failedHost);}
private void flume_f4333_0() throws FlumeException
{    connect(connectTimeout, TimeUnit.MILLISECONDS);}
private void flume_f4334_0(long timeout, TimeUnit tu) throws FlumeException
{    callTimeoutPool = Executors.newCachedThreadPool(new TransceiverThreadFactory("Flume Avro RPC Client Call Invoker"));    NioClientSocketChannelFactory socketChannelFactory = null;    try {        ExecutorService bossExecutor = Executors.newCachedThreadPool(new TransceiverThreadFactory("Avro " + NettyTransceiver.class.getSimpleName() + " Boss"));        ExecutorService workerExecutor = Executors.newCachedThreadPool(new TransceiverThreadFactory("Avro " + NettyTransceiver.class.getSimpleName() + " I/O Worker"));        if (enableDeflateCompression || enableSsl) {            if (maxIoWorkers >= 1) {                socketChannelFactory = new SSLCompressionChannelFactory(bossExecutor, workerExecutor, enableDeflateCompression, enableSsl, trustAllCerts, compressionLevel, truststore, truststorePassword, truststoreType, excludeProtocols, includeProtocols, excludeCipherSuites, includeCipherSuites, maxIoWorkers);            } else {                socketChannelFactory = new SSLCompressionChannelFactory(bossExecutor, workerExecutor, enableDeflateCompression, enableSsl, trustAllCerts, compressionLevel, truststore, truststorePassword, truststoreType, excludeProtocols, includeProtocols, excludeCipherSuites, includeCipherSuites);            }        } else {            if (maxIoWorkers >= 1) {                socketChannelFactory = new NioClientSocketChannelFactory(bossExecutor, workerExecutor, maxIoWorkers);            } else {                socketChannelFactory = new NioClientSocketChannelFactory(bossExecutor, workerExecutor);            }        }        transceiver = new NettyTransceiver(this.address, socketChannelFactory, tu.toMillis(timeout));        avroClient = SpecificRequestor.getClient(AvroSourceProtocol.Callback.class, transceiver);    } catch (Throwable t) {        if (callTimeoutPool != null) {            callTimeoutPool.shutdownNow();        }        if (socketChannelFactory != null) {            socketChannelFactory.releaseExternalResources();        }        if (t instanceof IOException) {            throw new FlumeException(this + ": RPC connection error", t);        } else if (t instanceof FlumeException) {            throw (FlumeException) t;        } else if (t instanceof Error) {            throw (Error) t;        } else {            throw new FlumeException(this + ": Unexpected exception", t);        }    }    setState(ConnState.READY);}
public void flume_f4335_1() throws FlumeException
{    if (callTimeoutPool != null) {        callTimeoutPool.shutdown();        try {            if (!callTimeoutPool.awaitTermination(requestTimeout, TimeUnit.MILLISECONDS)) {                callTimeoutPool.shutdownNow();                if (!callTimeoutPool.awaitTermination(requestTimeout, TimeUnit.MILLISECONDS)) {                                    }            }        } catch (InterruptedException ex) {                                    callTimeoutPool.shutdownNow();                        Thread.currentThread().interrupt();        }        callTimeoutPool = null;    }    try {        transceiver.close();    } catch (IOException ex) {        throw new FlumeException(this + ": Error closing transceiver.", ex);    } finally {        setState(ConnState.DEAD);    }}
public String flume_f4336_0()
{    return "NettyAvroRpcClient { host: " + address.getHostName() + ", port: " + address.getPort() + " }";}
public void flume_f4337_0(Event event) throws EventDeliveryException
{    try {        append(event, requestTimeout, TimeUnit.MILLISECONDS);    } catch (Throwable t) {                        setState(ConnState.DEAD);        if (t instanceof Error) {            throw (Error) t;        }        if (t instanceof TimeoutException) {            throw new EventDeliveryException(this + ": Failed to send event. " + "RPC request timed out after " + requestTimeout + "ms", t);        }        throw new EventDeliveryException(this + ": Failed to send event", t);    }}
private void flume_f4338_0(Event event, long timeout, TimeUnit tu) throws EventDeliveryException
{    assertReady();    final CallFuture<Status> callFuture = new CallFuture<Status>();    final AvroFlumeEvent avroEvent = new AvroFlumeEvent();    avroEvent.setBody(ByteBuffer.wrap(event.getBody()));    avroEvent.setHeaders(toCharSeqMap(event.getHeaders()));    Future<Void> handshake;    try {                handshake = callTimeoutPool.submit(new Callable<Void>() {            @Override            public Void call() throws Exception {                avroClient.append(avroEvent, callFuture);                return null;            }        });    } catch (RejectedExecutionException ex) {        throw new EventDeliveryException(this + ": Executor error", ex);    }    try {        handshake.get(connectTimeout, TimeUnit.MILLISECONDS);    } catch (TimeoutException ex) {        throw new EventDeliveryException(this + ": Handshake timed out after " + connectTimeout + " ms", ex);    } catch (InterruptedException ex) {        throw new EventDeliveryException(this + ": Interrupted in handshake", ex);    } catch (ExecutionException ex) {        throw new EventDeliveryException(this + ": RPC request exception", ex);    } catch (CancellationException ex) {        throw new EventDeliveryException(this + ": RPC request cancelled", ex);    } finally {        if (!handshake.isDone()) {            handshake.cancel(true);        }    }    waitForStatusOK(callFuture, timeout, tu);}
public Void flume_f4339_0() throws Exception
{    avroClient.append(avroEvent, callFuture);    return null;}
public void flume_f4340_0(List<Event> events) throws EventDeliveryException
{    try {        appendBatch(events, requestTimeout, TimeUnit.MILLISECONDS);    } catch (Throwable t) {                        setState(ConnState.DEAD);        if (t instanceof Error) {            throw (Error) t;        }        if (t instanceof TimeoutException) {            throw new EventDeliveryException(this + ": Failed to send event. " + "RPC request timed out after " + requestTimeout + " ms", t);        }        throw new EventDeliveryException(this + ": Failed to send batch", t);    }}
private void flume_f4341_0(List<Event> events, long timeout, TimeUnit tu) throws EventDeliveryException
{    assertReady();    Iterator<Event> iter = events.iterator();    final List<AvroFlumeEvent> avroEvents = new LinkedList<AvroFlumeEvent>();        while (iter.hasNext()) {        avroEvents.clear();        for (int i = 0; i < batchSize && iter.hasNext(); i++) {            Event event = iter.next();            AvroFlumeEvent avroEvent = new AvroFlumeEvent();            avroEvent.setBody(ByteBuffer.wrap(event.getBody()));            avroEvent.setHeaders(toCharSeqMap(event.getHeaders()));            avroEvents.add(avroEvent);        }        final CallFuture<Status> callFuture = new CallFuture<Status>();        Future<Void> handshake;        try {                        handshake = callTimeoutPool.submit(new Callable<Void>() {                @Override                public Void call() throws Exception {                    avroClient.appendBatch(avroEvents, callFuture);                    return null;                }            });        } catch (RejectedExecutionException ex) {            throw new EventDeliveryException(this + ": Executor error", ex);        }        try {            handshake.get(connectTimeout, TimeUnit.MILLISECONDS);        } catch (TimeoutException ex) {            throw new EventDeliveryException(this + ": Handshake timed out after " + connectTimeout + "ms", ex);        } catch (InterruptedException ex) {            throw new EventDeliveryException(this + ": Interrupted in handshake", ex);        } catch (ExecutionException ex) {            throw new EventDeliveryException(this + ": RPC request exception", ex);        } catch (CancellationException ex) {            throw new EventDeliveryException(this + ": RPC request cancelled", ex);        } finally {            if (!handshake.isDone()) {                handshake.cancel(true);            }        }        waitForStatusOK(callFuture, timeout, tu);    }}
public Void flume_f4342_0() throws Exception
{    avroClient.appendBatch(avroEvents, callFuture);    return null;}
private void flume_f4343_0(CallFuture<Status> callFuture, long timeout, TimeUnit tu) throws EventDeliveryException
{    try {        Status status = callFuture.get(timeout, tu);        if (status != Status.OK) {            throw new EventDeliveryException(this + ": Avro RPC call returned " + "Status: " + status);        }    } catch (CancellationException ex) {        throw new EventDeliveryException(this + ": RPC future was cancelled", ex);    } catch (ExecutionException ex) {        throw new EventDeliveryException(this + ": Exception thrown from " + "remote handler", ex);    } catch (TimeoutException ex) {        throw new EventDeliveryException(this + ": RPC request timed out", ex);    } catch (InterruptedException ex) {        Thread.currentThread().interrupt();        throw new EventDeliveryException(this + ": RPC request interrupted", ex);    }}
private void flume_f4344_0(ConnState newState)
{    stateLock.lock();    try {        if (connState == ConnState.DEAD && connState != newState) {            throw new IllegalStateException("Cannot transition from CLOSED state.");        }        connState = newState;    } finally {        stateLock.unlock();    }}
private void flume_f4345_0() throws EventDeliveryException
{    stateLock.lock();    try {        ConnState curState = connState;        if (curState != ConnState.READY) {            throw new EventDeliveryException("RPC failed, client in an invalid " + "state: " + curState);        }    } finally {        stateLock.unlock();    }}
private static Map<CharSequence, CharSequence> flume_f4346_0(Map<String, String> stringMap)
{    Map<CharSequence, CharSequence> charSeqMap = new HashMap<CharSequence, CharSequence>();    for (Map.Entry<String, String> entry : stringMap.entrySet()) {        charSeqMap.put(entry.getKey(), entry.getValue());    }    return charSeqMap;}
public boolean flume_f4347_0()
{    stateLock.lock();    try {        return (connState == ConnState.READY);    } finally {        stateLock.unlock();    }}
public Thread flume_f4349_0(Runnable r)
{    Thread thread = new Thread(r);    thread.setDaemon(true);    thread.setName(prefix + " " + threadId.incrementAndGet());    return thread;}
public SocketChannel flume_f4350_1(ChannelPipeline pipeline)
{    TrustManager[] managers;    try {        if (enableCompression) {            ZlibEncoder encoder = new ZlibEncoder(compressionLevel);            pipeline.addFirst("deflater", encoder);            pipeline.addFirst("inflater", new ZlibDecoder());        }        if (enableSsl) {            if (trustAllCerts) {                                managers = new TrustManager[] { new PermissiveTrustManager() };            } else {                KeyStore keystore = null;                if (truststore != null) {                    InputStream truststoreStream = new FileInputStream(truststore);                    keystore = KeyStore.getInstance(truststoreType);                    keystore.load(truststoreStream, truststorePassword != null ? truststorePassword.toCharArray() : null);                }                TrustManagerFactory tmf = TrustManagerFactory.getInstance("SunX509");                                                tmf.init(keystore);                managers = tmf.getTrustManagers();            }            SSLContext sslContext = SSLContext.getInstance("TLS");            sslContext.init(null, managers, null);            SSLEngine sslEngine = sslContext.createSSLEngine();            sslEngine.setUseClientMode(true);            List<String> enabledProtocols = new ArrayList<String>();            for (String protocol : sslEngine.getEnabledProtocols()) {                if ((includeProtocols.isEmpty() || includeProtocols.contains(protocol)) && !excludeProtocols.contains(protocol)) {                    enabledProtocols.add(protocol);                }            }            sslEngine.setEnabledProtocols(enabledProtocols.toArray(new String[0]));            List<String> enabledCipherSuites = new ArrayList<String>();            for (String suite : sslEngine.getEnabledCipherSuites()) {                if ((includeCipherSuites.isEmpty() || includeCipherSuites.contains(suite)) && !excludeCipherSuites.contains(suite)) {                    enabledCipherSuites.add(suite);                }            }            sslEngine.setEnabledCipherSuites(enabledCipherSuites.toArray(new String[0]));                                                                        pipeline.addFirst("ssl", new SslHandler(sslEngine));        }        return super.newChannel(pipeline);    } catch (Exception ex) {                throw new RuntimeException("Cannot create SSL channel", ex);    }}
public void flume_f4351_0(X509Certificate[] certs, String s)
{}
public void flume_f4352_0(X509Certificate[] certs, String s)
{}
public X509Certificate[] flume_f4353_0()
{    return new X509Certificate[0];}
public static RpcClient flume_f4354_0(Properties properties) throws FlumeException
{    String type = null;    type = properties.getProperty(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE);    if (type == null || type.isEmpty()) {        type = ClientType.DEFAULT.getClientClassName();    }    Class<? extends AbstractRpcClient> clazz;    AbstractRpcClient client;    try {        String clientClassType = type;        ClientType clientType = null;        try {            clientType = ClientType.valueOf(type.toUpperCase(Locale.ENGLISH));        } catch (IllegalArgumentException e) {            clientType = ClientType.OTHER;        }        if (!clientType.equals(ClientType.OTHER)) {            clientClassType = clientType.getClientClassName();        }        clazz = (Class<? extends AbstractRpcClient>) Class.forName(clientClassType);    } catch (ClassNotFoundException e) {        throw new FlumeException("No such client!", e);    }    try {        client = clazz.newInstance();    } catch (InstantiationException e) {        throw new FlumeException("Cannot instantiate client. " + "Exception follows:", e);    } catch (IllegalAccessException e) {        throw new FlumeException("Cannot instantiate client. " + "Exception follows:", e);    }    client.configure(properties);    return client;}
public static RpcClient flume_f4355_0(File propertiesFile) throws FileNotFoundException, IOException
{    Reader reader = new FileReader(propertiesFile);    Properties props = new Properties();    props.load(reader);    return getInstance(props);}
public static RpcClient flume_f4356_0(String hostname, Integer port) throws FlumeException
{    return getDefaultInstance(hostname, port);}
public static RpcClient flume_f4357_0(String hostname, Integer port) throws FlumeException
{    return getDefaultInstance(hostname, port, 0);}
public static RpcClient flume_f4358_0(String hostname, Integer port, Integer batchSize) throws FlumeException
{    return getDefaultInstance(hostname, port, batchSize);}
public static RpcClient flume_f4359_0(String hostname, Integer port, Integer batchSize) throws FlumeException
{    if (hostname == null) {        throw new NullPointerException("hostname must not be null");    }    if (port == null) {        throw new NullPointerException("port must not be null");    }    if (batchSize == null) {        throw new NullPointerException("batchSize must not be null");    }    Properties props = new Properties();    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "h1");    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "h1", hostname + ":" + port.intValue());    props.setProperty(RpcClientConfigurationConstants.CONFIG_BATCH_SIZE, batchSize.toString());    NettyAvroRpcClient client = new NettyAvroRpcClient();    client.configure(props);    return client;}
public static RpcClient flume_f4360_0(String hostname, Integer port, Integer batchSize)
{    if (hostname == null) {        throw new NullPointerException("hostname must not be null");    }    if (port == null) {        throw new NullPointerException("port must not be null");    }    if (batchSize == null) {        throw new NullPointerException("batchSize must not be null");    }    Properties props = new Properties();    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "h1");    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "h1", hostname + ":" + port.intValue());    props.setProperty(RpcClientConfigurationConstants.CONFIG_BATCH_SIZE, batchSize.toString());    ThriftRpcClient client = new ThriftRpcClient();    client.configure(props);    return client;}
public static RpcClient flume_f4361_0(String hostname, Integer port)
{    return getThriftInstance(hostname, port, RpcClientConfigurationConstants.DEFAULT_BATCH_SIZE);}
public static RpcClient flume_f4362_0(Properties props)
{    props.setProperty(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, ClientType.THRIFT.clientClassName);    return getInstance(props);}
protected String flume_f4363_0()
{    return this.clientClassName;}
protected void flume_f4364_0(Properties properties) throws FlumeException
{    enableSsl = Boolean.parseBoolean(properties.getProperty(RpcClientConfigurationConstants.CONFIG_SSL));    trustAllCerts = Boolean.parseBoolean(properties.getProperty(RpcClientConfigurationConstants.CONFIG_TRUST_ALL_CERTS));    truststore = properties.getProperty(RpcClientConfigurationConstants.CONFIG_TRUSTSTORE, SSLUtil.getGlobalTruststorePath());    truststorePassword = properties.getProperty(RpcClientConfigurationConstants.CONFIG_TRUSTSTORE_PASSWORD, SSLUtil.getGlobalTruststorePassword());    truststoreType = properties.getProperty(RpcClientConfigurationConstants.CONFIG_TRUSTSTORE_TYPE, SSLUtil.getGlobalTruststoreType("JKS"));    parseList(properties.getProperty(RpcClientConfigurationConstants.CONFIG_EXCLUDE_PROTOCOLS, SSLUtil.getGlobalExcludeProtocols()), excludeProtocols);    parseList(properties.getProperty(RpcClientConfigurationConstants.CONFIG_INCLUDE_PROTOCOLS, SSLUtil.getGlobalIncludeProtocols()), includeProtocols);    parseList(properties.getProperty(RpcClientConfigurationConstants.CONFIG_EXCLUDE_CIPHER_SUITES, SSLUtil.getGlobalExcludeCipherSuites()), excludeCipherSuites);    parseList(properties.getProperty(RpcClientConfigurationConstants.CONFIG_INCLUDE_CIPHER_SUITES, SSLUtil.getGlobalIncludeCipherSuites()), includeCipherSuites);}
private void flume_f4365_0(String value, Set<String> set)
{    if (Objects.nonNull(value)) {        set.addAll(Arrays.asList(value.split(" ")));    }}
public Thread flume_f4366_0(Runnable r)
{    Thread t = new Thread(r);    t.setName("Flume Thrift RPC thread - " + String.valueOf(threadCounter.incrementAndGet()));    return t;}
public void flume_f4367_0(Event event) throws EventDeliveryException
{            ClientWrapper client = null;    boolean destroyedClient = false;    try {        if (!isActive()) {            throw new EventDeliveryException("Client was closed due to error. " + "Please create a new client");        }        client = connectionManager.checkout();        final ThriftFlumeEvent thriftEvent = new ThriftFlumeEvent(event.getHeaders(), ByteBuffer.wrap(event.getBody()));        doAppend(client, thriftEvent).get(requestTimeout, TimeUnit.MILLISECONDS);    } catch (Throwable e) {        if (e instanceof ExecutionException) {            Throwable cause = e.getCause();            if (cause instanceof EventDeliveryException) {                throw (EventDeliveryException) cause;            } else if (cause instanceof TimeoutException) {                throw new EventDeliveryException("Append call timeout", cause);            }        }        destroyedClient = true;                if (client != null) {            connectionManager.destroy(client);        }        if (e instanceof Error) {            throw (Error) e;        } else if (e instanceof RuntimeException) {            throw (RuntimeException) e;        }        throw new EventDeliveryException("Failed to send event. ", e);    } finally {        if (client != null && !destroyedClient) {            connectionManager.checkIn(client);        }    }}
public void flume_f4368_0(List<Event> events) throws EventDeliveryException
{            ClientWrapper client = null;    boolean destroyedClient = false;    try {        if (!isActive()) {            throw new EventDeliveryException("Client was closed " + "due to error or is not yet configured.");        }        client = connectionManager.checkout();        final List<ThriftFlumeEvent> thriftFlumeEvents = new ArrayList<ThriftFlumeEvent>();        Iterator<Event> eventsIter = events.iterator();        while (eventsIter.hasNext()) {            thriftFlumeEvents.clear();            for (int i = 0; i < batchSize && eventsIter.hasNext(); i++) {                Event event = eventsIter.next();                thriftFlumeEvents.add(new ThriftFlumeEvent(event.getHeaders(), ByteBuffer.wrap(event.getBody())));            }            if (!thriftFlumeEvents.isEmpty()) {                doAppendBatch(client, thriftFlumeEvents).get(requestTimeout, TimeUnit.MILLISECONDS);            }        }    } catch (Throwable e) {        if (e instanceof ExecutionException) {            Throwable cause = e.getCause();            if (cause instanceof EventDeliveryException) {                throw (EventDeliveryException) cause;            } else if (cause instanceof TimeoutException) {                throw new EventDeliveryException("Append call timeout", cause);            }        }        destroyedClient = true;                if (client != null) {            connectionManager.destroy(client);        }        if (e instanceof Error) {            throw (Error) e;        } else if (e instanceof RuntimeException) {            throw (RuntimeException) e;        }        throw new EventDeliveryException("Failed to send event. ", e);    } finally {        if (client != null && !destroyedClient) {            connectionManager.checkIn(client);        }    }}
private Future<Void> flume_f4369_0(final ClientWrapper client, final ThriftFlumeEvent e) throws Exception
{    return callTimeoutPool.submit(new Callable<Void>() {        @Override        public Void call() throws Exception {            Status status = client.client.append(e);            if (status != Status.OK) {                throw new EventDeliveryException("Failed to deliver events. Server " + "returned status : " + status.name());            }            return null;        }    });}
public Void flume_f4370_0() throws Exception
{    Status status = client.client.append(e);    if (status != Status.OK) {        throw new EventDeliveryException("Failed to deliver events. Server " + "returned status : " + status.name());    }    return null;}
private Future<Void> flume_f4371_0(final ClientWrapper client, final List<ThriftFlumeEvent> e) throws Exception
{    return callTimeoutPool.submit(new Callable<Void>() {        @Override        public Void call() throws Exception {            Status status = client.client.appendBatch(e);            if (status != Status.OK) {                throw new EventDeliveryException("Failed to deliver events. Server " + "returned status : " + status.name());            }            return null;        }    });}
public Void flume_f4372_0() throws Exception
{    Status status = client.client.appendBatch(e);    if (status != Status.OK) {        throw new EventDeliveryException("Failed to deliver events. Server " + "returned status : " + status.name());    }    return null;}
public boolean flume_f4373_0()
{    stateLock.lock();    try {        return (connState == State.READY);    } finally {        stateLock.unlock();    }}
public void flume_f4374_0() throws FlumeException
{    try {                stateLock.lock();        connState = State.DEAD;        connectionManager.closeAll();        callTimeoutPool.shutdown();        if (!callTimeoutPool.awaitTermination(5, TimeUnit.SECONDS)) {            callTimeoutPool.shutdownNow();        }    } catch (Throwable ex) {        if (ex instanceof Error) {            throw (Error) ex;        } else if (ex instanceof RuntimeException) {            throw (RuntimeException) ex;        }        throw new FlumeException("Failed to close RPC client. ", ex);    } finally {        stateLock.unlock();    }}
protected void flume_f4375_1(Properties properties) throws FlumeException
{    if (isActive()) {        throw new FlumeException("Attempting to re-configured an already " + "configured client!");    }    stateLock.lock();    try {        HostInfo host = HostInfo.getHostInfoList(properties).get(0);        hostname = host.getHostName();        port = host.getPortNumber();        protocol = properties.getProperty(CONFIG_PROTOCOL);        if (protocol == null) {                        protocol = COMPACT_PROTOCOL;        }                if (!(protocol.equalsIgnoreCase(BINARY_PROTOCOL) || protocol.equalsIgnoreCase(COMPACT_PROTOCOL))) {                        protocol = COMPACT_PROTOCOL;        }        batchSize = parseBatchSize(properties);        requestTimeout = Long.parseLong(properties.getProperty(RpcClientConfigurationConstants.CONFIG_REQUEST_TIMEOUT, String.valueOf(RpcClientConfigurationConstants.DEFAULT_REQUEST_TIMEOUT_MILLIS)));        if (requestTimeout < 1000) {                        requestTimeout = RpcClientConfigurationConstants.DEFAULT_REQUEST_TIMEOUT_MILLIS;        }        int connectionPoolSize = Integer.parseInt(properties.getProperty(RpcClientConfigurationConstants.CONFIG_CONNECTION_POOL_SIZE, String.valueOf(RpcClientConfigurationConstants.DEFAULT_CONNECTION_POOL_SIZE)));        if (connectionPoolSize < 1) {                        connectionPoolSize = RpcClientConfigurationConstants.DEFAULT_CONNECTION_POOL_SIZE;        }        configureSSL(properties);        connectionManager = new ConnectionPoolManager(connectionPoolSize);        connState = State.READY;    } catch (Throwable ex) {                connState = State.DEAD;        if (ex instanceof Error) {            throw (Error) ex;        } else if (ex instanceof RuntimeException) {            throw (RuntimeException) ex;        }        throw new FlumeException("Error while configuring RpcClient. ", ex);    } finally {        stateLock.unlock();    }}
protected TTransport flume_f4376_0(TSocket tsocket) throws Exception
{    return new TFastFramedTransport(tsocket);}
public boolean flume_f4377_0(Object o)
{    if (o == null) {        return false;    }        if (this == o) {        return true;    }    return false;}
public int flume_f4378_0()
{    return hashCode;}
public ClientWrapper flume_f4379_0() throws Exception
{    ClientWrapper ret = null;    poolLock.lock();    try {        if (availableClients.isEmpty() && currentPoolSize < maxPoolSize) {            ret = new ClientWrapper();            currentPoolSize++;            checkedOutClients.add(ret);            return ret;        }        while (availableClients.isEmpty()) {            availableClientsCondition.await();        }        ret = availableClients.poll();        checkedOutClients.add(ret);    } finally {        poolLock.unlock();    }    return ret;}
public void flume_f4380_0(ClientWrapper client)
{    poolLock.lock();    try {        availableClients.add(client);        checkedOutClients.remove(client);        availableClientsCondition.signal();    } finally {        poolLock.unlock();    }}
public void flume_f4381_0(ClientWrapper client)
{    poolLock.lock();    try {        checkedOutClients.remove(client);        currentPoolSize--;    } finally {        poolLock.unlock();    }    client.transport.close();}
public void flume_f4382_0()
{    poolLock.lock();    try {        for (ClientWrapper c : availableClients) {            c.transport.close();            currentPoolSize--;        }                for (ClientWrapper c : checkedOutClients) {            c.transport.close();            currentPoolSize--;        }    } finally {        poolLock.unlock();    }}
private static SSLContext flume_f4383_0(String truststore, String truststorePassword, String truststoreType) throws FlumeException
{    SSLContext ctx;    try {        ctx = SSLContext.getInstance("TLS");        TrustManagerFactory tmf;        tmf = TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm());        KeyStore ts = null;        if (truststore != null && truststoreType != null) {            ts = KeyStore.getInstance(truststoreType);            ts.load(new FileInputStream(truststore), truststorePassword != null ? truststorePassword.toCharArray() : null);            tmf.init(ts);        }        tmf.init(ts);        ctx.init(null, tmf.getTrustManagers(), null);    } catch (Exception e) {        throw new FlumeException("Error creating the transport", e);    }    return ctx;}
private static TSocket flume_f4384_0(SSLSocketFactory factory, String host, int port, int timeout, Set<String> excludeProtocols, Set<String> includeProtocols, Set<String> excludeCipherSuites, Set<String> includeCipherSuites) throws FlumeException
{    try {        SSLSocket socket = (SSLSocket) factory.createSocket(host, port);        socket.setSoTimeout(timeout);        List<String> enabledProtocols = new ArrayList<String>();        for (String protocol : socket.getEnabledProtocols()) {            if ((includeProtocols.isEmpty() || includeProtocols.contains(protocol)) && !excludeProtocols.contains(protocol)) {                enabledProtocols.add(protocol);            }        }        socket.setEnabledProtocols(enabledProtocols.toArray(new String[0]));        List<String> enabledCipherSuites = new ArrayList<String>();        for (String suite : socket.getEnabledCipherSuites()) {            if ((includeCipherSuites.isEmpty() || includeCipherSuites.contains(suite)) && !excludeCipherSuites.contains(suite)) {                enabledCipherSuites.add(suite);            }        }        socket.setEnabledCipherSuites(enabledCipherSuites.toArray(new String[0]));        return new TSocket(socket);    } catch (Exception e) {        throw new FlumeException("Could not connect to " + host + " on port " + port, e);    }}
public static Event flume_f4385_0(byte[] body, Map<String, String> headers)
{    Event event = new SimpleEvent();    if (body == null) {        body = new byte[0];    }    event.setBody(body);    if (headers != null) {        event.setHeaders(new HashMap<String, String>(headers));    }    return event;}
public static Event flume_f4386_0(byte[] body)
{    return withBody(body, null);}
public static Event flume_f4387_0(String body, Charset charset, Map<String, String> headers)
{    return withBody(body.getBytes(charset), headers);}
public static Event flume_f4388_0(String body, Charset charset)
{    return withBody(body, charset, null);}
public Map<String, String> flume_f4389_0()
{    return headers;}
public void flume_f4390_0(Map<String, String> headers)
{    this.headers = headers;}
public byte[] flume_f4391_0()
{    if (body != null) {        try {            return body.getBytes(charset);        } catch (UnsupportedEncodingException ex) {            throw new FlumeException(String.format("%s encoding not supported", charset), ex);        }    } else {        return new byte[0];    }}
public void flume_f4392_0(byte[] body)
{    if (body != null) {        this.body = new String(body);    } else {        this.body = "";    }}
public void flume_f4393_0(String charset)
{    this.charset = charset;}
public Map<String, String> flume_f4394_0()
{    return headers;}
public void flume_f4395_0(Map<String, String> headers)
{    this.headers = headers;}
public byte[] flume_f4396_0()
{    return body;}
public void flume_f4397_0(byte[] body)
{    if (body == null) {        body = new byte[0];    }    this.body = body;}
public String flume_f4398_0()
{    Integer bodyLen = null;    if (body != null)        bodyLen = body.length;    return "[Event headers = " + headers + ", body.length = " + bodyLen + " ]";}
public int flume_f4399_0()
{    return value;}
public static Status flume_f4400_0(int value)
{    switch(value) {        case 0:            return OK;        case 1:            return FAILED;        case 2:            return ERROR;        case 3:            return UNKNOWN;        default:            return null;    }}
public static _Fields flume_f4401_0(int fieldId)
{    switch(fieldId) {        case         1:            return HEADERS;        case         2:            return BODY;        default:            return null;    }}
public static _Fields flume_f4402_0(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
public static _Fields flume_f4403_0(String name)
{    return byName.get(name);}
public short flume_f4404_0()
{    return _thriftId;}
public String flume_f4405_0()
{    return _fieldName;}
public ThriftFlumeEvent flume_f4406_0()
{    return new ThriftFlumeEvent(this);}
public void flume_f4407_0()
{    this.headers = null;    this.body = null;}
public int flume_f4408_0()
{    return (this.headers == null) ? 0 : this.headers.size();}
public void flume_f4409_0(String key, String val)
{    if (this.headers == null) {        this.headers = new HashMap<String, String>();    }    this.headers.put(key, val);}
public Map<String, String> flume_f4410_0()
{    return this.headers;}
public ThriftFlumeEvent flume_f4411_0(Map<String, String> headers)
{    this.headers = headers;    return this;}
public void flume_f4412_0()
{    this.headers = null;}
public boolean flume_f4413_0()
{    return this.headers != null;}
public void flume_f4414_0(boolean value)
{    if (!value) {        this.headers = null;    }}
public byte[] flume_f4415_0()
{    setBody(org.apache.thrift.TBaseHelper.rightSize(body));    return body == null ? null : body.array();}
public ByteBuffer flume_f4416_0()
{    return org.apache.thrift.TBaseHelper.copyBinary(body);}
public ThriftFlumeEvent flume_f4417_0(byte[] body)
{    this.body = body == null ? (ByteBuffer) null : ByteBuffer.wrap(Arrays.copyOf(body, body.length));    return this;}
public ThriftFlumeEvent flume_f4418_0(ByteBuffer body)
{    this.body = org.apache.thrift.TBaseHelper.copyBinary(body);    return this;}
public void flume_f4419_0()
{    this.body = null;}
public boolean flume_f4420_0()
{    return this.body != null;}
public void flume_f4421_0(boolean value)
{    if (!value) {        this.body = null;    }}
public void flume_f4422_0(_Fields field, Object value)
{    switch(field) {        case HEADERS:            if (value == null) {                unsetHeaders();            } else {                setHeaders((Map<String, String>) value);            }            break;        case BODY:            if (value == null) {                unsetBody();            } else {                setBody((ByteBuffer) value);            }            break;    }}
public Object flume_f4423_0(_Fields field)
{    switch(field) {        case HEADERS:            return getHeaders();        case BODY:            return getBody();    }    throw new IllegalStateException();}
public boolean flume_f4424_0(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case HEADERS:            return isSetHeaders();        case BODY:            return isSetBody();    }    throw new IllegalStateException();}
public boolean flume_f4425_0(Object that)
{    if (that == null)        return false;    if (that instanceof ThriftFlumeEvent)        return this.equals((ThriftFlumeEvent) that);    return false;}
public boolean flume_f4426_0(ThriftFlumeEvent that)
{    if (that == null)        return false;    boolean this_present_headers = true && this.isSetHeaders();    boolean that_present_headers = true && that.isSetHeaders();    if (this_present_headers || that_present_headers) {        if (!(this_present_headers && that_present_headers))            return false;        if (!this.headers.equals(that.headers))            return false;    }    boolean this_present_body = true && this.isSetBody();    boolean that_present_body = true && that.isSetBody();    if (this_present_body || that_present_body) {        if (!(this_present_body && that_present_body))            return false;        if (!this.body.equals(that.body))            return false;    }    return true;}
public int flume_f4427_0()
{    List<Object> list = new ArrayList<Object>();    boolean present_headers = true && (isSetHeaders());    list.add(present_headers);    if (present_headers)        list.add(headers);    boolean present_body = true && (isSetBody());    list.add(present_body);    if (present_body)        list.add(body);    return list.hashCode();}
public int flume_f4428_0(ThriftFlumeEvent other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetHeaders()).compareTo(other.isSetHeaders());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetHeaders()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.headers, other.headers);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetBody()).compareTo(other.isSetBody());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetBody()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.body, other.body);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
public _Fields flume_f4429_0(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
public void flume_f4430_0(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
public void flume_f4431_0(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
public String flume_f4432_0()
{    StringBuilder sb = new StringBuilder("ThriftFlumeEvent(");    boolean first = true;    sb.append("headers:");    if (this.headers == null) {        sb.append("null");    } else {        sb.append(this.headers);    }    first = false;    if (!first)        sb.append(", ");    sb.append("body:");    if (this.body == null) {        sb.append("null");    } else {        org.apache.thrift.TBaseHelper.toString(this.body, sb);    }    first = false;    sb.append(")");    return sb.toString();}
public void flume_f4433_0() throws org.apache.thrift.TException
{        if (headers == null) {        throw new org.apache.thrift.protocol.TProtocolException("Required field 'headers' was not present! Struct: " + toString());    }    if (body == null) {        throw new org.apache.thrift.protocol.TProtocolException("Required field 'body' was not present! Struct: " + toString());    }}
private void flume_f4434_0(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
private void flume_f4435_0(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
public ThriftFlumeEventStandardScheme flume_f4436_0()
{    return new ThriftFlumeEventStandardScheme();}
public void flume_f4437_0(org.apache.thrift.protocol.TProtocol iprot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.thrift.protocol.TType.MAP) {                    {                        org.apache.thrift.protocol.TMap _map0 = iprot.readMapBegin();                        struct.headers = new HashMap<String, String>(2 * _map0.size);                        String _key1;                        String _val2;                        for (int _i3 = 0; _i3 < _map0.size; ++_i3) {                            _key1 = iprot.readString();                            _val2 = iprot.readString();                            struct.headers.put(_key1, _val2);                        }                        iprot.readMapEnd();                    }                    struct.setHeadersIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             2:                if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {                    struct.body = iprot.readBinary();                    struct.setBodyIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
public void flume_f4438_0(org.apache.thrift.protocol.TProtocol oprot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.headers != null) {        oprot.writeFieldBegin(HEADERS_FIELD_DESC);        {            oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, struct.headers.size()));            for (Map.Entry<String, String> _iter4 : struct.headers.entrySet()) {                oprot.writeString(_iter4.getKey());                oprot.writeString(_iter4.getValue());            }            oprot.writeMapEnd();        }        oprot.writeFieldEnd();    }    if (struct.body != null) {        oprot.writeFieldBegin(BODY_FIELD_DESC);        oprot.writeBinary(struct.body);        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
public ThriftFlumeEventTupleScheme flume_f4439_0()
{    return new ThriftFlumeEventTupleScheme();}
public void flume_f4440_0(org.apache.thrift.protocol.TProtocol prot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    {        oprot.writeI32(struct.headers.size());        for (Map.Entry<String, String> _iter5 : struct.headers.entrySet()) {            oprot.writeString(_iter5.getKey());            oprot.writeString(_iter5.getValue());        }    }    oprot.writeBinary(struct.body);}
public void flume_f4441_0(org.apache.thrift.protocol.TProtocol prot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    {        org.apache.thrift.protocol.TMap _map6 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());        struct.headers = new HashMap<String, String>(2 * _map6.size);        String _key7;        String _val8;        for (int _i9 = 0; _i9 < _map6.size; ++_i9) {            _key7 = iprot.readString();            _val8 = iprot.readString();            struct.headers.put(_key7, _val8);        }    }    struct.setHeadersIsSet(true);    struct.body = iprot.readBinary();    struct.setBodyIsSet(true);}
public Client flume_f4442_0(org.apache.thrift.protocol.TProtocol prot)
{    return new Client(prot);}
public Client flume_f4443_0(org.apache.thrift.protocol.TProtocol iprot, org.apache.thrift.protocol.TProtocol oprot)
{    return new Client(iprot, oprot);}
public Status flume_f4444_0(ThriftFlumeEvent event) throws org.apache.thrift.TException
{    send_append(event);    return recv_append();}
public void flume_f4445_0(ThriftFlumeEvent event) throws org.apache.thrift.TException
{    append_args args = new append_args();    args.setEvent(event);    sendBase("append", args);}
public Status flume_f4446_0() throws org.apache.thrift.TException
{    append_result result = new append_result();    receiveBase(result, "append");    if (result.isSetSuccess()) {        return result.success;    }    throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.MISSING_RESULT, "append failed: unknown result");}
public Status flume_f4447_0(List<ThriftFlumeEvent> events) throws org.apache.thrift.TException
{    send_appendBatch(events);    return recv_appendBatch();}
public void flume_f4448_0(List<ThriftFlumeEvent> events) throws org.apache.thrift.TException
{    appendBatch_args args = new appendBatch_args();    args.setEvents(events);    sendBase("appendBatch", args);}
public Status flume_f4449_0() throws org.apache.thrift.TException
{    appendBatch_result result = new appendBatch_result();    receiveBase(result, "appendBatch");    if (result.isSetSuccess()) {        return result.success;    }    throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.MISSING_RESULT, "appendBatch failed: unknown result");}
public AsyncClient flume_f4450_0(org.apache.thrift.transport.TNonblockingTransport transport)
{    return new AsyncClient(protocolFactory, clientManager, transport);}
public void flume_f4451_0(ThriftFlumeEvent event, org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException
{    checkReady();    append_call method_call = new append_call(event, resultHandler, this, ___protocolFactory, ___transport);    this.___currentMethod = method_call;    ___manager.call(method_call);}
public void flume_f4452_0(org.apache.thrift.protocol.TProtocol prot) throws org.apache.thrift.TException
{    prot.writeMessageBegin(new org.apache.thrift.protocol.TMessage("append", org.apache.thrift.protocol.TMessageType.CALL, 0));    append_args args = new append_args();    args.setEvent(event);    args.write(prot);    prot.writeMessageEnd();}
public Status flume_f4453_0() throws org.apache.thrift.TException
{    if (getState() != org.apache.thrift.async.TAsyncMethodCall.State.RESPONSE_READ) {        throw new IllegalStateException("Method call not finished!");    }    org.apache.thrift.transport.TMemoryInputTransport memoryTransport = new org.apache.thrift.transport.TMemoryInputTransport(getFrameBuffer().array());    org.apache.thrift.protocol.TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);    return (new Client(prot)).recv_append();}
public void flume_f4454_0(List<ThriftFlumeEvent> events, org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException
{    checkReady();    appendBatch_call method_call = new appendBatch_call(events, resultHandler, this, ___protocolFactory, ___transport);    this.___currentMethod = method_call;    ___manager.call(method_call);}
public void flume_f4455_0(org.apache.thrift.protocol.TProtocol prot) throws org.apache.thrift.TException
{    prot.writeMessageBegin(new org.apache.thrift.protocol.TMessage("appendBatch", org.apache.thrift.protocol.TMessageType.CALL, 0));    appendBatch_args args = new appendBatch_args();    args.setEvents(events);    args.write(prot);    prot.writeMessageEnd();}
public Status flume_f4456_0() throws org.apache.thrift.TException
{    if (getState() != org.apache.thrift.async.TAsyncMethodCall.State.RESPONSE_READ) {        throw new IllegalStateException("Method call not finished!");    }    org.apache.thrift.transport.TMemoryInputTransport memoryTransport = new org.apache.thrift.transport.TMemoryInputTransport(getFrameBuffer().array());    org.apache.thrift.protocol.TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);    return (new Client(prot)).recv_appendBatch();}
private static Map<String, org.apache.thrift.ProcessFunction<I, ? extends org.apache.thrift.TBase>> flume_f4457_0(Map<String, org.apache.thrift.ProcessFunction<I, ? extends org.apache.thrift.TBase>> processMap)
{    processMap.put("append", new append());    processMap.put("appendBatch", new appendBatch());    return processMap;}
public append_args flume_f4458_0()
{    return new append_args();}
protected boolean flume_f4459_0()
{    return false;}
public append_result flume_f4460_0(I iface, append_args args) throws org.apache.thrift.TException
{    append_result result = new append_result();    result.success = iface.append(args.event);    return result;}
public appendBatch_args flume_f4461_0()
{    return new appendBatch_args();}
protected boolean flume_f4462_0()
{    return false;}
public appendBatch_result flume_f4463_0(I iface, appendBatch_args args) throws org.apache.thrift.TException
{    appendBatch_result result = new appendBatch_result();    result.success = iface.appendBatch(args.events);    return result;}
private static Map<String, org.apache.thrift.AsyncProcessFunction<I, ? extends org.apache.thrift.TBase, ?>> flume_f4464_0(Map<String, org.apache.thrift.AsyncProcessFunction<I, ? extends org.apache.thrift.TBase, ?>> processMap)
{    processMap.put("append", new append());    processMap.put("appendBatch", new appendBatch());    return processMap;}
public append_args flume_f4465_0()
{    return new append_args();}
public AsyncMethodCallback<Status> flume_f4466_1(final AsyncFrameBuffer fb, final int seqid)
{    final org.apache.thrift.AsyncProcessFunction fcall = this;    return new AsyncMethodCallback<Status>() {        public void onComplete(Status o) {            append_result result = new append_result();            result.success = o;            try {                fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);                return;            } catch (Exception e) {                            }            fb.close();        }        public void onError(Exception e) {            byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;            org.apache.thrift.TBase msg;            append_result result = new append_result();            {                msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;                msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());            }            try {                fcall.sendResponse(fb, msg, msgType, seqid);                return;            } catch (Exception ex) {                            }            fb.close();        }    };}
public void flume_f4467_1(Status o)
{    append_result result = new append_result();    result.success = o;    try {        fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);        return;    } catch (Exception e) {            }    fb.close();}
public void flume_f4468_1(Exception e)
{    byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;    org.apache.thrift.TBase msg;    append_result result = new append_result();    {        msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;        msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());    }    try {        fcall.sendResponse(fb, msg, msgType, seqid);        return;    } catch (Exception ex) {            }    fb.close();}
protected boolean flume_f4469_0()
{    return false;}
public void flume_f4470_0(I iface, append_args args, org.apache.thrift.async.AsyncMethodCallback<Status> resultHandler) throws TException
{    iface.append(args.event, resultHandler);}
public appendBatch_args flume_f4471_0()
{    return new appendBatch_args();}
public AsyncMethodCallback<Status> flume_f4472_1(final AsyncFrameBuffer fb, final int seqid)
{    final org.apache.thrift.AsyncProcessFunction fcall = this;    return new AsyncMethodCallback<Status>() {        public void onComplete(Status o) {            appendBatch_result result = new appendBatch_result();            result.success = o;            try {                fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);                return;            } catch (Exception e) {                            }            fb.close();        }        public void onError(Exception e) {            byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;            org.apache.thrift.TBase msg;            appendBatch_result result = new appendBatch_result();            {                msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;                msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());            }            try {                fcall.sendResponse(fb, msg, msgType, seqid);                return;            } catch (Exception ex) {                            }            fb.close();        }    };}
public void flume_f4473_1(Status o)
{    appendBatch_result result = new appendBatch_result();    result.success = o;    try {        fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);        return;    } catch (Exception e) {            }    fb.close();}
public void flume_f4474_1(Exception e)
{    byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;    org.apache.thrift.TBase msg;    appendBatch_result result = new appendBatch_result();    {        msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;        msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());    }    try {        fcall.sendResponse(fb, msg, msgType, seqid);        return;    } catch (Exception ex) {            }    fb.close();}
protected boolean flume_f4475_0()
{    return false;}
public void flume_f4476_0(I iface, appendBatch_args args, org.apache.thrift.async.AsyncMethodCallback<Status> resultHandler) throws TException
{    iface.appendBatch(args.events, resultHandler);}
public static _Fields flume_f4477_0(int fieldId)
{    switch(fieldId) {        case         1:            return EVENT;        default:            return null;    }}
public static _Fields flume_f4478_0(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
public static _Fields flume_f4479_0(String name)
{    return byName.get(name);}
public short flume_f4480_0()
{    return _thriftId;}
public String flume_f4481_0()
{    return _fieldName;}
public append_args flume_f4482_0()
{    return new append_args(this);}
public void flume_f4483_0()
{    this.event = null;}
public ThriftFlumeEvent flume_f4484_0()
{    return this.event;}
public append_args flume_f4485_0(ThriftFlumeEvent event)
{    this.event = event;    return this;}
public void flume_f4486_0()
{    this.event = null;}
public boolean flume_f4487_0()
{    return this.event != null;}
public void flume_f4488_0(boolean value)
{    if (!value) {        this.event = null;    }}
public void flume_f4489_0(_Fields field, Object value)
{    switch(field) {        case EVENT:            if (value == null) {                unsetEvent();            } else {                setEvent((ThriftFlumeEvent) value);            }            break;    }}
public Object flume_f4490_0(_Fields field)
{    switch(field) {        case EVENT:            return getEvent();    }    throw new IllegalStateException();}
public boolean flume_f4491_0(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case EVENT:            return isSetEvent();    }    throw new IllegalStateException();}
public boolean flume_f4492_0(Object that)
{    if (that == null)        return false;    if (that instanceof append_args)        return this.equals((append_args) that);    return false;}
public boolean flume_f4493_0(append_args that)
{    if (that == null)        return false;    boolean this_present_event = true && this.isSetEvent();    boolean that_present_event = true && that.isSetEvent();    if (this_present_event || that_present_event) {        if (!(this_present_event && that_present_event))            return false;        if (!this.event.equals(that.event))            return false;    }    return true;}
public int flume_f4494_0()
{    List<Object> list = new ArrayList<Object>();    boolean present_event = true && (isSetEvent());    list.add(present_event);    if (present_event)        list.add(event);    return list.hashCode();}
public int flume_f4495_0(append_args other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetEvent()).compareTo(other.isSetEvent());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetEvent()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.event, other.event);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
public _Fields flume_f4496_0(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
public void flume_f4497_0(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
public void flume_f4498_0(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
public String flume_f4499_0()
{    StringBuilder sb = new StringBuilder("append_args(");    boolean first = true;    sb.append("event:");    if (this.event == null) {        sb.append("null");    } else {        sb.append(this.event);    }    first = false;    sb.append(")");    return sb.toString();}
public void flume_f4500_0() throws org.apache.thrift.TException
{        if (event != null) {        event.validate();    }}
private void flume_f4501_0(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
private void flume_f4502_0(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
public append_argsStandardScheme flume_f4503_0()
{    return new append_argsStandardScheme();}
public void flume_f4504_0(org.apache.thrift.protocol.TProtocol iprot, append_args struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.thrift.protocol.TType.STRUCT) {                    struct.event = new ThriftFlumeEvent();                    struct.event.read(iprot);                    struct.setEventIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
public void flume_f4505_0(org.apache.thrift.protocol.TProtocol oprot, append_args struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.event != null) {        oprot.writeFieldBegin(EVENT_FIELD_DESC);        struct.event.write(oprot);        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
public append_argsTupleScheme flume_f4506_0()
{    return new append_argsTupleScheme();}
public void flume_f4507_0(org.apache.thrift.protocol.TProtocol prot, append_args struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetEvent()) {        optionals.set(0);    }    oprot.writeBitSet(optionals, 1);    if (struct.isSetEvent()) {        struct.event.write(oprot);    }}
public void flume_f4508_0(org.apache.thrift.protocol.TProtocol prot, append_args struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(1);    if (incoming.get(0)) {        struct.event = new ThriftFlumeEvent();        struct.event.read(iprot);        struct.setEventIsSet(true);    }}
public static _Fields flume_f4509_0(int fieldId)
{    switch(fieldId) {        case         0:            return SUCCESS;        default:            return null;    }}
public static _Fields flume_f4510_0(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
public static _Fields flume_f4511_0(String name)
{    return byName.get(name);}
public short flume_f4512_0()
{    return _thriftId;}
public String flume_f4513_0()
{    return _fieldName;}
public append_result flume_f4514_0()
{    return new append_result(this);}
public void flume_f4515_0()
{    this.success = null;}
public Status flume_f4516_0()
{    return this.success;}
public append_result flume_f4517_0(Status success)
{    this.success = success;    return this;}
public void flume_f4518_0()
{    this.success = null;}
public boolean flume_f4519_0()
{    return this.success != null;}
public void flume_f4520_0(boolean value)
{    if (!value) {        this.success = null;    }}
public void flume_f4521_0(_Fields field, Object value)
{    switch(field) {        case SUCCESS:            if (value == null) {                unsetSuccess();            } else {                setSuccess((Status) value);            }            break;    }}
public Object flume_f4522_0(_Fields field)
{    switch(field) {        case SUCCESS:            return getSuccess();    }    throw new IllegalStateException();}
public boolean flume_f4523_0(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case SUCCESS:            return isSetSuccess();    }    throw new IllegalStateException();}
public boolean flume_f4524_0(Object that)
{    if (that == null)        return false;    if (that instanceof append_result)        return this.equals((append_result) that);    return false;}
public boolean flume_f4525_0(append_result that)
{    if (that == null)        return false;    boolean this_present_success = true && this.isSetSuccess();    boolean that_present_success = true && that.isSetSuccess();    if (this_present_success || that_present_success) {        if (!(this_present_success && that_present_success))            return false;        if (!this.success.equals(that.success))            return false;    }    return true;}
public int flume_f4526_0()
{    List<Object> list = new ArrayList<Object>();    boolean present_success = true && (isSetSuccess());    list.add(present_success);    if (present_success)        list.add(success.getValue());    return list.hashCode();}
public int flume_f4527_0(append_result other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(other.isSetSuccess());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetSuccess()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.success, other.success);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
public _Fields flume_f4528_0(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
public void flume_f4529_0(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
public void flume_f4530_0(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
public String flume_f4531_0()
{    StringBuilder sb = new StringBuilder("append_result(");    boolean first = true;    sb.append("success:");    if (this.success == null) {        sb.append("null");    } else {        sb.append(this.success);    }    first = false;    sb.append(")");    return sb.toString();}
public void flume_f4532_0() throws org.apache.thrift.TException
{}
private void flume_f4533_0(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
private void flume_f4534_0(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
public append_resultStandardScheme flume_f4535_0()
{    return new append_resultStandardScheme();}
public void flume_f4536_0(org.apache.thrift.protocol.TProtocol iprot, append_result struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             0:                if (schemeField.type == org.apache.thrift.protocol.TType.I32) {                    struct.success = org.apache.flume.thrift.Status.findByValue(iprot.readI32());                    struct.setSuccessIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
public void flume_f4537_0(org.apache.thrift.protocol.TProtocol oprot, append_result struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.success != null) {        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);        oprot.writeI32(struct.success.getValue());        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
public append_resultTupleScheme flume_f4538_0()
{    return new append_resultTupleScheme();}
public void flume_f4539_0(org.apache.thrift.protocol.TProtocol prot, append_result struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetSuccess()) {        optionals.set(0);    }    oprot.writeBitSet(optionals, 1);    if (struct.isSetSuccess()) {        oprot.writeI32(struct.success.getValue());    }}
public void flume_f4540_0(org.apache.thrift.protocol.TProtocol prot, append_result struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(1);    if (incoming.get(0)) {        struct.success = org.apache.flume.thrift.Status.findByValue(iprot.readI32());        struct.setSuccessIsSet(true);    }}
public static _Fields flume_f4541_0(int fieldId)
{    switch(fieldId) {        case         1:            return EVENTS;        default:            return null;    }}
public static _Fields flume_f4542_0(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
public static _Fields flume_f4543_0(String name)
{    return byName.get(name);}
public short flume_f4544_0()
{    return _thriftId;}
public String flume_f4545_0()
{    return _fieldName;}
public appendBatch_args flume_f4546_0()
{    return new appendBatch_args(this);}
public void flume_f4547_0()
{    this.events = null;}
public int flume_f4548_0()
{    return (this.events == null) ? 0 : this.events.size();}
public java.util.Iterator<ThriftFlumeEvent> flume_f4549_0()
{    return (this.events == null) ? null : this.events.iterator();}
public void flume_f4550_0(ThriftFlumeEvent elem)
{    if (this.events == null) {        this.events = new ArrayList<ThriftFlumeEvent>();    }    this.events.add(elem);}
public List<ThriftFlumeEvent> flume_f4551_0()
{    return this.events;}
public appendBatch_args flume_f4552_0(List<ThriftFlumeEvent> events)
{    this.events = events;    return this;}
public void flume_f4553_0()
{    this.events = null;}
public boolean flume_f4554_0()
{    return this.events != null;}
public void flume_f4555_0(boolean value)
{    if (!value) {        this.events = null;    }}
public void flume_f4556_0(_Fields field, Object value)
{    switch(field) {        case EVENTS:            if (value == null) {                unsetEvents();            } else {                setEvents((List<ThriftFlumeEvent>) value);            }            break;    }}
public Object flume_f4557_0(_Fields field)
{    switch(field) {        case EVENTS:            return getEvents();    }    throw new IllegalStateException();}
public boolean flume_f4558_0(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case EVENTS:            return isSetEvents();    }    throw new IllegalStateException();}
public boolean flume_f4559_0(Object that)
{    if (that == null)        return false;    if (that instanceof appendBatch_args)        return this.equals((appendBatch_args) that);    return false;}
public boolean flume_f4560_0(appendBatch_args that)
{    if (that == null)        return false;    boolean this_present_events = true && this.isSetEvents();    boolean that_present_events = true && that.isSetEvents();    if (this_present_events || that_present_events) {        if (!(this_present_events && that_present_events))            return false;        if (!this.events.equals(that.events))            return false;    }    return true;}
public int flume_f4561_0()
{    List<Object> list = new ArrayList<Object>();    boolean present_events = true && (isSetEvents());    list.add(present_events);    if (present_events)        list.add(events);    return list.hashCode();}
public int flume_f4562_0(appendBatch_args other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetEvents()).compareTo(other.isSetEvents());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetEvents()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.events, other.events);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
public _Fields flume_f4563_0(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
public void flume_f4564_0(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
public void flume_f4565_0(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
public String flume_f4566_0()
{    StringBuilder sb = new StringBuilder("appendBatch_args(");    boolean first = true;    sb.append("events:");    if (this.events == null) {        sb.append("null");    } else {        sb.append(this.events);    }    first = false;    sb.append(")");    return sb.toString();}
public void flume_f4567_0() throws org.apache.thrift.TException
{}
private void flume_f4568_0(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
private void flume_f4569_0(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
public appendBatch_argsStandardScheme flume_f4570_0()
{    return new appendBatch_argsStandardScheme();}
public void flume_f4571_0(org.apache.thrift.protocol.TProtocol iprot, appendBatch_args struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.thrift.protocol.TType.LIST) {                    {                        org.apache.thrift.protocol.TList _list10 = iprot.readListBegin();                        struct.events = new ArrayList<ThriftFlumeEvent>(_list10.size);                        ThriftFlumeEvent _elem11;                        for (int _i12 = 0; _i12 < _list10.size; ++_i12) {                            _elem11 = new ThriftFlumeEvent();                            _elem11.read(iprot);                            struct.events.add(_elem11);                        }                        iprot.readListEnd();                    }                    struct.setEventsIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
public void flume_f4572_0(org.apache.thrift.protocol.TProtocol oprot, appendBatch_args struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.events != null) {        oprot.writeFieldBegin(EVENTS_FIELD_DESC);        {            oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.events.size()));            for (ThriftFlumeEvent _iter13 : struct.events) {                _iter13.write(oprot);            }            oprot.writeListEnd();        }        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
public appendBatch_argsTupleScheme flume_f4573_0()
{    return new appendBatch_argsTupleScheme();}
public void flume_f4574_0(org.apache.thrift.protocol.TProtocol prot, appendBatch_args struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetEvents()) {        optionals.set(0);    }    oprot.writeBitSet(optionals, 1);    if (struct.isSetEvents()) {        {            oprot.writeI32(struct.events.size());            for (ThriftFlumeEvent _iter14 : struct.events) {                _iter14.write(oprot);            }        }    }}
public void flume_f4575_0(org.apache.thrift.protocol.TProtocol prot, appendBatch_args struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(1);    if (incoming.get(0)) {        {            org.apache.thrift.protocol.TList _list15 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());            struct.events = new ArrayList<ThriftFlumeEvent>(_list15.size);            ThriftFlumeEvent _elem16;            for (int _i17 = 0; _i17 < _list15.size; ++_i17) {                _elem16 = new ThriftFlumeEvent();                _elem16.read(iprot);                struct.events.add(_elem16);            }        }        struct.setEventsIsSet(true);    }}
public static _Fields flume_f4576_0(int fieldId)
{    switch(fieldId) {        case         0:            return SUCCESS;        default:            return null;    }}
public static _Fields flume_f4577_0(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
public static _Fields flume_f4578_0(String name)
{    return byName.get(name);}
public short flume_f4579_0()
{    return _thriftId;}
public String flume_f4580_0()
{    return _fieldName;}
public appendBatch_result flume_f4581_0()
{    return new appendBatch_result(this);}
public void flume_f4582_0()
{    this.success = null;}
public Status flume_f4583_0()
{    return this.success;}
public appendBatch_result flume_f4584_0(Status success)
{    this.success = success;    return this;}
public void flume_f4585_0()
{    this.success = null;}
public boolean flume_f4586_0()
{    return this.success != null;}
public void flume_f4587_0(boolean value)
{    if (!value) {        this.success = null;    }}
public void flume_f4588_0(_Fields field, Object value)
{    switch(field) {        case SUCCESS:            if (value == null) {                unsetSuccess();            } else {                setSuccess((Status) value);            }            break;    }}
public Object flume_f4589_0(_Fields field)
{    switch(field) {        case SUCCESS:            return getSuccess();    }    throw new IllegalStateException();}
public boolean flume_f4590_0(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case SUCCESS:            return isSetSuccess();    }    throw new IllegalStateException();}
public boolean flume_f4591_0(Object that)
{    if (that == null)        return false;    if (that instanceof appendBatch_result)        return this.equals((appendBatch_result) that);    return false;}
public boolean flume_f4592_0(appendBatch_result that)
{    if (that == null)        return false;    boolean this_present_success = true && this.isSetSuccess();    boolean that_present_success = true && that.isSetSuccess();    if (this_present_success || that_present_success) {        if (!(this_present_success && that_present_success))            return false;        if (!this.success.equals(that.success))            return false;    }    return true;}
public int flume_f4593_0()
{    List<Object> list = new ArrayList<Object>();    boolean present_success = true && (isSetSuccess());    list.add(present_success);    if (present_success)        list.add(success.getValue());    return list.hashCode();}
public int flume_f4594_0(appendBatch_result other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(other.isSetSuccess());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetSuccess()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.success, other.success);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
public _Fields flume_f4595_0(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
public void flume_f4596_0(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
public void flume_f4597_0(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
public String flume_f4598_0()
{    StringBuilder sb = new StringBuilder("appendBatch_result(");    boolean first = true;    sb.append("success:");    if (this.success == null) {        sb.append("null");    } else {        sb.append(this.success);    }    first = false;    sb.append(")");    return sb.toString();}
public void flume_f4599_0() throws org.apache.thrift.TException
{}
private void flume_f4600_0(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
private void flume_f4601_0(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
public appendBatch_resultStandardScheme flume_f4602_0()
{    return new appendBatch_resultStandardScheme();}
public void flume_f4603_0(org.apache.thrift.protocol.TProtocol iprot, appendBatch_result struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             0:                if (schemeField.type == org.apache.thrift.protocol.TType.I32) {                    struct.success = org.apache.flume.thrift.Status.findByValue(iprot.readI32());                    struct.setSuccessIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
public void flume_f4604_0(org.apache.thrift.protocol.TProtocol oprot, appendBatch_result struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.success != null) {        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);        oprot.writeI32(struct.success.getValue());        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
public appendBatch_resultTupleScheme flume_f4605_0()
{    return new appendBatch_resultTupleScheme();}
public void flume_f4606_0(org.apache.thrift.protocol.TProtocol prot, appendBatch_result struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetSuccess()) {        optionals.set(0);    }    oprot.writeBitSet(optionals, 1);    if (struct.isSetSuccess()) {        oprot.writeI32(struct.success.getValue());    }}
public void flume_f4607_0(org.apache.thrift.protocol.TProtocol prot, appendBatch_result struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(1);    if (incoming.get(0)) {        struct.success = org.apache.flume.thrift.Status.findByValue(iprot.readI32());        struct.setSuccessIsSet(true);    }}
public void flume_f4608_0(List<T> objects)
{    for (T sink : objects) {        FailureState state = new FailureState();        stateMap.put(sink, state);    }}
public List<T> flume_f4609_0()
{    return new ArrayList<T>(stateMap.keySet());}
public void flume_f4610_0(T failedObject)
{        if (!shouldBackOff) {        return;    }    FailureState state = stateMap.get(failedObject);    long now = System.currentTimeMillis();    long delta = now - state.lastFail;    /*     * When do we increase the backoff period?     * We basically calculate the time difference between the last failure     * and the current one. If this failure happened within one hour of the     * last backoff period getting over, then we increase the timeout,     * since the object did not recover yet. Else we assume this is a fresh     * failure and reset the count.     */    long lastBackoffLength = Math.min(maxTimeout, 1000 * (1 << state.sequentialFails));    long allowableDiff = lastBackoffLength + CONSIDER_SEQUENTIAL_RANGE;    if (allowableDiff > delta) {        if (state.sequentialFails < EXP_BACKOFF_COUNTER_LIMIT) {            state.sequentialFails++;        }    } else {        state.sequentialFails = 1;    }    state.lastFail = now;                state.restoreTime = now + Math.min(maxTimeout, 1000 * (1 << state.sequentialFails));}
protected List<Integer> flume_f4611_0()
{    long now = System.currentTimeMillis();    List<Integer> indexList = new ArrayList<Integer>();    int i = 0;    for (T obj : stateMap.keySet()) {        if (!isShouldBackOff() || stateMap.get(obj).restoreTime < now) {            indexList.add(i);        }        i++;    }    return indexList;}
public boolean flume_f4612_0()
{    return shouldBackOff;}
public void flume_f4613_0(long timeout)
{    this.maxTimeout = timeout;}
public long flume_f4614_0()
{    return this.maxTimeout;}
public synchronized Iterator<T> flume_f4615_0()
{    List<Integer> indexList = getIndexList();    int size = indexList.size();    int[] indexOrder = new int[size];    while (indexList.size() != 1) {        int pick = random.nextInt(indexList.size());        indexOrder[indexList.size() - 1] = indexList.remove(pick);    }    indexOrder[0] = indexList.get(0);    return new SpecificOrderIterator<T>(indexOrder, getObjects());}
public Iterator<T> flume_f4616_0()
{    List<Integer> activeIndices = getIndexList();    int size = activeIndices.size();        if (nextHead >= size) {        nextHead = 0;    }    int begin = nextHead++;    if (nextHead == activeIndices.size()) {        nextHead = 0;    }    int[] indexOrder = new int[size];    for (int i = 0; i < size; i++) {        indexOrder[i] = activeIndices.get((begin + i) % size);    }    return new SpecificOrderIterator<T>(indexOrder, getObjects());}
public boolean flume_f4617_0()
{    return index < order.length;}
public T flume_f4618_0()
{    return items.get(order[index++]);}
public void flume_f4619_0()
{    throw new UnsupportedOperationException();}
public static void flume_f4620_0()
{    initSysPropFromEnvVar(SYS_PROP_KEYSTORE_PATH, ENV_VAR_KEYSTORE_PATH, DESCR_KEYSTORE_PATH);    initSysPropFromEnvVar(SYS_PROP_KEYSTORE_PASSWORD, ENV_VAR_KEYSTORE_PASSWORD, DESCR_KEYSTORE_PASSWORD);    initSysPropFromEnvVar(SYS_PROP_KEYSTORE_TYPE, ENV_VAR_KEYSTORE_TYPE, DESCR_KEYSTORE_TYPE);    initSysPropFromEnvVar(SYS_PROP_TRUSTSTORE_PATH, ENV_VAR_TRUSTSTORE_PATH, DESCR_TRUSTSTORE_PATH);    initSysPropFromEnvVar(SYS_PROP_TRUSTSTORE_PASSWORD, ENV_VAR_TRUSTSTORE_PASSWORD, DESCR_TRUSTSTORE_PASSWORD);    initSysPropFromEnvVar(SYS_PROP_TRUSTSTORE_TYPE, ENV_VAR_TRUSTSTORE_TYPE, DESCR_TRUSTSTORE_TYPE);    initSysPropFromEnvVar(SYS_PROP_INCLUDE_PROTOCOLS, ENV_VAR_INCLUDE_PROTOCOLS, DESCR_INCLUDE_PROTOCOLS);    initSysPropFromEnvVar(SYS_PROP_EXCLUDE_PROTOCOLS, ENV_VAR_EXCLUDE_PROTOCOLS, DESCR_EXCLUDE_PROTOCOLS);    initSysPropFromEnvVar(SYS_PROP_INCLUDE_CIPHERSUITES, ENV_VAR_INCLUDE_CIPHERSUITES, DESCR_INCLUDE_CIPHERSUITES);    initSysPropFromEnvVar(SYS_PROP_EXCLUDE_CIPHERSUITES, ENV_VAR_EXCLUDE_CIPHERSUITES, DESCR_EXCLUDE_CIPHERSUITES);}
private static void flume_f4621_1(String sysPropName, String envVarName, String description)
{    if (System.getProperty(sysPropName) != null) {            } else {        String envVarValue = System.getenv(envVarName);        if (envVarValue != null) {            System.setProperty(sysPropName, envVarValue);                    } else {                    }    }}
public static String flume_f4622_0()
{    return System.getProperty(SYS_PROP_KEYSTORE_PATH);}
public static String flume_f4623_0()
{    return System.getProperty(SYS_PROP_KEYSTORE_PASSWORD);}
public static String flume_f4624_0(String defaultValue)
{    String sysPropValue = System.getProperty(SYS_PROP_KEYSTORE_TYPE);    return sysPropValue != null ? sysPropValue : defaultValue;}
public static String flume_f4625_0()
{    return System.getProperty(SYS_PROP_TRUSTSTORE_PATH);}
public static String flume_f4626_0()
{    return System.getProperty(SYS_PROP_TRUSTSTORE_PASSWORD);}
public static String flume_f4627_0(String defaultValue)
{    String sysPropValue = System.getProperty(SYS_PROP_TRUSTSTORE_TYPE);    return sysPropValue != null ? sysPropValue : defaultValue;}
public static String flume_f4628_0()
{    return normalizeProperty(SYS_PROP_EXCLUDE_PROTOCOLS);}
public static String flume_f4629_0()
{    return normalizeProperty(SYS_PROP_INCLUDE_PROTOCOLS);}
public static String flume_f4630_0()
{    return normalizeProperty(SYS_PROP_EXCLUDE_CIPHERSUITES);}
public static String flume_f4631_0()
{    return normalizeProperty(SYS_PROP_INCLUDE_CIPHERSUITES);}
private static String flume_f4632_0(String name)
{    String property = System.getProperty(name);    return property == null ? null : property.replaceAll(",", " ");}
public static void flume_f4633_0(AvroSourceProtocol handler) throws FlumeException, EventDeliveryException
{    handlerSimpleAppendTest(handler, false, false, 0);}
public static void flume_f4634_0(AvroSourceProtocol handler, boolean enableServerCompression, boolean enableClientCompression, int compressionLevel) throws FlumeException, EventDeliveryException
{    NettyAvroRpcClient client = null;    Server server = startServer(handler, 0, enableServerCompression);    try {        Properties starterProp = new Properties();        if (enableClientCompression) {            starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_TYPE, "deflate");            starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_LEVEL, "" + compressionLevel);        } else {            starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_TYPE, "none");        }        client = getStockLocalClient(server.getPort(), starterProp);        boolean isActive = client.isActive();        Assert.assertTrue("Client should be active", isActive);        client.append(EventBuilder.withBody("wheee!!!", Charset.forName("UTF8")));    } finally {        stopServer(server);        if (client != null)            client.close();    }}
public static void flume_f4635_0(AvroSourceProtocol handler) throws FlumeException, EventDeliveryException
{    handlerBatchAppendTest(handler, false, false, 0);}
public static void flume_f4636_0(AvroSourceProtocol handler, boolean enableServerCompression, boolean enableClientCompression, int compressionLevel) throws FlumeException, EventDeliveryException
{    NettyAvroRpcClient client = null;    Server server = startServer(handler, 0, enableServerCompression);    try {        Properties starterProp = new Properties();        if (enableClientCompression) {            starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_TYPE, "deflate");            starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_LEVEL, "" + compressionLevel);        } else {            starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_TYPE, "none");        }        client = getStockLocalClient(server.getPort(), starterProp);        boolean isActive = client.isActive();        Assert.assertTrue("Client should be active", isActive);        int batchSize = client.getBatchSize();        List<Event> events = new ArrayList<Event>();        for (int i = 0; i < batchSize; i++) {            events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));        }        client.appendBatch(events);    } finally {        stopServer(server);        if (client != null)            client.close();    }}
public static NettyAvroRpcClient flume_f4637_0(int port)
{    Properties props = new Properties();    return getStockLocalClient(port, props);}
public static NettyAvroRpcClient flume_f4638_0(int port, Properties starterProp)
{    starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "h1");    starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "h1", "127.0.0.1" + ":" + port);    NettyAvroRpcClient client = new NettyAvroRpcClient();    client.configure(starterProp);    return client;}
public static Server flume_f4639_1(AvroSourceProtocol handler, int port, boolean enableCompression)
{    Responder responder = new SpecificResponder(AvroSourceProtocol.class, handler);    Server server;    if (enableCompression) {        server = new NettyServer(responder, new InetSocketAddress(localhost, port), new NioServerSocketChannelFactory(Executors.newCachedThreadPool(), Executors.newCachedThreadPool()), new CompressionChannelPipelineFactory(), null);    } else {        server = new NettyServer(responder, new InetSocketAddress(localhost, port));    }    server.start();        try {        Thread.sleep(300L);    } catch (InterruptedException ex) {                Thread.currentThread().interrupt();    }    return server;}
public static Server flume_f4640_0(AvroSourceProtocol handler)
{    return startServer(handler, 0, false);}
public static Server flume_f4641_0(AvroSourceProtocol handler, int port)
{    return startServer(handler, port, false);}
public static void flume_f4642_1(Server server)
{    try {        server.close();        server.join();    } catch (InterruptedException ex) {                Thread.currentThread().interrupt();    }}
public int flume_f4643_0()
{    return appendCount;}
public int flume_f4644_0()
{    return appendBatchCount;}
public boolean flume_f4645_0()
{    return failed;}
public void flume_f4646_0()
{    this.failed = true;}
public void flume_f4647_0()
{    this.failed = false;}
public Status flume_f4648_1(AvroFlumeEvent event) throws AvroRemoteException
{    if (failed) {                return Status.FAILED;    }        appendCount++;    return Status.OK;}
public Status flume_f4649_1(List<AvroFlumeEvent> events) throws AvroRemoteException
{    if (failed) {                return Status.FAILED;    }        appendBatchCount++;    return Status.OK;}
public Status flume_f4650_1(AvroFlumeEvent event) throws AvroRemoteException
{        return Status.OK;}
public Status flume_f4651_1(List<AvroFlumeEvent> events) throws AvroRemoteException
{        return Status.OK;}
public Status flume_f4652_1(AvroFlumeEvent event) throws AvroRemoteException
{        return Status.FAILED;}
public Status flume_f4653_1(List<AvroFlumeEvent> events) throws AvroRemoteException
{        return Status.FAILED;}
public Status flume_f4654_1(AvroFlumeEvent event) throws AvroRemoteException
{        return Status.UNKNOWN;}
public Status flume_f4655_1(List<AvroFlumeEvent> events) throws AvroRemoteException
{        return Status.UNKNOWN;}
public Status flume_f4656_1(AvroFlumeEvent event) throws AvroRemoteException
{        throw new AvroRemoteException("Handler smash!");}
public Status flume_f4657_1(List<AvroFlumeEvent> events) throws AvroRemoteException
{        throw new AvroRemoteException("Handler smash!");}
public ChannelPipeline flume_f4658_0() throws Exception
{    ChannelPipeline pipeline = Channels.pipeline();    ZlibEncoder encoder = new ZlibEncoder(6);    pipeline.addFirst("deflater", encoder);    pipeline.addFirst("inflater", new ZlibDecoder());    return pipeline;}
public void flume_f4659_0() throws FlumeException, EventDeliveryException, InterruptedException
{    FailoverRpcClient client = null;    Server server1 = RpcTestUtils.startServer(new OKAvroHandler());    Server server2 = RpcTestUtils.startServer(new OKAvroHandler());    Server server3 = RpcTestUtils.startServer(new OKAvroHandler());    Properties props = new Properties();    int s1Port = server1.getPort();    int s2Port = server2.getPort();    int s3Port = server3.getPort();    props.put("client.type", "default_failover");    props.put("hosts", "host1 host2 host3");    props.put("hosts.host1", "127.0.0.1:" + String.valueOf(s1Port));    props.put("hosts.host2", "127.0.0.1:" + String.valueOf(s2Port));    props.put("hosts.host3", "127.0.0.1:" + String.valueOf(s3Port));    client = (FailoverRpcClient) RpcClientFactory.getInstance(props);    List<Event> events = new ArrayList<Event>();    for (int i = 0; i < 50; i++) {        events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));    }    client.appendBatch(events);    Assert.assertEquals(client.getLastConnectedServerAddress(), new InetSocketAddress("127.0.0.1", server1.getPort()));    server1.close();        Thread.sleep(1000L);    events = new ArrayList<Event>();    for (int i = 0; i < 50; i++) {        events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));    }    client.appendBatch(events);    Assert.assertEquals(new InetSocketAddress("localhost", server2.getPort()), client.getLastConnectedServerAddress());    server2.close();        Thread.sleep(1000L);    client.append(EventBuilder.withBody("Had a sandwich?", Charset.forName("UTF8")));    Assert.assertEquals(new InetSocketAddress("localhost", server3.getPort()), client.getLastConnectedServerAddress());        Server server4 = RpcTestUtils.startServer(new OKAvroHandler(), s2Port);    server3.close();        Thread.sleep(1000L);    events = new ArrayList<Event>();    for (int i = 0; i < 50; i++) {        events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));    }    client.appendBatch(events);    Assert.assertEquals(new InetSocketAddress("localhost", s2Port), client.getLastConnectedServerAddress());    Server server5 = RpcTestUtils.startServer(new OKAvroHandler(), s1Port);        client.append(EventBuilder.withBody("Had a mango?", Charset.forName("UTF8")));    Assert.assertEquals(new InetSocketAddress("localhost", s2Port), client.getLastConnectedServerAddress());    server4.close();        Thread.sleep(1000L);    events = new ArrayList<Event>();    for (int i = 0; i < 50; i++) {        events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));    }    client.appendBatch(events);    Assert.assertEquals(new InetSocketAddress("localhost", s1Port), client.getLastConnectedServerAddress());    server5.close();        Thread.sleep(1000L);    Server server6 = RpcTestUtils.startServer(new OKAvroHandler(), s1Port);    client.append(EventBuilder.withBody("Had a whole watermelon?", Charset.forName("UTF8")));    Assert.assertEquals(new InetSocketAddress("localhost", s1Port), client.getLastConnectedServerAddress());    server6.close();        Thread.sleep(1000L);    Server server7 = RpcTestUtils.startServer(new OKAvroHandler(), s3Port);    events = new ArrayList<Event>();    for (int i = 0; i < 50; i++) {        events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));    }    client.appendBatch(events);    Assert.assertEquals(new InetSocketAddress("localhost", s3Port), client.getLastConnectedServerAddress());    server7.close();}
public void flume_f4660_0() throws FlumeException, EventDeliveryException
{    FailoverRpcClient client = null;    Server server1 = RpcTestUtils.startServer(new OKAvroHandler());    Server server2 = RpcTestUtils.startServer(new OKAvroHandler());    Server server3 = RpcTestUtils.startServer(new OKAvroHandler());    Properties props = new Properties();    props.put("client.type", "default_failover");    props.put("hosts", "host1 host2 host3");    props.put("hosts.host1", "localhost:" + String.valueOf(server1.getPort()));    props.put("hosts.host2", "localhost:" + String.valueOf(server2.getPort()));    props.put("hosts.host3", " localhost:" + String.valueOf(server3.getPort()));    client = (FailoverRpcClient) RpcClientFactory.getInstance(props);    List<Event> events = new ArrayList<Event>();    for (int i = 0; i < 50; i++) {        events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));    }    client.appendBatch(events);    server1.close();    server2.close();    server3.close();    events = new ArrayList<Event>();    for (int i = 0; i < 50; i++) {        events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));    }    client.appendBatch(events);}
public void flume_f4661_0()
{    Server server1 = null;    RpcClient c = null;    try {        server1 = RpcTestUtils.startServer(new OKAvroHandler());        Properties p = new Properties();        p.put("host1", "127.0.0.1:" + server1.getPort());        p.put("hosts", "host1");        p.put("client.type", "default_loadbalance");        RpcClientFactory.getInstance(p);    } finally {        if (server1 != null)            server1.close();        if (c != null)            c.close();    }}
public void flume_f4662_0() throws Exception
{    Server s1 = null;    Server s2 = null;    RpcClient c = null;    try {        LoadBalancedAvroHandler h1 = new LoadBalancedAvroHandler();        LoadBalancedAvroHandler h2 = new LoadBalancedAvroHandler();        s1 = RpcTestUtils.startServer(h1);        s2 = RpcTestUtils.startServer(h2);        Properties p = new Properties();        p.put("hosts", "h1 h2");        p.put("client.type", "default_loadbalance");        p.put("hosts.h1", "127.0.0.1:" + s1.getPort());        p.put("hosts.h2", "127.0.0.1:" + s2.getPort());        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < 100; i++) {            if (i == 20) {                h2.setFailed();            } else if (i == 40) {                h2.setOK();            }            c.append(getEvent(i));        }        Assert.assertEquals(60, h1.getAppendCount());        Assert.assertEquals(40, h2.getAppendCount());    } finally {        if (s1 != null)            s1.close();        if (s2 != null)            s2.close();        if (c != null)            c.close();    }}
public void flume_f4663_0() throws Exception
{    Server s1 = null;    Server s2 = null;    RpcClient c = null;    try {        LoadBalancedAvroHandler h1 = new LoadBalancedAvroHandler();        LoadBalancedAvroHandler h2 = new LoadBalancedAvroHandler();        s1 = RpcTestUtils.startServer(h1);        s2 = RpcTestUtils.startServer(h2);        Properties p = new Properties();        p.put("hosts", "h1 h2");        p.put("client.type", "default_loadbalance");        p.put("hosts.h1", "127.0.0.1:" + s1.getPort());        p.put("hosts.h2", "127.0.0.1:" + s2.getPort());        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < 100; i++) {            if (i == 20) {                h2.setFailed();            } else if (i == 40) {                h2.setOK();            }            c.append(getEvent(i));        }        Assert.assertEquals(60, h1.getAppendCount());        Assert.assertEquals(40, h2.getAppendCount());        if (c != null)            c.close();        c.append(getEvent(3));        Assert.fail();    } finally {        if (s1 != null)            s1.close();        if (s2 != null)            s2.close();    }}
public void flume_f4664_1() throws Exception
{        Server s1 = null;    RpcClient c1 = null;    RpcClient c2 = null;    try {        LoadBalancedAvroHandler h1 = new LoadBalancedAvroHandler();        s1 = RpcTestUtils.startServer(h1);                Properties p = new Properties();        p.put("hosts", "h1 h2");        p.put("client.type", "default_loadbalance");                p.put("hosts.h1", "127.0.0.1:" + 0);        p.put("hosts.h2", "127.0.0.1:" + s1.getPort());                c1 = RpcClientFactory.getInstance(p);        Assert.assertTrue(c1 instanceof LoadBalancingRpcClient);        for (int i = 0; i < 10; i++) {            c1.appendBatch(getBatchedEvent(i));        }        Assert.assertEquals(10, h1.getAppendBatchCount());                c2 = RpcClientFactory.getInstance(p);        Assert.assertTrue(c2 instanceof LoadBalancingRpcClient);        for (int i = 0; i < 10; i++) {            c2.append(getEvent(i));        }        Assert.assertEquals(10, h1.getAppendCount());    } finally {        if (s1 != null)            s1.close();        if (c1 != null)            c1.close();        if (c2 != null)            c2.close();    }}
public void flume_f4665_0() throws Exception
{    Server s1 = null;    Server s2 = null;    RpcClient c = null;    try {        LoadBalancedAvroHandler h1 = new LoadBalancedAvroHandler();        LoadBalancedAvroHandler h2 = new LoadBalancedAvroHandler();        s1 = RpcTestUtils.startServer(h1);        s2 = RpcTestUtils.startServer(h2);        Properties p = new Properties();        p.put("hosts", "h1 h2");        p.put("client.type", "default_loadbalance");        p.put("hosts.h1", "127.0.0.1:" + s1.getPort());        p.put("hosts.h2", "127.0.0.1:" + s2.getPort());        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < 100; i++) {            if (i == 20) {                h2.setFailed();            } else if (i == 40) {                h2.setOK();            }            c.appendBatch(getBatchedEvent(i));        }        Assert.assertEquals(60, h1.getAppendBatchCount());        Assert.assertEquals(40, h2.getAppendBatchCount());    } finally {        if (s1 != null)            s1.close();        if (s2 != null)            s2.close();        if (c != null)            c.close();    }}
public void flume_f4666_0() throws Exception
{    Server s1 = null;    Server s2 = null;    RpcClient c = null;    try {        LoadBalancedAvroHandler h1 = new LoadBalancedAvroHandler();        LoadBalancedAvroHandler h2 = new LoadBalancedAvroHandler();        s1 = RpcTestUtils.startServer(h1);        s2 = RpcTestUtils.startServer(h2);        Properties p = new Properties();        p.put("hosts", "h1 h2");        p.put("client.type", "default_loadbalance");        p.put("hosts.h1", "127.0.0.1:" + s1.getPort());        p.put("hosts.h2", "127.0.0.1:" + s2.getPort());        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < 100; i++) {            c.append(getEvent(i));        }        Assert.assertEquals(50, h1.getAppendCount());        Assert.assertEquals(50, h2.getAppendCount());    } finally {        if (s1 != null)            s1.close();        if (s2 != null)            s2.close();        if (c != null)            c.close();    }}
public void flume_f4667_0() throws Exception
{    Server s1 = null;    Server s2 = null;    RpcClient c = null;    try {        LoadBalancedAvroHandler h1 = new LoadBalancedAvroHandler();        LoadBalancedAvroHandler h2 = new LoadBalancedAvroHandler();        s1 = RpcTestUtils.startServer(h1);        s2 = RpcTestUtils.startServer(h2);        Properties p = new Properties();        p.put("hosts", "h1 h2");        p.put("client.type", "default_loadbalance");        p.put("hosts.h1", "127.0.0.1:" + s1.getPort());        p.put("hosts.h2", "127.0.0.1:" + s2.getPort());        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < 100; i++) {            c.appendBatch(getBatchedEvent(i));        }        Assert.assertEquals(50, h1.getAppendBatchCount());        Assert.assertEquals(50, h2.getAppendBatchCount());    } finally {        if (s1 != null)            s1.close();        if (s2 != null)            s2.close();        if (c != null)            c.close();    }}
public void flume_f4668_0() throws Exception
{    final int NUM_HOSTS = 10;    final int NUM_EVENTS = 1000;    Server[] s = new Server[NUM_HOSTS];    LoadBalancedAvroHandler[] h = new LoadBalancedAvroHandler[NUM_HOSTS];    RpcClient c = null;    try {        Properties p = new Properties();        StringBuilder hostList = new StringBuilder("");        for (int i = 0; i < NUM_HOSTS; i++) {            h[i] = new LoadBalancedAvroHandler();            s[i] = RpcTestUtils.startServer(h[i]);            String name = "h" + i;            p.put("hosts." + name, "127.0.0.1:" + s[i].getPort());            hostList.append(name).append(" ");        }        p.put("hosts", hostList.toString().trim());        p.put("client.type", "default_loadbalance");        p.put("host-selector", "random");        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < NUM_EVENTS; i++) {            c.append(getEvent(i));        }        Set<Integer> counts = new HashSet<Integer>();        int total = 0;        for (LoadBalancedAvroHandler handler : h) {            total += handler.getAppendCount();            counts.add(handler.getAppendCount());        }        Assert.assertTrue("Very unusual distribution", counts.size() > 2);        Assert.assertTrue("Missing events", total == NUM_EVENTS);    } finally {        for (int i = 0; i < NUM_HOSTS; i++) {            if (s[i] != null)                s[i].close();        }    }}
public void flume_f4669_0() throws Exception
{    final int NUM_HOSTS = 10;    final int NUM_EVENTS = 1000;    Server[] s = new Server[NUM_HOSTS];    LoadBalancedAvroHandler[] h = new LoadBalancedAvroHandler[NUM_HOSTS];    RpcClient c = null;    try {        Properties p = new Properties();        StringBuilder hostList = new StringBuilder("");        for (int i = 0; i < NUM_HOSTS; i++) {            h[i] = new LoadBalancedAvroHandler();            s[i] = RpcTestUtils.startServer(h[i]);            String name = "h" + i;            p.put("hosts." + name, "127.0.0.1:" + s[i].getPort());            hostList.append(name).append(" ");        }        p.put("hosts", hostList.toString().trim());        p.put("client.type", "default_loadbalance");        p.put("host-selector", "random");        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < NUM_EVENTS; i++) {            c.appendBatch(getBatchedEvent(i));        }        Set<Integer> counts = new HashSet<Integer>();        int total = 0;        for (LoadBalancedAvroHandler handler : h) {            total += handler.getAppendBatchCount();            counts.add(handler.getAppendBatchCount());        }        Assert.assertTrue("Very unusual distribution", counts.size() > 2);        Assert.assertTrue("Missing events", total == NUM_EVENTS);    } finally {        for (int i = 0; i < NUM_HOSTS; i++) {            if (s[i] != null)                s[i].close();        }    }}
public void flume_f4670_0() throws Exception
{    final int NUM_HOSTS = 10;    final int NUM_EVENTS = 1000;    Server[] s = new Server[NUM_HOSTS];    LoadBalancedAvroHandler[] h = new LoadBalancedAvroHandler[NUM_HOSTS];    RpcClient c = null;    try {        Properties p = new Properties();        StringBuilder hostList = new StringBuilder("");        for (int i = 0; i < NUM_HOSTS; i++) {            h[i] = new LoadBalancedAvroHandler();            s[i] = RpcTestUtils.startServer(h[i]);            String name = "h" + i;            p.put("hosts." + name, "127.0.0.1:" + s[i].getPort());            hostList.append(name).append(" ");        }        p.put("hosts", hostList.toString().trim());        p.put("client.type", "default_loadbalance");        p.put("host-selector", "round_robin");        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < NUM_EVENTS; i++) {            c.append(getEvent(i));        }        Set<Integer> counts = new HashSet<Integer>();        int total = 0;        for (LoadBalancedAvroHandler handler : h) {            total += handler.getAppendCount();            counts.add(handler.getAppendCount());        }        Assert.assertTrue("Very unusual distribution", counts.size() == 1);        Assert.assertTrue("Missing events", total == NUM_EVENTS);    } finally {        for (int i = 0; i < NUM_HOSTS; i++) {            if (s[i] != null)                s[i].close();        }    }}
public void flume_f4671_0() throws Exception
{    final int NUM_HOSTS = 10;    final int NUM_EVENTS = 1000;    Server[] s = new Server[NUM_HOSTS];    LoadBalancedAvroHandler[] h = new LoadBalancedAvroHandler[NUM_HOSTS];    RpcClient c = null;    try {        Properties p = new Properties();        StringBuilder hostList = new StringBuilder("");        for (int i = 0; i < NUM_HOSTS; i++) {            h[i] = new LoadBalancedAvroHandler();            s[i] = RpcTestUtils.startServer(h[i]);            String name = "h" + i;            p.put("hosts." + name, "127.0.0.1:" + s[i].getPort());            hostList.append(name).append(" ");        }        p.put("hosts", hostList.toString().trim());        p.put("client.type", "default_loadbalance");        p.put("host-selector", "round_robin");        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < NUM_EVENTS; i++) {            c.appendBatch(getBatchedEvent(i));        }        Set<Integer> counts = new HashSet<Integer>();        int total = 0;        for (LoadBalancedAvroHandler handler : h) {            total += handler.getAppendBatchCount();            counts.add(handler.getAppendBatchCount());        }        Assert.assertTrue("Very unusual distribution", counts.size() == 1);        Assert.assertTrue("Missing events", total == NUM_EVENTS);    } finally {        for (int i = 0; i < NUM_HOSTS; i++) {            if (s[i] != null)                s[i].close();        }    }}
public void flume_f4672_0() throws Exception
{    Properties p = new Properties();    List<LoadBalancedAvroHandler> hosts = new ArrayList<LoadBalancedAvroHandler>();    List<Server> servers = new ArrayList<Server>();    StringBuilder hostList = new StringBuilder("");    for (int i = 0; i < 3; i++) {        LoadBalancedAvroHandler s = new LoadBalancedAvroHandler();        hosts.add(s);        Server srv = RpcTestUtils.startServer(s);        servers.add(srv);        String name = "h" + i;        p.put("hosts." + name, "127.0.0.1:" + srv.getPort());        hostList.append(name).append(" ");    }    p.put("hosts", hostList.toString().trim());    p.put("client.type", "default_loadbalance");    p.put("host-selector", "random");    p.put("backoff", "true");    hosts.get(0).setFailed();    hosts.get(2).setFailed();    RpcClient c = RpcClientFactory.getInstance(p);    Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < 50; i++) {                c.append(EventBuilder.withBody(("test" + String.valueOf(i)).getBytes()));    }    Assert.assertEquals(50, hosts.get(1).getAppendCount());    Assert.assertEquals(0, hosts.get(0).getAppendCount());    Assert.assertEquals(0, hosts.get(2).getAppendCount());    hosts.get(0).setOK();        hosts.get(1).setFailed();    try {        c.append(EventBuilder.withBody("shouldfail".getBytes()));                Assert.fail("Expected EventDeliveryException");    } catch (EventDeliveryException e) {        }        Thread.sleep(2500);    for (int i = 0; i < 50; i++) {                c.append(EventBuilder.withBody(("test" + String.valueOf(i)).getBytes()));    }    Assert.assertEquals(50, hosts.get(0).getAppendCount());    Assert.assertEquals(50, hosts.get(1).getAppendCount());    Assert.assertEquals(0, hosts.get(2).getAppendCount());}
public void flume_f4673_0() throws EventDeliveryException
{    Properties p = new Properties();    List<LoadBalancedAvroHandler> hosts = new ArrayList<LoadBalancedAvroHandler>();    List<Server> servers = new ArrayList<Server>();    StringBuilder hostList = new StringBuilder("");    for (int i = 0; i < 3; i++) {        LoadBalancedAvroHandler s = new LoadBalancedAvroHandler();        hosts.add(s);        Server srv = RpcTestUtils.startServer(s);        servers.add(srv);        String name = "h" + i;        p.put("hosts." + name, "127.0.0.1:" + srv.getPort());        hostList.append(name).append(" ");    }    p.put("hosts", hostList.toString().trim());    p.put("client.type", "default_loadbalance");    p.put("host-selector", "round_robin");    p.put("backoff", "true");    RpcClient c = RpcClientFactory.getInstance(p);    Assert.assertTrue(c instanceof LoadBalancingRpcClient);    for (int i = 0; i < 3; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    hosts.get(1).setFailed();    for (int i = 0; i < 3; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    hosts.get(1).setOK();        for (int i = 0; i < 3; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    Assert.assertEquals(1 + 2 + 1, hosts.get(0).getAppendCount());    Assert.assertEquals(1, hosts.get(1).getAppendCount());    Assert.assertEquals(1 + 1 + 2, hosts.get(2).getAppendCount());}
public void flume_f4674_0() throws Exception
{    Properties p = new Properties();    List<LoadBalancedAvroHandler> hosts = new ArrayList<LoadBalancedAvroHandler>();    List<Server> servers = new ArrayList<Server>();    StringBuilder hostList = new StringBuilder("");    for (int i = 0; i < 3; i++) {        LoadBalancedAvroHandler s = new LoadBalancedAvroHandler();        hosts.add(s);        if (i == 1) {            s.setFailed();        }        Server srv = RpcTestUtils.startServer(s);        servers.add(srv);        String name = "h" + i;        p.put("hosts." + name, "127.0.0.1:" + srv.getPort());        hostList.append(name).append(" ");    }    p.put("hosts", hostList.toString().trim());    p.put("client.type", "default_loadbalance");    p.put("host-selector", "round_robin");    p.put("backoff", "true");    RpcClient c = RpcClientFactory.getInstance(p);    Assert.assertTrue(c instanceof LoadBalancingRpcClient);    for (int i = 0; i < 3; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    Assert.assertEquals(0, hosts.get(1).getAppendCount());    Thread.sleep(2100);        for (int i = 0; i < 3; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    Assert.assertEquals(0, hosts.get(1).getAppendCount());    hosts.get(1).setOK();    Thread.sleep(2100);        for (int i = 0; i < 3; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    Assert.assertEquals(0, hosts.get(1).getAppendCount());        Thread.sleep(2500);    int numEvents = 60;    for (int i = 0; i < numEvents; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    Assert.assertEquals(2 + 2 + 1 + (numEvents / 3), hosts.get(0).getAppendCount());    Assert.assertEquals((numEvents / 3), hosts.get(1).getAppendCount());    Assert.assertEquals(1 + 1 + 2 + (numEvents / 3), hosts.get(2).getAppendCount());}
public void flume_f4675_0() throws EventDeliveryException, InterruptedException
{    Properties p = new Properties();    List<LoadBalancedAvroHandler> hosts = new ArrayList<LoadBalancedAvroHandler>();    List<Server> servers = new ArrayList<Server>();    StringBuilder hostList = new StringBuilder("");    for (int i = 0; i < 3; i++) {        LoadBalancedAvroHandler s = new LoadBalancedAvroHandler();        hosts.add(s);        if (i == 1) {            s.setFailed();        }        Server srv = RpcTestUtils.startServer(s);        servers.add(srv);        String name = "h" + i;        p.put("hosts." + name, "127.0.0.1:" + srv.getPort());        hostList.append(name).append(" ");    }    p.put("hosts", hostList.toString().trim());    p.put("client.type", "default_loadbalance");    p.put("host-selector", "round_robin");    p.put("backoff", "true");    RpcClient c = RpcClientFactory.getInstance(p);    Assert.assertTrue(c instanceof LoadBalancingRpcClient);    for (int i = 0; i < 3; i++) {        c.append(EventBuilder.withBody("recovery test".getBytes()));    }    hosts.get(1).setOK();    Thread.sleep(3000);    int numEvents = 60;    for (int i = 0; i < numEvents; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    Assert.assertEquals(2 + (numEvents / 3), hosts.get(0).getAppendCount());    Assert.assertEquals(0 + (numEvents / 3), hosts.get(1).getAppendCount());    Assert.assertEquals(1 + (numEvents / 3), hosts.get(2).getAppendCount());}
private List<Event> flume_f4676_0(int index)
{    List<Event> result = new ArrayList<Event>();    result.add(getEvent(index));    return result;}
private Event flume_f4677_0(int index)
{    return EventBuilder.withBody(("event: " + index).getBytes());}
public void flume_f4678_0() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new OKAvroHandler());}
public void flume_f4679_0() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new OKAvroHandler(), true, true, 6);}
public void flume_f4680_0() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new OKAvroHandler(), true, true, 0);}
public void flume_f4681_0() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new OKAvroHandler(), false, true, 6);}
public void flume_f4682_0() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new OKAvroHandler(), true, false, 6);}
public void flume_f4683_0() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new OKAvroHandler());}
public void flume_f4684_0() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new OKAvroHandler(), true, true, 0);}
public void flume_f4685_0() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new OKAvroHandler(), true, true, 6);}
public void flume_f4686_0() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new OKAvroHandler(), true, false, 6);}
public void flume_f4687_0() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new OKAvroHandler(), false, true, 6);}
public void flume_f4688_0() throws FlumeException
{    @SuppressWarnings("unused")    NettyAvroRpcClient client = new NettyAvroRpcClient();    Properties props = new Properties();    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "localhost");    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "localhost", localhost + ":" + 1);    client.configure(props);}
public void flume_f4689_0() throws FlumeException, EventDeliveryException
{    int batchSize = 10;    int moreThanBatchSize = batchSize + 1;    NettyAvroRpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    Properties props = new Properties();    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "localhost");    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "localhost", localhost + ":" + server.getPort());    props.setProperty(RpcClientConfigurationConstants.CONFIG_BATCH_SIZE, "" + batchSize);    try {        client = new NettyAvroRpcClient();        client.configure(props);                List<Event> events = new ArrayList<Event>();        for (int i = 0; i < moreThanBatchSize; i++) {            events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));        }        client.appendBatch(events);    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
public void flume_f4690_1() throws FlumeException, EventDeliveryException, InterruptedException
{    NettyAvroRpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        client = RpcTestUtils.getStockLocalClient(server.getPort());        server.close();                Thread.sleep(1000L);        try {            server.join();        } catch (InterruptedException ex) {                        Thread.currentThread().interrupt();        }        try {            client.append(EventBuilder.withBody("hello", Charset.forName("UTF8")));        } finally {            Assert.assertFalse("Client should not be active", client.isActive());        }    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
public void flume_f4691_0() throws FlumeException, EventDeliveryException
{    NettyAvroRpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        client = RpcTestUtils.getStockLocalClient(server.getPort());        client.close();        Assert.assertFalse("Client should not be active", client.isActive());        System.out.println("Yaya! I am not active after client close!");        client.append(EventBuilder.withBody("hello", Charset.forName("UTF8")));    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
public void flume_f4692_1() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new FailedAvroHandler());    }
public void flume_f4693_1() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new UnknownAvroHandler());    }
public void flume_f4694_1() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new ThrowingAvroHandler());    }
public void flume_f4695_1() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new FailedAvroHandler());    }
public void flume_f4696_1() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new UnknownAvroHandler());    }
public void flume_f4697_1() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new ThrowingAvroHandler());    }
public void flume_f4698_0() throws FlumeException, EventDeliveryException
{    NettyAvroRpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    Properties props = new Properties();    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "localhost");    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "localhost", localhost + ":" + server.getPort());    props.setProperty(RpcClientConfigurationConstants.MAX_IO_WORKERS, Integer.toString(2));    try {        client = new NettyAvroRpcClient();        client.configure(props);        for (int i = 0; i < 5; i++) {            client.append(EventBuilder.withBody("evt:" + i, Charset.forName("UTF8")));        }    } finally {        RpcTestUtils.stopServer(server);        if (client != null) {            client.close();        }    }}
public void flume_f4699_0() throws FlumeException, EventDeliveryException
{    NettyAvroRpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler(), 0, true);    Properties props = new Properties();    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "localhost");    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "localhost", localhost + ":" + server.getPort());    props.setProperty(RpcClientConfigurationConstants.MAX_IO_WORKERS, Integer.toString(2));    props.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_TYPE, "deflate");    props.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_LEVEL, "" + 0);    try {        client = new NettyAvroRpcClient();        client.configure(props);        for (int i = 0; i < 5; i++) {            client.append(EventBuilder.withBody("evt:" + i, Charset.forName("UTF8")));        }    } finally {        RpcTestUtils.stopServer(server);        if (client != null) {            client.close();        }    }}
public void flume_f4700_0() throws FlumeException, EventDeliveryException
{    RpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        client = RpcClientFactory.getDefaultInstance(localhost, server.getPort());        client.append(EventBuilder.withBody("wheee!!!", Charset.forName("UTF8")));    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
public void flume_f4701_0() throws FlumeException, EventDeliveryException
{    RpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        client = RpcClientFactory.getInstance(localhost, server.getPort());        client.append(EventBuilder.withBody("wheee!!!", Charset.forName("UTF8")));    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
public void flume_f4702_0() throws FlumeException, EventDeliveryException
{    RpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        client = RpcClientFactory.getInstance(localhost, server.getPort(), 3);        Assert.assertEquals("Batch size was specified", 3, client.getBatchSize());        client.append(EventBuilder.withBody("wheee!!!", Charset.forName("UTF8")));    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
public void flume_f4703_0() throws FlumeException, EventDeliveryException
{    int batchSize = 7;    RpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        client = RpcClientFactory.getDefaultInstance(localhost, server.getPort(), batchSize);        List<Event> events = new ArrayList<Event>();        for (int i = 0; i < batchSize; i++) {            events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));        }        client.appendBatch(events);    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
public void flume_f4704_0() throws FlumeException, EventDeliveryException
{    int batchSize = 7;    RpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        Properties p = new Properties();        p.put("hosts", "host1");        p.put("hosts.host1", localhost + ":" + String.valueOf(server.getPort()));        p.put("batch-size", String.valueOf(batchSize));        client = RpcClientFactory.getInstance(p);        List<Event> events = new ArrayList<Event>();        for (int i = 0; i < batchSize; i++) {            events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));        }        client.appendBatch(events);    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
public void flume_f4705_0() throws FlumeException, EventDeliveryException
{    RpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        client = RpcClientFactory.getDefaultInstance(localhost, server.getPort());        int batchSize = client.getBatchSize();        int moreThanBatch = batchSize + 1;        List<Event> events = new ArrayList<Event>();        for (int i = 0; i < moreThanBatch; i++) {            events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));        }        client.appendBatch(events);    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
public void flume_f4706_0() throws Exception
{    props.setProperty("hosts", "h1");    try (ServerSocket socket = new ServerSocket(0)) {        port = socket.getLocalPort();    }    props.setProperty(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, "thrift");    props.setProperty("hosts.h1", "0.0.0.0:" + String.valueOf(port));    props.setProperty(RpcClientConfigurationConstants.CONFIG_BATCH_SIZE, "10");    props.setProperty(RpcClientConfigurationConstants.CONFIG_REQUEST_TIMEOUT, "2000");    props.setProperty(ThriftRpcClient.CONFIG_PROTOCOL, ThriftRpcClient.COMPACT_PROTOCOL);}
public void flume_f4707_0() throws Exception
{    src.stop();}
private static void flume_f4708_0(RpcClient client, int count) throws Exception
{    for (int i = 0; i < count; i++) {        Map<String, String> header = new HashMap<String, String>();        header.put(SEQ, String.valueOf(i));        client.append(EventBuilder.withBody(String.valueOf(i).getBytes(), header));    }}
private static void flume_f4709_0(RpcClient client, int start, int limit) throws Exception
{    List<Event> events = new ArrayList<Event>();    for (int i = start; i <= limit; i++) {        Map<String, String> header = new HashMap<String, String>();        header.put(SEQ, String.valueOf(i));        events.add(EventBuilder.withBody(String.valueOf(i).getBytes(), header));    }    client.appendBatch(events);}
public void flume_f4710_0() throws Exception
{    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.OK.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    client = (ThriftRpcClient) RpcClientFactory.getInstance(props);        insertEvents(client, 10);        insertAsBatch(client, 10, 25);        insertAsBatch(client, 26, 37);    int count = 0;    Assert.assertEquals(38, src.flumeEvents.size());    for (Event e : src.flumeEvents) {        Assert.assertEquals(new String(e.getBody()), String.valueOf(count++));    }    Assert.assertEquals(10, src.individualCount);    Assert.assertEquals(4, src.batchCount);    Assert.assertEquals(2, src.incompleteBatches);}
public void flume_f4711_0() throws Exception
{    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.SLOW.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    client = (ThriftRpcClient) RpcClientFactory.getInstance(props);        insertEvents(client, 2);        insertAsBatch(client, 2, 25);        insertAsBatch(client, 26, 37);    int count = 0;    Assert.assertEquals(38, src.flumeEvents.size());    for (Event e : src.flumeEvents) {        Assert.assertEquals(new String(e.getBody()), String.valueOf(count++));    }    Assert.assertEquals(2, src.individualCount);    Assert.assertEquals(5, src.batchCount);    Assert.assertEquals(2, src.incompleteBatches);}
public void flume_f4712_0() throws Exception
{    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.FAIL.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    client = (ThriftRpcClient) RpcClientFactory.getInstance(props);        insertEvents(client, 2);    Assert.fail("Expected EventDeliveryException to be thrown.");}
public void flume_f4713_0() throws Throwable
{    try {        src = new ThriftTestingSource(ThriftTestingSource.HandlerType.ERROR.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);        client = (ThriftRpcClient) RpcClientFactory.getThriftInstance("0.0.0.0", port);                insertEvents(client, 2);    } catch (EventDeliveryException ex) {        Assert.assertEquals("Failed to send event. ", ex.getMessage());    }}
public void flume_f4714_0() throws Throwable
{    try {        src = new ThriftTestingSource(ThriftTestingSource.HandlerType.TIMEOUT.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);        client = (ThriftRpcClient) RpcClientFactory.getThriftInstance(props);                insertEvents(client, 2);    } catch (EventDeliveryException ex) {        throw ex.getCause();    }}
public void flume_f4715_0() throws Throwable
{    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.OK.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    client = (ThriftRpcClient) RpcClientFactory.getThriftInstance("0.0.0.0", port, 10);    int threadCount = 100;    ExecutorService submissionSvc = Executors.newFixedThreadPool(threadCount);    ArrayList<Future<?>> futures = new ArrayList<Future<?>>(threadCount);    for (int i = 0; i < threadCount; i++) {        futures.add(submissionSvc.submit(new Runnable() {            @Override            public void run() {                try {                    insertAsBatch(client, 0, 9);                } catch (Exception e) {                                        e.printStackTrace();                                }            }        }));    }    for (int i = 0; i < threadCount; i++) {        futures.get(i).get();    }    ArrayList<String> events = new ArrayList<String>();    for (Event e : src.flumeEvents) {        events.add(new String(e.getBody()));    }    int count = 0;    Collections.sort(events);    for (int i = 0; i < events.size(); ) {        for (int j = 0; j < threadCount; j++) {            Assert.assertEquals(String.valueOf(count), events.get(i++));        }        count++;    }}
public void flume_f4716_0()
{    try {        insertAsBatch(client, 0, 9);    } catch (Exception e) {                e.printStackTrace();        }}
public void flume_f4717_0(AtomicLong delay)
{    this.delay = delay;}
public Status flume_f4718_0(ThriftFlumeEvent event) throws TException
{    flumeEvents.add(EventBuilder.withBody(event.getBody(), event.getHeaders()));    individualCount++;    return Status.OK;}
public Status flume_f4719_0(List<ThriftFlumeEvent> events) throws TException
{    batchCount++;    if (events.size() < 10) {        incompleteBatches++;    }    for (ThriftFlumeEvent event : events) {        flumeEvents.add(EventBuilder.withBody(event.getBody(), event.getHeaders()));    }    return Status.OK;}
public Status flume_f4720_0(ThriftFlumeEvent event) throws TException
{    return Status.FAILED;}
public Status flume_f4721_0(List<ThriftFlumeEvent> events) throws TException
{    return Status.FAILED;}
public Status flume_f4722_0(ThriftFlumeEvent event) throws TException
{    throw new FlumeException("Forced Error");}
public Status flume_f4723_0(List<ThriftFlumeEvent> events) throws TException
{    throw new FlumeException("Forced Error");}
public Status flume_f4724_0(ThriftFlumeEvent event) throws TException
{    try {        TimeUnit.MILLISECONDS.sleep(1550);    } catch (InterruptedException e) {        throw new FlumeException("Error", e);    }    return super.append(event);}
public Status flume_f4725_0(List<ThriftFlumeEvent> events) throws TException
{    try {        TimeUnit.MILLISECONDS.sleep(1550);    } catch (InterruptedException e) {        throw new FlumeException("Error", e);    }    return super.appendBatch(events);}
public Status flume_f4726_0(ThriftFlumeEvent event) throws TException
{    try {        TimeUnit.MILLISECONDS.sleep(5000);    } catch (InterruptedException e) {        throw new FlumeException("Error", e);    }    return super.append(event);}
public Status flume_f4727_0(List<ThriftFlumeEvent> events) throws TException
{    try {        TimeUnit.MILLISECONDS.sleep(5000);    } catch (InterruptedException e) {        throw new FlumeException("Error", e);    }    return super.appendBatch(events);}
public Status flume_f4728_0(ThriftFlumeEvent event) throws TException
{    try {        if (delay != null) {            TimeUnit.MILLISECONDS.sleep(delay.get());        }    } catch (InterruptedException e) {        throw new FlumeException("Error", e);    }    return super.append(event);}
public Status flume_f4729_0(List<ThriftFlumeEvent> events) throws TException
{    try {        if (delay != null) {            TimeUnit.MILLISECONDS.sleep(delay.get());        }    } catch (InterruptedException e) {        throw new FlumeException("Error", e);    }    return super.appendBatch(events);}
private ThriftSourceProtocol.Iface flume_f4730_0(String handlerName)
{    ThriftSourceProtocol.Iface handler = null;    if (handlerName.equals(HandlerType.OK.name())) {        handler = new ThriftOKHandler();    } else if (handlerName.equals(HandlerType.FAIL.name())) {        handler = new ThriftFailHandler();    } else if (handlerName.equals(HandlerType.ERROR.name())) {        handler = new ThriftErrorHandler();    } else if (handlerName.equals(HandlerType.SLOW.name())) {        handler = new ThriftSlowHandler();    } else if (handlerName.equals(HandlerType.TIMEOUT.name())) {        handler = new ThriftTimeoutHandler();    } else if (handlerName.equals(HandlerType.ALTERNATE.name())) {        handler = new ThriftAlternateHandler();    }    return handler;}
public void flume_f4731_0()
{    server.serve();}
public void flume_f4732_0()
{    server.serve();}
public void flume_f4733_0()
{    server.stop();}
public void flume_f4734_0()
{    Event e1 = EventBuilder.withBody("e1".getBytes());    Assert.assertNotNull(e1);    Assert.assertArrayEquals("body is correct", "e1".getBytes(), e1.getBody());    Event e2 = EventBuilder.withBody(Long.valueOf(2).toString().getBytes());    Assert.assertNotNull(e2);    Assert.assertArrayEquals("body is correct", Long.valueOf(2L).toString().getBytes(), e2.getBody());}
public void flume_f4735_0()
{    Map<String, String> headers = new HashMap<String, String>();    headers.put("one", "1");    headers.put("two", "2");    Event e1 = EventBuilder.withBody("e1".getBytes(), headers);    Assert.assertNotNull(e1);    Assert.assertArrayEquals("e1 has the proper body", "e1".getBytes(), e1.getBody());    Assert.assertEquals("e1 has the proper headers", 2, e1.getHeaders().size());    Assert.assertEquals("e1 has a one key", "1", e1.getHeaders().get("one"));}
public void flume_f4736_0()
{    JSONEvent jsonEvent = new JSONEvent();    jsonEvent.setCharset("dummy");    jsonEvent.setBody("This is json event".getBytes());    jsonEvent.getBody();}
public static Collection<?> flume_f4737_0()
{    return Arrays.asList(new Object[][] {     { null, null, null }, { "sysprop", null, "sysprop" }, { "sysprop,sysprop", null, "sysprop sysprop" }, { null, "envvar", "envvar" }, { null, "envvar,envvar", "envvar envvar" }, { "sysprop", "envvar", "sysprop" }, { "sysprop,sysprop", "envvar,envvar", "sysprop sysprop" } });}
public static Collection<?> flume_f4738_0()
{    return Arrays.asList(new Object[][] {     { null, null, null }, { "sysprop", null, "sysprop" }, { null, "envvar", "envvar" }, { "sysprop", "envvar", "sysprop" } });}
public void flume_f4739_0()
{    setSysProp(getSysPropName(), sysPropValue);    setEnvVar(getEnvVarName(), envVarValue);}
public void flume_f4740_0()
{    setSysProp(getSysPropName(), null);    setEnvVar(getEnvVarName(), null);}
private static void flume_f4741_0(String name, String value)
{    if (value != null) {        System.setProperty(name, value);    } else {        System.clearProperty(name);    }}
private static void flume_f4742_0(String name, String value)
{    try {        injectEnvironmentVariable(name, value);    } catch (ReflectiveOperationException e) {        throw new AssertionError("Test setup  failed.", e);    }}
private static void flume_f4743_0(String key, String value) throws ReflectiveOperationException
{    Class<?> processEnvironment = Class.forName("java.lang.ProcessEnvironment");    Field unmodifiableMapField = getAccessibleField(processEnvironment, "theUnmodifiableEnvironment");    Object unmodifiableMap = unmodifiableMapField.get(null);    injectIntoUnmodifiableMap(key, value, unmodifiableMap);    Field mapField = getAccessibleField(processEnvironment, "theEnvironment");    Map<String, String> map = (Map<String, String>) mapField.get(null);    if (value != null) {        map.put(key, value);    } else {        map.remove(key);    }}
private static Field flume_f4744_0(Class<?> clazz, String fieldName) throws NoSuchFieldException
{    Field field = clazz.getDeclaredField(fieldName);    field.setAccessible(true);    return field;}
private static void flume_f4745_0(String key, String value, Object map) throws ReflectiveOperationException
{    Class unmodifiableMap = Class.forName("java.util.Collections$UnmodifiableMap");    Field field = getAccessibleField(unmodifiableMap, "m");    Object obj = field.get(map);    if (value != null) {        ((Map<String, String>) obj).put(key, value);    } else {        ((Map<String, String>) obj).remove(key);    }}
protected String flume_f4746_0()
{    return "flume.ssl.exclude.cipherSuites";}
protected String flume_f4747_0()
{    return "FLUME_SSL_EXCLUDE_CIPHERSUITES";}
public void flume_f4748_0()
{    SSLUtil.initGlobalSSLParameters();    String actualValue = SSLUtil.getGlobalExcludeCipherSuites();    Assert.assertEquals(expectedValue, actualValue);}
protected String flume_f4749_0()
{    return "flume.ssl.exclude.protocols";}
protected String flume_f4750_0()
{    return "FLUME_SSL_EXCLUDE_PROTOCOLS";}
public void flume_f4751_0()
{    SSLUtil.initGlobalSSLParameters();    String actualValue = SSLUtil.getGlobalExcludeProtocols();    Assert.assertEquals(expectedValue, actualValue);}
protected String flume_f4752_0()
{    return "flume.ssl.include.cipherSuites";}
protected String flume_f4753_0()
{    return "FLUME_SSL_INCLUDE_CIPHERSUITES";}
public void flume_f4754_0()
{    SSLUtil.initGlobalSSLParameters();    String actualValue = SSLUtil.getGlobalIncludeCipherSuites();    Assert.assertEquals(expectedValue, actualValue);}
protected String flume_f4755_0()
{    return "flume.ssl.include.protocols";}
protected String flume_f4756_0()
{    return "FLUME_SSL_INCLUDE_PROTOCOLS";}
public void flume_f4757_0()
{    SSLUtil.initGlobalSSLParameters();    String actualValue = SSLUtil.getGlobalIncludeProtocols();    Assert.assertEquals(expectedValue, actualValue);}
protected String flume_f4758_0()
{    return "javax.net.ssl.keyStorePassword";}
protected String flume_f4759_0()
{    return "FLUME_SSL_KEYSTORE_PASSWORD";}
public void flume_f4760_0()
{    SSLUtil.initGlobalSSLParameters();    String keystorePassword = SSLUtil.getGlobalKeystorePassword();    Assert.assertEquals(expectedValue, keystorePassword);}
protected String flume_f4761_0()
{    return "javax.net.ssl.keyStore";}
protected String flume_f4762_0()
{    return "FLUME_SSL_KEYSTORE_PATH";}
public void flume_f4763_0()
{    SSLUtil.initGlobalSSLParameters();    String keystorePath = SSLUtil.getGlobalKeystorePath();    Assert.assertEquals(expectedValue, keystorePath);}
protected String flume_f4764_0()
{    return "javax.net.ssl.keyStoreType";}
protected String flume_f4765_0()
{    return "FLUME_SSL_KEYSTORE_TYPE";}
public void flume_f4766_0()
{    SSLUtil.initGlobalSSLParameters();    String keystoreType = SSLUtil.getGlobalKeystoreType(null);    Assert.assertEquals(expectedValue, keystoreType);}
public static Collection<?> flume_f4767_0()
{    return Arrays.asList(new Object[][] {     { null, null, "default" }, { "sysprop", null, "sysprop" }, { null, "envvar", "envvar" }, { "sysprop", "envvar", "sysprop" } });}
protected String flume_f4768_0()
{    return "javax.net.ssl.keyStoreType";}
protected String flume_f4769_0()
{    return "FLUME_SSL_KEYSTORE_TYPE";}
public void flume_f4770_0()
{    SSLUtil.initGlobalSSLParameters();    String keystoreType = SSLUtil.getGlobalKeystoreType("default");    Assert.assertEquals(expectedValue, keystoreType);}
protected String flume_f4771_0()
{    return "javax.net.ssl.trustStorePassword";}
protected String flume_f4772_0()
{    return "FLUME_SSL_TRUSTSTORE_PASSWORD";}
public void flume_f4773_0()
{    SSLUtil.initGlobalSSLParameters();    String truststorePassword = SSLUtil.getGlobalTruststorePassword();    Assert.assertEquals(expectedValue, truststorePassword);}
protected String flume_f4774_0()
{    return "javax.net.ssl.trustStore";}
protected String flume_f4775_0()
{    return "FLUME_SSL_TRUSTSTORE_PATH";}
public void flume_f4776_0()
{    SSLUtil.initGlobalSSLParameters();    String truststorePath = SSLUtil.getGlobalTruststorePath();    Assert.assertEquals(expectedValue, truststorePath);}
protected String flume_f4777_0()
{    return "javax.net.ssl.trustStoreType";}
protected String flume_f4778_0()
{    return "FLUME_SSL_TRUSTSTORE_TYPE";}
public void flume_f4779_0()
{    SSLUtil.initGlobalSSLParameters();    String truststoreType = SSLUtil.getGlobalTruststoreType(null);    Assert.assertEquals(expectedValue, truststoreType);}
public static Collection<?> flume_f4780_0()
{    return Arrays.asList(new Object[][] {     { null, null, "default" }, { "sysprop", null, "sysprop" }, { null, "envvar", "envvar" }, { "sysprop", "envvar", "sysprop" } });}
protected String flume_f4781_0()
{    return "javax.net.ssl.trustStoreType";}
protected String flume_f4782_0()
{    return "FLUME_SSL_TRUSTSTORE_TYPE";}
public void flume_f4783_0()
{    SSLUtil.initGlobalSSLParameters();    String truststoreType = SSLUtil.getGlobalTruststoreType("default");    Assert.assertEquals(expectedValue, truststoreType);}
protected List<String> flume_f4784_0()
{    return Lists.newArrayList("avro", "parquet");}
public void flume_f4785_0(Context context)
{    this.context = context;    String principal = context.getString(AUTH_PRINCIPAL);    String keytab = context.getString(AUTH_KEYTAB);    String effectiveUser = context.getString(AUTH_PROXY_USER);    this.privilegedExecutor = FlumeAuthenticationUtil.getAuthenticator(principal, keytab).proxyAs(effectiveUser);        String datasetURI = context.getString(CONFIG_KITE_DATASET_URI);    if (datasetURI != null) {        this.datasetUri = URI.create(datasetURI);        this.datasetName = uriToName(datasetUri);    } else {        String repositoryURI = context.getString(CONFIG_KITE_REPO_URI);        Preconditions.checkNotNull(repositoryURI, "No dataset configured. Setting " + CONFIG_KITE_DATASET_URI + " is required.");        this.datasetName = context.getString(CONFIG_KITE_DATASET_NAME);        Preconditions.checkNotNull(datasetName, "No dataset configured. Setting " + CONFIG_KITE_DATASET_URI + " is required.");        String namespace = context.getString(CONFIG_KITE_DATASET_NAMESPACE, DEFAULT_NAMESPACE);        this.datasetUri = new URIBuilder(repositoryURI, namespace, datasetName).build();    }    this.setName(datasetUri.toString());    if (context.getBoolean(CONFIG_SYNCABLE_SYNC_ON_BATCH, DEFAULT_SYNCABLE_SYNC_ON_BATCH)) {        Preconditions.checkArgument(context.getBoolean(CONFIG_FLUSHABLE_COMMIT_ON_BATCH, DEFAULT_FLUSHABLE_COMMIT_ON_BATCH), "Configuration error: " + CONFIG_FLUSHABLE_COMMIT_ON_BATCH + " must be set to true when " + CONFIG_SYNCABLE_SYNC_ON_BATCH + " is set to true.");    }        this.failurePolicy = FAILURE_POLICY_FACTORY.newPolicy(context);        this.batchSize = context.getLong(CONFIG_KITE_BATCH_SIZE, DEFAULT_BATCH_SIZE);    this.rollIntervalSeconds = context.getInteger(CONFIG_KITE_ROLL_INTERVAL, DEFAULT_ROLL_INTERVAL);    this.counter = new SinkCounter(datasetName);}
public synchronized void flume_f4786_1()
{    this.lastRolledMillis = System.currentTimeMillis();    counter.start();            super.start();}
 void flume_f4787_0()
{    this.lastRolledMillis = 0L;}
 DatasetWriter<GenericRecord> flume_f4788_0()
{    return writer;}
 void flume_f4789_0(DatasetWriter<GenericRecord> writer)
{    this.writer = writer;}
 void flume_f4790_0(EntityParser<GenericRecord> parser)
{    this.parser = parser;}
 void flume_f4791_0(FailurePolicy failurePolicy)
{    this.failurePolicy = failurePolicy;}
public synchronized void flume_f4792_1()
{    counter.stop();    try {                        closeWriter();        commitTransaction();    } catch (EventDeliveryException ex) {        rollbackTransaction();                            }            super.stop();}
public Status flume_f4793_1() throws EventDeliveryException
{    long processedEvents = 0;    try {        if (shouldRoll()) {            closeWriter();            commitTransaction();            createWriter();        }                Preconditions.checkNotNull(writer, "Can't process events with a null writer. This is likely a bug.");        Channel channel = getChannel();                enterTransaction(channel);        for (; processedEvents < batchSize; processedEvents += 1) {            Event event = channel.take();            if (event == null) {                                break;            }            write(event);        }                if (commitOnBatch) {                        if (syncOnBatch && writer instanceof Syncable) {                ((Syncable) writer).sync();            } else if (writer instanceof Flushable) {                ((Flushable) writer).flush();            }            boolean committed = commitTransaction();            Preconditions.checkState(committed, "Tried to commit a batch when there was no transaction");            committedBatch |= committed;        }    } catch (Throwable th) {                        rollbackTransaction();        if (commitOnBatch && committedBatch) {            try {                closeWriter();            } catch (EventDeliveryException ex) {                                            }        } else {            this.writer = null;        }                Throwables.propagateIfInstanceOf(th, Error.class);        Throwables.propagateIfInstanceOf(th, EventDeliveryException.class);        throw new EventDeliveryException(th);    }    if (processedEvents == 0) {        counter.incrementBatchEmptyCount();        return Status.BACKOFF;    } else if (processedEvents < batchSize) {        counter.incrementBatchUnderflowCount();    } else {        counter.incrementBatchCompleteCount();    }    counter.addToEventDrainSuccessCount(processedEvents);    return Status.READY;}
 void flume_f4794_0(Event event) throws EventDeliveryException
{    try {        this.entity = parser.parse(event, reuseEntity ? entity : null);        this.bytesParsed += event.getBody().length;                                        writer.write(entity);    } catch (NonRecoverableEventException ex) {        failurePolicy.handle(event, ex);    } catch (DataFileWriter.AppendWriteException ex) {        failurePolicy.handle(event, ex);    } catch (RuntimeException ex) {        Throwables.propagateIfInstanceOf(ex, EventDeliveryException.class);        throw new EventDeliveryException(ex);    }}
 void flume_f4795_0() throws EventDeliveryException
{        committedBatch = false;    try {        View<GenericRecord> view;        view = privilegedExecutor.execute(new PrivilegedAction<Dataset<GenericRecord>>() {            @Override            public Dataset<GenericRecord> run() {                return Datasets.load(datasetUri);            }        });        DatasetDescriptor descriptor = view.getDataset().getDescriptor();        Format format = descriptor.getFormat();        Preconditions.checkArgument(allowedFormats().contains(format.getName()), "Unsupported format: " + format.getName());        Schema newSchema = descriptor.getSchema();        if (datasetSchema == null || !newSchema.equals(datasetSchema)) {            this.datasetSchema = descriptor.getSchema();                        parser = ENTITY_PARSER_FACTORY.newParser(datasetSchema, context);        }        this.reuseEntity = !(Formats.PARQUET.equals(format));                        this.commitOnBatch = context.getBoolean(CONFIG_FLUSHABLE_COMMIT_ON_BATCH, DEFAULT_FLUSHABLE_COMMIT_ON_BATCH) && (Formats.AVRO.equals(format));                        this.syncOnBatch = context.getBoolean(CONFIG_SYNCABLE_SYNC_ON_BATCH, DEFAULT_SYNCABLE_SYNC_ON_BATCH) && (Formats.AVRO.equals(format));        this.datasetName = view.getDataset().getName();        this.writer = view.newWriter();                this.lastRolledMillis = System.currentTimeMillis();        this.bytesParsed = 0L;    } catch (DatasetNotFoundException ex) {        throw new EventDeliveryException("Dataset " + datasetUri + " not found." + " The dataset must be created before Flume can write to it.", ex);    } catch (RuntimeException ex) {        throw new EventDeliveryException("Error trying to open a new" + " writer for dataset " + datasetUri, ex);    }}
public Dataset<GenericRecord> flume_f4796_0()
{    return Datasets.load(datasetUri);}
private boolean flume_f4797_1()
{    long currentTimeMillis = System.currentTimeMillis();    long elapsedTimeSeconds = TimeUnit.MILLISECONDS.toSeconds(currentTimeMillis - lastRolledMillis);        return elapsedTimeSeconds >= rollIntervalSeconds || writer == null;}
 void flume_f4798_1() throws EventDeliveryException
{    if (writer != null) {        try {            writer.close();            long elapsedTimeSeconds = TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis() - lastRolledMillis);                    } catch (DatasetIOException ex) {            throw new EventDeliveryException("Check HDFS permissions/health. IO" + " error trying to close the  writer for dataset " + datasetUri, ex);        } catch (RuntimeException ex) {            throw new EventDeliveryException("Error trying to close the  writer for" + " dataset " + datasetUri, ex);        } finally {                                                                        this.writer = null;            failurePolicy.close();        }    }}
private void flume_f4799_0(Channel channel) throws EventDeliveryException
{        if (transaction == null) {        this.transaction = channel.getTransaction();        transaction.begin();        failurePolicy = FAILURE_POLICY_FACTORY.newPolicy(context);    }}
 boolean flume_f4800_0() throws EventDeliveryException
{    if (transaction != null) {        failurePolicy.sync();        transaction.commit();        transaction.close();        this.transaction = null;        return true;    } else {        return false;    }}
private void flume_f4801_1()
{    if (transaction != null) {        try {                                    transaction.rollback();        } catch (RuntimeException ex) {                                } finally {            transaction.close();            this.transaction = null;        }    }}
private static String flume_f4802_0(URI uri)
{    return Registration.lookupDatasetUri(URI.create(uri.getRawSchemeSpecificPart())).second().get("dataset");}
public long flume_f4803_0()
{    return batchSize;}
public Schema flume_f4804_0(String literal)
{    Preconditions.checkNotNull(literal, "Schema literal cannot be null without a Schema URL");    return new Schema.Parser().parse(literal);}
public Schema flume_f4805_0(String url) throws IOException
{    Schema.Parser parser = new Schema.Parser();    InputStream is = null;    try {        FileSystem fs = FileSystem.get(URI.create(url), conf);        if (url.toLowerCase(Locale.ENGLISH).startsWith("hdfs:/")) {            is = fs.open(new Path(url));        } else {            is = new URL(url).openStream();        }        return parser.parse(is);    } finally {        if (is != null) {            is.close();        }    }}
public DatumReader<GenericRecord> flume_f4806_0(Schema schema)
{        return new GenericDatumReader<GenericRecord>(schema, datasetSchema);}
public GenericRecord flume_f4807_0(Event event, GenericRecord reuse) throws EventDeliveryException, NonRecoverableEventException
{    decoder = DecoderFactory.get().binaryDecoder(event.getBody(), decoder);    try {        DatumReader<GenericRecord> reader = readers.getUnchecked(schema(event));        return reader.read(reuse, decoder);    } catch (IOException ex) {        throw new NonRecoverableEventException("Cannot deserialize event", ex);    } catch (RuntimeException ex) {        throw new NonRecoverableEventException("Cannot deserialize event", ex);    }}
private static Schema flume_f4808_0(Event event) throws EventDeliveryException, NonRecoverableEventException
{    Map<String, String> headers = event.getHeaders();    String schemaURL = headers.get(AVRO_SCHEMA_URL_HEADER);    try {        if (schemaURL != null) {            return schemasFromURL.get(schemaURL);        } else {            String schemaLiteral = headers.get(AVRO_SCHEMA_LITERAL_HEADER);            if (schemaLiteral == null) {                throw new NonRecoverableEventException("No schema in event headers." + " Headers must include either " + AVRO_SCHEMA_URL_HEADER + " or " + AVRO_SCHEMA_LITERAL_HEADER);            }            return schemasFromLiteral.get(schemaLiteral);        }    } catch (ExecutionException ex) {        throw new EventDeliveryException("Cannot get schema", ex.getCause());    } catch (UncheckedExecutionException ex) {        throw new NonRecoverableEventException("Cannot parse schema", ex.getCause());    }}
public EntityParser<GenericRecord> flume_f4809_0(Schema datasetSchema, Context config)
{    return new AvroParser(datasetSchema);}
public EntityParser<GenericRecord> flume_f4810_0(Schema datasetSchema, Context config)
{    EntityParser<GenericRecord> parser;    String parserType = config.getString(CONFIG_ENTITY_PARSER, DEFAULT_ENTITY_PARSER);    if (parserType.equals(AVRO_ENTITY_PARSER)) {        parser = new AvroParser.Builder().build(datasetSchema, config);    } else {        Class<? extends EntityParser.Builder> builderClass;        Class c;        try {            c = Class.forName(parserType);        } catch (ClassNotFoundException ex) {            throw new IllegalArgumentException("EntityParser.Builder class " + parserType + " not found. Must set " + CONFIG_ENTITY_PARSER + " to a class that implements EntityParser.Builder or to a builtin" + " parser: " + Arrays.toString(AVAILABLE_PARSERS), ex);        }        if (c != null && EntityParser.Builder.class.isAssignableFrom(c)) {            builderClass = c;        } else {            throw new IllegalArgumentException("Class " + parserType + " does not" + " implement EntityParser.Builder. Must set " + CONFIG_ENTITY_PARSER + " to a class that extends" + " EntityParser.Builder or to a builtin parser: " + Arrays.toString(AVAILABLE_PARSERS));        }        EntityParser.Builder<GenericRecord> builder;        try {            builder = builderClass.newInstance();        } catch (InstantiationException ex) {            throw new IllegalArgumentException("Can't instantiate class " + parserType + ". Must set " + CONFIG_ENTITY_PARSER + " to a class" + " that extends EntityParser.Builder or to a builtin parser: " + Arrays.toString(AVAILABLE_PARSERS), ex);        } catch (IllegalAccessException ex) {            throw new IllegalArgumentException("Can't instantiate class " + parserType + ". Must set " + CONFIG_ENTITY_PARSER + " to a class" + " that extends EntityParser.Builder or to a builtin parser: " + Arrays.toString(AVAILABLE_PARSERS), ex);        }        parser = builder.build(datasetSchema, config);    }    return parser;}
public FailurePolicy flume_f4811_0(Context config)
{    FailurePolicy policy;    String policyType = config.getString(CONFIG_FAILURE_POLICY, DEFAULT_FAILURE_POLICY);    if (policyType.equals(RETRY_FAILURE_POLICY)) {        policy = new RetryPolicy.Builder().build(config);    } else if (policyType.equals(SAVE_FAILURE_POLICY)) {        policy = new SavePolicy.Builder().build(config);    } else {        Class<? extends FailurePolicy.Builder> builderClass;        Class c;        try {            c = Class.forName(policyType);        } catch (ClassNotFoundException ex) {            throw new IllegalArgumentException("FailurePolicy.Builder class " + policyType + " not found. Must set " + CONFIG_FAILURE_POLICY + " to a class that implements FailurePolicy.Builder or to a builtin" + " policy: " + Arrays.toString(AVAILABLE_POLICIES), ex);        }        if (c != null && FailurePolicy.Builder.class.isAssignableFrom(c)) {            builderClass = c;        } else {            throw new IllegalArgumentException("Class " + policyType + " does not" + " implement FailurePolicy.Builder. Must set " + CONFIG_FAILURE_POLICY + " to a class that extends" + " FailurePolicy.Builder or to a builtin policy: " + Arrays.toString(AVAILABLE_POLICIES));        }        FailurePolicy.Builder builder;        try {            builder = builderClass.newInstance();        } catch (InstantiationException ex) {            throw new IllegalArgumentException("Can't instantiate class " + policyType + ". Must set " + CONFIG_FAILURE_POLICY + " to a class" + " that extends FailurePolicy.Builder or to a builtin policy: " + Arrays.toString(AVAILABLE_POLICIES), ex);        } catch (IllegalAccessException ex) {            throw new IllegalArgumentException("Can't instantiate class " + policyType + ". Must set " + CONFIG_FAILURE_POLICY + " to a class" + " that extends FailurePolicy.Builder or to a builtin policy: " + Arrays.toString(AVAILABLE_POLICIES), ex);        }        policy = builder.build(config);    }    return policy;}
public void flume_f4812_1(Event event, Throwable cause) throws EventDeliveryException
{            throw new EventDeliveryException(cause);}
public void flume_f4813_0() throws EventDeliveryException
{}
public void flume_f4814_0() throws EventDeliveryException
{}
public FailurePolicy flume_f4815_0(Context config)
{    return new RetryPolicy();}
public void flume_f4816_0(Event event, Throwable cause) throws EventDeliveryException
{    try {        if (writer == null) {            writer = dataset.newWriter();        }        final AvroFlumeEvent avroEvent = new AvroFlumeEvent();        avroEvent.setBody(ByteBuffer.wrap(event.getBody()));        avroEvent.setHeaders(toCharSeqMap(event.getHeaders()));        writer.write(avroEvent);        nEventsHandled++;    } catch (RuntimeException ex) {        throw new EventDeliveryException(ex);    }}
public void flume_f4817_0() throws EventDeliveryException
{    if (nEventsHandled > 0) {        if (Formats.PARQUET.equals(dataset.getDataset().getDescriptor().getFormat())) {                                    close();        } else {            if (writer instanceof Syncable) {                ((Syncable) writer).sync();            }        }    }}
public void flume_f4818_0() throws EventDeliveryException
{    if (nEventsHandled > 0) {        try {            writer.close();        } catch (RuntimeException ex) {            throw new EventDeliveryException(ex);        } finally {            writer = null;            nEventsHandled = 0;        }    }}
private static Map<CharSequence, CharSequence> flume_f4819_0(Map<String, String> map)
{    return Maps.<CharSequence, CharSequence>newHashMap(map);}
public FailurePolicy flume_f4820_0(Context config)
{    return new SavePolicy(config);}
public static void flume_f4821_0() throws IOException
{    oldTestBuildDataProp = System.getProperty(TEST_BUILD_DATA_KEY);    System.setProperty(TEST_BUILD_DATA_KEY, DFS_DIR);    FileWriter schema = new FileWriter(SCHEMA_FILE);    schema.append(RECORD_SCHEMA.toString());    schema.close();}
public static void flume_f4822_0()
{    FileUtils.deleteQuietly(new File(DFS_DIR));    if (oldTestBuildDataProp != null) {        System.setProperty(TEST_BUILD_DATA_KEY, oldTestBuildDataProp);    }}
public void flume_f4823_0() throws EventDeliveryException
{    Datasets.delete(FILE_DATASET_URI);    Datasets.create(FILE_DATASET_URI, DESCRIPTOR);    this.config = new Context();    config.put("keep-alive", "0");    this.in = new MemoryChannel();    Configurables.configure(in, config);    config.put(DatasetSinkConstants.CONFIG_KITE_DATASET_URI, FILE_DATASET_URI);    GenericRecordBuilder builder = new GenericRecordBuilder(RECORD_SCHEMA);    expected = Lists.<GenericRecord>newArrayList(builder.set("id", "1").set("msg", "msg1").build(), builder.set("id", "2").set("msg", "msg2").build(), builder.set("id", "3").set("msg", "msg3").build());    putToChannel(in, Iterables.transform(expected, new Function<GenericRecord, Event>() {        private int i = 0;        @Override        public Event apply(@Nullable GenericRecord rec) {            this.i += 1;            boolean useURI = (i % 2) == 0;            return event(rec, RECORD_SCHEMA, SCHEMA_FILE, useURI);        }    }));}
public Event flume_f4824_0(@Nullable GenericRecord rec)
{    this.i += 1;    boolean useURI = (i % 2) == 0;    return event(rec, RECORD_SCHEMA, SCHEMA_FILE, useURI);}
public void flume_f4825_0()
{    Datasets.delete(FILE_DATASET_URI);}
public void flume_f4826_0() throws EventDeliveryException
{    config.put(DatasetSinkConstants.CONFIG_KITE_DATASET_URI, null);    config.put(DatasetSinkConstants.CONFIG_KITE_REPO_URI, FILE_REPO_URI);    config.put(DatasetSinkConstants.CONFIG_KITE_DATASET_NAME, DATASET_NAME);    DatasetSink sink = sink(in, config);        sink.start();    sink.process();    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));}
public void flume_f4827_0() throws EventDeliveryException
{        config.put(DatasetSinkConstants.CONFIG_KITE_REPO_URI, "bad uri");    config.put(DatasetSinkConstants.CONFIG_KITE_DATASET_NAME, "");    DatasetSink sink = sink(in, config);        sink.start();    sink.process();    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));}
public void flume_f4828_0() throws EventDeliveryException, NonRecoverableEventException, NonRecoverableEventException
{    DatasetSink sink = sink(in, config);        sink.start();    sink.process();    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));}
public void flume_f4829_0() throws EventDeliveryException
{    Datasets.delete(FILE_DATASET_URI);    Dataset<GenericRecord> created = Datasets.create(FILE_DATASET_URI, new DatasetDescriptor.Builder(DESCRIPTOR).format("parquet").build());    DatasetSink sink = sink(in, config);        sink.start();    sink.process();        assertThrows("Transaction should still be open", IllegalStateException.class, new Callable() {        @Override        public Object call() throws EventDeliveryException {            in.getTransaction().begin();            return null;        }    });        Assert.assertEquals("Should not have committed", 0, read(created).size());    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(created));    Assert.assertEquals("Should have committed", 0, remaining(in));}
public Object flume_f4830_0() throws EventDeliveryException
{    in.getTransaction().begin();    return null;}
public void flume_f4831_0() throws EventDeliveryException
{    URI partitionedUri = URI.create("dataset:file:target/test_repo/partitioned");    try {        Datasets.create(partitionedUri, new DatasetDescriptor.Builder(DESCRIPTOR).partitionStrategy(new PartitionStrategy.Builder().identity("id",         10).build()).build());        config.put(DatasetSinkConstants.CONFIG_KITE_DATASET_URI, partitionedUri.toString());        DatasetSink sink = sink(in, config);                sink.start();        sink.process();        sink.stop();        Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(partitionedUri)));        Assert.assertEquals("Should have committed", 0, remaining(in));    } finally {        if (Datasets.exists(partitionedUri)) {            Datasets.delete(partitionedUri);        }    }}
public void flume_f4832_0() throws EventDeliveryException
{        Datasets.delete(FILE_DATASET_URI);    DatasetSink sink = sink(in, config);        sink.start();        try {        sink.process();        Assert.fail("Should have thrown an exception: no such dataset");    } catch (EventDeliveryException e) {        }        Datasets.create(FILE_DATASET_URI, DESCRIPTOR);        sink.process();    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));}
public void flume_f4833_0() throws EventDeliveryException
{        GenericRecordBuilder updatedBuilder = new GenericRecordBuilder(UPDATED_SCHEMA);    GenericData.Record updatedRecord = updatedBuilder.set("id", "0").set("priority", 1).set("msg", "Priority 1 message!").build();        Set<GenericRecord> expectedAsUpdated = Sets.newHashSet();    for (GenericRecord record : expected) {        expectedAsUpdated.add(updatedBuilder.clear("priority").set("id", record.get("id")).set("msg", record.get("msg")).build());    }    expectedAsUpdated.add(updatedRecord);    DatasetSink sink = sink(in, config);        sink.start();    sink.process();        DatasetDescriptor updated = new DatasetDescriptor.Builder(Datasets.load(FILE_DATASET_URI).getDataset().getDescriptor()).schema(UPDATED_SCHEMA).build();    Datasets.update(FILE_DATASET_URI, updated);        sink.roll();        putToChannel(in, event(updatedRecord, UPDATED_SCHEMA, null, false));        sink.process();    sink.stop();    Assert.assertEquals(expectedAsUpdated, read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));}
public void flume_f4834_0() throws EventDeliveryException, IOException
{        MiniDFSCluster cluster = new MiniDFSCluster.Builder(new Configuration()).build();    FileSystem dfs = cluster.getFileSystem();    Configuration conf = dfs.getConf();    URI hdfsUri = URI.create("dataset:" + conf.get("fs.defaultFS") + "/tmp/repo" + DATASET_NAME);    try {                Datasets.create(hdfsUri, DESCRIPTOR);                config.put(DatasetSinkConstants.CONFIG_KITE_DATASET_URI, hdfsUri.toString());        DatasetSink sink = sink(in, config);                sink.start();        sink.process();        sink.stop();        Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(hdfsUri)));        Assert.assertEquals("Should have committed", 0, remaining(in));    } finally {        if (Datasets.exists(hdfsUri)) {            Datasets.delete(hdfsUri);        }        cluster.shutdown();    }}
public void flume_f4835_0() throws EventDeliveryException
{    DatasetSink sink = sink(in, config);        config.put("kite.batchSize", "2");    Configurables.configure(sink, config);    sink.start();        sink.process();        sink.roll();        sink.process();    Assert.assertEquals(Sets.newHashSet(expected.subList(0, 2)), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));        sink.roll();        sink.process();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    sink.stop();}
public void flume_f4836_0() throws EventDeliveryException, InterruptedException
{            config.put("kite.rollInterval", "1");    DatasetSink sink = sink(in, config);    Dataset<GenericRecord> records = Datasets.load(FILE_DATASET_URI);        sink.start();    sink.process();    Assert.assertEquals("Should have committed", 0, remaining(in));        Thread.sleep(1100);        sink.process();    Assert.assertEquals(Sets.newHashSet(expected), read(records));        sink.stop();}
public void flume_f4837_0() throws EventDeliveryException
{    DatasetSink sink = sink(in, config);        GenericRecordBuilder compatBuilder = new GenericRecordBuilder(COMPATIBLE_SCHEMA);    GenericData.Record compatibleRecord = compatBuilder.set("id", "0").build();        putToChannel(in, event(compatibleRecord, COMPATIBLE_SCHEMA, null, false));            GenericRecordBuilder builder = new GenericRecordBuilder(RECORD_SCHEMA);    GenericData.Record expectedRecord = builder.set("id", "0").build();    expected.add(expectedRecord);        sink.start();    sink.process();    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));}
public void flume_f4838_0() throws EventDeliveryException
{    final DatasetSink sink = sink(in, config);    GenericRecordBuilder builder = new GenericRecordBuilder(INCOMPATIBLE_SCHEMA);    GenericData.Record rec = builder.set("username", "koala").build();    putToChannel(in, event(rec, INCOMPATIBLE_SCHEMA, null, false));        sink.start();    assertThrows("Should fail", EventDeliveryException.class, new Callable() {        @Override        public Object call() throws EventDeliveryException {            sink.process();            return null;        }    });    sink.stop();    Assert.assertEquals("Should have rolled back", expected.size() + 1, remaining(in));}
public Object flume_f4839_0() throws EventDeliveryException
{    sink.process();    return null;}
public void flume_f4840_0() throws EventDeliveryException
{    final DatasetSink sink = sink(in, config);    Event badEvent = new SimpleEvent();    badEvent.setHeaders(Maps.<String, String>newHashMap());    badEvent.setBody(serialize(expected.get(0), RECORD_SCHEMA));    putToChannel(in, badEvent);        sink.start();    assertThrows("Should fail", EventDeliveryException.class, new Callable() {        @Override        public Object call() throws EventDeliveryException {            sink.process();            return null;        }    });    sink.stop();    Assert.assertEquals("Should have rolled back", expected.size() + 1, remaining(in));}
public Object flume_f4841_0() throws EventDeliveryException
{    sink.process();    return null;}
public void flume_f4842_0() throws EventDeliveryException
{    if (Datasets.exists(ERROR_DATASET_URI)) {        Datasets.delete(ERROR_DATASET_URI);    }    config.put(DatasetSinkConstants.CONFIG_FAILURE_POLICY, DatasetSinkConstants.SAVE_FAILURE_POLICY);    config.put(DatasetSinkConstants.CONFIG_KITE_ERROR_DATASET_URI, ERROR_DATASET_URI);    DatasetSink sink = sink(in, config);        sink.start();    sink.process();    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));}
public void flume_f4843_0() throws EventDeliveryException
{    if (Datasets.exists(ERROR_DATASET_URI)) {        Datasets.delete(ERROR_DATASET_URI);    }    config.put(DatasetSinkConstants.CONFIG_FAILURE_POLICY, DatasetSinkConstants.SAVE_FAILURE_POLICY);    config.put(DatasetSinkConstants.CONFIG_KITE_ERROR_DATASET_URI, ERROR_DATASET_URI);    final DatasetSink sink = sink(in, config);    Event badEvent = new SimpleEvent();    badEvent.setHeaders(Maps.<String, String>newHashMap());    badEvent.setBody(serialize(expected.get(0), RECORD_SCHEMA));    putToChannel(in, badEvent);        sink.start();    sink.process();    sink.stop();    Assert.assertEquals("Good records should have been written", Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should not have rolled back", 0, remaining(in));    Assert.assertEquals("Should have saved the bad event", Sets.newHashSet(AvroFlumeEvent.newBuilder().setBody(ByteBuffer.wrap(badEvent.getBody())).setHeaders(toUtf8Map(badEvent.getHeaders())).build()), read(Datasets.load(ERROR_DATASET_URI, AvroFlumeEvent.class)));}
public void flume_f4844_0() throws EventDeliveryException
{    if (Datasets.exists(ERROR_DATASET_URI)) {        Datasets.delete(ERROR_DATASET_URI);    }    config.put(DatasetSinkConstants.CONFIG_FAILURE_POLICY, DatasetSinkConstants.SAVE_FAILURE_POLICY);    config.put(DatasetSinkConstants.CONFIG_KITE_ERROR_DATASET_URI, ERROR_DATASET_URI);    final DatasetSink sink = sink(in, config);    GenericRecordBuilder builder = new GenericRecordBuilder(INCOMPATIBLE_SCHEMA);    GenericData.Record rec = builder.set("username", "koala").build();            Event badEvent = event(rec, INCOMPATIBLE_SCHEMA, SCHEMA_FILE, true);    putToChannel(in, badEvent);        sink.start();    sink.process();    sink.stop();    Assert.assertEquals("Good records should have been written", Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should not have rolled back", 0, remaining(in));    Assert.assertEquals("Should have saved the bad event", Sets.newHashSet(AvroFlumeEvent.newBuilder().setBody(ByteBuffer.wrap(badEvent.getBody())).setHeaders(toUtf8Map(badEvent.getHeaders())).build()), read(Datasets.load(ERROR_DATASET_URI, AvroFlumeEvent.class)));}
public void flume_f4845_0() throws EventDeliveryException
{    final DatasetSink sink = sink(in, config);    GenericRecordBuilder builder = new GenericRecordBuilder(INCOMPATIBLE_SCHEMA);    GenericData.Record rec = builder.set("username", "koala").build();            putToChannel(in, event(rec, INCOMPATIBLE_SCHEMA, SCHEMA_FILE, true));        sink.start();    assertThrows("Should fail", EventDeliveryException.class, new Callable() {        @Override        public Object call() throws EventDeliveryException {            sink.process();            return null;        }    });    sink.stop();    Assert.assertEquals("Should have rolled back", expected.size() + 1, remaining(in));}
public Object flume_f4846_0() throws EventDeliveryException
{    sink.process();    return null;}
public void flume_f4847_0() throws EventDeliveryException
{    DatasetSink sink = sink(in, config);        sink.start();    sink.process();        Assert.assertEquals("Should have committed", 0, remaining(in));        Assert.assertEquals(0, read(Datasets.load(FILE_DATASET_URI)).size());    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));}
public void flume_f4848_0() throws EventDeliveryException
{    config.put(DatasetSinkConstants.CONFIG_FLUSHABLE_COMMIT_ON_BATCH, Boolean.toString(false));    config.put(DatasetSinkConstants.CONFIG_SYNCABLE_SYNC_ON_BATCH, Boolean.toString(false));    DatasetSink sink = sink(in, config);        sink.start();    sink.process();        assertThrows("Transaction should still be open", IllegalStateException.class, new Callable() {        @Override        public Object call() throws EventDeliveryException {            in.getTransaction().begin();            return null;        }    });        Assert.assertEquals(0, read(Datasets.load(FILE_DATASET_URI)).size());    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));        Assert.assertEquals("Should have committed", 0, remaining(in));}
public Object flume_f4849_0() throws EventDeliveryException
{    in.getTransaction().begin();    return null;}
public void flume_f4850_0() throws EventDeliveryException
{    config.put(DatasetSinkConstants.CONFIG_FLUSHABLE_COMMIT_ON_BATCH, Boolean.toString(false));    config.put(DatasetSinkConstants.CONFIG_SYNCABLE_SYNC_ON_BATCH, Boolean.toString(true));    try {        sink(in, config);        Assert.fail("Should have thrown IllegalArgumentException");    } catch (IllegalArgumentException ex) {        }}
public void flume_f4851_0() throws EventDeliveryException
{    config.put(DatasetSinkConstants.CONFIG_FLUSHABLE_COMMIT_ON_BATCH, Boolean.toString(false));    config.put(DatasetSinkConstants.CONFIG_SYNCABLE_SYNC_ON_BATCH, Boolean.toString(false));    DatasetSink sink = sink(in, config);        sink.start();    sink.process();    sink.closeWriter();    sink.commitTransaction();    sink.createWriter();    Assert.assertNotNull("Writer should not be null", sink.getWriter());    Assert.assertEquals("Should have committed", 0, remaining(in));    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));}
public void flume_f4852_0() throws EventDeliveryException
{    config.put(DatasetSinkConstants.CONFIG_FLUSHABLE_COMMIT_ON_BATCH, Boolean.toString(false));    config.put(DatasetSinkConstants.CONFIG_SYNCABLE_SYNC_ON_BATCH, Boolean.toString(false));    DatasetSink sink = sink(in, config);        sink.start();    sink.process();    sink.closeWriter();    sink.commitTransaction();    Assert.assertNull("Writer should be null", sink.getWriter());    Assert.assertEquals("Should have committed", 0, remaining(in));    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));}
public void flume_f4853_0() throws EventDeliveryException
{    config.put(DatasetSinkConstants.CONFIG_FLUSHABLE_COMMIT_ON_BATCH, Boolean.toString(false));    config.put(DatasetSinkConstants.CONFIG_SYNCABLE_SYNC_ON_BATCH, Boolean.toString(false));    DatasetSink sink = sink(in, config);        sink.start();    sink.process();    sink.commitTransaction();    sink.createWriter();    Assert.assertNotNull("Writer should not be null", sink.getWriter());    Assert.assertEquals("Should have committed", 0, remaining(in));    sink.stop();    Assert.assertEquals(0, read(Datasets.load(FILE_DATASET_URI)).size());}
public void flume_f4854_0() throws EventDeliveryException, NonRecoverableEventException
{    DatasetSink sink = sink(in, config);        sink.start();    sink.process();        Event mockEvent = mock(Event.class);    when(mockEvent.getBody()).thenReturn(new byte[] { 0x01 });        GenericRecord mockRecord = mock(GenericRecord.class);        EntityParser<GenericRecord> mockParser = mock(EntityParser.class);    when(mockParser.parse(eq(mockEvent), any(GenericRecord.class))).thenReturn(mockRecord);    sink.setParser(mockParser);        FailurePolicy mockFailurePolicy = mock(FailurePolicy.class);    sink.setFailurePolicy(mockFailurePolicy);        DatasetWriter<GenericRecord> mockWriter = mock(DatasetWriter.class);    doThrow(new DataFileWriter.AppendWriteException(new IOException())).when(mockWriter).write(mockRecord);    sink.setWriter(mockWriter);    sink.write(mockEvent);        verify(mockFailurePolicy).handle(eq(mockEvent), any(Throwable.class));    sink.stop();}
public void flume_f4855_0() throws EventDeliveryException, NonRecoverableEventException
{    DatasetSink sink = sink(in, config);        sink.start();    sink.process();        Event mockEvent = mock(Event.class);    when(mockEvent.getBody()).thenReturn(new byte[] { 0x01 });        GenericRecord mockRecord = mock(GenericRecord.class);        EntityParser<GenericRecord> mockParser = mock(EntityParser.class);    when(mockParser.parse(eq(mockEvent), any(GenericRecord.class))).thenReturn(mockRecord);    sink.setParser(mockParser);        FailurePolicy mockFailurePolicy = mock(FailurePolicy.class);    sink.setFailurePolicy(mockFailurePolicy);        DatasetWriter<GenericRecord> mockWriter = mock(DatasetWriter.class);    doThrow(new RuntimeException()).when(mockWriter).write(mockRecord);    sink.setWriter(mockWriter);    try {        sink.write(mockEvent);        Assert.fail("Should throw EventDeliveryException");    } catch (EventDeliveryException ex) {    }        verify(mockFailurePolicy, never()).handle(eq(mockEvent), any(Throwable.class));    sink.stop();}
public void flume_f4856_0() throws EventDeliveryException, NonRecoverableEventException, NonRecoverableEventException
{    DatasetSink sink = sink(in, config);        sink.start();    sink.process();        sink.setWriter(null);        sink.process();    sink.stop();    Assert.assertEquals("Should have committed", 0, remaining(in));}
public static DatasetSink flume_f4857_0(Channel in, Context config)
{    DatasetSink sink = new DatasetSink();    sink.setChannel(in);    Configurables.configure(sink, config);    return sink;}
public static HashSet<T> flume_f4858_0(View<T> view)
{    DatasetReader<T> reader = null;    try {        reader = view.newReader();        return Sets.newHashSet(reader.iterator());    } finally {        if (reader != null) {            reader.close();        }    }}
public static int flume_f4859_0(Channel ch) throws EventDeliveryException
{    Transaction t = ch.getTransaction();    try {        t.begin();        int count = 0;        while (ch.take() != null) {            count += 1;        }        t.commit();        return count;    } catch (Throwable th) {        t.rollback();        Throwables.propagateIfInstanceOf(th, Error.class);        Throwables.propagateIfInstanceOf(th, EventDeliveryException.class);        throw new EventDeliveryException(th);    } finally {        t.close();    }}
public static void flume_f4860_0(Channel in, Event... records) throws EventDeliveryException
{    putToChannel(in, Arrays.asList(records));}
public static void flume_f4861_0(Channel in, Iterable<Event> records) throws EventDeliveryException
{    Transaction t = in.getTransaction();    try {        t.begin();        for (Event record : records) {            in.put(record);        }        t.commit();    } catch (Throwable th) {        t.rollback();        Throwables.propagateIfInstanceOf(th, Error.class);        Throwables.propagateIfInstanceOf(th, EventDeliveryException.class);        throw new EventDeliveryException(th);    } finally {        t.close();    }}
public static Event flume_f4862_0(Object datum, Schema schema, File file, boolean useURI)
{    Map<String, String> headers = Maps.newHashMap();    if (useURI) {        headers.put(DatasetSinkConstants.AVRO_SCHEMA_URL_HEADER, file.getAbsoluteFile().toURI().toString());    } else {        headers.put(DatasetSinkConstants.AVRO_SCHEMA_LITERAL_HEADER, schema.toString());    }    Event e = new SimpleEvent();    e.setBody(serialize(datum, schema));    e.setHeaders(headers);    return e;}
public static byte[] flume_f4863_0(Object datum, Schema schema)
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    Encoder encoder = EncoderFactory.get().binaryEncoder(out, null);    ReflectDatumWriter writer = new ReflectDatumWriter(schema);    try {        writer.write(datum, encoder);        encoder.flush();    } catch (IOException ex) {        Throwables.propagate(ex);    }    return out.toByteArray();}
public static void flume_f4864_0(String message, Class<? extends Exception> expected, Callable callable)
{    try {        callable.call();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        Assert.assertEquals(message, expected, actual.getClass());    }}
public static Map<CharSequence, CharSequence> flume_f4865_0(Map<String, String> map)
{    Map<CharSequence, CharSequence> utf8Map = Maps.newHashMap();    for (Map.Entry<String, String> entry : map.entrySet()) {        utf8Map.put(new Utf8(entry.getKey()), new Utf8(entry.getValue()));    }    return utf8Map;}
public void flume_f4866_1(Context context)
{    configuredMinReplicas = context.getInteger("hdfs.minBlockReplicas");    if (configuredMinReplicas != null) {        Preconditions.checkArgument(configuredMinReplicas >= 0, "hdfs.minBlockReplicas must be greater than or equal to 0");    }    numberOfCloseRetries = context.getInteger("hdfs.closeTries", 1) - 1;    if (numberOfCloseRetries > 1) {        try {                        timeBetweenCloseRetries = context.getLong("hdfs.callTimeout", 30000L);        } catch (NumberFormatException e) {                    }        timeBetweenCloseRetries = Math.max(timeBetweenCloseRetries / numberOfCloseRetries, 1000);    }}
public boolean flume_f4867_1()
{    try {        int numBlocks = getNumCurrentReplicas();        if (numBlocks == -1) {            return false;        }        int desiredBlocks;        if (configuredMinReplicas != null) {            desiredBlocks = configuredMinReplicas;        } else {            desiredBlocks = getFsDesiredReplication();        }        return numBlocks < desiredBlocks;    } catch (IllegalAccessException e) {            } catch (InvocationTargetException e) {            } catch (IllegalArgumentException e) {            }    return false;}
protected void flume_f4868_0(FSDataOutputStream outputStream, FileSystem fs, Path destPath)
{    Preconditions.checkNotNull(outputStream, "outputStream must not be null");    Preconditions.checkNotNull(fs, "fs must not be null");    Preconditions.checkNotNull(destPath, "destPath must not be null");    this.outputStream = outputStream;    this.fs = fs;    this.destPath = destPath;    this.refGetNumCurrentReplicas = reflectGetNumCurrentReplicas(outputStream);    this.refGetDefaultReplication = reflectGetDefaultReplication(fs);    this.refHflushOrSync = reflectHflushOrSync(outputStream);}
protected void flume_f4869_0()
{    this.outputStream = null;    this.fs = null;    this.destPath = null;    this.refGetNumCurrentReplicas = null;    this.refGetDefaultReplication = null;}
public int flume_f4870_1()
{    short replication = 0;    if (fs != null && destPath != null) {        if (refGetDefaultReplication != null) {            try {                replication = (Short) refGetDefaultReplication.invoke(fs, destPath);            } catch (IllegalAccessException e) {                            } catch (InvocationTargetException e) {                            }        } else {                        replication = fs.getDefaultReplication();        }    }    return replication;}
public int flume_f4871_0() throws IllegalArgumentException, IllegalAccessException, InvocationTargetException
{    if (refGetNumCurrentReplicas != null && outputStream != null) {        OutputStream dfsOutputStream = outputStream.getWrappedStream();        if (dfsOutputStream != null) {            Object repl = refGetNumCurrentReplicas.invoke(dfsOutputStream, NO_ARGS);            if (repl instanceof Integer) {                return ((Integer) repl).intValue();            }        }    }    return -1;}
private Method flume_f4874_1(FSDataOutputStream os)
{    Method m = null;    if (os != null) {        Class<?> fsDataOutputStreamClass = os.getClass();        try {            m = fsDataOutputStreamClass.getMethod("hflush");        } catch (NoSuchMethodException ex) {                        try {                m = fsDataOutputStreamClass.getMethod("sync");            } catch (Exception ex1) {                String msg = "Neither hflush not sync were found. That seems to be " + "a problem!";                                throw new FlumeException(msg, ex1);            }        }    }    return m;}
protected void flume_f4875_1(FSDataOutputStream os) throws IOException
{    try {                        this.refHflushOrSync.invoke(os);    } catch (InvocationTargetException e) {        String msg = "Error while trying to hflushOrSync!";                Throwable cause = e.getCause();        if (cause != null && cause instanceof IOException) {            throw (IOException) cause;        }        throw new FlumeException(msg, e);    } catch (Exception e) {        String msg = "Error while trying to hflushOrSync!";                throw new FlumeException(msg, e);    }}
public void flume_f4876_0(Context context)
{    syncIntervalBytes = context.getInteger(SYNC_INTERVAL_BYTES, DEFAULT_SYNC_INTERVAL_BYTES);    compressionCodec = context.getString(COMPRESSION_CODEC, DEFAULT_COMPRESSION_CODEC);    staticSchemaURL = context.getString(STATIC_SCHEMA_URL, DEFAULT_STATIC_SCHEMA_URL);}
public void flume_f4877_0() throws IOException
{}
public void flume_f4878_0() throws IOException
{        throw new UnsupportedOperationException("Avro API doesn't support append");}
public void flume_f4879_0(Event event) throws IOException
{    if (dataFileWriter == null) {        initialize(event);    }    dataFileWriter.appendEncoded(ByteBuffer.wrap(event.getBody()));}
private void flume_f4880_1(Event event) throws IOException
{    Schema schema = null;    String schemaUrl = event.getHeaders().get(AVRO_SCHEMA_URL_HEADER);    String schemaString = event.getHeaders().get(AVRO_SCHEMA_LITERAL_HEADER);    if (schemaUrl != null) {                schema = schemaCache.get(schemaUrl);        if (schema == null) {            schema = loadFromUrl(schemaUrl);            schemaCache.put(schemaUrl, schema);        }    } else if (schemaString != null) {                schema = new Schema.Parser().parse(schemaString);    } else if (staticSchemaURL != null) {                schema = schemaCache.get(staticSchemaURL);        if (schema == null) {            schema = loadFromUrl(staticSchemaURL);            schemaCache.put(staticSchemaURL, schema);        }    } else {                throw new FlumeException("Could not find schema for event " + event);    }    writer = new GenericDatumWriter<Object>(schema);    dataFileWriter = new DataFileWriter<Object>(writer);    dataFileWriter.setSyncInterval(syncIntervalBytes);    try {        CodecFactory codecFactory = CodecFactory.fromString(compressionCodec);        dataFileWriter.setCodec(codecFactory);    } catch (AvroRuntimeException e) {            }    dataFileWriter.create(schema, out);}
private Schema flume_f4881_0(String schemaUrl) throws IOException
{    Configuration conf = new Configuration();    Schema.Parser parser = new Schema.Parser();    if (schemaUrl.toLowerCase(Locale.ENGLISH).startsWith("hdfs://")) {        FileSystem fs = FileSystem.get(conf);        FSDataInputStream input = null;        try {            input = fs.open(new Path(schemaUrl));            return parser.parse(input);        } finally {            if (input != null) {                input.close();            }        }    } else {        InputStream is = null;        try {            is = new URL(schemaUrl).openStream();            return parser.parse(is);        } finally {            if (is != null) {                is.close();            }        }    }}
public void flume_f4882_0() throws IOException
{    dataFileWriter.flush();}
public void flume_f4883_0() throws IOException
{}
public boolean flume_f4884_0()
{    return false;}
public EventSerializer flume_f4885_0(Context context, OutputStream out)
{    AvroEventSerializer writer = new AvroEventSerializer(out);    writer.configure(context);    return writer;}
 void flume_f4886_0(FileSystem fs)
{    this.fileSystem = fs;    mockFsInjected = true;}
private void flume_f4887_0()
{    eventCounter = 0;    processSize = 0;    batchCounter = 0;}
private Method flume_f4888_1()
{    try {        return fileSystem.getClass().getMethod("isFileClosed", Path.class);    } catch (Exception e) {                return null;    }}
private Boolean flume_f4889_0(FileSystem fs, Path tmpFilePath) throws Exception
{    return (Boolean) (isClosedMethod.invoke(fs, tmpFilePath));}
private void flume_f4890_1() throws IOException, InterruptedException
{    if ((filePath == null) || (writer == null)) {        throw new IOException("Invalid file settings");    }    final Configuration config = new Configuration();        config.setBoolean("fs.automatic.close", false);        synchronized (staticLock) {        checkAndThrowInterruptedException();        try {            long counter = fileExtensionCounter.incrementAndGet();            String fullFileName = fileName + "." + counter;            if (fileSuffix != null && fileSuffix.length() > 0) {                fullFileName += fileSuffix;            } else if (codeC != null) {                fullFileName += codeC.getDefaultExtension();            }            bucketPath = filePath + "/" + inUsePrefix + fullFileName + inUseSuffix;            targetPath = filePath + "/" + fullFileName;                        callWithTimeout(new CallRunner<Void>() {                @Override                public Void call() throws Exception {                    if (codeC == null) {                                                if (!mockFsInjected) {                            fileSystem = new Path(bucketPath).getFileSystem(config);                        }                        writer.open(bucketPath);                    } else {                                                if (!mockFsInjected) {                            fileSystem = new Path(bucketPath).getFileSystem(config);                        }                        writer.open(bucketPath, codeC, compType);                    }                    return null;                }            });        } catch (Exception ex) {            sinkCounter.incrementConnectionFailedCount();            if (ex instanceof IOException) {                throw (IOException) ex;            } else {                throw Throwables.propagate(ex);            }        }    }    isClosedMethod = getRefIsClosed();    sinkCounter.incrementConnectionCreatedCount();    resetCounters();        if (rollInterval > 0) {        Callable<Void> action = new Callable<Void>() {            public Void call() throws Exception {                                try {                                        close(true);                } catch (Throwable t) {                                    }                return null;            }        };        timedRollFuture = timedRollerPool.schedule(action, rollInterval, TimeUnit.SECONDS);    }    isOpen = true;}
public Void flume_f4891_0() throws Exception
{    if (codeC == null) {                if (!mockFsInjected) {            fileSystem = new Path(bucketPath).getFileSystem(config);        }        writer.open(bucketPath);    } else {                if (!mockFsInjected) {            fileSystem = new Path(bucketPath).getFileSystem(config);        }        writer.open(bucketPath, codeC, compType);    }    return null;}
public Void flume_f4892_1() throws Exception
{        try {                close(true);    } catch (Throwable t) {            }    return null;}
public void flume_f4893_0() throws InterruptedException
{    close(false);}
private CallRunner<Void> flume_f4894_0()
{    return new CallRunner<Void>() {        @Override        public Void call() throws Exception {                        writer.close();            return null;        }    };}
public Void flume_f4895_0() throws Exception
{        writer.close();    return null;}
public Void flume_f4896_0() throws Exception
{    close(false);    return null;}
public void flume_f4897_1(boolean immediate)
{    closeTries++;    boolean shouldRetry = closeTries < maxRetries && !immediate;    try {        callWithTimeout(createCloseCallRunner());        sinkCounter.incrementConnectionClosedCount();    } catch (InterruptedException | IOException e) {                if (timedRollerPool != null && !timedRollerPool.isTerminated()) {            if (shouldRetry) {                timedRollerPool.schedule(this, retryInterval, TimeUnit.SECONDS);            }        } else {                    }        if (!shouldRetry) {                        sinkCounter.incrementConnectionFailedCount();            recoverLease();        }    }}
public Void flume_f4898_1() throws Exception
{    if (renameTries >= maxRetries) {                return null;    }    renameTries++;    try {        renameBucket(path, finalPath, fs);    } catch (Exception e) {                timedRollerPool.schedule(this, retryInterval, TimeUnit.SECONDS);        return null;    }    return null;}
private synchronized void flume_f4899_1()
{    if (bucketPath != null && fileSystem instanceof DistributedFileSystem) {        try {                        ((DistributedFileSystem) fileSystem).recoverLease(new Path(bucketPath));        } catch (IOException ex) {                    }    }}
public void flume_f4900_0(boolean callCloseCallback) throws InterruptedException
{    close(callCloseCallback, false);}
public void flume_f4901_1(boolean callCloseCallback, boolean immediate) throws InterruptedException
{    if (callCloseCallback) {        if (closed.compareAndSet(false, true)) {                        runCloseAction();        } else {                    }    }    doClose(immediate);}
private synchronized void flume_f4902_1(boolean immediate) throws InterruptedException
{    checkAndThrowInterruptedException();    try {        flush();    } catch (IOException e) {            }        if (isOpen) {        new CloseHandler().close(immediate);        isOpen = false;    } else {            }        if (timedRollFuture != null && !timedRollFuture.isDone()) {                timedRollFuture.cancel(false);        timedRollFuture = null;    }    if (idleFuture != null && !idleFuture.isDone()) {                idleFuture.cancel(false);        idleFuture = null;    }    if (bucketPath != null && fileSystem != null) {                try {            renameBucket(bucketPath, targetPath, fileSystem);        } catch (Exception e) {                        sinkCounter.incrementConnectionFailedCount();            final Callable<Void> scheduledRename = new ScheduledRenameCallable();            timedRollerPool.schedule(scheduledRename, retryInterval, TimeUnit.SECONDS);        }    }}
public synchronized void flume_f4903_1() throws IOException, InterruptedException
{    checkAndThrowInterruptedException();    if (!isBatchComplete()) {        doFlush();        if (idleTimeout > 0) {                        if (idleFuture == null || idleFuture.cancel(false)) {                Callable<Void> idleAction = new Callable<Void>() {                    public Void call() throws Exception {                                                if (isOpen) {                            close(true);                        }                        return null;                    }                };                idleFuture = timedRollerPool.schedule(idleAction, idleTimeout, TimeUnit.SECONDS);            }        }    }}
public Void flume_f4904_1() throws Exception
{        if (isOpen) {        close(true);    }    return null;}
private void flume_f4905_1()
{    try {        if (onCloseCallback != null) {            onCloseCallback.run(onCloseCallbackPath);        }    } catch (Throwable t) {            }}
private void flume_f4906_0() throws IOException, InterruptedException
{    callWithTimeout(new CallRunner<Void>() {        @Override        public Void call() throws Exception {                        writer.sync();            return null;        }    });    batchCounter = 0;}
public Void flume_f4907_0() throws Exception
{        writer.sync();    return null;}
public Void flume_f4909_0() throws Exception
{        writer.append(event);    return null;}
private boolean flume_f4910_1()
{    boolean doRotate = false;    if (writer.isUnderReplicated()) {        this.isUnderReplicated = true;        doRotate = true;    } else {        this.isUnderReplicated = false;    }    if ((rollCount > 0) && (rollCount <= eventCounter)) {                doRotate = true;    }    if ((rollSize > 0) && (rollSize <= processSize)) {                doRotate = true;    }    return doRotate;}
private void flume_f4911_1(String bucketPath, String targetPath, final FileSystem fs) throws IOException, InterruptedException
{    if (bucketPath.equals(targetPath)) {        return;    }    final Path srcPath = new Path(bucketPath);    final Path dstPath = new Path(targetPath);    callWithTimeout(new CallRunner<Void>() {        @Override        public Void call() throws Exception {            if (fs.exists(srcPath)) {                                                renameTries.incrementAndGet();                                fs.rename(srcPath, dstPath);            }            return null;        }    });}
public Void flume_f4912_1() throws Exception
{    if (fs.exists(srcPath)) {                        renameTries.incrementAndGet();                fs.rename(srcPath, dstPath);    }    return null;}
public String flume_f4913_0()
{    return "[ " + this.getClass().getSimpleName() + " targetPath = " + targetPath + ", bucketPath = " + bucketPath + " ]";}
private boolean flume_f4914_0()
{    return (batchCounter == 0);}
private static void flume_f4915_0() throws InterruptedException
{    if (Thread.currentThread().interrupted()) {        throw new InterruptedException("Timed out before HDFS call was made. " + "Your hdfs.callTimeout might be set too low or HDFS calls are " + "taking too long.");    }}
private T flume_f4916_1(final CallRunner<T> callRunner) throws IOException, InterruptedException
{    Future<T> future = callTimeoutPool.submit(new Callable<T>() {        @Override        public T call() throws Exception {            return proxyUser.execute(new PrivilegedExceptionAction<T>() {                @Override                public T run() throws Exception {                    return callRunner.call();                }            });        }    });    try {        if (callTimeout > 0) {            return future.get(callTimeout, TimeUnit.MILLISECONDS);        } else {            return future.get();        }    } catch (TimeoutException eT) {        future.cancel(true);        sinkCounter.incrementConnectionFailedCount();        throw new IOException("Callable timed out after " + callTimeout + " ms" + " on file: " + bucketPath, eT);    } catch (ExecutionException e1) {        sinkCounter.incrementConnectionFailedCount();        Throwable cause = e1.getCause();        if (cause instanceof IOException) {            throw (IOException) cause;        } else if (cause instanceof InterruptedException) {            throw (InterruptedException) cause;        } else if (cause instanceof RuntimeException) {            throw (RuntimeException) cause;        } else if (cause instanceof Error) {            throw (Error) cause;        } else {            throw new RuntimeException(e1);        }    } catch (CancellationException ce) {        throw new InterruptedException("Blocked callable interrupted by rotation event");    } catch (InterruptedException ex) {                throw ex;    }}
public T flume_f4917_0() throws Exception
{    return proxyUser.execute(new PrivilegedExceptionAction<T>() {        @Override        public T run() throws Exception {            return callRunner.call();        }    });}
public T flume_f4918_0() throws Exception
{    return callRunner.call();}
public void flume_f4919_1(Context context)
{    super.configure(context);    serializerType = context.getString("serializer", "TEXT");    useRawLocalFileSystem = context.getBoolean("hdfs.useRawLocalFileSystem", false);    serializerContext = new Context(context.getSubProperties(EventSerializer.CTX_PREFIX));    }
public void flume_f4920_0(String filePath) throws IOException
{    DefaultCodec defCodec = new DefaultCodec();    CompressionType cType = CompressionType.BLOCK;    open(filePath, defCodec, cType);}
public void flume_f4921_1(String filePath, CompressionCodec codec, CompressionType cType) throws IOException
{    Configuration conf = new Configuration();    Path dstPath = new Path(filePath);    FileSystem hdfs = dstPath.getFileSystem(conf);    if (useRawLocalFileSystem) {        if (hdfs instanceof LocalFileSystem) {            hdfs = ((LocalFileSystem) hdfs).getRaw();        } else {                    }    }    boolean appending = false;    if (conf.getBoolean("hdfs.append.support", false) == true && hdfs.isFile(dstPath)) {        fsOut = hdfs.append(dstPath);        appending = true;    } else {        fsOut = hdfs.create(dstPath);    }    if (compressor == null) {        compressor = CodecPool.getCompressor(codec, conf);    }    cmpOut = codec.createOutputStream(fsOut, compressor);    serializer = EventSerializerFactory.getInstance(serializerType, serializerContext, cmpOut);    if (appending && !serializer.supportsReopen()) {        cmpOut.close();        serializer = null;        throw new IOException("serializer (" + serializerType + ") does not support append");    }    registerCurrentStream(fsOut, hdfs, dstPath);    if (appending) {        serializer.afterReopen();    } else {        serializer.afterCreate();    }    isFinished = false;}
public void flume_f4922_0(Event e) throws IOException
{    if (isFinished) {        cmpOut.resetState();        isFinished = false;    }    serializer.write(e);}
public void flume_f4923_0() throws IOException
{                        serializer.flush();    if (!isFinished) {        cmpOut.finish();        isFinished = true;    }    fsOut.flush();    hflushOrSync(this.fsOut);}
public void flume_f4924_0() throws IOException
{    serializer.flush();    serializer.beforeClose();    if (!isFinished) {        cmpOut.finish();        isFinished = true;    }    fsOut.flush();    hflushOrSync(fsOut);    cmpOut.close();    if (compressor != null) {        CodecPool.returnCompressor(compressor);        compressor = null;    }    unregisterCurrentStream();}
public void flume_f4925_1(Context context)
{    super.configure(context);    serializerType = context.getString("serializer", "TEXT");    useRawLocalFileSystem = context.getBoolean("hdfs.useRawLocalFileSystem", false);    serializerContext = new Context(context.getSubProperties(EventSerializer.CTX_PREFIX));    }
protected FileSystem flume_f4926_0(Configuration conf, Path dstPath) throws IOException
{    return dstPath.getFileSystem(conf);}
protected void flume_f4927_1(Configuration conf, Path dstPath, FileSystem hdfs) throws IOException
{    if (useRawLocalFileSystem) {        if (hdfs instanceof LocalFileSystem) {            hdfs = ((LocalFileSystem) hdfs).getRaw();        } else {                    }    }    boolean appending = false;    if (conf.getBoolean("hdfs.append.support", false) == true && hdfs.isFile(dstPath)) {        outStream = hdfs.append(dstPath);        appending = true;    } else {        outStream = hdfs.create(dstPath);    }    serializer = EventSerializerFactory.getInstance(serializerType, serializerContext, outStream);    if (appending && !serializer.supportsReopen()) {        outStream.close();        serializer = null;        throw new IOException("serializer (" + serializerType + ") does not support append");    }        registerCurrentStream(outStream, hdfs, dstPath);    if (appending) {        serializer.afterReopen();    } else {        serializer.afterCreate();    }}
public void flume_f4928_0(String filePath) throws IOException
{    Configuration conf = new Configuration();    Path dstPath = new Path(filePath);    FileSystem hdfs = getDfs(conf, dstPath);    doOpen(conf, dstPath, hdfs);}
public void flume_f4929_0(String filePath, CompressionCodec codec, CompressionType cType) throws IOException
{    open(filePath);}
public void flume_f4930_0(Event e) throws IOException
{    serializer.write(e);}
public void flume_f4931_0() throws IOException
{    serializer.flush();    outStream.flush();    hflushOrSync(outStream);}
public void flume_f4932_0() throws IOException
{    serializer.flush();    serializer.beforeClose();    outStream.flush();    hflushOrSync(outStream);    outStream.close();    unregisterCurrentStream();}
protected boolean flume_f4933_1(Entry<String, BucketWriter> eldest)
{    if (size() > maxOpenFiles) {                try {            eldest.getValue().close();        } catch (InterruptedException e) {                        Thread.currentThread().interrupt();        }        return true;    } else {        return false;    }}
 Map<String, BucketWriter> flume_f4934_0()
{    return sfWriters;}
public void flume_f4935_1(Context context)
{    this.context = context;    filePath = Preconditions.checkNotNull(context.getString("hdfs.path"), "hdfs.path is required");    fileName = context.getString("hdfs.filePrefix", defaultFileName);    this.suffix = context.getString("hdfs.fileSuffix", defaultSuffix);    inUsePrefix = context.getString("hdfs.inUsePrefix", defaultInUsePrefix);    boolean emptyInUseSuffix = context.getBoolean("hdfs.emptyInUseSuffix", false);    if (emptyInUseSuffix) {        inUseSuffix = "";        String tmpInUseSuffix = context.getString(IN_USE_SUFFIX_PARAM_NAME);        if (tmpInUseSuffix != null) {                    }    } else {        inUseSuffix = context.getString(IN_USE_SUFFIX_PARAM_NAME, defaultInUseSuffix);    }    String tzName = context.getString("hdfs.timeZone");    timeZone = tzName == null ? null : TimeZone.getTimeZone(tzName);    rollInterval = context.getLong("hdfs.rollInterval", defaultRollInterval);    rollSize = context.getLong("hdfs.rollSize", defaultRollSize);    rollCount = context.getLong("hdfs.rollCount", defaultRollCount);    batchSize = context.getLong("hdfs.batchSize", defaultBatchSize);    idleTimeout = context.getInteger("hdfs.idleTimeout", 0);    String codecName = context.getString("hdfs.codeC");    fileType = context.getString("hdfs.fileType", defaultFileType);    maxOpenFiles = context.getInteger("hdfs.maxOpenFiles", defaultMaxOpenFiles);    callTimeout = context.getLong("hdfs.callTimeout", defaultCallTimeout);    threadsPoolSize = context.getInteger("hdfs.threadsPoolSize", defaultThreadPoolSize);    rollTimerPoolSize = context.getInteger("hdfs.rollTimerPoolSize", defaultRollTimerPoolSize);    String kerbConfPrincipal = context.getString("hdfs.kerberosPrincipal");    String kerbKeytab = context.getString("hdfs.kerberosKeytab");    String proxyUser = context.getString("hdfs.proxyUser");    tryCount = context.getInteger("hdfs.closeTries", defaultTryCount);    if (tryCount <= 0) {                tryCount = defaultTryCount;    }    retryInterval = context.getLong("hdfs.retryInterval", defaultRetryInterval);    if (retryInterval <= 0) {                tryCount = 1;    }    Preconditions.checkArgument(batchSize > 0, "batchSize must be greater than 0");    if (codecName == null) {        codeC = null;        compType = CompressionType.NONE;    } else {        codeC = getCodec(codecName);                compType = CompressionType.BLOCK;    }        if (fileType.equalsIgnoreCase(HDFSWriterFactory.DataStreamType) && codecName != null) {        throw new IllegalArgumentException("fileType: " + fileType + " which does NOT support compressed output. Please don't set codeC" + " or change the fileType if compressed output is desired.");    }    if (fileType.equalsIgnoreCase(HDFSWriterFactory.CompStreamType)) {        Preconditions.checkNotNull(codeC, "It's essential to set compress codec" + " when fileType is: " + fileType);    }        this.privExecutor = FlumeAuthenticationUtil.getAuthenticator(kerbConfPrincipal, kerbKeytab).proxyAs(proxyUser);    needRounding = context.getBoolean("hdfs.round", false);    if (needRounding) {        String unit = context.getString("hdfs.roundUnit", "second");        if (unit.equalsIgnoreCase("hour")) {            this.roundUnit = Calendar.HOUR_OF_DAY;        } else if (unit.equalsIgnoreCase("minute")) {            this.roundUnit = Calendar.MINUTE;        } else if (unit.equalsIgnoreCase("second")) {            this.roundUnit = Calendar.SECOND;        } else {                        needRounding = false;        }        this.roundValue = context.getInteger("hdfs.roundValue", 1);        if (roundUnit == Calendar.SECOND || roundUnit == Calendar.MINUTE) {            Preconditions.checkArgument(roundValue > 0 && roundValue <= 60, "Round value" + "must be > 0 and <= 60");        } else if (roundUnit == Calendar.HOUR_OF_DAY) {            Preconditions.checkArgument(roundValue > 0 && roundValue <= 24, "Round value" + "must be > 0 and <= 24");        }    }    this.useLocalTime = context.getBoolean("hdfs.useLocalTimeStamp", false);    if (useLocalTime) {        clock = new SystemClock();    }    if (sinkCounter == null) {        sinkCounter = new SinkCounter(getName());    }}
private static boolean flume_f4936_0(Class<? extends CompressionCodec> cls, String codecName)
{    String simpleName = cls.getSimpleName();    if (cls.getName().equals(codecName) || simpleName.equalsIgnoreCase(codecName)) {        return true;    }    if (simpleName.endsWith("Codec")) {        String prefix = simpleName.substring(0, simpleName.length() - "Codec".length());        if (prefix.equalsIgnoreCase(codecName)) {            return true;        }    }    return false;}
 static CompressionCodec flume_f4937_1(String codecName)
{    Configuration conf = new Configuration();    List<Class<? extends CompressionCodec>> codecs = CompressionCodecFactory.getCodecClasses(conf);            CompressionCodec codec = null;    ArrayList<String> codecStrs = new ArrayList<String>();    codecStrs.add("None");    for (Class<? extends CompressionCodec> cls : codecs) {        codecStrs.add(cls.getSimpleName());        if (codecMatches(cls, codecName)) {            try {                codec = cls.newInstance();            } catch (InstantiationException e) {                            } catch (IllegalAccessException e) {                            }        }    }    if (codec == null) {        if (!codecName.equalsIgnoreCase("None")) {            throw new IllegalArgumentException("Unsupported compression codec " + codecName + ".  Please choose from: " + codecStrs);        }    } else if (codec instanceof org.apache.hadoop.conf.Configurable) {                                ((org.apache.hadoop.conf.Configurable) codec).setConf(conf);    }    return codec;}
public Status flume_f4938_1() throws EventDeliveryException
{    Channel channel = getChannel();    Transaction transaction = channel.getTransaction();    transaction.begin();    try {        Set<BucketWriter> writers = new LinkedHashSet<>();        int txnEventCount = 0;        for (txnEventCount = 0; txnEventCount < batchSize; txnEventCount++) {            Event event = channel.take();            if (event == null) {                break;            }                        String realPath = BucketPath.escapeString(filePath, event.getHeaders(), timeZone, needRounding, roundUnit, roundValue, useLocalTime);            String realName = BucketPath.escapeString(fileName, event.getHeaders(), timeZone, needRounding, roundUnit, roundValue, useLocalTime);            String lookupPath = realPath + DIRECTORY_DELIMITER + realName;            BucketWriter bucketWriter;            HDFSWriter hdfsWriter = null;                                                WriterCallback closeCallback = new WriterCallback() {                @Override                public void run(String bucketPath) {                                        synchronized (sfWritersLock) {                        sfWriters.remove(bucketPath);                    }                }            };            synchronized (sfWritersLock) {                bucketWriter = sfWriters.get(lookupPath);                                if (bucketWriter == null) {                    hdfsWriter = writerFactory.getWriter(fileType);                    bucketWriter = initializeBucketWriter(realPath, realName, lookupPath, hdfsWriter, closeCallback);                    sfWriters.put(lookupPath, bucketWriter);                }            }                        try {                bucketWriter.append(event);            } catch (BucketClosedException ex) {                                hdfsWriter = writerFactory.getWriter(fileType);                bucketWriter = initializeBucketWriter(realPath, realName, lookupPath, hdfsWriter, closeCallback);                synchronized (sfWritersLock) {                    sfWriters.put(lookupPath, bucketWriter);                }                bucketWriter.append(event);            }                        if (!writers.contains(bucketWriter)) {                writers.add(bucketWriter);            }        }        if (txnEventCount == 0) {            sinkCounter.incrementBatchEmptyCount();        } else if (txnEventCount == batchSize) {            sinkCounter.incrementBatchCompleteCount();        } else {            sinkCounter.incrementBatchUnderflowCount();        }                for (BucketWriter bucketWriter : writers) {            bucketWriter.flush();        }        transaction.commit();        if (txnEventCount < 1) {            return Status.BACKOFF;        } else {            sinkCounter.addToEventDrainSuccessCount(txnEventCount);            return Status.READY;        }    } catch (IOException eIO) {        transaction.rollback();                sinkCounter.incrementEventWriteFail();        return Status.BACKOFF;    } catch (Throwable th) {        transaction.rollback();                sinkCounter.incrementEventWriteOrChannelFail(th);        if (th instanceof Error) {            throw (Error) th;        } else {            throw new EventDeliveryException(th);        }    } finally {        transaction.close();    }}
public void flume_f4939_1(String bucketPath)
{        synchronized (sfWritersLock) {        sfWriters.remove(bucketPath);    }}
 BucketWriter flume_f4940_0(String realPath, String realName, String lookupPath, HDFSWriter hdfsWriter, WriterCallback closeCallback)
{    HDFSWriter actualHdfsWriter = mockFs == null ? hdfsWriter : mockWriter;    BucketWriter bucketWriter = new BucketWriter(rollInterval, rollSize, rollCount, batchSize, context, realPath, realName, inUsePrefix, inUseSuffix, suffix, codeC, compType, actualHdfsWriter, timedRollerPool, privExecutor, sinkCounter, idleTimeout, closeCallback, lookupPath, callTimeout, callTimeoutPool, retryInterval, tryCount);    if (mockFs != null) {        bucketWriter.setFileSystem(mockFs);    }    return bucketWriter;}
public void flume_f4941_1()
{        synchronized (sfWritersLock) {        for (Entry<String, BucketWriter> entry : sfWriters.entrySet()) {                        try {                entry.getValue().close(false, true);            } catch (Exception ex) {                                if (ex instanceof InterruptedException) {                    Thread.currentThread().interrupt();                }            }        }    }        ExecutorService[] toShutdown = { callTimeoutPool, timedRollerPool };    for (ExecutorService execService : toShutdown) {        execService.shutdown();        try {            while (execService.isTerminated() == false) {                execService.awaitTermination(Math.max(defaultCallTimeout, callTimeout), TimeUnit.MILLISECONDS);            }        } catch (InterruptedException ex) {                    }    }    callTimeoutPool = null;    timedRollerPool = null;    synchronized (sfWritersLock) {        sfWriters.clear();        sfWriters = null;    }    sinkCounter.stop();    super.stop();}
public void flume_f4942_0()
{    String timeoutName = "hdfs-" + getName() + "-call-runner-%d";    callTimeoutPool = Executors.newFixedThreadPool(threadsPoolSize, new ThreadFactoryBuilder().setNameFormat(timeoutName).build());    String rollerName = "hdfs-" + getName() + "-roll-timer-%d";    timedRollerPool = Executors.newScheduledThreadPool(rollTimerPoolSize, new ThreadFactoryBuilder().setNameFormat(rollerName).build());    this.sfWriters = new WriterLinkedHashMap(maxOpenFiles);    sinkCounter.start();    super.start();}
public String flume_f4943_0()
{    return "{ Sink type:" + getClass().getSimpleName() + ", name:" + getName() + " }";}
 void flume_f4944_0(Clock clock)
{    BucketPath.setClock(clock);}
 void flume_f4945_0(FileSystem mockFs)
{    this.mockFs = mockFs;}
 void flume_f4946_0(HDFSWriter writer)
{    this.mockWriter = writer;}
 int flume_f4947_0()
{    return tryCount;}
public long flume_f4948_0()
{    return batchSize;}
public void flume_f4949_1(Context context)
{    super.configure(context);        writeFormat = context.getString("hdfs.writeFormat", SequenceFileSerializerType.Writable.name());    useRawLocalFileSystem = context.getBoolean("hdfs.useRawLocalFileSystem", false);    serializerContext = new Context(context.getSubProperties(SequenceFileSerializerFactory.CTX_PREFIX));    serializer = SequenceFileSerializerFactory.getSerializer(writeFormat, serializerContext);    }
public void flume_f4950_0(String filePath) throws IOException
{    open(filePath, null, CompressionType.NONE);}
public void flume_f4951_0(String filePath, CompressionCodec codeC, CompressionType compType) throws IOException
{    Configuration conf = new Configuration();    Path dstPath = new Path(filePath);    FileSystem hdfs = dstPath.getFileSystem(conf);    open(dstPath, codeC, compType, conf, hdfs);}
protected void flume_f4952_1(Path dstPath, CompressionCodec codeC, CompressionType compType, Configuration conf, FileSystem hdfs) throws IOException
{    if (useRawLocalFileSystem) {        if (hdfs instanceof LocalFileSystem) {            hdfs = ((LocalFileSystem) hdfs).getRaw();        } else {                    }    }    if (conf.getBoolean("hdfs.append.support", false) == true && hdfs.isFile(dstPath)) {        outStream = hdfs.append(dstPath);    } else {        outStream = hdfs.create(dstPath);    }    writer = SequenceFile.createWriter(conf, outStream, serializer.getKeyClass(), serializer.getValueClass(), compType, codeC);    registerCurrentStream(outStream, hdfs, dstPath);}
public void flume_f4953_0(Event e) throws IOException
{    for (SequenceFileSerializer.Record record : serializer.serialize(e)) {        writer.append(record.getKey(), record.getValue());    }}
public void flume_f4954_0() throws IOException
{    writer.sync();    hflushOrSync(outStream);}
public void flume_f4955_0() throws IOException
{    writer.close();    outStream.close();    unregisterCurrentStream();}
private Text flume_f4956_0(Event e)
{    Text textObject = new Text();    textObject.set(e.getBody(), 0, e.getBody().length);    return textObject;}
public Class<LongWritable> flume_f4957_0()
{    return LongWritable.class;}
public Class<Text> flume_f4958_0()
{    return Text.class;}
public Iterable<Record> flume_f4959_0(Event e)
{    Object key = getKey(e);    Object value = getValue(e);    return Collections.singletonList(new Record(key, value));}
private Object flume_f4960_0(Event e)
{        String timestamp = e.getHeaders().get("timestamp");    long eventStamp;    if (timestamp == null) {        eventStamp = System.currentTimeMillis();    } else {        eventStamp = Long.valueOf(timestamp);    }    return new LongWritable(eventStamp);}
private Object flume_f4961_0(Event e)
{    return makeText(e);}
public SequenceFileSerializer flume_f4962_0(Context context)
{    return new HDFSTextSerializer();}
private BytesWritable flume_f4963_0(Event e)
{    BytesWritable bytesObject = new BytesWritable();    bytesObject.set(e.getBody(), 0, e.getBody().length);    return bytesObject;}
public Class<LongWritable> flume_f4964_0()
{    return LongWritable.class;}
public Class<BytesWritable> flume_f4965_0()
{    return BytesWritable.class;}
public Iterable<Record> flume_f4966_0(Event e)
{    Object key = getKey(e);    Object value = getValue(e);    return Collections.singletonList(new Record(key, value));}
private Object flume_f4967_0(Event e)
{    String timestamp = e.getHeaders().get("timestamp");    long eventStamp;    if (timestamp == null) {        eventStamp = System.currentTimeMillis();    } else {        eventStamp = Long.valueOf(timestamp);    }    return new LongWritable(eventStamp);}
private Object flume_f4968_0(Event e)
{    return makeByteWritable(e);}
public SequenceFileSerializer flume_f4969_0(Context context)
{    return new HDFSWritableSerializer();}
public HDFSWriter flume_f4970_0(String fileType) throws IOException
{    if (fileType.equalsIgnoreCase(SequenceFileType)) {        return new HDFSSequenceFile();    } else if (fileType.equalsIgnoreCase(DataStreamType)) {        return new HDFSDataStream();    } else if (fileType.equalsIgnoreCase(CompStreamType)) {        return new HDFSCompressedDataStream();    } else {        throw new IOException("File type " + fileType + " not supported");    }}
public String flume_f4971_0()
{    return principal;}
public String flume_f4972_0()
{    return keyTab;}
public boolean flume_f4973_0(Object obj)
{    if (obj == null) {        return false;    }    if (getClass() != obj.getClass()) {        return false;    }    final KerberosUser other = (KerberosUser) obj;    if ((this.principal == null) ? (other.principal != null) : !this.principal.equals(other.principal)) {        return false;    }    if ((this.keyTab == null) ? (other.keyTab != null) : !this.keyTab.equals(other.keyTab)) {        return false;    }    return true;}
public int flume_f4974_0()
{    int hash = 7;    hash = 41 * hash + (this.principal != null ? this.principal.hashCode() : 0);    hash = 41 * hash + (this.keyTab != null ? this.keyTab.hashCode() : 0);    return hash;}
public String flume_f4975_0()
{    return "{ principal: " + principal + ", keytab: " + keyTab + " }";}
public Object flume_f4976_0()
{    return key;}
public Object flume_f4977_0()
{    return value;}
 static SequenceFileSerializer flume_f4978_1(String formatType, Context context)
{    Preconditions.checkNotNull(formatType, "serialize type must not be null");        SequenceFileSerializerType type;    try {        type = SequenceFileSerializerType.valueOf(formatType);    } catch (IllegalArgumentException e) {                type = SequenceFileSerializerType.Other;    }    Class<? extends SequenceFileSerializer.Builder> builderClass = type.getBuilderClass();        if (builderClass == null) {        try {            Class c = Class.forName(formatType);            if (c != null && SequenceFileSerializer.Builder.class.isAssignableFrom(c)) {                builderClass = (Class<? extends SequenceFileSerializer.Builder>) c;            } else {                                return null;            }        } catch (ClassNotFoundException ex) {                        return null;        } catch (ClassCastException ex) {                        return null;        }    }        SequenceFileSerializer.Builder builder;    try {        builder = builderClass.newInstance();    } catch (InstantiationException ex) {                return null;    } catch (IllegalAccessException ex) {                return null;    }    return builder.build(context);}
public Class<? extends SequenceFileSerializer.Builder> flume_f4979_0()
{    return builderClass;}
public void flume_f4980_0(Event e) throws IOException
{    if (e.getHeaders().containsKey("fault")) {        throw new IOException("Injected fault");    } else if (e.getHeaders().containsKey("slow")) {        long waitTime = Long.parseLong(e.getHeaders().get("slow"));        try {            Thread.sleep(waitTime);        } catch (InterruptedException eT) {            throw new IOException("append interrupted", eT);        }    }    super.append(e);}
public void flume_f4981_0(String filePath, CompressionCodec codeC, CompressionType compType) throws IOException
{    super.open(filePath, codeC, compType);    if (closed) {        opened = true;    }}
public void flume_f4982_0(Event e) throws IOException
{    if (e.getHeaders().containsKey("fault")) {        throw new IOException("Injected fault");    } else if (e.getHeaders().containsKey("fault-once")) {        e.getHeaders().remove("fault-once");        throw new IOException("Injected fault");    } else if (e.getHeaders().containsKey("fault-until-reopen")) {                if (openCount == 1) {            throw new IOException("Injected fault-until-reopen");        }    } else if (e.getHeaders().containsKey("slow")) {        long waitTime = Long.parseLong(e.getHeaders().get("slow"));        try {            Thread.sleep(waitTime);        } catch (InterruptedException eT) {            throw new IOException("append interrupted", eT);        }    }    super.append(e);}
public void flume_f4983_0() throws IOException
{    closed = true;    super.close();}
public HDFSWriter flume_f4984_0(String fileType) throws IOException
{    if (fileType == TestSequenceFileType) {        return new HDFSTestSeqWriter(openCount.incrementAndGet());    } else if (fileType == BadDataStreamType) {        return new HDFSBadDataStream();    } else {        throw new IOException("File type " + fileType + " not supported");    }}
protected FileSystem flume_f4985_0(Configuration conf, Path dstPath) throws IOException
{    return fs;}
public FSDataOutputStream flume_f4986_0(Path arg0, int arg1, Progressable arg2) throws IOException
{    latestOutputStream = new MockFsDataOutputStream(fs.append(arg0, arg1, arg2), closeSucceed);    return latestOutputStream;}
public FSDataOutputStream flume_f4987_0(Path arg0) throws IOException
{    latestOutputStream = new MockFsDataOutputStream(fs.create(arg0), closeSucceed);    return latestOutputStream;}
public FSDataOutputStream flume_f4988_0(Path arg0, FsPermission arg1, boolean arg2, int arg3, short arg4, long arg5, Progressable arg6) throws IOException
{    throw new IOException("Not a real file system");}
public boolean flume_f4989_0(Path arg0) throws IOException
{    return fs.delete(arg0);}
public boolean flume_f4990_0(Path arg0, boolean arg1) throws IOException
{    return fs.delete(arg0, arg1);}
public FileStatus flume_f4991_0(Path arg0) throws IOException
{    return fs.getFileStatus(arg0);}
public URI flume_f4992_0()
{    return fs.getUri();}
public Path flume_f4993_0()
{    return fs.getWorkingDirectory();}
public FileStatus[] flume_f4994_0(Path arg0) throws IOException
{    return fs.listStatus(arg0);}
public boolean flume_f4995_0(Path arg0, FsPermission arg1) throws IOException
{        return fs.mkdirs(arg0, arg1);}
public FSDataInputStream flume_f4996_0(Path arg0, int arg1) throws IOException
{    return fs.open(arg0, arg1);}
public boolean flume_f4997_1(Path arg0, Path arg1) throws IOException
{    currentRenameAttempts++;        if (currentRenameAttempts >= numberOfRetriesRequired || numberOfRetriesRequired == 0) {                return fs.rename(arg0, arg1);    } else {        throw new IOException("MockIOException");    }}
public void flume_f4998_0(Path arg0)
{    fs.setWorkingDirectory(arg0);}
public void flume_f4999_1() throws IOException
{        if (closeSucceed) {                super.close();    } else {        throw new IOException("MockIOException");    }}
public int flume_f5000_0()
{    return filesOpened;}
public int flume_f5001_0()
{    return filesClosed;}
public int flume_f5002_0()
{    return bytesWritten;}
public int flume_f5003_0()
{    return eventsWritten;}
public String flume_f5004_0()
{    return filePath;}
public void flume_f5005_0(Context context)
{}
public void flume_f5006_0(String filePath) throws IOException
{    this.filePath = filePath;    filesOpened++;}
public void flume_f5007_0(String filePath, CompressionCodec codec, CompressionType cType) throws IOException
{    this.filePath = filePath;    filesOpened++;}
public void flume_f5008_0(Event e) throws IOException
{    eventsWritten++;    bytesWritten += e.getBody().length;}
public void flume_f5009_0() throws IOException
{}
public void flume_f5010_1() throws IOException
{    filesClosed++;    int curr = currentCloseAttempts.incrementAndGet();        if (curr >= numberOfRetriesRequired || numberOfRetriesRequired == 0) {            } else {        throw new IOException("MockIOException");    }}
public boolean flume_f5011_0()
{    return false;}
public Class<LongWritable> flume_f5012_0()
{    return LongWritable.class;}
public Class<BytesWritable> flume_f5013_0()
{    return BytesWritable.class;}
public Iterable<Record> flume_f5014_0(Event e)
{    return Arrays.asList(new Record(new LongWritable(1234L), new BytesWritable(new byte[10])), new Record(new LongWritable(4567L), new BytesWritable(new byte[20])));}
public SequenceFileSerializer flume_f5015_0(Context context)
{    return new MyCustomSerializer();}
public void flume_f5016_0() throws Exception
{    file = File.createTempFile(getClass().getSimpleName(), "");}
public void flume_f5017_0() throws Exception
{    file.delete();}
public void flume_f5018_0() throws IOException
{    createAvroFile(file, null, false, false);    validateAvroFile(file);}
public void flume_f5019_0() throws IOException
{    createAvroFile(file, "null", false, false);    validateAvroFile(file);}
public void flume_f5020_0() throws IOException
{    createAvroFile(file, "deflate", false, false);    validateAvroFile(file);}
public void flume_f5021_0() throws IOException
{    createAvroFile(file, "snappy", false, false);    validateAvroFile(file);}
public void flume_f5022_0() throws IOException
{    createAvroFile(file, null, true, false);    validateAvroFile(file);}
public void flume_f5023_0() throws IOException
{    createAvroFile(file, null, false, true);    validateAvroFile(file);}
public void flume_f5024_0() throws IOException
{    createAvroFile(file, null, true, true);    validateAvroFile(file);}
public void flume_f5025_0(File file, String codec, boolean useSchemaUrl, boolean useStaticSchemaUrl) throws IOException
{        OutputStream out = new FileOutputStream(file);    Context ctx = new Context();    if (codec != null) {        ctx.put("compressionCodec", codec);    }    Schema schema = Schema.createRecord("myrecord", null, null, false);    schema.setFields(Arrays.asList(new Schema.Field[] { new Schema.Field("message", Schema.create(Schema.Type.STRING), null, null) }));    GenericRecordBuilder recordBuilder = new GenericRecordBuilder(schema);    File schemaFile = null;    if (useSchemaUrl || useStaticSchemaUrl) {        schemaFile = File.createTempFile(getClass().getSimpleName(), ".avsc");        Files.write(schema.toString(), schemaFile, Charsets.UTF_8);    }    if (useStaticSchemaUrl) {        ctx.put(AvroEventSerializerConfigurationConstants.STATIC_SCHEMA_URL, schemaFile.toURI().toURL().toExternalForm());    }    EventSerializer.Builder builder = new AvroEventSerializer.Builder();    EventSerializer serializer = builder.build(ctx, out);    serializer.afterCreate();    for (int i = 0; i < 3; i++) {        GenericRecord record = recordBuilder.set("message", "Hello " + i).build();        Event event = EventBuilder.withBody(serializeAvro(record, schema));        if (schemaFile == null && !useSchemaUrl) {            event.getHeaders().put(AvroEventSerializer.AVRO_SCHEMA_LITERAL_HEADER, schema.toString());        } else if (useSchemaUrl) {            event.getHeaders().put(AvroEventSerializer.AVRO_SCHEMA_URL_HEADER, schemaFile.toURI().toURL().toExternalForm());        }        serializer.write(event);    }    serializer.flush();    serializer.beforeClose();    out.flush();    out.close();    if (schemaFile != null) {        schemaFile.delete();    }}
private byte[] flume_f5026_0(Object datum, Schema schema) throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    ReflectDatumWriter<Object> writer = new ReflectDatumWriter<Object>(schema);    BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(out, null);    out.reset();    writer.write(datum, encoder);    encoder.flush();    return out.toByteArray();}
public void flume_f5027_0(File file) throws IOException
{        DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>();    DataFileReader<GenericRecord> fileReader = new DataFileReader<GenericRecord>(file, reader);    GenericRecord record = new GenericData.Record(fileReader.getSchema());    int numEvents = 0;    while (fileReader.hasNext()) {        fileReader.next(record);        String bodyStr = record.get("message").toString();        System.out.println(bodyStr);        numEvents++;    }    fileReader.close();    Assert.assertEquals("Should have found a total of 3 events", 3, numEvents);}
public static void flume_f5028_0()
{    timedRollerPool = Executors.newSingleThreadScheduledExecutor();    proxy = FlumeAuthenticationUtil.getAuthenticator(null, null).proxyAs(null);}
public static void flume_f5029_0() throws InterruptedException
{    timedRollerPool.shutdown();    timedRollerPool.awaitTermination(2, TimeUnit.SECONDS);    timedRollerPool.shutdownNow();}
public void flume_f5030_1() throws IOException, InterruptedException
{    int maxEvents = 100;    MockHDFSWriter hdfsWriter = new MockHDFSWriter();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollCount(maxEvents).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    for (int i = 0; i < 1000; i++) {        bucketWriter.append(e);    }                Assert.assertEquals("events written", 1000, hdfsWriter.getEventsWritten());    Assert.assertEquals("bytes written", 3000, hdfsWriter.getBytesWritten());    Assert.assertEquals("files opened", 10, hdfsWriter.getFilesOpened());}
public void flume_f5031_1() throws IOException, InterruptedException
{    int maxBytes = 300;    MockHDFSWriter hdfsWriter = new MockHDFSWriter();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollSize(maxBytes).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    for (int i = 0; i < 1000; i++) {        bucketWriter.append(e);    }                Assert.assertEquals("events written", 1000, hdfsWriter.getEventsWritten());    Assert.assertEquals("bytes written", 3000, hdfsWriter.getBytesWritten());    Assert.assertEquals("files opened", 10, hdfsWriter.getFilesOpened());}
public void flume_f5032_1() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1;    final int NUM_EVENTS = 10;    final AtomicBoolean calledBack = new AtomicBoolean(false);    MockHDFSWriter hdfsWriter = new MockHDFSWriter();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).setOnCloseCallback(new HDFSEventSink.WriterCallback() {        @Override        public void run(String filePath) {            calledBack.set(true);        }    }).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    long startNanos = System.nanoTime();    for (int i = 0; i < NUM_EVENTS - 1; i++) {        bucketWriter.append(e);    }        Thread.sleep(2 * ROLL_INTERVAL * 1000L);    Assert.assertTrue(bucketWriter.closed.get());    Assert.assertTrue(calledBack.get());    bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).build();        bucketWriter.append(e);    long elapsedMillis = TimeUnit.MILLISECONDS.convert(System.nanoTime() - startNanos, TimeUnit.NANOSECONDS);    long elapsedSeconds = elapsedMillis / 1000L;                        Assert.assertEquals("events written", NUM_EVENTS, hdfsWriter.getEventsWritten());    Assert.assertEquals("bytes written", e.getBody().length * NUM_EVENTS, hdfsWriter.getBytesWritten());    Assert.assertEquals("files opened", 2, hdfsWriter.getFilesOpened());        Assert.assertEquals("files closed", 1, hdfsWriter.getFilesClosed());        Thread.sleep(2 * ROLL_INTERVAL * 1000L);        Assert.assertEquals("files closed", 2, hdfsWriter.getFilesClosed());}
public void flume_f5033_0(String filePath)
{    calledBack.set(true);}
public void flume_f5034_0() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1;    final int NUM_EVENTS = 10;    HDFSWriter hdfsWriter = new HDFSWriter() {        private volatile boolean open = false;        public void configure(Context context) {        }        public void sync() throws IOException {            if (!open) {                throw new IOException("closed");            }        }        public void open(String filePath, CompressionCodec codec, CompressionType cType) throws IOException {            open = true;        }        public void open(String filePath) throws IOException {            open = true;        }        public void close() throws IOException {            open = false;        }        @Override        public boolean isUnderReplicated() {            return false;        }        public void append(Event e) throws IOException {                        open = true;        }    };    File tmpFile = File.createTempFile("flume", "test");    tmpFile.deleteOnExit();    String path = tmpFile.getParent();    String name = tmpFile.getName();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).setFilePath(path).setFileName(name).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    for (int i = 0; i < NUM_EVENTS - 1; i++) {        bucketWriter.append(e);    }        Thread.sleep(2 * ROLL_INTERVAL * 1000L);        bucketWriter.flush();}
public void flume_f5035_0(Context context)
{}
public void flume_f5036_0() throws IOException
{    if (!open) {        throw new IOException("closed");    }}
public void flume_f5037_0(String filePath, CompressionCodec codec, CompressionType cType) throws IOException
{    open = true;}
public void flume_f5038_0(String filePath) throws IOException
{    open = true;}
public void flume_f5039_0() throws IOException
{    open = false;}
public boolean flume_f5040_0()
{    return false;}
public void flume_f5041_0(Event e) throws IOException
{        open = true;}
public void flume_f5042_0() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1000;    final String suffix = null;        final long testTime = System.currentTimeMillis();    Clock testClock = new Clock() {        public long currentTimeMillis() {            return testTime;        }    };    MockHDFSWriter hdfsWriter = new MockHDFSWriter();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).setFileSuffix(suffix).setClock(testClock).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    bucketWriter.append(e);    Assert.assertTrue("Incorrect suffix", hdfsWriter.getOpenedFilePath().endsWith(Long.toString(testTime + 1) + ".tmp"));}
public long flume_f5043_0()
{    return testTime;}
public void flume_f5044_0() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1000;    final String suffix = ".avro";        final long testTime = System.currentTimeMillis();    Clock testClock = new Clock() {        public long currentTimeMillis() {            return testTime;        }    };    MockHDFSWriter hdfsWriter = new MockHDFSWriter();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).setFileSuffix(suffix).setClock(testClock).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    bucketWriter.append(e);    Assert.assertTrue("Incorrect suffix", hdfsWriter.getOpenedFilePath().endsWith(Long.toString(testTime + 1) + suffix + ".tmp"));}
public long flume_f5045_0()
{    return testTime;}
public void flume_f5046_0() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1000;    final String suffix = ".foo";    MockHDFSWriter hdfsWriter = new MockHDFSWriter();        final long testTime = System.currentTimeMillis();    Clock testClock = new Clock() {        public long currentTimeMillis() {            return testTime;        }    };    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).setFileSuffix(suffix).setCodeC(HDFSEventSink.getCodec("gzip")).setCompType(SequenceFile.CompressionType.BLOCK).setClock(testClock).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    bucketWriter.append(e);    Assert.assertTrue("Incorrect suffix", hdfsWriter.getOpenedFilePath().endsWith(Long.toString(testTime + 1) + suffix + ".tmp"));}
public long flume_f5047_0()
{    return testTime;}
public void flume_f5048_0() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1000;    final String PREFIX = "BRNO_IS_CITY_IN_CZECH_REPUBLIC";    MockHDFSWriter hdfsWriter = new MockHDFSWriter();    HDFSTextSerializer formatter = new HDFSTextSerializer();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).setInUsePrefix(PREFIX).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    bucketWriter.append(e);    Assert.assertTrue("Incorrect in use prefix", hdfsWriter.getOpenedFilePath().contains(PREFIX));}
public void flume_f5049_0() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1000;    final String SUFFIX = "WELCOME_TO_THE_HELLMOUNTH";    MockHDFSWriter hdfsWriter = new MockHDFSWriter();    HDFSTextSerializer serializer = new HDFSTextSerializer();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).setInUseSuffix(SUFFIX).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    bucketWriter.append(e);    Assert.assertTrue("Incorrect in use suffix", hdfsWriter.getOpenedFilePath().contains(SUFFIX));}
public void flume_f5050_0() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1000;    final String SUFFIX = "WELCOME_TO_THE_EREBOR";    final AtomicBoolean callbackCalled = new AtomicBoolean(false);    BucketWriter bucketWriter = new BucketWriterBuilder().setRollInterval(ROLL_INTERVAL).setInUseSuffix(SUFFIX).setOnCloseCallback(new HDFSEventSink.WriterCallback() {        @Override        public void run(String filePath) {            callbackCalled.set(true);        }    }).setOnCloseCallbackPath("blah").build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    bucketWriter.append(e);    bucketWriter.close(true);    Assert.assertTrue(callbackCalled.get());}
public void flume_f5051_0(String filePath)
{    callbackCalled.set(true);}
public void flume_f5052_0() throws Exception
{    sequenceFileRenameRetryCoreTest(1, true);    sequenceFileRenameRetryCoreTest(5, true);    sequenceFileRenameRetryCoreTest(2, true);    sequenceFileRenameRetryCoreTest(1, false);    sequenceFileRenameRetryCoreTest(5, false);    sequenceFileRenameRetryCoreTest(2, false);}
public void flume_f5053_0() throws Exception
{    sequenceFileCloseRetryCoreTest(5);    sequenceFileCloseRetryCoreTest(1);}
public void flume_f5054_0(int numberOfRetriesRequired, boolean closeSucceed) throws Exception
{    String hdfsPath = "file:///tmp/flume-test." + Calendar.getInstance().getTimeInMillis() + "." + Thread.currentThread().getId();    Context context = new Context();    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(hdfsPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    context.put("hdfs.path", hdfsPath);    context.put("hdfs.closeTries", String.valueOf(numberOfRetriesRequired));    context.put("hdfs.rollCount", "1");    context.put("hdfs.retryInterval", "1");    context.put("hdfs.callTimeout", Long.toString(1000));    MockFileSystem mockFs = new MockFileSystem(fs, numberOfRetriesRequired, closeSucceed);    MockDataStream writer = new MockDataStream(mockFs);    BucketWriter bucketWriter = new BucketWriterBuilder(writer).setRollCount(1).setBatchSize(1).setFilePath(hdfsPath).setFileName(hdfsPath).setInUsePrefix("singleBucket").setCompType(null).setRetryInterval(1).setMaxCloseTries(numberOfRetriesRequired).setWriter(writer).build();    bucketWriter.setFileSystem(mockFs);            Event event = EventBuilder.withBody("test", Charsets.UTF_8);    bucketWriter.append(event);        bucketWriter.append(event);    TimeUnit.SECONDS.sleep(numberOfRetriesRequired + 2);    Assert.assertTrue("Expected " + numberOfRetriesRequired + " " + "but got " + bucketWriter.renameTries.get(), bucketWriter.renameTries.get() == numberOfRetriesRequired);}
private void flume_f5055_0(int numberOfRetriesRequired) throws Exception
{    String hdfsPath = "file:///tmp/flume-test." + Calendar.getInstance().getTimeInMillis() + "." + Thread.currentThread().getId();    Context context = new Context();    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(hdfsPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    context.put("hdfs.path", hdfsPath);    context.put("hdfs.closeTries", String.valueOf(numberOfRetriesRequired));    context.put("hdfs.rollCount", "1");    context.put("hdfs.retryInterval", "1");    context.put("hdfs.callTimeout", Long.toString(1000));    MockHDFSWriter mockHDFSWriter = new MockHDFSWriter(Integer.MAX_VALUE);    ExecutorService executorService = Executors.newSingleThreadExecutor();    BucketWriter bucketWriter = new BucketWriter(0, 0, 1, 1, ctx, hdfsPath, hdfsPath, "singleBucket", ".tmp", null, null, null, mockHDFSWriter, timedRollerPool, proxy, new SinkCounter("test-bucket-writer-" + System.currentTimeMillis()), 0, null, null, 30000, executorService, 1, numberOfRetriesRequired);    Event event = EventBuilder.withBody("test", Charsets.UTF_8);    bucketWriter.append(event);    bucketWriter.close(false);    TimeUnit.SECONDS.sleep(numberOfRetriesRequired + 2);    Assert.assertEquals("ExcceutorService should be empty", executorService.shutdownNow().size(), 0);    Assert.assertEquals("Expected " + numberOfRetriesRequired + " " + "but got " + mockHDFSWriter.currentCloseAttempts, mockHDFSWriter.currentCloseAttempts.get(), numberOfRetriesRequired);}
public void flume_f5056_1() throws IOException, InterruptedException
{    MockHDFSWriter hdfsWriter = Mockito.spy(new MockHDFSWriter());    PrivilegedExecutor ugiProxy = FlumeAuthenticationUtil.getAuthenticator(null, null).proxyAs("alice");        final int ROLL_COUNT = 1;    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setProxyUser(ugiProxy).setRollCount(ROLL_COUNT).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);        bucketWriter.append(e);        IOException expectedIOException = new IOException("Test injected IOException");    Mockito.doThrow(expectedIOException).when(hdfsWriter).append(Mockito.any(Event.class));        try {        bucketWriter.append(e);        Assert.fail("Expected IOException wasn't thrown during append");    } catch (IOException ex) {        Assert.assertEquals(expectedIOException, ex);            }        try {        bucketWriter.append(e);        Assert.fail("BucketWriter should be already closed, BucketClosedException expected");    } catch (BucketClosedException ex) {            }    Assert.assertEquals("events written", 1, hdfsWriter.getEventsWritten());    Assert.assertEquals("2 files should be closed", 2, hdfsWriter.getFilesClosed());}
public BucketWriterBuilder flume_f5057_0(long rollInterval)
{    this.rollInterval = rollInterval;    return this;}
public BucketWriterBuilder flume_f5058_0(long rollSize)
{    this.rollSize = rollSize;    return this;}
public BucketWriterBuilder flume_f5059_0(long rollCount)
{    this.rollCount = rollCount;    return this;}
public BucketWriterBuilder flume_f5060_0(long batchSize)
{    this.batchSize = batchSize;    return this;}
public BucketWriterBuilder flume_f5061_0(Context context)
{    this.context = context;    return this;}
public BucketWriterBuilder flume_f5062_0(String filePath)
{    this.filePath = filePath;    return this;}
public BucketWriterBuilder flume_f5063_0(String fileName)
{    this.fileName = fileName;    return this;}
public BucketWriterBuilder flume_f5064_0(String inUsePrefix)
{    this.inUsePrefix = inUsePrefix;    return this;}
public BucketWriterBuilder flume_f5065_0(String inUseSuffix)
{    this.inUseSuffix = inUseSuffix;    return this;}
public BucketWriterBuilder flume_f5066_0(String fileSuffix)
{    this.fileSuffix = fileSuffix;    return this;}
public BucketWriterBuilder flume_f5067_0(CompressionCodec codeC)
{    this.codeC = codeC;    return this;}
public BucketWriterBuilder flume_f5068_0(CompressionType compType)
{    this.compType = compType;    return this;}
public BucketWriterBuilder flume_f5069_0(ScheduledExecutorService timedRollerPool)
{    this.timedRollerPool = timedRollerPool;    return this;}
public BucketWriterBuilder flume_f5070_0(PrivilegedExecutor proxyUser)
{    this.proxyUser = proxyUser;    return this;}
public BucketWriterBuilder flume_f5071_0(SinkCounter sinkCounter)
{    this.sinkCounter = sinkCounter;    return this;}
public BucketWriterBuilder flume_f5072_0(int idleTimeout)
{    this.idleTimeout = idleTimeout;    return this;}
public BucketWriterBuilder flume_f5073_0(WriterCallback onCloseCallback)
{    this.onCloseCallback = onCloseCallback;    return this;}
public BucketWriterBuilder flume_f5074_0(String onCloseCallbackPath)
{    this.onCloseCallbackPath = onCloseCallbackPath;    return this;}
public BucketWriterBuilder flume_f5075_0(long callTimeout)
{    this.callTimeout = callTimeout;    return this;}
public BucketWriterBuilder flume_f5076_0(ExecutorService callTimeoutPool)
{    this.callTimeoutPool = callTimeoutPool;    return this;}
public BucketWriterBuilder flume_f5077_0(long retryInterval)
{    this.retryInterval = retryInterval;    return this;}
public BucketWriterBuilder flume_f5078_0(int maxCloseTries)
{    this.maxCloseTries = maxCloseTries;    return this;}
public BucketWriterBuilder flume_f5079_0(HDFSWriter writer)
{    this.writer = writer;    return this;}
public BucketWriterBuilder flume_f5080_0(Clock clock)
{    this.clock = clock;    return this;}
public BucketWriter flume_f5081_0()
{    if (clock == null) {        clock = new SystemClock();    }    if (writer == null) {        writer = new MockHDFSWriter();    }    return new BucketWriter(rollInterval, rollSize, rollCount, batchSize, context, filePath, fileName, inUsePrefix, inUseSuffix, fileSuffix, codeC, compType, writer, timedRollerPool, proxyUser, sinkCounter, idleTimeout, onCloseCallback, onCloseCallbackPath, callTimeout, callTimeoutPool, retryInterval, maxCloseTries, clock);}
public void flume_f5082_1() throws Exception
{    this.file = new File("target/test/data/foo.gz");    this.fileURI = file.getAbsoluteFile().toURI().toString();        Configuration conf = new Configuration();        conf.set("fs.file.impl", "org.apache.hadoop.fs.RawLocalFileSystem");    Path path = new Path(fileURI);        path.getFileSystem(conf);    this.factory = new CompressionCodecFactory(conf);}
public void flume_f5083_0() throws Exception
{    Context context = new Context();    HDFSCompressedDataStream writer = new HDFSCompressedDataStream();    writer.configure(context);    writer.open(fileURI, factory.getCodec(new Path(fileURI)), SequenceFile.CompressionType.BLOCK);    String[] bodies = { "yarf!" };    writeBodies(writer, bodies);    byte[] buf = new byte[256];    GZIPInputStream cmpIn = new GZIPInputStream(new FileInputStream(file));    int len = cmpIn.read(buf);    String result = new String(buf, 0, len, Charsets.UTF_8);        result = result.trim();    Assert.assertEquals("input and output must match", bodies[0], result);}
public void flume_f5084_0() throws Exception
{    Context context = new Context();    context.put("serializer", "AVRO_EVENT");    HDFSCompressedDataStream writer = new HDFSCompressedDataStream();    writer.configure(context);    writer.open(fileURI, factory.getCodec(new Path(fileURI)), SequenceFile.CompressionType.BLOCK);    String[] bodies = { "yarf!", "yarfing!" };    writeBodies(writer, bodies);    int found = 0;    int expected = bodies.length;    List<String> expectedBodies = Lists.newArrayList(bodies);    GZIPInputStream cmpIn = new GZIPInputStream(new FileInputStream(file));    DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>();    DataFileStream<GenericRecord> avroStream = new DataFileStream<GenericRecord>(cmpIn, reader);    GenericRecord record = new GenericData.Record(avroStream.getSchema());    while (avroStream.hasNext()) {        avroStream.next(record);        CharsetDecoder decoder = Charsets.UTF_8.newDecoder();        String bodyStr = decoder.decode((ByteBuffer) record.get("body")).toString();        expectedBodies.remove(bodyStr);        found++;    }    avroStream.close();    cmpIn.close();    Assert.assertTrue("Found = " + found + ", Expected = " + expected + ", Left = " + expectedBodies.size() + " " + expectedBodies, expectedBodies.size() == 0);}
private void flume_f5085_0(HDFSCompressedDataStream writer, String... bodies) throws Exception
{    for (String body : bodies) {        Event evt = EventBuilder.withBody(body, Charsets.UTF_8);        writer.append(evt);    }    writer.sync();}
private void flume_f5086_1()
{    Configuration conf = new Configuration();    try {        FileSystem fs = FileSystem.get(conf);        Path dirPath = new Path(testPath);        if (fs.exists(dirPath)) {            fs.delete(dirPath, true);        }    } catch (IOException eIO) {            }}
public void flume_f5087_1()
{        /*     * FIXME: Use a dynamic path to support concurrent test execution. Also,     * beware of the case where this path is used for something or when the     * Hadoop config points at file:/     * better way of testing HDFS related functionality.     */    testPath = "file:///tmp/flume-test." + Calendar.getInstance().getTimeInMillis() + "." + Thread.currentThread().getId();    sink = new HDFSEventSink();    sink.setName("HDFSEventSink-" + UUID.randomUUID().toString());    dirCleanup();}
public void flume_f5088_0()
{    if (System.getenv("hdfs_keepFiles") == null)        dirCleanup();}
public void flume_f5089_0() throws Exception
{    doTestTextBatchAppend(false);}
public void flume_f5090_0() throws Exception
{    doTestTextBatchAppend(true);}
public void flume_f5091_1(boolean useRawLocalFileSystem) throws Exception
{        final long rollCount = 10;    final long batchSize = 2;    final String fileName = "FlumeData";    String newPath = testPath + "/singleTextBucket";    int totalEvents = 0;    int i = 1, j = 1;        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();        context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.rollInterval", "0");    context.put("hdfs.rollSize", "0");    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.writeFormat", "Text");    context.put("hdfs.useRawLocalFileSystem", Boolean.toString(useRawLocalFileSystem));    context.put("hdfs.fileType", "DataStream");    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i <= (rollCount * 10) / batchSize; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);            totalEvents++;        }        txn.commit();        txn.close();                sink.process();    }    sink.stop();        FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] fList = FileUtil.stat2Paths(dirStat);        long expectedFiles = totalEvents / rollCount;    if (totalEvents % rollCount > 0)        expectedFiles++;    Assert.assertEquals("num files wrong, found: " + Lists.newArrayList(fList), expectedFiles, fList.length);        verifyOutputTextFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
public void flume_f5092_1() throws InterruptedException, LifecycleException
{        Context context = new Context();    context.put("hdfs.path", testPath);    /*     * context.put("hdfs.rollInterval", String.class);     * context.get("hdfs.rollSize", String.class); context.get("hdfs.rollCount",     * String.class);     */    Configurables.configure(sink, context);    sink.setChannel(new MemoryChannel());    sink.start();    sink.stop();}
public void flume_f5093_1() throws InterruptedException, LifecycleException, EventDeliveryException
{        Context context = new Context();    Channel channel = new MemoryChannel();    context.put("hdfs.path", testPath);    context.put("keep-alive", "0");    Configurables.configure(sink, context);    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Assert.assertEquals(Status.BACKOFF, sink.process());    sink.stop();}
public void flume_f5094_1() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final String fileName = "FlumeData";    final long rollCount = 5;    final long batchSize = 2;    String newPath = testPath + "/singleBucket";    String kerbConfPrincipal = "user1/localhost@EXAMPLE.COM";    String kerbKeytab = "/usr/lib/flume/nonexistkeytabfile";        Configuration conf = new Configuration();    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION, "kerberos");    UserGroupInformation.setConfiguration(conf);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.kerberosPrincipal", kerbConfPrincipal);    context.put("hdfs.kerberosKeytab", kerbKeytab);    try {        Configurables.configure(sink, context);        Assert.fail("no exception thrown");    } catch (IllegalArgumentException expected) {        Assert.assertTrue(expected.getMessage().contains("Keytab is not a readable file"));    } finally {                conf.set(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION, "simple");        UserGroupInformation.setConfiguration(conf);    }}
public void flume_f5095_1() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final long rollCount = 3;    final long batchSize = 2;    final String fileName = "FlumeData";    String newPath = testPath + "/singleTextBucket";    int totalEvents = 0;    int i = 1, j = 1;        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();        context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.writeFormat", "Text");    context.put("hdfs.fileType", "DataStream");    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i < 4; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);            totalEvents++;        }        txn.commit();        txn.close();                sink.process();    }    sink.stop();        FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] fList = FileUtil.stat2Paths(dirStat);        long expectedFiles = totalEvents / rollCount;    if (totalEvents % rollCount > 0)        expectedFiles++;    Assert.assertEquals("num files wrong, found: " + Lists.newArrayList(fList), expectedFiles, fList.length);    verifyOutputTextFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
public void flume_f5096_1() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final long rollCount = 3;    final long batchSize = 2;    final String fileName = "FlumeData";    String newPath = testPath + "/singleTextBucket";    int totalEvents = 0;    int i = 1, j = 1;        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();        context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.writeFormat", "Text");    context.put("hdfs.fileType", "DataStream");    context.put("serializer", "AVRO_EVENT");    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i < 4; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);            totalEvents++;        }        txn.commit();        txn.close();                sink.process();    }    sink.stop();        FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] fList = FileUtil.stat2Paths(dirStat);        long expectedFiles = totalEvents / rollCount;    if (totalEvents % rollCount > 0)        expectedFiles++;    Assert.assertEquals("num files wrong, found: " + Lists.newArrayList(fList), expectedFiles, fList.length);    verifyOutputAvroFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
public void flume_f5097_1() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final String fileName = "FlumeData";    final long rollCount = 5;    final long batchSize = 2;    final int numBatches = 4;    String newPath = testPath + "/singleBucket";    int totalEvents = 0;    int i = 1, j = 1;        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i < numBatches; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);            totalEvents++;        }        txn.commit();        txn.close();                sink.process();    }    sink.stop();        FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] fList = FileUtil.stat2Paths(dirStat);        long expectedFiles = totalEvents / rollCount;    if (totalEvents % rollCount > 0)        expectedFiles++;    Assert.assertEquals("num files wrong, found: " + Lists.newArrayList(fList), expectedFiles, fList.length);    verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
public void flume_f5098_1() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{    final long currentTime = System.currentTimeMillis();    Clock clk = new Clock() {        @Override        public long currentTimeMillis() {            return currentTime;        }    };        final String fileName = "FlumeData";    final long rollCount = 5;    final long batchSize = 2;    final int numBatches = 4;    String newPath = testPath + "/singleBucket/%s";    String expectedPath = testPath + "/singleBucket/" + String.valueOf(currentTime / 1000);    int totalEvents = 0;    int i = 1, j = 1;        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(expectedPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.useLocalTimeStamp", String.valueOf(true));    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.setBucketClock(clk);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i < numBatches; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);            totalEvents++;        }        txn.commit();        txn.close();                sink.process();    }    sink.stop();        FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] fList = FileUtil.stat2Paths(dirStat);        long expectedFiles = totalEvents / rollCount;    if (totalEvents % rollCount > 0)        expectedFiles++;    Assert.assertEquals("num files wrong, found: " + Lists.newArrayList(fList), expectedFiles, fList.length);    verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);        sink.setBucketClock(new SystemClock());}
public long flume_f5099_0()
{    return currentTime;}
public void flume_f5100_1() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final long rollCount = 3;    final long batchSize = 2;    final String fileName = "FlumeData";        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(testPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", testPath + "/%Y-%m-%d/%H");    context.put("hdfs.timeZone", "UTC");    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (int i = 1; i < 4; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (int j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);        }        txn.commit();        txn.close();                sink.process();    }    sink.stop();    verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
public void flume_f5101_1() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final String fileName = "FlumeData";    final long rollCount = 5;    final long batchSize = 2;    final int numBatches = 4;    String newPath = testPath + "/singleBucket";    int totalEvents = 0;    int i = 1, j = 1;    HDFSTestWriterFactory badWriterFactory = new HDFSTestWriterFactory();    sink = new HDFSEventSink(badWriterFactory);        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.fileType", HDFSTestWriterFactory.TestSequenceFileType);    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i < numBatches; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);                        if ((totalEvents % 30) == 1) {                event.getHeaders().put("fault-once", "");            }            channel.put(event);            totalEvents++;        }        txn.commit();        txn.close();            }                sink.stop();    verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);    SinkCounter sc = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sc.getEventWriteFail());}
private List<String> flume_f5102_0(String input)
{    List<String> output = Lists.newArrayList();    File dir = new File(input);    if (dir.isFile()) {        output.add(dir.getAbsolutePath());    } else if (dir.isDirectory()) {        for (String file : dir.list()) {            File subDir = new File(dir, file);            output.addAll(getAllFiles(subDir.getAbsolutePath()));        }    }    return output;}
private void flume_f5103_1(FileSystem fs, Configuration conf, String dir, String prefix, List<String> bodies) throws IOException
{    int found = 0;    int expected = bodies.size();    for (String outputFile : getAllFiles(dir)) {        String name = (new File(outputFile)).getName();        if (name.startsWith(prefix)) {            SequenceFile.Reader reader = new SequenceFile.Reader(fs, new Path(outputFile), conf);            LongWritable key = new LongWritable();            BytesWritable value = new BytesWritable();            while (reader.next(key, value)) {                String body = new String(value.getBytes(), 0, value.getLength());                if (bodies.contains(body)) {                                        bodies.remove(body);                    found++;                }            }            reader.close();        }    }    if (!bodies.isEmpty()) {        for (String body : bodies) {                    }    }    Assert.assertTrue("Found = " + found + ", Expected = " + expected + ", Left = " + bodies.size() + " " + bodies, bodies.size() == 0);}
private void flume_f5104_0(FileSystem fs, Configuration conf, String dir, String prefix, List<String> bodies) throws IOException
{    int found = 0;    int expected = bodies.size();    for (String outputFile : getAllFiles(dir)) {        String name = (new File(outputFile)).getName();        if (name.startsWith(prefix)) {            FSDataInputStream input = fs.open(new Path(outputFile));            BufferedReader reader = new BufferedReader(new InputStreamReader(input));            String body = null;            while ((body = reader.readLine()) != null) {                bodies.remove(body);                found++;            }            reader.close();        }    }    Assert.assertTrue("Found = " + found + ", Expected = " + expected + ", Left = " + bodies.size() + " " + bodies, bodies.size() == 0);}
private void flume_f5105_1(FileSystem fs, Configuration conf, String dir, String prefix, List<String> bodies) throws IOException
{    int found = 0;    int expected = bodies.size();    for (String outputFile : getAllFiles(dir)) {        String name = (new File(outputFile)).getName();        if (name.startsWith(prefix)) {            FSDataInputStream input = fs.open(new Path(outputFile));            DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>();            DataFileStream<GenericRecord> avroStream = new DataFileStream<GenericRecord>(input, reader);            GenericRecord record = new GenericData.Record(avroStream.getSchema());            while (avroStream.hasNext()) {                avroStream.next(record);                ByteBuffer body = (ByteBuffer) record.get("body");                CharsetDecoder decoder = Charsets.UTF_8.newDecoder();                String bodyStr = decoder.decode(body).toString();                                bodies.remove(bodyStr);                found++;            }            avroStream.close();            input.close();        }    }    Assert.assertTrue("Found = " + found + ", Expected = " + expected + ", Left = " + bodies.size() + " " + bodies, bodies.size() == 0);}
public void flume_f5106_1() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final int numBatches = 4;    final String fileName = "FlumeData";    final long rollCount = 5;    final long batchSize = 2;    String newPath = testPath + "/singleBucket";    int i = 1, j = 1;    HDFSTestWriterFactory badWriterFactory = new HDFSTestWriterFactory();    sink = new HDFSEventSink(badWriterFactory);        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.fileType", HDFSTestWriterFactory.TestSequenceFileType);    Configurables.configure(sink, context);    MemoryChannel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i < numBatches; i++) {        channel.getTransaction().begin();        try {            for (j = 1; j <= batchSize; j++) {                Event event = new SimpleEvent();                eventDate.clear();                                eventDate.set(2011, i, i, i, 0);                event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));                event.getHeaders().put("hostname", "Host" + i);                String body = "Test." + i + "." + j;                event.setBody(body.getBytes());                bodies.add(body);                                event.getHeaders().put("fault-until-reopen", "");                channel.put(event);            }            channel.getTransaction().commit();        } finally {            channel.getTransaction().close();        }            }        sink.stop();    verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);    SinkCounter sc = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sc.getEventWriteFail());}
public void flume_f5107_1() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final int numBatches = 4;    final String fileName = "FlumeData";    final long batchSize = 2;    String newPath = testPath + "/singleBucket";    int i = 1, j = 1;    HDFSTestWriterFactory badWriterFactory = new HDFSTestWriterFactory();    sink = new HDFSEventSink(badWriterFactory);        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(0));    context.put("hdfs.rollSize", String.valueOf(0));    context.put("hdfs.rollInterval", String.valueOf(2));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.fileType", HDFSTestWriterFactory.TestSequenceFileType);    Configurables.configure(sink, context);    MemoryChannel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i < numBatches; i++) {        channel.getTransaction().begin();        try {            for (j = 1; j <= batchSize; j++) {                Event event = new SimpleEvent();                eventDate.clear();                                eventDate.set(2011, i, i, i, 0);                event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));                event.getHeaders().put("hostname", "Host" + i);                String body = "Test." + i + "." + j;                event.setBody(body.getBytes());                bodies.add(body);                                event.getHeaders().put("count-check", "");                channel.put(event);            }            channel.getTransaction().commit();        } finally {            channel.getTransaction().close();        }                        if (i == 1) {            Thread.sleep(2001);        }    }        sink.stop();    Assert.assertTrue(badWriterFactory.openCount.get() >= 2);        verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
public void flume_f5108_1() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final String fileName = "FlumeData";    final long batchSize = 2;    String newPath = testPath + "/singleBucket";    int i = 1, j = 1;    HDFSTestWriterFactory badWriterFactory = new HDFSTestWriterFactory();    sink = new HDFSEventSink(badWriterFactory);        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(0));    context.put("hdfs.rollSize", String.valueOf(0));    context.put("hdfs.rollInterval", String.valueOf(1));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.fileType", HDFSTestWriterFactory.TestSequenceFileType);    String expectedLookupPath = newPath + "/FlumeData";    Configurables.configure(sink, context);    MemoryChannel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        channel.getTransaction().begin();    try {        for (j = 1; j <= 2 * batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);                        event.getHeaders().put("count-check", "");            channel.put(event);        }        channel.getTransaction().commit();    } finally {        channel.getTransaction().close();    }        Assert.assertTrue(sink.getSfWriters().containsKey(expectedLookupPath));        Thread.sleep(2001);    Assert.assertFalse(sink.getSfWriters().containsKey(expectedLookupPath));                Assert.assertTrue(sink.getSfWriters().containsKey(expectedLookupPath));    sink.stop();        verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
public void flume_f5109_1() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final String fileName = "FlumeData";    final long rollCount = 5;    final long batchSize = 2;    final int numBatches = 2;    String newPath = testPath + "/singleBucket";    int i = 1, j = 1;        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);        HDFSTestWriterFactory badWriterFactory = new HDFSTestWriterFactory();    sink = new HDFSEventSink(badWriterFactory);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.fileType", HDFSTestWriterFactory.TestSequenceFileType);    context.put("hdfs.callTimeout", Long.toString(1000));    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();        for (i = 0; i < numBatches; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            event.getHeaders().put("slow", "1500");            event.setBody(("Test." + i + "." + j).getBytes());            channel.put(event);        }        txn.commit();        txn.close();                Status satus = sink.process();                Assert.assertEquals(satus, Status.BACKOFF);    }    sink.stop();    SinkCounter sc = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(2, sc.getEventWriteFail());}
private void flume_f5110_0(long appendTimeout) throws InterruptedException, IOException, LifecycleException, EventDeliveryException, IOException
{    final String fileName = "FlumeData";    final long rollCount = 5;    final long batchSize = 2;    final int numBatches = 2;    String newPath = testPath + "/singleBucket";    int totalEvents = 0;    int i = 1, j = 1;        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);        HDFSTestWriterFactory badWriterFactory = new HDFSTestWriterFactory();    sink = new HDFSEventSink(badWriterFactory);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.fileType", HDFSTestWriterFactory.TestSequenceFileType);    context.put("hdfs.appendTimeout", String.valueOf(appendTimeout));    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 0; i < numBatches; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            event.getHeaders().put("slow", "1500");            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);            totalEvents++;        }        txn.commit();        txn.close();                sink.process();    }    sink.stop();        FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] fList = FileUtil.stat2Paths(dirStat);            long expectedFiles = totalEvents / rollCount;    if (totalEvents % rollCount > 0)        expectedFiles++;    Assert.assertEquals("num files wrong, found: " + Lists.newArrayList(fList), expectedFiles, fList.length);    verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
public void flume_f5111_1() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        slowAppendTestHelper(3000);}
public void flume_f5112_1() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        slowAppendTestHelper(0);}
public void flume_f5113_0() throws IOException, EventDeliveryException, InterruptedException
{    String hdfsPath = testPath + "/idleClose";    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(hdfsPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", hdfsPath);    /*     * All three rolling methods are disabled so the only     * way a file can roll is through the idle timeout.     */    context.put("hdfs.rollCount", "0");    context.put("hdfs.rollSize", "0");    context.put("hdfs.rollInterval", "0");    context.put("hdfs.batchSize", "2");    context.put("hdfs.idleTimeout", "1");    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 10; i++) {        Event event = new SimpleEvent();        event.setBody(("test event " + i).getBytes());        channel.put(event);    }    txn.commit();    txn.close();    sink.process();    sink.process();    Thread.sleep(1001);                        sink.process();    sink.process();        Thread.sleep(500);    sink.process();    sink.process();    sink.stop();    FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] fList = FileUtil.stat2Paths(dirStat);    Assert.assertEquals("Incorrect content of the directory " + StringUtils.join(fList, ","), 2, fList.length);    Assert.assertTrue(!fList[0].getName().endsWith(".tmp") && !fList[1].getName().endsWith(".tmp"));    fs.close();}
public void flume_f5114_0() throws IOException, EventDeliveryException
{    String hdfsPath = testPath + "/sequenceFileWriterSync";    FileSystem fs = FileSystem.get(new Configuration());        fs.setVerifyChecksum(false);    fs.setWriteChecksum(false);        String[] codecs = { "BZip2Codec", "DeflateCodec" };    for (String codec : codecs) {        sequenceFileWriteAndVerifyEvents(fs, hdfsPath, codec, Collections.singletonList("single-event"));        sequenceFileWriteAndVerifyEvents(fs, hdfsPath, codec, Arrays.asList("multiple-events-1", "multiple-events-2", "multiple-events-3", "multiple-events-4", "multiple-events-5"));    }    fs.close();}
private void flume_f5115_0(FileSystem fs, String hdfsPath, String codec, Collection<String> eventBodies) throws IOException, EventDeliveryException
{    Path dirPath = new Path(hdfsPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", hdfsPath);        context.put("hdfs.rollCount", String.valueOf(eventBodies.size() + 1));    context.put("hdfs.rollSize", "0");    context.put("hdfs.rollInterval", "0");    context.put("hdfs.batchSize", "1");    context.put("hdfs.fileType", "SequenceFile");    context.put("hdfs.codeC", codec);    context.put("hdfs.writeFormat", "Writable");    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    for (String eventBody : eventBodies) {        Transaction txn = channel.getTransaction();        txn.begin();        Event event = new SimpleEvent();        event.setBody(eventBody.getBytes());        channel.put(event);        txn.commit();        txn.close();        sink.process();    }            FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] paths = FileUtil.stat2Paths(dirStat);    Assert.assertEquals(1, paths.length);    SequenceFile.Reader reader = new SequenceFile.Reader(fs.getConf(), SequenceFile.Reader.stream(fs.open(paths[0])));    LongWritable key = new LongWritable();    BytesWritable value = new BytesWritable();    for (String eventBody : eventBodies) {        Assert.assertTrue(reader.next(key, value));        Assert.assertArrayEquals(eventBody.getBytes(), value.copyBytes());    }    Assert.assertFalse(reader.next(key, value));}
private Context flume_f5116_0()
{    Context context = new Context();    context.put("hdfs.path", testPath + "/%{retryHeader}");    context.put("hdfs.filePrefix", "test");    context.put("hdfs.batchSize", String.valueOf(100));    context.put("hdfs.fileType", "DataStream");    context.put("hdfs.serializer", "text");    context.put("hdfs.closeTries", "3");    context.put("hdfs.rollCount", "1");    context.put("hdfs.retryInterval", "1");    return context;}
public void flume_f5117_0() throws Exception
{    Context context = getContextForRetryTests();    context.put("hdfs.retryInterval", "0");    Configurables.configure(sink, context);    Assert.assertEquals(1, sink.getTryCount());}
public void flume_f5118_0() throws Exception
{    Context context = getContextForRetryTests();    context.put("hdfs.retryInterval", "-1");    Configurables.configure(sink, context);    Assert.assertEquals(1, sink.getTryCount());}
public void flume_f5119_0() throws Exception
{    Context context = getContextForRetryTests();    context.put("hdfs.closeTries", "0");    Configurables.configure(sink, context);    Assert.assertEquals(Integer.MAX_VALUE, sink.getTryCount());}
public void flume_f5120_0() throws Exception
{    Context context = getContextForRetryTests();    context.put("hdfs.closeTries", "-4");    Configurables.configure(sink, context);    Assert.assertEquals(Integer.MAX_VALUE, sink.getTryCount());}
public void flume_f5121_0() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{    testRetryRename(true);    testRetryRename(false);}
private void flume_f5122_1(boolean closeSucceed) throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        String newPath = testPath + "/retryBucket";        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    MockFileSystem mockFs = new MockFileSystem(fs, 6, closeSucceed);    Context context = getContextForRetryTests();    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.setMockFs(mockFs);    HDFSWriter hdfsWriter = new MockDataStream(mockFs);    hdfsWriter.configure(context);    sink.setMockWriter(hdfsWriter);    sink.start();        for (int i = 0; i < 2; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        Map<String, String> hdr = Maps.newHashMap();        hdr.put("retryHeader", "v1");        channel.put(EventBuilder.withBody("random".getBytes(), hdr));        txn.commit();        txn.close();                sink.process();    }        for (int i = 0; i < 2; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        Map<String, String> hdr = Maps.newHashMap();        hdr.put("retryHeader", "v2");        channel.put(EventBuilder.withBody("random".getBytes(), hdr));        txn.commit();        txn.close();                sink.process();    }        TimeUnit.SECONDS.sleep(5);    Collection<BucketWriter> writers = sink.getSfWriters().values();    int totalRenameAttempts = 0;    for (BucketWriter writer : writers) {                totalRenameAttempts += writer.renameTries.get();    }            sink.stop();    Assert.assertEquals(6, totalRenameAttempts);}
public void flume_f5123_0() throws Exception
{    final Set<BucketWriter> bucketWriters = new HashSet<>();    sink = new HDFSEventSink() {        @Override        BucketWriter initializeBucketWriter(String realPath, String realName, String lookupPath, HDFSWriter hdfsWriter, WriterCallback closeCallback) {            BucketWriter bw = Mockito.spy(super.initializeBucketWriter(realPath, realName, lookupPath, hdfsWriter, closeCallback));            try {                                                Mockito.doCallRealMethod().doThrow(BucketClosedException.class).when(bw).append(Mockito.any(Event.class));            } catch (IOException | InterruptedException e) {                Assert.fail("This shouldn't happen, as append() is called during mocking.");            }            bucketWriters.add(bw);            return bw;        }    };    Context context = new Context(ImmutableMap.of("hdfs.path", testPath));    Configurables.configure(sink, context);    Channel channel = Mockito.spy(new MemoryChannel());    Configurables.configure(channel, new Context());    final Iterator<Event> events = Iterators.forArray(EventBuilder.withBody("test1".getBytes()), EventBuilder.withBody("test2".getBytes()));    Mockito.doAnswer(new Answer() {        @Override        public Object answer(InvocationOnMock invocation) throws Throwable {            return events.hasNext() ? events.next() : null;        }    }).when(channel).take();    sink.setChannel(channel);    sink.start();    sink.process();        Mockito.verify(channel, Mockito.times(3)).take();    FileSystem fs = FileSystem.get(new Configuration());    int fileCount = 0;    for (RemoteIterator<LocatedFileStatus> i = fs.listFiles(new Path(testPath), false); i.hasNext(); i.next()) {        fileCount++;    }    Assert.assertEquals(2, fileCount);    Assert.assertEquals(2, bucketWriters.size());        for (BucketWriter bw : bucketWriters) {        Mockito.verify(bw, Mockito.times(1)).flush();    }    sink.stop();}
 BucketWriter flume_f5124_0(String realPath, String realName, String lookupPath, HDFSWriter hdfsWriter, WriterCallback closeCallback)
{    BucketWriter bw = Mockito.spy(super.initializeBucketWriter(realPath, realName, lookupPath, hdfsWriter, closeCallback));    try {                        Mockito.doCallRealMethod().doThrow(BucketClosedException.class).when(bw).append(Mockito.any(Event.class));    } catch (IOException | InterruptedException e) {        Assert.fail("This shouldn't happen, as append() is called during mocking.");    }    bucketWriters.add(bw);    return bw;}
public Object flume_f5125_0(InvocationOnMock invocation) throws Throwable
{    return events.hasNext() ? events.next() : null;}
public void flume_f5126_1()
{        Context context = new Context();    context.put("hdfs.path", testPath);    context.put("keep-alive", "0");    Configurables.configure(sink, context);    Channel channel = Mockito.mock(Channel.class);    Mockito.when(channel.take()).thenThrow(new ChannelException("dummy"));    Mockito.when(channel.getTransaction()).thenReturn(Mockito.mock(BasicTransactionSemantics.class));    sink.setChannel(channel);    sink.start();    try {        sink.process();    } catch (EventDeliveryException e) {        }    sink.stop();    SinkCounter sc = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sc.getChannelReadFail());}
public void flume_f5127_0()
{    String inUseSuffixConf = "aaaa";    Context context = new Context();    context.put("hdfs.path", testPath);    context.put("hdfs.inUseSuffix", inUseSuffixConf);        Configurables.configure(sink, context);    String inUseSuffix = (String) Whitebox.getInternalState(sink, "inUseSuffix");    Assert.assertEquals(inUseSuffixConf, inUseSuffix);    context.put("hdfs.emptyInUseSuffix", "true");    Configurables.configure(sink, context);    inUseSuffix = (String) Whitebox.getInternalState(sink, "inUseSuffix");    Assert.assertEquals("", inUseSuffix);    context.put("hdfs.emptyInUseSuffix", "false");    Configurables.configure(sink, context);    inUseSuffix = (String) Whitebox.getInternalState(sink, "inUseSuffix");    Assert.assertEquals(inUseSuffixConf, inUseSuffix);}
public static void flume_f5128_0(String... args)
{    HDFSEventSink sink = new HDFSEventSink();    sink.setName("HDFSEventSink");    Context context = new Context(ImmutableMap.of("hdfs.path", "file:///tmp/flume-test/bucket-%t", "hdfs.filePrefix", "flumetest", "hdfs.rollInterval", "1", "hdfs.maxOpenFiles", "1", "hdfs.useLocalTimeStamp", "true"));    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    final SequenceGeneratorSource source = new SequenceGeneratorSource();    Configurables.configure(source, new Context());    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(Collections.singletonList(channel));    source.setChannelProcessor(new ChannelProcessor(rcs));    sink.setChannel(channel);    channel.start();    source.start();    SinkProcessor sinkProcessor = new DefaultSinkProcessor();    sinkProcessor.setSinks(Collections.singletonList(sink));    SinkRunner sinkRunner = new SinkRunner();    sinkRunner.setSink(sinkProcessor);    sinkRunner.start();    ScheduledExecutorService executor = Executors.newScheduledThreadPool(3);    executor.execute(new Runnable() {        @Override        public void run() {            int i = 0;            while (true) {                try {                    source.process();                    System.out.println(i++);                    if (i == 250) {                        System.out.println("No deadlock found after 250 iterations, exiting");                        System.exit(0);                    }                    Thread.sleep((long) (Math.random() * 100 + 950));                } catch (Exception e) {                                }            }        }    });    executor.scheduleAtFixedRate(new Runnable() {        @Override        public void run() {            ThreadMXBean bean = ManagementFactory.getThreadMXBean();            long[] threadIds = bean.findDeadlockedThreads();            if (threadIds != null) {                System.out.println("Deadlocked threads found");                printThreadStackTraces(threadIds);                System.exit(1);            }        }    }, 0, 1, TimeUnit.SECONDS);}
public void flume_f5129_0()
{    int i = 0;    while (true) {        try {            source.process();            System.out.println(i++);            if (i == 250) {                System.out.println("No deadlock found after 250 iterations, exiting");                System.exit(0);            }            Thread.sleep((long) (Math.random() * 100 + 950));        } catch (Exception e) {                }    }}
public void flume_f5130_0()
{    ThreadMXBean bean = ManagementFactory.getThreadMXBean();    long[] threadIds = bean.findDeadlockedThreads();    if (threadIds != null) {        System.out.println("Deadlocked threads found");        printThreadStackTraces(threadIds);        System.exit(1);    }}
private static void flume_f5131_0(long[] threadIds)
{    Set<Long> threadIdSet = new HashSet<>(Longs.asList(threadIds));    for (Thread th : Thread.getAllStackTraces().keySet()) {        if (threadIdSet.contains(th.getId())) {            System.out.println("Thread: " + th);            for (StackTraceElement e : th.getStackTrace()) {                System.out.println("\t" + e);            }            System.out.println("-----------------------------");        }    }}
public static void flume_f5132_0() throws IOException
{        File dfsDir = new File(DFS_DIR);    if (!dfsDir.isDirectory()) {        dfsDir.mkdirs();    }        oldTestBuildDataProp = System.getProperty(TEST_BUILD_DATA_KEY);    System.setProperty(TEST_BUILD_DATA_KEY, DFS_DIR);}
private static String flume_f5133_0(MiniDFSCluster cluster)
{    int nnPort = cluster.getNameNode().getNameNodeAddress().getPort();    return "hdfs://localhost:" + nnPort;}
public void flume_f5134_1() throws EventDeliveryException, IOException
{    cluster = new MiniDFSCluster(new Configuration(), 1, true, null);    cluster.waitActive();    String outputDir = "/flume/simpleHDFSTest";    Path outputDirPath = new Path(outputDir);        FileSystem fs = cluster.getFileSystem();        if (fs.exists(outputDirPath)) {        fs.delete(outputDirPath, true);    }    String nnURL = getNameNodeURL(cluster);        Context chanCtx = new Context();    MemoryChannel channel = new MemoryChannel();    channel.setName("simpleHDFSTest-mem-chan");    channel.configure(chanCtx);    channel.start();    Context sinkCtx = new Context();    sinkCtx.put("hdfs.path", nnURL + outputDir);    sinkCtx.put("hdfs.fileType", HDFSWriterFactory.DataStreamType);    sinkCtx.put("hdfs.batchSize", Integer.toString(1));    HDFSEventSink sink = new HDFSEventSink();    sink.setName("simpleHDFSTest-hdfs-sink");    sink.configure(sinkCtx);    sink.setChannel(channel);    sink.start();        String EVENT_BODY = "yarg!";    channel.getTransaction().begin();    try {        channel.put(EventBuilder.withBody(EVENT_BODY, Charsets.UTF_8));        channel.getTransaction().commit();    } finally {        channel.getTransaction().close();    }        sink.process();        sink.stop();    channel.stop();        FileStatus[] statuses = fs.listStatus(outputDirPath);    Assert.assertNotNull("No files found written to HDFS", statuses);    Assert.assertEquals("Only one file expected", 1, statuses.length);    for (FileStatus status : statuses) {        Path filePath = status.getPath();                FSDataInputStream stream = fs.open(filePath);        BufferedReader reader = new BufferedReader(new InputStreamReader(stream));        String line = reader.readLine();                Assert.assertEquals(EVENT_BODY, line);    }    if (!KEEP_DATA) {        fs.delete(outputDirPath, true);    }    cluster.shutdown();    cluster = null;}
public void flume_f5135_1() throws EventDeliveryException, IOException
{    cluster = new MiniDFSCluster(new Configuration(), 1, true, null);    cluster.waitActive();    String outputDir = "/flume/simpleHDFSGZipCompressedTest";    Path outputDirPath = new Path(outputDir);        FileSystem fs = cluster.getFileSystem();        if (fs.exists(outputDirPath)) {        fs.delete(outputDirPath, true);    }    String nnURL = getNameNodeURL(cluster);        Context chanCtx = new Context();    MemoryChannel channel = new MemoryChannel();    channel.setName("simpleHDFSTest-mem-chan");    channel.configure(chanCtx);    channel.start();    Context sinkCtx = new Context();    sinkCtx.put("hdfs.path", nnURL + outputDir);    sinkCtx.put("hdfs.fileType", HDFSWriterFactory.CompStreamType);    sinkCtx.put("hdfs.batchSize", Integer.toString(1));    sinkCtx.put("hdfs.codeC", "gzip");    HDFSEventSink sink = new HDFSEventSink();    sink.setName("simpleHDFSTest-hdfs-sink");    sink.configure(sinkCtx);    sink.setChannel(channel);    sink.start();        String EVENT_BODY_1 = "yarg1";    String EVENT_BODY_2 = "yarg2";    channel.getTransaction().begin();    try {        channel.put(EventBuilder.withBody(EVENT_BODY_1, Charsets.UTF_8));        channel.put(EventBuilder.withBody(EVENT_BODY_2, Charsets.UTF_8));        channel.getTransaction().commit();    } finally {        channel.getTransaction().close();    }        sink.process();        sink.stop();    channel.stop();        FileStatus[] statuses = fs.listStatus(outputDirPath);    Assert.assertNotNull("No files found written to HDFS", statuses);    Assert.assertEquals("Only one file expected", 1, statuses.length);    for (FileStatus status : statuses) {        Path filePath = status.getPath();                FSDataInputStream stream = fs.open(filePath);        BufferedReader reader = new BufferedReader(new InputStreamReader(new GZIPInputStream(stream)));        String line = reader.readLine();                Assert.assertEquals(EVENT_BODY_1, line);                                                        }    if (!KEEP_DATA) {        fs.delete(outputDirPath, true);    }    cluster.shutdown();    cluster = null;}
public void flume_f5136_1() throws EventDeliveryException, IOException
{    Configuration conf = new Configuration();    conf.set("dfs.replication", String.valueOf(3));    cluster = new MiniDFSCluster(conf, 3, true, null);    cluster.waitActive();    String outputDir = "/flume/underReplicationTest";    Path outputDirPath = new Path(outputDir);        FileSystem fs = cluster.getFileSystem();        if (fs.exists(outputDirPath)) {        fs.delete(outputDirPath, true);    }    String nnURL = getNameNodeURL(cluster);        Context chanCtx = new Context();    MemoryChannel channel = new MemoryChannel();    channel.setName("simpleHDFSTest-mem-chan");    channel.configure(chanCtx);    channel.start();    Context sinkCtx = new Context();    sinkCtx.put("hdfs.path", nnURL + outputDir);    sinkCtx.put("hdfs.fileType", HDFSWriterFactory.DataStreamType);    sinkCtx.put("hdfs.batchSize", Integer.toString(1));        sinkCtx.put("hdfs.retryInterval", "10");    HDFSEventSink sink = new HDFSEventSink();    sink.setName("simpleHDFSTest-hdfs-sink");    sink.configure(sinkCtx);    sink.setChannel(channel);    sink.start();        channel.getTransaction().begin();    try {        channel.put(EventBuilder.withBody("yarg 1", Charsets.UTF_8));        channel.put(EventBuilder.withBody("yarg 2", Charsets.UTF_8));        channel.put(EventBuilder.withBody("yarg 3", Charsets.UTF_8));        channel.put(EventBuilder.withBody("yarg 4", Charsets.UTF_8));        channel.put(EventBuilder.withBody("yarg 5", Charsets.UTF_8));        channel.put(EventBuilder.withBody("yarg 5", Charsets.UTF_8));        channel.getTransaction().commit();    } finally {        channel.getTransaction().close();    }                sink.process();        sink.process();            cluster.stopDataNode(0);                    sink.process();        sink.process();        sink.process();        sink.process();        sink.stop();    channel.stop();        FileStatus[] statuses = fs.listStatus(outputDirPath);    Assert.assertNotNull("No files found written to HDFS", statuses);    for (FileStatus status : statuses) {        Path filePath = status.getPath();                FSDataInputStream stream = fs.open(filePath);        BufferedReader reader = new BufferedReader(new InputStreamReader(stream));        String line = reader.readLine();                Assert.assertTrue(line.startsWith("yarg"));    }    Assert.assertTrue("4 or 5 files expected, found " + statuses.length, statuses.length == 4 || statuses.length == 5);    System.out.println("There are " + statuses.length + " files.");    if (!KEEP_DATA) {        fs.delete(outputDirPath, true);    }    cluster.shutdown();    cluster = null;}
public void flume_f5137_1() throws EventDeliveryException, IOException
{    Configuration conf = new Configuration();    conf.set("dfs.replication", String.valueOf(3));    cluster = new MiniDFSCluster(conf, 3, true, null);    cluster.waitActive();    String outputDir = "/flume/underReplicationTest";    Path outputDirPath = new Path(outputDir);        FileSystem fs = cluster.getFileSystem();        if (fs.exists(outputDirPath)) {        fs.delete(outputDirPath, true);    }    String nnURL = getNameNodeURL(cluster);        Context chanCtx = new Context();    MemoryChannel channel = new MemoryChannel();    channel.setName("simpleHDFSTest-mem-chan");    channel.configure(chanCtx);    channel.start();    Context sinkCtx = new Context();    sinkCtx.put("hdfs.path", nnURL + outputDir);    sinkCtx.put("hdfs.fileType", HDFSWriterFactory.DataStreamType);    sinkCtx.put("hdfs.batchSize", Integer.toString(1));    HDFSEventSink sink = new HDFSEventSink();    sink.setName("simpleHDFSTest-hdfs-sink");    sink.configure(sinkCtx);    sink.setChannel(channel);    sink.start();        channel.getTransaction().begin();    try {        for (int i = 0; i < 50; i++) {            channel.put(EventBuilder.withBody("yarg " + i, Charsets.UTF_8));        }        channel.getTransaction().commit();    } finally {        channel.getTransaction().close();    }                sink.process();        sink.process();            cluster.stopDataNode(0);                    sink.process();    for (int i = 3; i < 50; i++) {                sink.process();    }        sink.stop();    channel.stop();        FileStatus[] statuses = fs.listStatus(outputDirPath);    Assert.assertNotNull("No files found written to HDFS", statuses);    for (FileStatus status : statuses) {        Path filePath = status.getPath();                FSDataInputStream stream = fs.open(filePath);        BufferedReader reader = new BufferedReader(new InputStreamReader(stream));        String line = reader.readLine();                Assert.assertTrue(line.startsWith("yarg"));    }    System.out.println("There are " + statuses.length + " files.");    Assert.assertEquals("31 files expected, found " + statuses.length, 31, statuses.length);    if (!KEEP_DATA) {        fs.delete(outputDirPath, true);    }    cluster.shutdown();    cluster = null;}
public void flume_f5138_0() throws Exception
{    testLeaseRecoveredIfCloseFails(new Callable<Void>() {        @Override        public Void call() throws Exception {            throw new IOException();        }    });}
public Void flume_f5139_0() throws Exception
{    throw new IOException();}
public void flume_f5140_0() throws Exception
{    testLeaseRecoveredIfCloseFails(new Callable<Void>() {        @Override        public Void call() throws Exception {            TimeUnit.SECONDS.sleep(30);            return null;        }    });}
public Void flume_f5141_0() throws Exception
{    TimeUnit.SECONDS.sleep(30);    return null;}
private void flume_f5142_1(final Callable<?> doThisInClose) throws Exception
{    cluster = new MiniDFSCluster.Builder(new Configuration()).numDataNodes(1).format(true).build();    cluster.waitActive();    String outputDir = "/flume/leaseRecovery";    Path outputDirPath = new Path(outputDir);        FileSystem fs = cluster.getFileSystem();        if (fs.exists(outputDirPath)) {        fs.delete(outputDirPath, true);    }    String nnURL = getNameNodeURL(cluster);    Context ctx = new Context();    MemoryChannel channel = new MemoryChannel();    channel.configure(ctx);    channel.start();    ctx.put("hdfs.path", nnURL + outputDir);    ctx.put("hdfs.fileType", HDFSWriterFactory.DataStreamType);    ctx.put("hdfs.batchSize", Integer.toString(1));    ctx.put("hdfs.callTimeout", Integer.toString(1000));        ctx.put("hdfs.retryInterval", "10");    HDFSWriter hdfsWriter = new HDFSDataStream() {        @Override        public void close() throws IOException {            try {                doThisInClose.call();            } catch (Throwable e) {                Throwables.propagateIfPossible(e, IOException.class);                throw new RuntimeException(e);            }        }    };    hdfsWriter.configure(ctx);    HDFSEventSink sink = new HDFSEventSink();    sink.configure(ctx);    sink.setMockFs(fs);    sink.setMockWriter(hdfsWriter);    sink.setChannel(channel);    sink.start();    Transaction txn = channel.getTransaction();    txn.begin();    try {        channel.put(EventBuilder.withBody("test", Charsets.UTF_8));        txn.commit();    } finally {        txn.close();    }    sink.process();    sink.stop();    channel.stop();    FileStatus[] statuses = fs.listStatus(outputDirPath);    Assert.assertEquals(1, statuses.length);    String filePath = statuses[0].getPath().toUri().getPath();        long leaseRenewalTime = NameNodeAdapter.getLeaseRenewalTime(cluster.getNameNode(), filePath);        for (int i = 0; (i < 10) && (leaseRenewalTime != -1L); i++) {        TimeUnit.SECONDS.sleep(1);        leaseRenewalTime = NameNodeAdapter.getLeaseRenewalTime(cluster.getNameNode(), filePath);    }            Assert.assertEquals(-1L, leaseRenewalTime);    if (!KEEP_DATA) {        fs.delete(outputDirPath, true);    }    cluster.shutdown();    cluster = null;}
public void flume_f5143_0() throws IOException
{    try {        doThisInClose.call();    } catch (Throwable e) {        Throwables.propagateIfPossible(e, IOException.class);        throw new RuntimeException(e);    }}
public static void flume_f5144_0()
{        if (oldTestBuildDataProp != null) {        System.setProperty(TEST_BUILD_DATA_KEY, oldTestBuildDataProp);    }    if (!KEEP_DATA) {        FileUtils.deleteQuietly(new File(DFS_DIR));    }}
public void flume_f5145_0()
{    SequenceFileSerializer formatter = SequenceFileSerializerFactory.getSerializer("Text", new Context());    assertTrue(formatter != null);    assertTrue(formatter.getClass().getName(), formatter instanceof HDFSTextSerializer);}
public void flume_f5146_0()
{    SequenceFileSerializer formatter = SequenceFileSerializerFactory.getSerializer("Writable", new Context());    assertTrue(formatter != null);    assertTrue(formatter.getClass().getName(), formatter instanceof HDFSWritableSerializer);}
public void flume_f5147_0()
{    SequenceFileSerializer formatter = SequenceFileSerializerFactory.getSerializer("org.apache.flume.sink.hdfs.MyCustomSerializer$Builder", new Context());    assertTrue(formatter != null);    assertTrue(formatter.getClass().getName(), formatter instanceof MyCustomSerializer);}
public void flume_f5148_0() throws Exception
{    baseDir = Files.createTempDir();    testFile = new File(baseDir.getAbsoluteFile(), "test");    context = new Context();    event = EventBuilder.withBody("test", Charsets.UTF_8);}
public void flume_f5149_0() throws Exception
{    FileUtils.deleteQuietly(baseDir);}
public void flume_f5150_0() throws Exception
{    String file = testFile.getCanonicalPath();    HDFSDataStream stream = new HDFSDataStream();    context.put("hdfs.useRawLocalFileSystem", "true");    stream.configure(context);    stream.open(file);    stream.append(event);    stream.sync();    Assert.assertTrue(testFile.length() > 0);}
public void flume_f5151_0() throws Exception
{    String file = testFile.getCanonicalPath();    HDFSCompressedDataStream stream = new HDFSCompressedDataStream();    context.put("hdfs.useRawLocalFileSystem", "true");    stream.configure(context);    stream.open(file, new GzipCodec(), CompressionType.RECORD);    stream.append(event);    stream.sync();    Assert.assertTrue(testFile.length() > 0);}
public void flume_f5152_0() throws Exception
{    String file = testFile.getCanonicalPath();    HDFSSequenceFile stream = new HDFSSequenceFile();    context.put("hdfs.useRawLocalFileSystem", "true");    stream.configure(context);    stream.open(file);    stream.append(event);    stream.sync();    Assert.assertTrue(testFile.length() > 0);}
public void flume_f5153_0(TransactionBatch txnBatch, Event e) throws StreamingException, IOException, InterruptedException
{    txnBatch.write(e.getBody());}
public void flume_f5154_0(TransactionBatch txnBatch, Collection<byte[]> events) throws StreamingException, IOException, InterruptedException
{    txnBatch.write(events);}
public RecordWriter flume_f5155_0(HiveEndPoint endPoint) throws StreamingException, IOException, ClassNotFoundException
{    if (serdeSeparator == null) {        return new DelimitedInputWriter(fieldToColMapping, delimiter, endPoint);    }    return new DelimitedInputWriter(fieldToColMapping, delimiter, endPoint, null, serdeSeparator);}
public void flume_f5156_0(Context context)
{    delimiter = parseDelimiterSpec(context.getString(SERIALIZER_DELIMITER, defaultDelimiter));    String fieldNames = context.getString(SERIALIZER_FIELDNAMES);    if (fieldNames == null) {        throw new IllegalArgumentException("serializer.fieldnames is not specified " + "for serializer " + this.getClass().getName());    }    String serdeSeparatorStr = context.getString(SERIALIZER_SERDE_SEPARATOR);    this.serdeSeparator = parseSerdeSeparatorSpec(serdeSeparatorStr);        fieldToColMapping = fieldNames.trim().split(",", -1);}
private static String flume_f5157_0(String delimiter)
{    if (delimiter == null) {        return null;    }    if (delimiter.charAt(0) == '"' && delimiter.charAt(delimiter.length() - 1) == '"') {        return delimiter.substring(1, delimiter.length() - 1);    }    return delimiter;}
private static Character flume_f5158_0(String separatorStr)
{    if (separatorStr == null) {        return null;    }    if (separatorStr.length() == 1) {        return separatorStr.charAt(0);    }    if (separatorStr.length() == 3 && separatorStr.charAt(2) == '\'' && separatorStr.charAt(separatorStr.length() - 1) == '\'') {        return separatorStr.charAt(1);    }    throw new IllegalArgumentException("serializer.serdeSeparator spec is invalid " + "for " + ALIAS + " serializer ");}
public void flume_f5159_0(TransactionBatch txnBatch, Event e) throws StreamingException, IOException, InterruptedException
{    txnBatch.write(e.getBody());}
public void flume_f5160_0(TransactionBatch txnBatch, Collection<byte[]> events) throws StreamingException, IOException, InterruptedException
{    txnBatch.write(events);}
public RecordWriter flume_f5161_0(HiveEndPoint endPoint) throws StreamingException, IOException, ClassNotFoundException
{    return new StrictJsonWriter(endPoint);}
public void flume_f5162_0(Context context)
{    return;}
 Map<HiveEndPoint, HiveWriter> flume_f5163_0()
{    return allWriters;}
public void flume_f5164_1(Context context)
{    metaStoreUri = context.getString(Config.HIVE_METASTORE);    if (metaStoreUri == null) {        throw new IllegalArgumentException(Config.HIVE_METASTORE + " config setting is not " + "specified for sink " + getName());    }    if (metaStoreUri.equalsIgnoreCase("null")) {                metaStoreUri = null;    }        proxyUser = null;    database = context.getString(Config.HIVE_DATABASE);    if (database == null) {        throw new IllegalArgumentException(Config.HIVE_DATABASE + " config setting is not " + "specified for sink " + getName());    }    table = context.getString(Config.HIVE_TABLE);    if (table == null) {        throw new IllegalArgumentException(Config.HIVE_TABLE + " config setting is not " + "specified for sink " + getName());    }    String partitions = context.getString(Config.HIVE_PARTITION);    if (partitions != null) {        partitionVals = Arrays.asList(partitions.split(","));    }    txnsPerBatchAsk = context.getInteger(Config.HIVE_TXNS_PER_BATCH_ASK, DEFAULT_TXNSPERBATCH);    if (txnsPerBatchAsk < 0) {                txnsPerBatchAsk = DEFAULT_TXNSPERBATCH;    }    batchSize = context.getInteger(Config.BATCH_SIZE, DEFAULT_BATCHSIZE);    if (batchSize < 0) {                batchSize = DEFAULT_BATCHSIZE;    }    idleTimeout = context.getInteger(Config.IDLE_TIMEOUT, DEFAULT_IDLETIMEOUT);    if (idleTimeout < 0) {                idleTimeout = DEFAULT_IDLETIMEOUT;    }    callTimeout = context.getInteger(Config.CALL_TIMEOUT, DEFAULT_CALLTIMEOUT);    if (callTimeout < 0) {                callTimeout = DEFAULT_CALLTIMEOUT;    }    heartBeatInterval = context.getInteger(Config.HEART_BEAT_INTERVAL, DEFAULT_HEARTBEATINTERVAL);    if (heartBeatInterval < 0) {                heartBeatInterval = DEFAULT_HEARTBEATINTERVAL;    }    maxOpenConnections = context.getInteger(Config.MAX_OPEN_CONNECTIONS, DEFAULT_MAXOPENCONNECTIONS);    autoCreatePartitions = context.getBoolean("autoCreatePartitions", true);        useLocalTime = context.getBoolean(Config.USE_LOCAL_TIME_STAMP, false);    String tzName = context.getString(Config.TIME_ZONE);    timeZone = (tzName == null) ? null : TimeZone.getTimeZone(tzName);    needRounding = context.getBoolean(Config.ROUND, false);    String unit = context.getString(Config.ROUND_UNIT, Config.MINUTE);    if (unit.equalsIgnoreCase(Config.HOUR)) {        this.roundUnit = Calendar.HOUR_OF_DAY;    } else if (unit.equalsIgnoreCase(Config.MINUTE)) {        this.roundUnit = Calendar.MINUTE;    } else if (unit.equalsIgnoreCase(Config.SECOND)) {        this.roundUnit = Calendar.SECOND;    } else {                needRounding = false;    }    this.roundValue = context.getInteger(Config.ROUND_VALUE, 1);    if (roundUnit == Calendar.SECOND || roundUnit == Calendar.MINUTE) {        Preconditions.checkArgument(roundValue > 0 && roundValue <= 60, "Round value must be > 0 and <= 60");    } else if (roundUnit == Calendar.HOUR_OF_DAY) {        Preconditions.checkArgument(roundValue > 0 && roundValue <= 24, "Round value must be > 0 and <= 24");    }        serializerType = context.getString(Config.SERIALIZER, "");    if (serializerType.isEmpty()) {        throw new IllegalArgumentException("serializer config setting is not " + "specified for sink " + getName());    }    serializer = createSerializer(serializerType);    serializer.configure(context);    Preconditions.checkArgument(batchSize > 0, "batchSize must be greater than 0");    if (sinkCounter == null) {        sinkCounter = new SinkCounter(getName());    }}
protected SinkCounter flume_f5165_0()
{    return sinkCounter;}
private HiveEventSerializer flume_f5166_0(String serializerName)
{    if (serializerName.compareToIgnoreCase(HiveDelimitedTextSerializer.ALIAS) == 0 || serializerName.compareTo(HiveDelimitedTextSerializer.class.getName()) == 0) {        return new HiveDelimitedTextSerializer();    } else if (serializerName.compareToIgnoreCase(HiveJsonSerializer.ALIAS) == 0 || serializerName.compareTo(HiveJsonSerializer.class.getName()) == 0) {        return new HiveJsonSerializer();    }    try {        return (HiveEventSerializer) Class.forName(serializerName).newInstance();    } catch (Exception e) {        throw new IllegalArgumentException("Unable to instantiate serializer: " + serializerName + " on sink: " + getName(), e);    }}
public Status flume_f5167_1() throws EventDeliveryException
{        Channel channel = getChannel();    Transaction transaction = channel.getTransaction();    transaction.begin();    boolean success = false;    try {                if (timeToSendHeartBeat.compareAndSet(true, false)) {            enableHeartBeatOnAllWriters();        }                int txnEventCount = drainOneBatch(channel);        transaction.commit();        success = true;                if (txnEventCount < 1) {            return Status.BACKOFF;        } else {            return Status.READY;        }    } catch (InterruptedException err) {                return Status.BACKOFF;    } catch (Exception e) {        sinkCounter.incrementEventWriteOrChannelFail(e);        throw new EventDeliveryException(e);    } finally {        if (!success) {            transaction.rollback();        }        transaction.close();    }}
private int flume_f5168_1(Channel channel) throws HiveWriter.Failure, InterruptedException
{    int txnEventCount = 0;    try {        Map<HiveEndPoint, HiveWriter> activeWriters = Maps.newHashMap();        for (; txnEventCount < batchSize; ++txnEventCount) {                        Event event = channel.take();            if (event == null) {                break;            }                        HiveEndPoint endPoint = makeEndPoint(metaStoreUri, database, table, partitionVals, event.getHeaders(), timeZone, needRounding, roundUnit, roundValue, useLocalTime);                        HiveWriter writer = getOrCreateWriter(activeWriters, endPoint);                                    writer.write(event);        }                if (txnEventCount == 0) {            sinkCounter.incrementBatchEmptyCount();        } else if (txnEventCount == batchSize) {            sinkCounter.incrementBatchCompleteCount();        } else {            sinkCounter.incrementBatchUnderflowCount();        }        sinkCounter.addToEventDrainAttemptCount(txnEventCount);                for (HiveWriter writer : activeWriters.values()) {            writer.flush(true);        }        sinkCounter.addToEventDrainSuccessCount(txnEventCount);        return txnEventCount;    } catch (HiveWriter.Failure e) {                        abortAllWriters();        closeAllWriters();        throw e;    }}
private void flume_f5169_0()
{    for (HiveWriter writer : allWriters.values()) {        writer.setHearbeatNeeded();    }}
private HiveWriter flume_f5170_1(Map<HiveEndPoint, HiveWriter> activeWriters, HiveEndPoint endPoint) throws HiveWriter.ConnectException, InterruptedException
{    try {        HiveWriter writer = allWriters.get(endPoint);        if (writer == null) {                        writer = new HiveWriter(endPoint, txnsPerBatchAsk, autoCreatePartitions, callTimeout, callTimeoutPool, proxyUser, serializer, sinkCounter);            sinkCounter.incrementConnectionCreatedCount();            if (allWriters.size() > maxOpenConnections) {                int retired = closeIdleWriters();                if (retired == 0) {                    closeEldestWriter();                }            }            allWriters.put(endPoint, writer);            activeWriters.put(endPoint, writer);        } else {            if (activeWriters.get(endPoint) == null) {                activeWriters.put(endPoint, writer);            }        }        return writer;    } catch (HiveWriter.ConnectException e) {        sinkCounter.incrementConnectionFailedCount();        throw e;    }}
private HiveEndPoint flume_f5171_0(String metaStoreUri, String database, String table, List<String> partVals, Map<String, String> headers, TimeZone timeZone, boolean needRounding, int roundUnit, Integer roundValue, boolean useLocalTime)
{    if (partVals == null) {        return new HiveEndPoint(metaStoreUri, database, table, null);    }    ArrayList<String> realPartVals = Lists.newArrayList();    for (String partVal : partVals) {        realPartVals.add(BucketPath.escapeString(partVal, headers, timeZone, needRounding, roundUnit, roundValue, useLocalTime));    }    return new HiveEndPoint(metaStoreUri, database, table, realPartVals);}
private void flume_f5172_1() throws InterruptedException
{    long oldestTimeStamp = System.currentTimeMillis();    HiveEndPoint eldest = null;    for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {        if (entry.getValue().getLastUsed() < oldestTimeStamp) {            eldest = entry.getKey();            oldestTimeStamp = entry.getValue().getLastUsed();        }    }    try {        sinkCounter.incrementConnectionCreatedCount();                allWriters.remove(eldest).close();    } catch (InterruptedException e) {                throw e;    }}
private int flume_f5173_1() throws InterruptedException
{    int count = 0;    long now = System.currentTimeMillis();    ArrayList<HiveEndPoint> retirees = Lists.newArrayList();        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {        if (now - entry.getValue().getLastUsed() > idleTimeout) {            ++count;            retirees.add(entry.getKey());        }    }        for (HiveEndPoint ep : retirees) {        sinkCounter.incrementConnectionClosedCount();                allWriters.remove(ep).close();    }    return count;}
private void flume_f5174_0() throws InterruptedException
{        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {        entry.getValue().close();    }        allWriters.clear();}
private void flume_f5175_0() throws InterruptedException
{    for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {        entry.getValue().abort();    }}
public void flume_f5176_1()
{        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {        try {            HiveWriter w = entry.getValue();            w.close();        } catch (InterruptedException ex) {            Thread.currentThread().interrupt();        }    }        callTimeoutPool.shutdown();    try {        while (callTimeoutPool.isTerminated() == false) {            callTimeoutPool.awaitTermination(Math.max(DEFAULT_CALLTIMEOUT, callTimeout), TimeUnit.MILLISECONDS);        }    } catch (InterruptedException ex) {            }    callTimeoutPool = null;    allWriters.clear();    allWriters = null;    sinkCounter.stop();    super.stop();    }
public void flume_f5177_1()
{    String timeoutName = "hive-" + getName() + "-call-runner-%d";        callTimeoutPool = Executors.newFixedThreadPool(1, new ThreadFactoryBuilder().setNameFormat(timeoutName).build());    this.allWriters = Maps.newHashMap();    sinkCounter.start();    super.start();    setupHeartBeatTimer();    }
private void flume_f5178_0()
{    if (heartBeatInterval > 0) {        heartBeatTimer.schedule(new TimerTask() {            @Override            public void run() {                timeToSendHeartBeat.set(true);                setupHeartBeatTimer();            }        }, heartBeatInterval * 1000);    }}
public void flume_f5179_0()
{    timeToSendHeartBeat.set(true);    setupHeartBeatTimer();}
public long flume_f5180_0()
{    return batchSize;}
public String flume_f5181_0()
{    return "{ Sink type:" + getClass().getSimpleName() + ", name:" + getName() + " }";}
public String flume_f5182_0()
{    return endPoint.toString();}
private void flume_f5183_0()
{    eventCounter = 0;    processSize = 0;    batchCounter = 0;}
 void flume_f5184_0()
{    hearbeatNeeded = true;}
public int flume_f5185_0()
{    return txnBatch.remainingTransactions();}
public synchronized void flume_f5186_0(final Event event) throws WriteException, InterruptedException
{    if (closed) {        throw new IllegalStateException("Writer closed. Cannot write to : " + endPoint);    }    batch.add(event);    if (batch.size() == writeBatchSz) {                writeEventBatchToSerializer();    }        processSize += event.getBody().length;    eventCounter++;}
private void flume_f5187_1() throws InterruptedException, WriteException
{    try {        timedCall(new CallRunner1<Void>() {            @Override            public Void call() throws InterruptedException, StreamingException {                try {                    for (Event event : batch) {                        try {                            serializer.write(txnBatch, event);                        } catch (SerializationError err) {                                                    }                    }                    return null;                } catch (IOException e) {                    throw new StreamingIOFailure(e.getMessage(), e);                }            }        });        batch.clear();    } catch (StreamingException e) {        throw new WriteException(endPoint, txnBatch.getCurrentTxnId(), e);    } catch (TimeoutException e) {        throw new WriteException(endPoint, txnBatch.getCurrentTxnId(), e);    }}
public Void flume_f5188_1() throws InterruptedException, StreamingException
{    try {        for (Event event : batch) {            try {                serializer.write(txnBatch, event);            } catch (SerializationError err) {                            }        }        return null;    } catch (IOException e) {        throw new StreamingIOFailure(e.getMessage(), e);    }}
public void flume_f5189_1(boolean rollToNext) throws CommitException, TxnBatchException, TxnFailure, InterruptedException, WriteException
{    if (!batch.isEmpty()) {        writeEventBatchToSerializer();        batch.clear();    }        if (hearbeatNeeded) {        hearbeatNeeded = false;        heartBeat();    }    lastUsed = System.currentTimeMillis();    try {                commitTxn();        if (txnBatch.remainingTransactions() == 0) {            closeTxnBatch();            txnBatch = null;            if (rollToNext) {                txnBatch = nextTxnBatch(recordWriter);            }        }                if (rollToNext) {                                    txnBatch.beginNextTransaction();        }    } catch (StreamingException e) {        throw new TxnFailure(txnBatch, e);    }}
public void flume_f5190_0() throws InterruptedException
{    batch.clear();    abortTxn();}
public void flume_f5191_1() throws InterruptedException
{        try {        timedCall(new CallRunner1<Void>() {            @Override            public Void call() throws StreamingException {                                txnBatch.heartbeat();                return null;            }        });    } catch (InterruptedException e) {        throw e;    } catch (Exception e) {                }}
public Void flume_f5192_1() throws StreamingException
{        txnBatch.heartbeat();    return null;}
public void flume_f5193_0() throws InterruptedException
{    batch.clear();    abortRemainingTxns();    closeTxnBatch();    closeConnection();    closed = true;}
private void flume_f5194_1() throws InterruptedException
{    try {        if (!isClosed(txnBatch.getCurrentTransactionState())) {            abortCurrTxnHelper();        }                if (txnBatch.remainingTransactions() > 0) {            timedCall(new CallRunner1<Void>() {                @Override                public Void call() throws StreamingException, InterruptedException {                    txnBatch.beginNextTransaction();                    return null;                }            });            abortRemainingTxns();        }    } catch (StreamingException e) {                return;    } catch (TimeoutException e) {                return;    }}
public Void flume_f5195_0() throws StreamingException, InterruptedException
{    txnBatch.beginNextTransaction();    return null;}
private void flume_f5196_1() throws TimeoutException, InterruptedException
{    try {        timedCall(new CallRunner1<Void>() {            @Override            public Void call() throws StreamingException, InterruptedException {                txnBatch.abort();                                return null;            }        });    } catch (StreamingException e) {                }}
public Void flume_f5197_1() throws StreamingException, InterruptedException
{    txnBatch.abort();        return null;}
private boolean flume_f5198_0(TransactionBatch.TxnState txnState)
{    if (txnState == TransactionBatch.TxnState.COMMITTED) {        return true;    }    if (txnState == TransactionBatch.TxnState.ABORTED) {        return true;    }    return false;}
public void flume_f5199_1() throws InterruptedException
{        try {        timedCall(new CallRunner1<Void>() {            @Override            public Void call() {                                connection.close();                return null;            }        });        sinkCounter.incrementConnectionClosedCount();    } catch (Exception e) {                }}
public Void flume_f5200_0()
{        connection.close();    return null;}
private void flume_f5201_1() throws CommitException, InterruptedException
{    if (LOG.isInfoEnabled()) {            }    try {        timedCall(new CallRunner1<Void>() {            @Override            public Void call() throws StreamingException, InterruptedException {                                txnBatch.commit();                return null;            }        });    } catch (Exception e) {        throw new CommitException(endPoint, txnBatch.getCurrentTxnId(), e);    }}
public Void flume_f5202_0() throws StreamingException, InterruptedException
{        txnBatch.commit();    return null;}
private void flume_f5203_1() throws InterruptedException
{        try {        timedCall(new CallRunner1<Void>() {            @Override            public Void call() throws StreamingException, InterruptedException {                                txnBatch.abort();                return null;            }        });    } catch (InterruptedException e) {        throw e;    } catch (TimeoutException e) {            } catch (Exception e) {                }}
public Void flume_f5204_0() throws StreamingException, InterruptedException
{        txnBatch.abort();    return null;}
private StreamingConnection flume_f5205_0(final String proxyUser) throws InterruptedException, ConnectException
{    try {        return timedCall(new CallRunner1<StreamingConnection>() {            @Override            public StreamingConnection call() throws InterruptedException, StreamingException {                                return endPoint.newConnection(autoCreatePartitions);            }        });    } catch (Exception e) {        throw new ConnectException(endPoint, e);    }}
public StreamingConnection flume_f5206_0() throws InterruptedException, StreamingException
{        return endPoint.newConnection(autoCreatePartitions);}
private TransactionBatch flume_f5207_1(final RecordWriter recordWriter) throws InterruptedException, TxnBatchException
{        TransactionBatch batch = null;    try {        batch = timedCall(new CallRunner1<TransactionBatch>() {            @Override            public TransactionBatch call() throws InterruptedException, StreamingException {                                return connection.fetchTransactionBatch(txnsPerBatch, recordWriter);            }        });            } catch (Exception e) {        throw new TxnBatchException(endPoint, e);    }    return batch;}
public TransactionBatch flume_f5208_0() throws InterruptedException, StreamingException
{        return connection.fetchTransactionBatch(txnsPerBatch, recordWriter);}
private void flume_f5209_1() throws InterruptedException
{    try {                timedCall(new CallRunner1<Void>() {            @Override            public Void call() throws InterruptedException, StreamingException {                                txnBatch.close();                return null;            }        });    } catch (InterruptedException e) {        throw e;    } catch (Exception e) {                }}
public Void flume_f5210_0() throws InterruptedException, StreamingException
{        txnBatch.close();    return null;}
private T flume_f5211_0(final CallRunner1<T> callRunner) throws TimeoutException, InterruptedException, StreamingException
{    Future<T> future = callTimeoutPool.submit(new Callable<T>() {        @Override        public T call() throws StreamingException, InterruptedException, Failure {            return callRunner.call();        }    });    try {        if (callTimeout > 0) {            return future.get(callTimeout, TimeUnit.MILLISECONDS);        } else {            return future.get();        }    } catch (TimeoutException eT) {        future.cancel(true);        sinkCounter.incrementConnectionFailedCount();        throw eT;    } catch (ExecutionException e1) {        sinkCounter.incrementConnectionFailedCount();        Throwable cause = e1.getCause();        if (cause instanceof IOException) {            throw new StreamingException("I/O Failure", (IOException) cause);        } else if (cause instanceof StreamingException) {            throw (StreamingException) cause;        } else if (cause instanceof TimeoutException) {            throw new StreamingException("Operation Timed Out.", (TimeoutException) cause);        } else if (cause instanceof RuntimeException) {            throw (RuntimeException) cause;        } else if (cause instanceof InterruptedException) {            throw (InterruptedException) cause;        }        throw new StreamingException(e1.getMessage(), e1);    }}
public T flume_f5212_0() throws StreamingException, InterruptedException, Failure
{    return callRunner.call();}
 long flume_f5213_0()
{    return lastUsed;}
public void flume_f5214_0() throws Exception
{    TestUtil.dropDB(conf, dbName);    sink = new HiveSink();    sink.setName("HiveSink-" + UUID.randomUUID().toString());    String dbLocation = dbFolder.newFolder(dbName).getCanonicalPath() + ".db";        dbLocation = dbLocation.replaceAll("\\\\", "/");    TestUtil.createDbAndTable(driver, dbName, tblName, partitionVals, colNames, colTypes, partNames, dbLocation);}
public void flume_f5215_0() throws MetaException, HiveException
{    TestUtil.dropDB(conf, dbName);}
public void flume_f5216_0(boolean partitioned, String dbName, String tblName, Channel pChannel) throws Exception
{    int totalRecords = 4;    int batchSize = 2;    int batchCount = totalRecords / batchSize;    Context context = new Context();    context.put("hive.metastore", metaStoreURI);    context.put("hive.database", dbName);    context.put("hive.table", tblName);    if (partitioned) {        context.put("hive.partition", PART1_VALUE + "," + PART2_VALUE);    }    context.put("autoCreatePartitions", "false");    context.put("batchSize", "" + batchSize);    context.put("serializer", HiveDelimitedTextSerializer.ALIAS);    context.put("serializer.fieldnames", COL1 + ",," + COL2 + ",");    context.put("heartBeatInterval", "0");    Channel channel = startSink(sink, context, pChannel);    List<String> bodies = Lists.newArrayList();        Transaction txn = channel.getTransaction();    txn.begin();    for (int j = 1; j <= totalRecords; j++) {        Event event = new SimpleEvent();        String body = j + ",blah,This is a log message,other stuff";        event.setBody(body.getBytes());        bodies.add(body);        channel.put(event);    }        txn.commit();    txn.close();    checkRecordCountInTable(0, dbName, tblName);    for (int i = 0; i < batchCount; i++) {        sink.process();    }    checkRecordCountInTable(totalRecords, dbName, tblName);    sink.stop();    checkRecordCountInTable(totalRecords, dbName, tblName);}
public void flume_f5217_0() throws Exception
{    testSingleWriter(true, dbName, tblName, null);}
public void flume_f5218_0() throws Exception
{    TestUtil.dropDB(conf, dbName2);    String dbLocation = dbFolder.newFolder(dbName2).getCanonicalPath() + ".db";        dbLocation = dbLocation.replaceAll("\\\\", "/");    TestUtil.createDbAndTable(driver, dbName2, tblName2, null, colNames2, colTypes2, null, dbLocation);    try {        testSingleWriter(false, dbName2, tblName2, null);    } finally {        TestUtil.dropDB(conf, dbName2);    }}
public void flume_f5219_0() throws Exception
{    String[] colNames = { COL1, COL2 };    String PART1_NAME = "country";    String PART2_NAME = "hour";    String[] partNames = { PART1_NAME, PART2_NAME };    List<String> partitionVals = null;    String PART1_VALUE = "%{" + PART1_NAME + "}";    String PART2_VALUE = "%y-%m-%d-%k";    partitionVals = new ArrayList<String>(2);    partitionVals.add(PART1_VALUE);    partitionVals.add(PART2_VALUE);    String tblName = "hourlydata";    TestUtil.dropDB(conf, dbName2);    String dbLocation = dbFolder.newFolder(dbName2).getCanonicalPath() + ".db";        dbLocation = dbLocation.replaceAll("\\\\", "/");    TestUtil.createDbAndTable(driver, dbName2, tblName, partitionVals, colNames, colTypes, partNames, dbLocation);    int totalRecords = 4;    int batchSize = 2;    int batchCount = totalRecords / batchSize;    Context context = new Context();    context.put("hive.metastore", metaStoreURI);    context.put("hive.database", dbName2);    context.put("hive.table", tblName);    context.put("hive.partition", PART1_VALUE + "," + PART2_VALUE);    context.put("autoCreatePartitions", "true");    context.put("useLocalTimeStamp", "false");    context.put("batchSize", "" + batchSize);    context.put("serializer", HiveDelimitedTextSerializer.ALIAS);    context.put("serializer.fieldnames", COL1 + ",," + COL2 + ",");    context.put("heartBeatInterval", "0");    Channel channel = startSink(sink, context);    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        Transaction txn = channel.getTransaction();    txn.begin();    for (int j = 1; j <= totalRecords; j++) {        Event event = new SimpleEvent();        String body = j + ",blah,This is a log message,other stuff";        event.setBody(body.getBytes());        eventDate.clear();                eventDate.set(2014, 03, 03, j % batchCount, 1);        event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));        event.getHeaders().put(PART1_NAME, "Asia");        bodies.add(body);        channel.put(event);    }        txn.commit();    txn.close();    checkRecordCountInTable(0, dbName2, tblName);    for (int i = 0; i < batchCount; i++) {        sink.process();    }    checkRecordCountInTable(totalRecords, dbName2, tblName);    sink.stop();        SinkCounter counter = sink.getCounter();    Assert.assertEquals(2, counter.getConnectionCreatedCount());    Assert.assertEquals(2, counter.getConnectionClosedCount());    Assert.assertEquals(2, counter.getBatchCompleteCount());    Assert.assertEquals(0, counter.getBatchEmptyCount());    Assert.assertEquals(0, counter.getConnectionFailedCount());    Assert.assertEquals(4, counter.getEventDrainAttemptCount());    Assert.assertEquals(4, counter.getEventDrainSuccessCount());}
public void flume_f5220_0() throws EventDeliveryException, IOException, CommandNeedRetryException
{    int batchSize = 2;    int batchCount = 3;    int totalRecords = batchCount * batchSize;    Context context = new Context();    context.put("hive.metastore", metaStoreURI);    context.put("hive.database", dbName);    context.put("hive.table", tblName);    context.put("hive.partition", PART1_VALUE + "," + PART2_VALUE);    context.put("autoCreatePartitions", "true");    context.put("batchSize", "" + batchSize);    context.put("serializer", HiveDelimitedTextSerializer.ALIAS);    context.put("serializer.fieldnames", COL1 + ",," + COL2 + ",");    context.put("hive.txnsPerBatchAsk", "20");        context.put("heartBeatInterval", "3");    Channel channel = startSink(sink, context);    List<String> bodies = Lists.newArrayList();        for (int i = 0; i < batchCount; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (int j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            String body = i * j + ",blah,This is a log message,other stuff";            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);        }                txn.commit();        txn.close();        sink.process();                sleep(3000);    }    sink.stop();    checkRecordCountInTable(totalRecords, dbName, tblName);}
public void flume_f5221_0() throws Exception
{    int batchSize = 2;    int batchCount = 2;    int totalRecords = batchCount * batchSize;    Context context = new Context();    context.put("hive.metastore", metaStoreURI);    context.put("hive.database", dbName);    context.put("hive.table", tblName);    context.put("hive.partition", PART1_VALUE + "," + PART2_VALUE);    context.put("autoCreatePartitions", "true");    context.put("batchSize", "" + batchSize);    context.put("serializer", HiveJsonSerializer.ALIAS);    context.put("serializer.fieldnames", COL1 + ",," + COL2 + ",");    context.put("heartBeatInterval", "0");    Channel channel = startSink(sink, context);    List<String> bodies = Lists.newArrayList();        for (int i = 0; i < batchCount; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (int j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            String body = "{\"id\" : 1, \"msg\" : \"using json serializer\"}";            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);        }                txn.commit();        txn.close();        sink.process();    }    checkRecordCountInTable(totalRecords, dbName, tblName);    sink.stop();    checkRecordCountInTable(totalRecords, dbName, tblName);}
public void flume_f5222_0() throws Exception
{    Channel channel = Mockito.mock(Channel.class);    Mockito.when(channel.take()).thenThrow(new ChannelException("dummy"));    Transaction transaction = Mockito.mock(BasicTransactionSemantics.class);    Mockito.when(channel.getTransaction()).thenReturn(transaction);    try {        testSingleWriter(true, dbName, tblName, channel);    } catch (EventDeliveryException e) {        }    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sinkCounter.getChannelReadFail());}
private void flume_f5223_0(int n)
{    try {        Thread.sleep(n);    } catch (InterruptedException e) {    }}
private static Channel flume_f5224_0(HiveSink sink, Context context)
{    return startSink(sink, context, null);}
private static Channel flume_f5225_0(HiveSink sink, Context context, Channel pChannel)
{    Configurables.configure(sink, context);    Channel channel = pChannel == null ? new MemoryChannel() : pChannel;    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    return channel;}
private void flume_f5226_0(int expectedCount, String db, String tbl) throws CommandNeedRetryException, IOException
{    int count = TestUtil.listRecordsInTable(driver, db, tbl).size();    Assert.assertEquals(expectedCount, count);}
public void flume_f5227_0() throws Exception
{        TxnDbUtil.cleanDb();    TxnDbUtil.prepDb();        TestUtil.dropDB(conf, dbName);    String dbLocation = dbFolder.newFolder(dbName).getCanonicalPath() + ".db";        dbLocation = dbLocation.replaceAll("\\\\", "/");    TestUtil.createDbAndTable(driver, dbName, tblName, partVals, colNames, colTypes, partNames, dbLocation);        Context ctx = new Context();    ctx.put("serializer.fieldnames", COL1 + ",," + COL2 + ",");    serializer = new HiveDelimitedTextSerializer();    serializer.configure(ctx);}
public void flume_f5228_0() throws Exception
{    HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    SinkCounter sinkCounter = new SinkCounter(this.getClass().getName());    HiveWriter writer = new HiveWriter(endPoint, 10, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter);    writer.close();}
public void flume_f5229_0() throws Exception
{    HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    SinkCounter sinkCounter = new SinkCounter(this.getClass().getName());    HiveWriter writer = new HiveWriter(endPoint, 10, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter);    writeEvents(writer, 3);    writer.flush(false);    writer.close();    checkRecordCountInTable(3);}
public void flume_f5230_0() throws Exception
{    HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    SinkCounter sinkCounter = new SinkCounter(this.getClass().getName());    HiveWriter writer = new HiveWriter(endPoint, 10, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter);    checkRecordCountInTable(0);    SimpleEvent event = new SimpleEvent();    String REC1 = "1,xyz,Hello world,abc";    event.setBody(REC1.getBytes());    writer.write(event);    checkRecordCountInTable(0);    writer.flush(true);    checkRecordCountInTable(1);    String REC2 = "2,xyz,Hello world,abc";    event.setBody(REC2.getBytes());    writer.write(event);    checkRecordCountInTable(1);    writer.flush(true);    checkRecordCountInTable(2);    String REC3 = "3,xyz,Hello world,abc";    event.setBody(REC3.getBytes());    writer.write(event);    writer.flush(true);    checkRecordCountInTable(3);    writer.close();    checkRecordCountInTable(3);}
public void flume_f5231_0() throws Exception
{            HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    SinkCounter sinkCounter = new SinkCounter(this.getClass().getName());    int txnPerBatch = 3;    HiveWriter writer = new HiveWriter(endPoint, txnPerBatch, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter);    Assert.assertEquals(writer.getRemainingTxns(), 2);    writer.flush(true);    Assert.assertEquals(writer.getRemainingTxns(), 1);    writer.flush(true);    Assert.assertEquals(writer.getRemainingTxns(), 0);    writer.flush(true);        Assert.assertEquals(writer.getRemainingTxns(), 2);    writer.flush(true);    Assert.assertEquals(writer.getRemainingTxns(), 1);    writer.close();}
private void flume_f5232_0(int expectedCount) throws CommandNeedRetryException, IOException
{    int count = TestUtil.listRecordsInTable(driver, dbName, tblName).size();    Assert.assertEquals(expectedCount, count);}
public void flume_f5233_0() throws Exception
{    HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    SinkCounter sinkCounter = new SinkCounter(this.getClass().getName());        int timeout = 5000;    HiveDelimitedTextSerializer serializer2 = new HiveDelimitedTextSerializer();    Context ctx = new Context();    ctx.put("serializer.fieldnames", COL1 + "," + COL2);    ctx.put("serializer.serdeSeparator", ",");    serializer2.configure(ctx);    HiveWriter writer = new HiveWriter(endPoint, 10, true, timeout, callTimeoutPool, "flumetest", serializer2, sinkCounter);    SimpleEvent event = new SimpleEvent();    event.setBody("1,Hello world 1".getBytes());    writer.write(event);    event.setBody("2,Hello world 2".getBytes());    writer.write(event);    event.setBody("3,Hello world 3".getBytes());    writer.write(event);    writer.flush(false);    writer.close();}
public void flume_f5234_0() throws Exception
{    HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    SinkCounter sinkCounter = new SinkCounter(this.getClass().getName());        int timeout = 10000;        HiveDelimitedTextSerializer serializer1 = new HiveDelimitedTextSerializer();    Context ctx = new Context();    ctx.put("serializer.fieldnames", COL1 + "," + COL2);    ctx.put("serializer.serdeSeparator", ",");    serializer1.configure(ctx);            HiveDelimitedTextSerializer serializer2 = new HiveDelimitedTextSerializer();    ctx = new Context();    ctx.put("serializer.fieldnames", COL1 + "," + COL2);    ctx.put("serializer.serdeSeparator", "'\t'");    serializer2.configure(ctx);            HiveDelimitedTextSerializer serializer3 = new HiveDelimitedTextSerializer();    ctx = new Context();    ctx.put("serializer.fieldnames", COL1 + "," + COL2);    ctx.put("serializer.serdeSeparator", "ab");    try {        serializer3.configure(ctx);        Assert.assertTrue("Bad serdeSeparator character was accepted", false);    } catch (Exception e) {        }}
public void flume_f5235_0() throws Exception
{        HiveEndPoint endPoint1 = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    ArrayList<String> partVals2 = new ArrayList<String>(2);    partVals2.add(PART1_VALUE);    partVals2.add("Nepal");    HiveEndPoint endPoint2 = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals2);    SinkCounter sinkCounter1 = new SinkCounter(this.getClass().getName());    SinkCounter sinkCounter2 = new SinkCounter(this.getClass().getName());    HiveWriter writer1 = new HiveWriter(endPoint1, 10, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter1);    writeEvents(writer1, 3);    HiveWriter writer2 = new HiveWriter(endPoint2, 10, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter2);    writeEvents(writer2, 3);        writer2.flush(false);        writer1.flush(false);    writer1.close();    writer2.close();}
public void flume_f5236_0() throws Exception
{        HiveEndPoint endPoint1 = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    ArrayList<String> partVals2 = new ArrayList<String>(2);    partVals2.add(PART1_VALUE);    partVals2.add("Nepal");    HiveEndPoint endPoint2 = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals2);    SinkCounter sinkCounter1 = new SinkCounter(this.getClass().getName());    SinkCounter sinkCounter2 = new SinkCounter(this.getClass().getName());    HiveWriter writer1 = new HiveWriter(endPoint1, 10, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter1);    writeEvents(writer1, 3);        writer1.flush(false);    HiveWriter writer2 = new HiveWriter(endPoint2, 10, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter2);    writeEvents(writer2, 3);        writer2.flush(false);    writer1.close();    writer2.close();}
private void flume_f5237_0(HiveWriter writer, int count) throws InterruptedException, HiveWriter.WriteException
{    SimpleEvent event = new SimpleEvent();    for (int i = 1; i <= count; i++) {        event.setBody((i + ",xyz,Hello world,abc").getBytes());        writer.write(event);    }}
public static void flume_f5238_0(HiveConf conf)
{    conf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, txnMgr);    conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, true);    conf.set("fs.raw.impl", RawFileSystem.class.getName());}
public static void flume_f5239_0(Driver driver, String databaseName, String tableName, List<String> partVals, String[] colNames, String[] colTypes, String[] partNames, String dbLocation) throws Exception
{    String dbUri = "raw://" + dbLocation;    String tableLoc = dbUri + Path.SEPARATOR + tableName;    runDDL(driver, "create database IF NOT EXISTS " + databaseName + " location '" + dbUri + "'");    runDDL(driver, "use " + databaseName);    String crtTbl = "create table " + tableName + " ( " + getTableColumnsStr(colNames, colTypes) + " )" + getPartitionStmtStr(partNames) + " clustered by ( " + colNames[0] + " )" + " into 10 buckets " + " stored as orc " + " location '" + tableLoc + "'" + " TBLPROPERTIES ('transactional'='true')";    runDDL(driver, crtTbl);    System.out.println("crtTbl = " + crtTbl);    if (partNames != null && partNames.length != 0) {        String addPart = "alter table " + tableName + " add partition ( " + getTablePartsStr2(partNames, partVals) + " )";        runDDL(driver, addPart);    }}
private static String flume_f5240_0(String[] partNames)
{    if (partNames == null || partNames.length == 0) {        return "";    }    return " partitioned by (" + getTablePartsStr(partNames) + " )";}
public static void flume_f5241_0(HiveConf conf, String databaseName) throws HiveException, MetaException
{    IMetaStoreClient client = new HiveMetaStoreClient(conf);    try {        for (String table : client.listTableNamesByFilter(databaseName, "", (short) -1)) {            client.dropTable(databaseName, table, true, true);        }        client.dropDatabase(databaseName);    } catch (TException e) {        client.close();    }}
private static String flume_f5242_0(String[] colNames, String[] colTypes)
{    StringBuffer sb = new StringBuffer();    for (int i = 0; i < colNames.length; ++i) {        sb.append(colNames[i] + " " + colTypes[i]);        if (i < colNames.length - 1) {            sb.append(",");        }    }    return sb.toString();}
private static String flume_f5243_0(String[] partNames)
{    if (partNames == null || partNames.length == 0) {        return "";    }    StringBuffer sb = new StringBuffer();    for (int i = 0; i < partNames.length; ++i) {        sb.append(partNames[i] + " string");        if (i < partNames.length - 1) {            sb.append(",");        }    }    return sb.toString();}
private static String flume_f5244_0(String[] partNames, List<String> partVals)
{    StringBuffer sb = new StringBuffer();    for (int i = 0; i < partVals.size(); ++i) {        sb.append(partNames[i] + " = '" + partVals.get(i) + "'");        if (i < partVals.size() - 1) {            sb.append(",");        }    }    return sb.toString();}
public static ArrayList<String> flume_f5245_0(Driver driver, String dbName, String tblName) throws CommandNeedRetryException, IOException
{    driver.run("select * from " + dbName + "." + tblName);    ArrayList<String> res = new ArrayList<String>();    driver.getResults(res);    return res;}
public static ArrayList<String> flume_f5246_0(Driver driver, String dbName, String tblName, String continent, String country) throws CommandNeedRetryException, IOException
{    driver.run("select * from " + dbName + "." + tblName + " where continent='" + continent + "' and country='" + country + "'");    ArrayList<String> res = new ArrayList<String>();    driver.getResults(res);    return res;}
public URI flume_f5247_0()
{    return NAME;}
 static String flume_f5248_0(File f, String... cmd) throws IOException
{    String[] args = new String[cmd.length + 1];    System.arraycopy(cmd, 0, args, 0, cmd.length);    args[cmd.length] = f.getCanonicalPath();    String output = Shell.execCommand(args);    return output;}
public FileStatus flume_f5249_0(Path path) throws IOException
{    File file = pathToFile(path);    if (!file.exists()) {        throw new FileNotFoundException("Can't find " + path);    }        short mod = 0;    if (file.canRead()) {        mod |= 0444;    }    if (file.canWrite()) {        mod |= 0200;    }    if (file.canExecute()) {        mod |= 0111;    }    ShimLoader.getHadoopShims();    return new FileStatus(file.length(), file.isDirectory(), 1, 1024, file.lastModified(), file.lastModified(), FsPermission.createImmutable(mod), "owen", "users", path);}
private static boolean flume_f5250_0(Driver driver, String sql) throws QueryFailedException
{        int retryCount = 1;    for (int attempt = 0; attempt <= retryCount; ++attempt) {        try {            driver.run(sql);            return true;        } catch (CommandNeedRetryException e) {            if (attempt == retryCount) {                throw new QueryFailedException(sql, e);            }            continue;        }    }        return false;}
public final void flume_f5251_1(final Context context)
{    String configuredEndpoint = context.getString("endpoint", "");        try {        endpointUrl = new URL(configuredEndpoint);    } catch (MalformedURLException e) {        throw new IllegalArgumentException("Endpoint URL invalid", e);    }    connectTimeout = context.getInteger("connectTimeout", DEFAULT_CONNECT_TIMEOUT);    if (connectTimeout <= 0) {        throw new IllegalArgumentException("Connect timeout must be a non-zero and positive");    }        requestTimeout = context.getInteger("requestTimeout", DEFAULT_REQUEST_TIMEOUT);    if (requestTimeout <= 0) {        throw new IllegalArgumentException("Request timeout must be a non-zero and positive");    }        acceptHeader = context.getString("acceptHeader", DEFAULT_ACCEPT_HEADER);        contentTypeHeader = context.getString("contentTypeHeader", DEFAULT_CONTENT_TYPE);        defaultBackoff = context.getBoolean("defaultBackoff", true);        defaultRollback = context.getBoolean("defaultRollback", true);        defaultIncrementMetrics = context.getBoolean("defaultIncrementMetrics", false);        parseConfigOverrides("backoff", context, backoffOverrides);    parseConfigOverrides("rollback", context, rollbackOverrides);    parseConfigOverrides("incrementMetrics", context, incrementMetricsOverrides);    if (this.sinkCounter == null) {        this.sinkCounter = new SinkCounter(this.getName());    }    connectionBuilder = new ConnectionBuilder();}
public final void flume_f5252_1()
{        sinkCounter.start();}
public final void flume_f5253_1()
{        sinkCounter.stop();}
public final Status flume_f5254_1() throws EventDeliveryException
{    Status status = null;    OutputStream outputStream = null;    Channel ch = getChannel();    Transaction txn = ch.getTransaction();    txn.begin();    try {        Event event = ch.take();        byte[] eventBody = null;        if (event != null) {            eventBody = event.getBody();        }        if (eventBody != null && eventBody.length > 0) {            sinkCounter.incrementEventDrainAttemptCount();                        try {                HttpURLConnection connection = connectionBuilder.getConnection();                outputStream = connection.getOutputStream();                outputStream.write(eventBody);                outputStream.flush();                outputStream.close();                int httpStatusCode = connection.getResponseCode();                                if (httpStatusCode < HttpURLConnection.HTTP_BAD_REQUEST) {                    connection.getInputStream().close();                } else {                                        connection.getErrorStream().close();                }                                if (httpStatusCode >= HTTP_STATUS_CONTINUE) {                    String httpStatusString = String.valueOf(httpStatusCode);                    boolean shouldRollback = findOverrideValue(httpStatusString, rollbackOverrides, defaultRollback);                    if (shouldRollback) {                        txn.rollback();                    } else {                        txn.commit();                    }                    boolean shouldBackoff = findOverrideValue(httpStatusString, backoffOverrides, defaultBackoff);                    if (shouldBackoff) {                        status = Status.BACKOFF;                    } else {                        status = Status.READY;                    }                    boolean shouldIncrementMetrics = findOverrideValue(httpStatusString, incrementMetricsOverrides, defaultIncrementMetrics);                    if (shouldIncrementMetrics) {                        sinkCounter.incrementEventDrainSuccessCount();                    }                    if (shouldRollback) {                        if (shouldBackoff) {                                                    } else {                                                    }                    }                } else {                    txn.rollback();                    status = Status.BACKOFF;                                    }            } catch (IOException e) {                txn.rollback();                status = Status.BACKOFF;                                sinkCounter.incrementEventWriteFail();            }        } else {            txn.commit();            status = Status.BACKOFF;                    }    } catch (Throwable t) {        txn.rollback();        status = Status.BACKOFF;                sinkCounter.incrementEventWriteOrChannelFail(t);                if (t instanceof Error) {            throw (Error) t;        }    } finally {        txn.close();        if (outputStream != null) {            try {                outputStream.close();            } catch (IOException e) {                        }        }    }    return status;}
private void flume_f5255_1(final String propertyName, final Context context, final Map<String, Boolean> override)
{    Map<String, String> config = context.getSubProperties(propertyName + ".");    if (config != null) {        for (Map.Entry<String, String> value : config.entrySet()) {                        if (override.containsKey(value.getKey())) {                            } else {                override.put(value.getKey(), Boolean.valueOf(value.getValue()));            }        }    }}
private boolean flume_f5256_0(final String statusCode, final HashMap<String, Boolean> overrides, final boolean defaultValue)
{    Boolean overrideValue = overrides.get(statusCode);    if (overrideValue == null) {        overrideValue = overrides.get(statusCode.substring(0, 1) + "XX");        if (overrideValue == null) {            overrideValue = defaultValue;        }    }    return overrideValue;}
 final void flume_f5257_0(final ConnectionBuilder builder)
{    this.connectionBuilder = builder;}
 final void flume_f5258_0(final SinkCounter newSinkCounter)
{    this.sinkCounter = newSinkCounter;}
public HttpURLConnection flume_f5259_0() throws IOException
{    HttpURLConnection connection = (HttpURLConnection) endpointUrl.openConnection();    connection.setRequestMethod("POST");    connection.setRequestProperty("Content-Type", contentTypeHeader);    connection.setRequestProperty("Accept", acceptHeader);    connection.setConnectTimeout(connectTimeout);    connection.setReadTimeout(requestTimeout);    connection.setDoOutput(true);    connection.setDoInput(true);    connection.connect();    return connection;}
public void flume_f5260_0()
{    whenDefaultStringConfig();    whenDefaultBooleanConfig();    when(configContext.getInteger(eq("connectTimeout"), Mockito.anyInt())).thenReturn(1000);    when(configContext.getInteger(eq("requestTimeout"), Mockito.anyInt())).thenReturn(1000);    new HttpSink().configure(configContext);    verify(configContext).getString("endpoint", "");    verify(configContext).getInteger(eq("connectTimeout"), Mockito.anyInt());    verify(configContext).getInteger(eq("requestTimeout"), Mockito.anyInt());    verify(configContext).getString(eq("acceptHeader"), Mockito.anyString());    verify(configContext).getString(eq("contentTypeHeader"), Mockito.anyString());    verify(configContext).getBoolean("defaultBackoff", true);    verify(configContext).getBoolean("defaultRollback", true);    verify(configContext).getBoolean("defaultIncrementMetrics", false);}
public void flume_f5261_0()
{    when(configContext.getString("endpoint", "")).thenReturn("");    new HttpSink().configure(configContext);}
public void flume_f5262_0()
{    when(configContext.getString("endpoint", "")).thenReturn("invalid url");    new HttpSink().configure(configContext);}
public void flume_f5263_0()
{    whenDefaultStringConfig();    when(configContext.getInteger("connectTimeout", 1000)).thenReturn(-1000);    when(configContext.getInteger(eq("requestTimeout"), Mockito.anyInt())).thenReturn(1000);    new HttpSink().configure(configContext);}
public void flume_f5264_0()
{    whenDefaultStringConfig();    when(configContext.getInteger("connectTimeout", DEFAULT_CONNECT_TIMEOUT)).thenReturn(1000);    when(configContext.getInteger(eq("requestTimeout"), Mockito.anyInt())).thenReturn(1000);    new HttpSink().configure(configContext);    verify(configContext).getInteger("connectTimeout", DEFAULT_CONNECT_TIMEOUT);}
public void flume_f5265_0()
{    whenDefaultStringConfig();    when(configContext.getInteger("requestTimeout", 1000)).thenReturn(-1000);    when(configContext.getInteger(eq("connectTimeout"), Mockito.anyInt())).thenReturn(1000);    new HttpSink().configure(configContext);}
public void flume_f5266_0()
{    whenDefaultStringConfig();    when(configContext.getInteger("requestTimeout", DEFAULT_REQUEST_TIMEOUT)).thenReturn(1000);    when(configContext.getInteger(eq("connectTimeout"), Mockito.anyInt())).thenReturn(1000);    new HttpSink().configure(configContext);    verify(configContext).getInteger("requestTimeout", DEFAULT_REQUEST_TIMEOUT);}
public void flume_f5267_0()
{    whenDefaultTimeouts();    whenDefaultStringConfig();    new HttpSink().configure(configContext);    verify(configContext).getString("acceptHeader", DEFAULT_ACCEPT_HEADER);}
public void flume_f5268_0()
{    whenDefaultTimeouts();    whenDefaultStringConfig();    new HttpSink().configure(configContext);    verify(configContext).getString("contentTypeHeader", DEFAULT_CONTENT_TYPE_HEADER);}
public void flume_f5269_0() throws Exception
{    when(channel.take()).thenReturn(null);    executeWithMocks(true);}
public void flume_f5270_0() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn(null);    executeWithMocks(true);}
public void flume_f5271_0() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn(new byte[] {});    executeWithMocks(true);}
public void flume_f5272_0() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn("something".getBytes());    Context context = new Context();    context.put("defaultRollback", "true");    context.put("defaultBackoff", "true");    context.put("defaultIncrementMetrics", "true");    executeWithMocks(false, Status.BACKOFF, true, true, context, HttpURLConnection.HTTP_OK);}
public void flume_f5273_0() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn("something".getBytes());    Context context = new Context();    context.put("defaultRollback", "false");    context.put("defaultBackoff", "false");    context.put("defaultIncrementMetrics", "false");    executeWithMocks(true, Status.READY, false, false, context, HttpURLConnection.HTTP_OK);}
public void flume_f5274_0() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn("something".getBytes());    Context context = new Context();    context.put("defaultRollback", "true");    context.put("defaultBackoff", "true");    context.put("defaultIncrementMetrics", "false");    context.put("rollback.200", "false");    context.put("backoff.200", "false");    context.put("incrementMetrics.200", "true");    executeWithMocks(true, Status.READY, true, true, context, HttpURLConnection.HTTP_OK);}
public void flume_f5275_0() throws Exception
{    RuntimeException exception = new RuntimeException("dummy");    when(channel.take()).thenThrow(exception);    Context context = new Context();    context.put("defaultRollback", "false");    context.put("defaultBackoff", "false");    context.put("defaultIncrementMetrics", "false");    executeWithMocks(false, Status.BACKOFF, false, false, context, HttpURLConnection.HTTP_OK);    inOrder(sinkCounter).verify(sinkCounter).incrementEventWriteOrChannelFail(exception);}
public void flume_f5276_0() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn("something".getBytes());    Context context = new Context();    context.put("defaultRollback", "true");    context.put("defaultBackoff", "true");    context.put("defaultIncrementMetrics", "false");    context.put("rollback.401", "false");    context.put("backoff.401", "false");    context.put("incrementMetrics.401", "false");    executeWithMocks(true, Status.READY, false, true, context, HttpURLConnection.HTTP_UNAUTHORIZED);}
public void flume_f5277_0() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn("something".getBytes());    Context context = new Context();    context.put("defaultRollback", "true");    context.put("defaultBackoff", "true");    context.put("defaultIncrementMetrics", "false");    context.put("rollback.2XX", "false");    context.put("backoff.2XX", "false");    context.put("incrementMetrics.2XX", "true");    executeWithMocks(true, Status.READY, true, true, context, HttpURLConnection.HTTP_OK);    executeWithMocks(true, Status.READY, true, true, context, HttpURLConnection.HTTP_NO_CONTENT);}
public void flume_f5278_0() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn("something".getBytes());    Context context = new Context();    context.put("rollback.2XX", "false");    context.put("backoff.2XX", "false");    context.put("incrementMetrics.2XX", "true");    context.put("rollback.200", "true");    context.put("backoff.200", "true");    context.put("incrementMetrics.200", "false");    executeWithMocks(true, Status.READY, true, true, context, HttpURLConnection.HTTP_NO_CONTENT);    executeWithMocks(false, Status.BACKOFF, false, true, context, HttpURLConnection.HTTP_OK);}
private void flume_f5279_0(boolean commit) throws Exception
{    Context context = new Context();    executeWithMocks(commit, Status.BACKOFF, false, false, context, HttpURLConnection.HTTP_OK);}
private void flume_f5280_0(boolean expectedCommit, Status expectedStatus, boolean expectedIncrementSuccessMetrics, boolean expectedIncrementAttemptMetrics, Context context, int httpStatus) throws Exception
{    context.put("endpoint", "http://localhost:8080/endpoint");    HttpSink httpSink = new HttpSink();    httpSink.configure(context);    httpSink.setConnectionBuilder(httpSink.new ConnectionBuilder() {        @Override        public HttpURLConnection getConnection() throws IOException {            return httpURLConnection;        }    });    httpSink.setChannel(channel);    httpSink.setSinkCounter(sinkCounter);    when(channel.getTransaction()).thenReturn(transaction);    when(httpURLConnection.getOutputStream()).thenReturn(outputStream);    when(httpURLConnection.getInputStream()).thenReturn(inputStream);    when(httpURLConnection.getErrorStream()).thenReturn(inputStream);    when(httpURLConnection.getResponseCode()).thenReturn(httpStatus);    Status actualStatus = httpSink.process();    assert (actualStatus == expectedStatus);    inOrder(transaction).verify(transaction).begin();    if (expectedIncrementAttemptMetrics) {        inOrder(sinkCounter).verify(sinkCounter).incrementEventDrainAttemptCount();    }    if (expectedCommit) {        inOrder(transaction).verify(transaction).commit();    } else {        inOrder(transaction).verify(transaction).rollback();    }    if (expectedIncrementSuccessMetrics) {        inOrder(sinkCounter).verify(sinkCounter).incrementEventDrainSuccessCount();    }    inOrder(transaction).verify(transaction).close();}
public HttpURLConnection flume_f5281_0() throws IOException
{    return httpURLConnection;}
private void flume_f5282_0()
{    when(configContext.getString("endpoint", "")).thenReturn("http://test.abc/");    when(configContext.getString("acceptHeader", "")).thenReturn("test/accept");    when(configContext.getString("contentTypeHeader", "")).thenReturn("test/content");}
private void flume_f5283_0()
{    when(configContext.getBoolean("defaultBackoff", true)).thenReturn(true);    when(configContext.getBoolean("defaultRollback", true)).thenReturn(true);    when(configContext.getBoolean("defaultIncrementMetrics", false)).thenReturn(true);}
private void flume_f5284_0()
{    when(configContext.getInteger(eq("requestTimeout"), Mockito.anyInt())).thenReturn(1000);    when(configContext.getInteger(eq("connectTimeout"), Mockito.anyInt())).thenReturn(1000);}
private static int flume_f5285_0()
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    } catch (IOException e) {        throw new AssertionError("Can not find free port.", e);    }}
public void flume_f5286_0()
{    if (httpSink == null) {        Context httpSinkContext = new Context();        httpSinkContext.put("endpoint", "http://localhost:" + port + "/endpoint");        httpSinkContext.put("requestTimeout", "2000");        httpSinkContext.put("connectTimeout", "1500");        httpSinkContext.put("acceptHeader", "application/json");        httpSinkContext.put("contentTypeHeader", "application/json");        httpSinkContext.put("backoff.200", "false");        httpSinkContext.put("rollback.200", "false");        httpSinkContext.put("backoff.401", "false");        httpSinkContext.put("rollback.401", "false");        httpSinkContext.put("incrementMetrics.200", "true");        Context memoryChannelContext = new Context();        channel = new MemoryChannel();        channel.configure(memoryChannelContext);        channel.start();        httpSink = new HttpSink();        httpSink.configure(httpSinkContext);        httpSink.setChannel(channel);        httpSink.start();    }}
public void flume_f5287_0() throws InterruptedException
{    httpSink.stop();    Thread.sleep(500);}
public void flume_f5288_0() throws Exception
{    service.stubFor(post(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("SUCCESS"))).willReturn(aResponse().withStatus(200)));    addEventToChannel(event("SUCCESS"));    service.verify(1, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("SUCCESS"))));}
public void flume_f5289_0() throws Exception
{    String errorScenario = "Error Scenario";    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs(STARTED).withRequestBody(equalToJson(event("TRANSIENT_ERROR"))).willReturn(aResponse().withStatus(503)).willSetStateTo("Error Sent"));    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs("Error Sent").withRequestBody(equalToJson(event("TRANSIENT_ERROR"))).willReturn(aResponse().withStatus(200)));    addEventToChannel(event("TRANSIENT_ERROR"), Status.BACKOFF);    addEventToChannel(event("TRANSIENT_ERROR"), Status.READY);    service.verify(2, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("TRANSIENT_ERROR"))));}
public void flume_f5290_0() throws Exception
{    String errorScenario = "Error skip scenario";    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs(STARTED).withRequestBody(equalToJson(event("UNAUTHORIZED REQUEST"))).willReturn(aResponse().withStatus(401).withHeader("Content-Type", "text/plain").withBody("Not allowed!")).willSetStateTo("Error Sent"));    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs("Error Sent").withRequestBody(equalToJson(event("NEXT EVENT"))).willReturn(aResponse().withStatus(200)));    addEventToChannel(event("UNAUTHORIZED REQUEST"), Status.READY);    addEventToChannel(event("NEXT EVENT"), Status.READY);    service.verify(1, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("UNAUTHORIZED REQUEST"))));    service.verify(1, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("NEXT EVENT"))));}
public void flume_f5291_0() throws Exception
{    String errorScenario = "Error Scenario";    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs(STARTED).withRequestBody(equalToJson(event("NETWORK_ERROR"))).willReturn(aResponse().withFault(Fault.RANDOM_DATA_THEN_CLOSE)).willSetStateTo("Error Sent"));    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs("Error Sent").withRequestBody(equalToJson(event("NETWORK_ERROR"))).willReturn(aResponse().withStatus(200)));    addEventToChannel(event("NETWORK_ERROR"), Status.BACKOFF);    addEventToChannel(event("NETWORK_ERROR"), Status.READY);    service.verify(2, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("NETWORK_ERROR"))));}
public void flume_f5292_0() throws Exception
{    final CountDownLatch firstRequestReceived = new CountDownLatch(1);    service.addSocketAcceptDelay(new RequestDelaySpec(CONNECT_TIMEOUT));    service.addMockServiceRequestListener(new RequestListener() {        @Override        public void requestReceived(Request request, Response response) {            service.addSocketAcceptDelay(new RequestDelaySpec(0));            firstRequestReceived.countDown();        }    });    service.stubFor(post(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("SLOW_SOCKET"))).willReturn(aResponse().withStatus(200)));    addEventToChannel(event("SLOW_SOCKET"), Status.BACKOFF);        firstRequestReceived.await(2000, TimeUnit.MILLISECONDS);    addEventToChannel(event("SLOW_SOCKET"), Status.READY);    service.verify(2, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("SLOW_SOCKET"))));}
public void flume_f5293_0(Request request, Response response)
{    service.addSocketAcceptDelay(new RequestDelaySpec(0));    firstRequestReceived.countDown();}
public void flume_f5294_0() throws Exception
{    String errorScenario = "Error Scenario";    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs(STARTED).withRequestBody(equalToJson(event("SLOW_RESPONSE"))).willReturn(aResponse().withFixedDelay(RESPONSE_TIMEOUT).withStatus(200)).willSetStateTo("Slow Response Sent"));    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs("Slow Response Sent").withRequestBody(equalToJson(event("SLOW_RESPONSE"))).willReturn(aResponse().withStatus(200)));    addEventToChannel(event("SLOW_RESPONSE"), Status.BACKOFF);    addEventToChannel(event("SLOW_RESPONSE"), Status.READY);    service.verify(2, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("SLOW_RESPONSE"))));}
public void flume_f5295_0() throws Exception
{        service.addSocketAcceptDelay(new RequestDelaySpec(1000));    service.stubFor(post(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("SUCCESS"))).willReturn(aResponse().withStatus(200)));    long startTime = System.currentTimeMillis();    addEventToChannel(event("SUCCESS"), Status.READY);    addEventToChannel(event("SUCCESS"), Status.READY);    addEventToChannel(event("SUCCESS"), Status.READY);    long endTime = System.currentTimeMillis();    assertTrue("Test should have completed faster", endTime - startTime < 2500);    service.verify(3, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("SUCCESS"))));}
private void flume_f5296_0(String line) throws EventDeliveryException
{    addEventToChannel(line, Status.READY);}
private void flume_f5297_0(String line, Status expectedStatus) throws EventDeliveryException
{    SimpleEvent event = new SimpleEvent();    event.setBody(line.getBytes());    Transaction channelTransaction = channel.getTransaction();    channelTransaction.begin();    channel.put(event);    channelTransaction.commit();    channelTransaction.close();    Sink.Status status = httpSink.process();    assertEquals(expectedStatus, status);}
private String flume_f5298_0(String id)
{    return "{'id':'" + id + "'}";}
public void flume_f5299_0()
{}
public void flume_f5300_1()
{    }
public void flume_f5301_1(String msg)
{    }
public void flume_f5302_1(int num, String msg)
{    }
public void flume_f5303_0(String chan, IRCUser u, String nickPass)
{}
public void flume_f5304_0(String chan, IRCUser u)
{}
public void flume_f5305_0(String chan, IRCUser u, String nickPass, String msg)
{}
public void flume_f5306_0(IRCUser u, String nickPass, String mode)
{}
public void flume_f5307_0(String chan, IRCUser u, IRCModeParser mp)
{}
public void flume_f5308_0(IRCUser u, String nickNew)
{}
public void flume_f5309_0(String target, IRCUser u, String msg)
{}
public void flume_f5310_0(String chan, IRCUser u, String msg)
{}
public void flume_f5311_0(String chan, IRCUser u, String msg)
{}
public void flume_f5312_0(IRCUser u, String msg)
{}
public void flume_f5313_0(int num, String value, String msg)
{}
public void flume_f5314_0(String chan, IRCUser u, String topic)
{}
public void flume_f5315_0(String p)
{}
public void flume_f5316_0(String a, String b, String c, String d)
{}
public void flume_f5317_0(Context context)
{    hostname = context.getString("hostname");    String portStr = context.getString("port");    nick = context.getString("nick");    password = context.getString("password");    user = context.getString("user");    name = context.getString("name");    chan = context.getString("chan");    splitLines = context.getBoolean("splitlines", false);    splitChars = context.getString("splitchars");    if (portStr != null) {        port = Integer.parseInt(portStr);    } else {        port = DEFAULT_PORT;    }    if (splitChars == null) {        splitChars = DEFAULT_SPLIT_CHARS;    }    Preconditions.checkState(hostname != null, "No hostname specified");    Preconditions.checkState(nick != null, "No nick specified");    Preconditions.checkState(chan != null, "No chan specified");}
private void flume_f5318_1() throws IOException
{    if (connection == null) {                connection = new IRCConnection(hostname, new int[] { port }, password, nick, user, name);        connection.addIRCEventListener(new IRCConnectionListener());        connection.setEncoding("UTF-8");        connection.setPong(true);        connection.setDaemon(false);        connection.setColors(false);        connection.connect();        connection.send("join " + IRC_CHANNEL_PREFIX + chan);    }}
private void flume_f5319_1()
{    if (connection != null) {                connection.close();    }    connection = null;}
public void flume_f5320_1()
{        try {        createConnection();    } catch (Exception e) {                /* Try to prevent leaking resources. */        destroyConnection();        /* FIXME: Mark ourselves as failed. */        return;    }    super.start();    }
public void flume_f5321_1()
{        destroyConnection();    super.stop();    }
private void flume_f5322_0(Event event)
{    String body = new String(event.getBody());    if (splitLines) {        String[] lines = body.split(splitChars);        for (String line : lines) {            connection.doPrivmsg(IRC_CHANNEL_PREFIX + this.chan, line);        }    } else {        connection.doPrivmsg(IRC_CHANNEL_PREFIX + this.chan, body);    }}
public Status flume_f5323_1() throws EventDeliveryException
{    Status status = Status.READY;    Channel channel = getChannel();    Transaction transaction = channel.getTransaction();    try {        transaction.begin();        createConnection();        Event event = channel.take();        if (event == null) {            counterGroup.incrementAndGet("event.empty");            status = Status.BACKOFF;        } else {            sendLine(event);            counterGroup.incrementAndGet("event.irc");        }        transaction.commit();    } catch (ChannelException e) {        transaction.rollback();                status = Status.BACKOFF;    } catch (Exception e) {        transaction.rollback();                status = Status.BACKOFF;        destroyConnection();    } finally {        transaction.close();    }    return status;}
private static int flume_f5324_0() throws IOException
{    ServerSocket socket = new ServerSocket(0);    int port = socket.getLocalPort();    socket.close();    return port;}
public void flume_f5325_0() throws IOException
{    ircServerPort = findFreePort();    dumbIRCServer = new DumbIRCServer(ircServerPort);    dumbIRCServer.start();    eventFile = folder.newFile("eventFile.txt");}
public void flume_f5326_0() throws Exception
{    dumbIRCServer.shutdownServer();}
public void flume_f5327_0()
{    Sink ircSink = new IRCSink();    ircSink.setName("IRC Sink - " + UUID.randomUUID().toString());    Context context = new Context();    context.put("hostname", "localhost");    context.put("port", String.valueOf(ircServerPort));    context.put("nick", "flume");    context.put("password", "flume");    context.put("user", "flume");    context.put("name", "flume-dev");    context.put("chan", "flume");    context.put("splitchars", "false");    Configurables.configure(ircSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    ircSink.setChannel(memoryChannel);    ircSink.start();    Transaction txn = memoryChannel.getTransaction();    txn.begin();    Event event = EventBuilder.withBody("Dummy Event".getBytes());    memoryChannel.put(event);    txn.commit();    txn.close();    try {        Sink.Status status = ircSink.process();        if (status == Sink.Status.BACKOFF) {            fail("Error occured");        }    } catch (EventDeliveryException eDelExcp) {        }}
public void flume_f5328_0()
{    try {        ss = new ServerSocket(port);        while (true) {            try {                Socket socket = ss.accept();                process(socket);            } catch (Exception ex) {            /* noop */            }        }    } catch (IOException e) {        }}
public void flume_f5329_0() throws Exception
{    ss.close();}
private void flume_f5330_0(Socket socket) throws IOException
{    FileOutputStream fileOutputStream = FileUtils.openOutputStream(eventFile);    List<String> input = IOUtils.readLines(socket.getInputStream());    for (String next : input) {        if (isPrivMessage(next)) {            fileOutputStream.write(next.getBytes());            fileOutputStream.write("\n".getBytes());        }    }    fileOutputStream.close();    socket.close();}
private boolean flume_f5331_0(String input)
{    return input.startsWith("PRIVMSG");}
public IndexRequestBuilder flume_f5332_0(Client client, String indexPrefix, String indexType, Event event) throws IOException
{    IndexRequestBuilder request = prepareIndex(client);    String realIndexPrefix = BucketPath.escapeString(indexPrefix, event.getHeaders());    String realIndexType = BucketPath.escapeString(indexType, event.getHeaders());    TimestampedEvent timestampedEvent = new TimestampedEvent(event);    long timestamp = timestampedEvent.getTimestamp();    String indexName = getIndexName(realIndexPrefix, timestamp);    prepareIndexRequest(request, indexName, realIndexType, timestampedEvent);    return request;}
 IndexRequestBuilder flume_f5333_0(Client client)
{    return client.prepareIndex();}
protected String flume_f5334_0(String indexPrefix, long timestamp)
{    return new StringBuilder(indexPrefix).append('-').append(fastDateFormat.format(timestamp)).toString();}
public ElasticSearchClient flume_f5335_0(String clientType, String[] hostNames, String clusterName, ElasticSearchEventSerializer serializer, ElasticSearchIndexRequestBuilderFactory indexBuilder) throws NoSuchClientTypeException
{    if (clientType.equalsIgnoreCase(TransportClient) && serializer != null) {        return new ElasticSearchTransportClient(hostNames, clusterName, serializer);    } else if (clientType.equalsIgnoreCase(TransportClient) && indexBuilder != null) {        return new ElasticSearchTransportClient(hostNames, clusterName, indexBuilder);    } else if (clientType.equalsIgnoreCase(RestClient) && serializer != null) {        return new ElasticSearchRestClient(hostNames, serializer);    }    throw new NoSuchClientTypeException();}
public ElasticSearchClient flume_f5336_0(String clientType, ElasticSearchEventSerializer serializer, ElasticSearchIndexRequestBuilderFactory indexBuilder) throws NoSuchClientTypeException
{    if (clientType.equalsIgnoreCase(TransportClient) && serializer != null) {        return new ElasticSearchTransportClient(serializer);    } else if (clientType.equalsIgnoreCase(TransportClient) && indexBuilder != null) {        return new ElasticSearchTransportClient(indexBuilder);    } else if (clientType.equalsIgnoreCase(RestClient)) {    }    throw new NoSuchClientTypeException();}
public void flume_f5337_0(Context context)
{}
public void flume_f5338_0()
{}
public void flume_f5339_0(Event event, IndexNameBuilder indexNameBuilder, String indexType, long ttlMs) throws Exception
{    BytesReference content = serializer.getContentBuilder(event).bytes();    Map<String, Map<String, String>> parameters = new HashMap<String, Map<String, String>>();    Map<String, String> indexParameters = new HashMap<String, String>();    indexParameters.put(INDEX_PARAM, indexNameBuilder.getIndexName(event));    indexParameters.put(TYPE_PARAM, indexType);    if (ttlMs > 0) {        indexParameters.put(TTL_PARAM, Long.toString(ttlMs));    }    parameters.put(INDEX_OPERATION_NAME, indexParameters);    Gson gson = new Gson();    synchronized (bulkBuilder) {        bulkBuilder.append(gson.toJson(parameters));        bulkBuilder.append("\n");        bulkBuilder.append(content.toBytesArray().toUtf8());        bulkBuilder.append("\n");    }}
public void flume_f5340_1() throws Exception
{    int statusCode = 0, triesCount = 0;    HttpResponse response = null;    String entity;    synchronized (bulkBuilder) {        entity = bulkBuilder.toString();        bulkBuilder = new StringBuilder();    }    while (statusCode != HttpStatus.SC_OK && triesCount < serversList.size()) {        triesCount++;        String host = serversList.get();        String url = host + "/" + BULK_ENDPOINT;        HttpPost httpRequest = new HttpPost(url);        httpRequest.setEntity(new StringEntity(entity));        response = httpClient.execute(httpRequest);        statusCode = response.getStatusLine().getStatusCode();                if (response.getEntity() != null) {                    }    }    if (statusCode != HttpStatus.SC_OK) {        if (response.getEntity() != null) {            throw new EventDeliveryException(EntityUtils.toString(response.getEntity(), "UTF-8"));        } else {            throw new EventDeliveryException("Elasticsearch status code was: " + statusCode);        }    }}
 InetSocketTransportAddress[] flume_f5341_0()
{    return serverAddresses;}
 void flume_f5342_0(BulkRequestBuilder bulkRequestBuilder)
{    this.bulkRequestBuilder = bulkRequestBuilder;}
private void flume_f5343_1(String[] hostNames)
{        serverAddresses = new InetSocketTransportAddress[hostNames.length];    for (int i = 0; i < hostNames.length; i++) {        String[] hostPort = hostNames[i].trim().split(":");        String host = hostPort[0].trim();        int port = hostPort.length == 2 ? Integer.parseInt(hostPort[1].trim()) : DEFAULT_PORT;        serverAddresses[i] = new InetSocketTransportAddress(host, port);    }}
public void flume_f5344_0()
{    if (client != null) {        client.close();    }    client = null;}
public void flume_f5345_0(Event event, IndexNameBuilder indexNameBuilder, String indexType, long ttlMs) throws Exception
{    if (bulkRequestBuilder == null) {        bulkRequestBuilder = client.prepareBulk();    }    IndexRequestBuilder indexRequestBuilder = null;    if (indexRequestBuilderFactory == null) {        indexRequestBuilder = client.prepareIndex(indexNameBuilder.getIndexName(event), indexType).setSource(serializer.getContentBuilder(event).bytes());    } else {        indexRequestBuilder = indexRequestBuilderFactory.createIndexRequest(client, indexNameBuilder.getIndexPrefix(event), indexType, event);    }    if (ttlMs > 0) {        indexRequestBuilder.setTTL(ttlMs);    }    bulkRequestBuilder.add(indexRequestBuilder);}
public void flume_f5346_0() throws Exception
{    try {        BulkResponse bulkResponse = bulkRequestBuilder.execute().actionGet();        if (bulkResponse.hasFailures()) {            throw new EventDeliveryException(bulkResponse.buildFailureMessage());        }    } finally {        bulkRequestBuilder = client.prepareBulk();    }}
private void flume_f5347_1(String clusterName)
{        Settings settings = ImmutableSettings.settingsBuilder().put("cluster.name", clusterName).build();    TransportClient transportClient = new TransportClient(settings);    for (InetSocketTransportAddress host : serverAddresses) {        transportClient.addTransportAddress(host);    }    if (client != null) {        client.close();    }    client = transportClient;}
private void flume_f5348_1()
{        Node node = NodeBuilder.nodeBuilder().client(true).local(true).node();    if (client != null) {        client.close();    }    client = node.client();}
public void flume_f5349_0(Context context)
{}
public synchronized T flume_f5350_0()
{    if (iterator.hasNext()) {        return iterator.next();    } else {        iterator = elements.iterator();        return iterator.next();    }}
public int flume_f5351_0()
{    return elements.size();}
public static void flume_f5352_0(XContentBuilder builder, String field, byte[] data) throws IOException
{    XContentType contentType = XContentFactory.xContentType(data);    if (contentType == null) {        addSimpleField(builder, field, data);    } else {        addComplexField(builder, field, contentType, data);    }}
public static void flume_f5353_0(XContentBuilder builder, String fieldName, byte[] data) throws IOException
{    builder.field(fieldName, new String(data, charset));}
public static void flume_f5354_0(XContentBuilder builder, String fieldName, XContentType contentType, byte[] data) throws IOException
{    XContentParser parser = null;    try {                                                                parser = XContentFactory.xContent(contentType).createParser(data);        while (parser.nextToken() != null) {        }        ;                parser = XContentFactory.xContent(contentType).createParser(data);                builder.field(fieldName);                builder.copyCurrentStructure(parser);    } catch (JsonParseException ex) {                                addSimpleField(builder, fieldName, data);    } finally {        if (parser != null) {            parser.close();        }    }}
public void flume_f5355_0(Context context)
{}
public void flume_f5356_0(ComponentConfiguration conf)
{}
public XContentBuilder flume_f5357_0(Event event) throws IOException
{    XContentBuilder builder = jsonBuilder().startObject();    appendBody(builder, event);    appendHeaders(builder, event);    return builder;}
private void flume_f5358_0(XContentBuilder builder, Event event) throws IOException
{    ContentBuilderUtil.appendField(builder, "body", event.getBody());}
private void flume_f5359_0(XContentBuilder builder, Event event) throws IOException
{    Map<String, String> headers = event.getHeaders();    for (String key : headers.keySet()) {        ContentBuilderUtil.appendField(builder, key, headers.get(key).getBytes(charset));    }}
public XContentBuilder flume_f5360_0(Event event) throws IOException
{    XContentBuilder builder = jsonBuilder().startObject();    appendBody(builder, event);    appendHeaders(builder, event);    return builder;}
private void flume_f5361_0(XContentBuilder builder, Event event) throws IOException, UnsupportedEncodingException
{    byte[] body = event.getBody();    ContentBuilderUtil.appendField(builder, "@message", body);}
private void flume_f5362_0(XContentBuilder builder, Event event) throws IOException
{    Map<String, String> headers = Maps.newHashMap(event.getHeaders());    String timestamp = headers.get("timestamp");    if (!StringUtils.isBlank(timestamp) && StringUtils.isBlank(headers.get("@timestamp"))) {        long timestampMs = Long.parseLong(timestamp);        builder.field("@timestamp", new Date(timestampMs));    }    String source = headers.get("source");    if (!StringUtils.isBlank(source) && StringUtils.isBlank(headers.get("@source"))) {        ContentBuilderUtil.appendField(builder, "@source", source.getBytes(charset));    }    String type = headers.get("type");    if (!StringUtils.isBlank(type) && StringUtils.isBlank(headers.get("@type"))) {        ContentBuilderUtil.appendField(builder, "@type", type.getBytes(charset));    }    String host = headers.get("host");    if (!StringUtils.isBlank(host) && StringUtils.isBlank(headers.get("@source_host"))) {        ContentBuilderUtil.appendField(builder, "@source_host", host.getBytes(charset));    }    String srcPath = headers.get("src_path");    if (!StringUtils.isBlank(srcPath) && StringUtils.isBlank(headers.get("@source_path"))) {        ContentBuilderUtil.appendField(builder, "@source_path", srcPath.getBytes(charset));    }    builder.startObject("@fields");    for (String key : headers.keySet()) {        byte[] val = headers.get(key).getBytes(charset);        ContentBuilderUtil.appendField(builder, key, val);    }    builder.endObject();}
public void flume_f5363_0(Context context)
{}
public void flume_f5364_0(ComponentConfiguration conf)
{}
 String[] flume_f5365_0()
{    return serverAddresses;}
 String flume_f5366_0()
{    return clusterName;}
 String flume_f5367_0()
{    return indexName;}
 String flume_f5368_0()
{    return indexType;}
 long flume_f5369_0()
{    return ttlMs;}
 ElasticSearchEventSerializer flume_f5370_0()
{    return eventSerializer;}
 IndexNameBuilder flume_f5371_0()
{    return indexNameBuilder;}
public long flume_f5372_0()
{    return batchSize;}
public Status flume_f5373_1() throws EventDeliveryException
{        Status status = Status.READY;    Channel channel = getChannel();    Transaction txn = channel.getTransaction();    try {        txn.begin();        int count;        for (count = 0; count < batchSize; ++count) {            Event event = channel.take();            if (event == null) {                break;            }            String realIndexType = BucketPath.escapeString(indexType, event.getHeaders());            client.addEvent(event, indexNameBuilder, realIndexType, ttlMs);        }        if (count <= 0) {            sinkCounter.incrementBatchEmptyCount();            counterGroup.incrementAndGet("channel.underflow");            status = Status.BACKOFF;        } else {            if (count < batchSize) {                sinkCounter.incrementBatchUnderflowCount();                status = Status.BACKOFF;            } else {                sinkCounter.incrementBatchCompleteCount();            }            sinkCounter.addToEventDrainAttemptCount(count);            client.execute();        }        txn.commit();        sinkCounter.addToEventDrainSuccessCount(count);        counterGroup.incrementAndGet("transaction.success");    } catch (Throwable ex) {        try {            txn.rollback();            counterGroup.incrementAndGet("transaction.rollback");        } catch (Exception ex2) {                    }        if (ex instanceof Error || ex instanceof RuntimeException) {                        Throwables.propagate(ex);        } else {                        throw new EventDeliveryException("Failed to commit transaction. Transaction rolled back.", ex);        }    } finally {        txn.close();    }    return status;}
public void flume_f5374_1(Context context)
{    if (!isLocal) {        if (StringUtils.isNotBlank(context.getString(HOSTNAMES))) {            serverAddresses = StringUtils.deleteWhitespace(context.getString(HOSTNAMES)).split(",");        }        Preconditions.checkState(serverAddresses != null && serverAddresses.length > 0, "Missing Param:" + HOSTNAMES);    }    if (StringUtils.isNotBlank(context.getString(INDEX_NAME))) {        this.indexName = context.getString(INDEX_NAME);    }    if (StringUtils.isNotBlank(context.getString(INDEX_TYPE))) {        this.indexType = context.getString(INDEX_TYPE);    }    if (StringUtils.isNotBlank(context.getString(CLUSTER_NAME))) {        this.clusterName = context.getString(CLUSTER_NAME);    }    if (StringUtils.isNotBlank(context.getString(BATCH_SIZE))) {        this.batchSize = Integer.parseInt(context.getString(BATCH_SIZE));    }    if (StringUtils.isNotBlank(context.getString(TTL))) {        this.ttlMs = parseTTL(context.getString(TTL));        Preconditions.checkState(ttlMs > 0, TTL + " must be greater than 0 or not set.");    }    if (StringUtils.isNotBlank(context.getString(CLIENT_TYPE))) {        clientType = context.getString(CLIENT_TYPE);    }    elasticSearchClientContext = new Context();    elasticSearchClientContext.putAll(context.getSubProperties(CLIENT_PREFIX));    String serializerClazz = DEFAULT_SERIALIZER_CLASS;    if (StringUtils.isNotBlank(context.getString(SERIALIZER))) {        serializerClazz = context.getString(SERIALIZER);    }    Context serializerContext = new Context();    serializerContext.putAll(context.getSubProperties(SERIALIZER_PREFIX));    try {        @SuppressWarnings("unchecked")        Class<? extends Configurable> clazz = (Class<? extends Configurable>) Class.forName(serializerClazz);        Configurable serializer = clazz.newInstance();        if (serializer instanceof ElasticSearchIndexRequestBuilderFactory) {            indexRequestFactory = (ElasticSearchIndexRequestBuilderFactory) serializer;            indexRequestFactory.configure(serializerContext);        } else if (serializer instanceof ElasticSearchEventSerializer) {            eventSerializer = (ElasticSearchEventSerializer) serializer;            eventSerializer.configure(serializerContext);        } else {            throw new IllegalArgumentException(serializerClazz + " is not an ElasticSearchEventSerializer");        }    } catch (Exception e) {                Throwables.propagate(e);    }    if (sinkCounter == null) {        sinkCounter = new SinkCounter(getName());    }    String indexNameBuilderClass = DEFAULT_INDEX_NAME_BUILDER_CLASS;    if (StringUtils.isNotBlank(context.getString(INDEX_NAME_BUILDER))) {        indexNameBuilderClass = context.getString(INDEX_NAME_BUILDER);    }    Context indexnameBuilderContext = new Context();    serializerContext.putAll(context.getSubProperties(INDEX_NAME_BUILDER_PREFIX));    try {        @SuppressWarnings("unchecked")        Class<? extends IndexNameBuilder> clazz = (Class<? extends IndexNameBuilder>) Class.forName(indexNameBuilderClass);        indexNameBuilder = clazz.newInstance();        indexnameBuilderContext.put(INDEX_NAME, indexName);        indexNameBuilder.configure(indexnameBuilderContext);    } catch (Exception e) {                Throwables.propagate(e);    }    if (sinkCounter == null) {        sinkCounter = new SinkCounter(getName());    }    Preconditions.checkState(StringUtils.isNotBlank(indexName), "Missing Param:" + INDEX_NAME);    Preconditions.checkState(StringUtils.isNotBlank(indexType), "Missing Param:" + INDEX_TYPE);    Preconditions.checkState(StringUtils.isNotBlank(clusterName), "Missing Param:" + CLUSTER_NAME);    Preconditions.checkState(batchSize >= 1, BATCH_SIZE + " must be greater than 0");}
public void flume_f5375_1()
{    ElasticSearchClientFactory clientFactory = new ElasticSearchClientFactory();        sinkCounter.start();    try {        if (isLocal) {            client = clientFactory.getLocalClient(clientType, eventSerializer, indexRequestFactory);        } else {            client = clientFactory.getClient(clientType, serverAddresses, clusterName, eventSerializer, indexRequestFactory);            client.configure(elasticSearchClientContext);        }        sinkCounter.incrementConnectionCreatedCount();    } catch (Exception ex) {        ex.printStackTrace();        sinkCounter.incrementConnectionFailedCount();        if (client != null) {            client.close();            sinkCounter.incrementConnectionClosedCount();        }    }    super.start();}
public void flume_f5376_1()
{        if (client != null) {        client.close();    }    sinkCounter.incrementConnectionClosedCount();    sinkCounter.stop();    super.stop();}
private long flume_f5377_1(String ttl)
{    matcher = matcher.reset(ttl);    while (matcher.find()) {        if (matcher.group(2).equals("ms")) {            return Long.parseLong(matcher.group(1));        } else if (matcher.group(2).equals("s")) {            return TimeUnit.SECONDS.toMillis(Integer.parseInt(matcher.group(1)));        } else if (matcher.group(2).equals("m")) {            return TimeUnit.MINUTES.toMillis(Integer.parseInt(matcher.group(1)));        } else if (matcher.group(2).equals("h")) {            return TimeUnit.HOURS.toMillis(Integer.parseInt(matcher.group(1)));        } else if (matcher.group(2).equals("d")) {            return TimeUnit.DAYS.toMillis(Integer.parseInt(matcher.group(1)));        } else if (matcher.group(2).equals("w")) {            return TimeUnit.DAYS.toMillis(7 * Integer.parseInt(matcher.group(1)));        } else if (matcher.group(2).equals("")) {                        return TimeUnit.DAYS.toMillis(Integer.parseInt(matcher.group(1)));        } else {                        return 0;        }    }        return 0;}
public void flume_f5378_0(Context context)
{    serializer.configure(context);}
public void flume_f5379_0(ComponentConfiguration config)
{    serializer.configure(config);}
protected void flume_f5380_0(IndexRequestBuilder indexRequest, String indexName, String indexType, Event event) throws IOException
{    BytesStream contentBuilder = serializer.getContentBuilder(event);    indexRequest.setIndex(indexName).setType(indexType).setSource(contentBuilder.bytes());}
public String flume_f5381_0(Event event)
{    return BucketPath.escapeString(indexName, event.getHeaders());}
public String flume_f5382_0(Event event)
{    return BucketPath.escapeString(indexName, event.getHeaders());}
public void flume_f5383_0(Context context)
{    indexName = context.getString(ElasticSearchSinkConstants.INDEX_NAME);}
public void flume_f5384_0(ComponentConfiguration conf)
{}
 FastDateFormat flume_f5385_0()
{    return fastDateFormat;}
public String flume_f5386_0(Event event)
{    TimestampedEvent timestampedEvent = new TimestampedEvent(event);    long timestamp = timestampedEvent.getTimestamp();    String realIndexPrefix = BucketPath.escapeString(indexPrefix, event.getHeaders());    return new StringBuilder(realIndexPrefix).append('-').append(fastDateFormat.format(timestamp)).toString();}
public String flume_f5387_0(Event event)
{    return BucketPath.escapeString(indexPrefix, event.getHeaders());}
public void flume_f5388_0(Context context)
{    String dateFormatString = context.getString(DATE_FORMAT);    String timeZoneString = context.getString(TIME_ZONE);    if (StringUtils.isBlank(dateFormatString)) {        dateFormatString = DEFAULT_DATE_FORMAT;    }    if (StringUtils.isBlank(timeZoneString)) {        timeZoneString = DEFAULT_TIME_ZONE;    }    fastDateFormat = FastDateFormat.getInstance(dateFormatString, TimeZone.getTimeZone(timeZoneString));    indexPrefix = context.getString(ElasticSearchSinkConstants.INDEX_NAME);}
public void flume_f5389_0(ComponentConfiguration conf)
{}
 long flume_f5390_0()
{    return timestamp;}
 void flume_f5391_0()
{    parameters = Maps.newHashMap();    parameters.put(INDEX_NAME, DEFAULT_INDEX_NAME);    parameters.put(INDEX_TYPE, DEFAULT_INDEX_TYPE);    parameters.put(CLUSTER_NAME, DEFAULT_CLUSTER_NAME);    parameters.put(BATCH_SIZE, "1");    parameters.put(TTL, "5");    timestampedIndexName = DEFAULT_INDEX_NAME + '-' + ElasticSearchIndexRequestBuilderFactory.df.format(FIXED_TIME_MILLIS);}
 void flume_f5392_0() throws Exception
{    Settings settings = ImmutableSettings.settingsBuilder().put("number_of_shards", 1).put("number_of_replicas", 0).put("routing.hash.type", "simple").put("gateway.type", "none").put("path.data", "target/es-test").build();    node = NodeBuilder.nodeBuilder().settings(settings).local(true).node();    client = node.client();    client.admin().cluster().prepareHealth().setWaitForGreenStatus().execute().actionGet();}
 void flume_f5393_0() throws Exception
{    ((InternalNode) node).injector().getInstance(Gateway.class).reset();    client.close();    node.close();}
public void flume_f5394_0()
{    DateTimeUtils.setCurrentMillisFixed(FIXED_TIME_MILLIS);}
public void flume_f5395_0()
{    DateTimeUtils.setCurrentMillisSystem();}
 Channel flume_f5396_0(ElasticSearchSink fixture)
{        Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());        fixture.setChannel(channel);    fixture.start();    return channel;}
 void flume_f5397_0(int expectedHits, Event... events)
{    assertSearch(expectedHits, performSearch(QueryBuilders.matchAllQuery()), null, events);}
 void flume_f5398_0(int expectedHits, Event... events)
{        assertSearch(expectedHits, performSearch(QueryBuilders.fieldQuery("@message", "event")), null, events);}
 SearchResponse flume_f5399_0(QueryBuilder query)
{    return client.prepareSearch(timestampedIndexName).setTypes(DEFAULT_INDEX_TYPE).setQuery(query).execute().actionGet();}
 void flume_f5400_0(int expectedHits, SearchResponse response, Map<String, Object> expectedBody, Event... events)
{    SearchHits hitResponse = response.getHits();    assertEquals(expectedHits, hitResponse.getTotalHits());    SearchHit[] hits = hitResponse.getHits();    Arrays.sort(hits, new Comparator<SearchHit>() {        @Override        public int compare(SearchHit o1, SearchHit o2) {            return o1.getSourceAsString().compareTo(o2.getSourceAsString());        }    });    for (int i = 0; i < events.length; i++) {        Event event = events[i];        SearchHit hit = hits[i];        Map<String, Object> source = hit.getSource();        if (expectedBody == null) {            assertEquals(new String(event.getBody()), source.get("@message"));        } else {            assertEquals(expectedBody, source.get("@message"));        }    }}
public int flume_f5401_0(SearchHit o1, SearchHit o2)
{    return o1.getSourceAsString().compareTo(o2.getSourceAsString());}
public void flume_f5402_0()
{    fixture = new RoundRobinList<String>(Arrays.asList("test1", "test2"));}
public void flume_f5403_0()
{    assertEquals("test1", fixture.get());    assertEquals("test2", fixture.get());    assertEquals("test1", fixture.get());    assertEquals("test2", fixture.get());    assertEquals("test1", fixture.get());}
public void flume_f5404_0()
{    initMocks(this);    factory = new ElasticSearchClientFactory();}
public void flume_f5405_0() throws Exception
{    String[] hostNames = { "127.0.0.1" };    Object o = factory.getClient(ElasticSearchClientFactory.TransportClient, hostNames, "test", serializer, null);    assertThat(o, instanceOf(ElasticSearchTransportClient.class));}
public void flume_f5406_0() throws NoSuchClientTypeException
{    String[] hostNames = { "127.0.0.1" };    Object o = factory.getClient(ElasticSearchClientFactory.RestClient, hostNames, "test", serializer, null);    assertThat(o, instanceOf(ElasticSearchRestClient.class));}
public void flume_f5407_0() throws NoSuchClientTypeException
{    String[] hostNames = { "127.0.0.1" };    factory.getClient("not_existing_client", hostNames, "test", null, null);}
public void flume_f5408_0() throws IOException
{    initMocks(this);    BytesReference bytesReference = mock(BytesReference.class);    BytesStream bytesStream = mock(BytesStream.class);    when(nameBuilder.getIndexName(any(Event.class))).thenReturn(INDEX_NAME);    when(bytesReference.toBytesArray()).thenReturn(new BytesArray(MESSAGE_CONTENT));    when(bytesStream.bytes()).thenReturn(bytesReference);    when(serializer.getContentBuilder(any(Event.class))).thenReturn(bytesStream);    fixture = new ElasticSearchRestClient(HOSTS, serializer, httpClient);}
public void flume_f5409_0() throws Exception
{    ArgumentCaptor<HttpPost> argument = ArgumentCaptor.forClass(HttpPost.class);    when(httpStatus.getStatusCode()).thenReturn(HttpStatus.SC_OK);    when(httpResponse.getStatusLine()).thenReturn(httpStatus);    when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(httpResponse);    fixture.addEvent(event, nameBuilder, "bar_type", -1);    fixture.execute();    verify(httpClient).execute(isA(HttpUriRequest.class));    verify(httpClient).execute(argument.capture());    assertEquals("http://host1/_bulk", argument.getValue().getURI().toString());    assertTrue(verifyJsonEvents("{\"index\":{\"_type\":\"bar_type\", \"_index\":\"foo_index\"}}\n", MESSAGE_CONTENT, EntityUtils.toString(argument.getValue().getEntity())));}
public void flume_f5410_0() throws Exception
{    ArgumentCaptor<HttpPost> argument = ArgumentCaptor.forClass(HttpPost.class);    when(httpStatus.getStatusCode()).thenReturn(HttpStatus.SC_OK);    when(httpResponse.getStatusLine()).thenReturn(httpStatus);    when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(httpResponse);    fixture.addEvent(event, nameBuilder, "bar_type", 123);    fixture.execute();    verify(httpClient).execute(isA(HttpUriRequest.class));    verify(httpClient).execute(argument.capture());    assertEquals("http://host1/_bulk", argument.getValue().getURI().toString());    assertTrue(verifyJsonEvents("{\"index\":{\"_type\":\"bar_type\",\"_index\":\"foo_index\",\"_ttl\":\"123\"}}\n", MESSAGE_CONTENT, EntityUtils.toString(argument.getValue().getEntity())));}
private boolean flume_f5411_0(String expectedIndex, String expectedBody, String actual)
{    Iterator<String> it = Splitter.on("\n").split(actual).iterator();    JsonParser parser = new JsonParser();    JsonObject[] arr = new JsonObject[2];    for (int i = 0; i < 2; i++) {        arr[i] = (JsonObject) parser.parse(it.next());    }    return arr[0].equals(parser.parse(expectedIndex)) && arr[1].equals(parser.parse(expectedBody));}
public void flume_f5412_0() throws Exception
{    ArgumentCaptor<HttpPost> argument = ArgumentCaptor.forClass(HttpPost.class);    when(httpStatus.getStatusCode()).thenReturn(HttpStatus.SC_INTERNAL_SERVER_ERROR);    when(httpResponse.getStatusLine()).thenReturn(httpStatus);    when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(httpResponse);    fixture.addEvent(event, nameBuilder, "bar_type", 123);    fixture.execute();}
public void flume_f5413_0() throws Exception
{    ArgumentCaptor<HttpPost> argument = ArgumentCaptor.forClass(HttpPost.class);    when(httpStatus.getStatusCode()).thenReturn(HttpStatus.SC_INTERNAL_SERVER_ERROR, HttpStatus.SC_OK);    when(httpResponse.getStatusLine()).thenReturn(httpStatus);    when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(httpResponse);    fixture.addEvent(event, nameBuilder, "bar_type", 123);    fixture.execute();    verify(httpClient, times(2)).execute(isA(HttpUriRequest.class));    verify(httpClient, times(2)).execute(argument.capture());    List<HttpPost> allValues = argument.getAllValues();    assertEquals("http://host1/_bulk", allValues.get(0).getURI().toString());    assertEquals("http://host2/_bulk", allValues.get(1).getURI().toString());}
public void flume_f5414_0() throws IOException
{    initMocks(this);    BytesReference bytesReference = mock(BytesReference.class);    BytesStream bytesStream = mock(BytesStream.class);    when(nameBuilder.getIndexName(any(Event.class))).thenReturn("foo_index");    when(bytesReference.toBytes()).thenReturn("{\"body\":\"test\"}".getBytes());    when(bytesStream.bytes()).thenReturn(bytesReference);    when(serializer.getContentBuilder(any(Event.class))).thenReturn(bytesStream);    when(elasticSearchClient.prepareIndex(anyString(), anyString())).thenReturn(indexRequestBuilder);    when(indexRequestBuilder.setSource(bytesReference)).thenReturn(indexRequestBuilder);    fixture = new ElasticSearchTransportClient(elasticSearchClient, serializer);    fixture.setBulkRequestBuilder(bulkRequestBuilder);}
public void flume_f5415_0() throws Exception
{    fixture.addEvent(event, nameBuilder, "bar_type", -1);    verify(indexRequestBuilder).setSource(serializer.getContentBuilder(event).bytes());    verify(bulkRequestBuilder).add(indexRequestBuilder);}
public void flume_f5416_0() throws Exception
{    fixture.addEvent(event, nameBuilder, "bar_type", 10);    verify(indexRequestBuilder).setTTL(10);    verify(indexRequestBuilder).setSource(serializer.getContentBuilder(event).bytes());}
public void flume_f5417_0() throws Exception
{    ListenableActionFuture<BulkResponse> action = (ListenableActionFuture<BulkResponse>) mock(ListenableActionFuture.class);    BulkResponse response = mock(BulkResponse.class);    when(bulkRequestBuilder.execute()).thenReturn(action);    when(action.actionGet()).thenReturn(response);    when(response.hasFailures()).thenReturn(false);    fixture.addEvent(event, nameBuilder, "bar_type", 10);    fixture.execute();    verify(bulkRequestBuilder).execute();}
public void flume_f5418_0() throws Exception
{    ListenableActionFuture<BulkResponse> action = (ListenableActionFuture<BulkResponse>) mock(ListenableActionFuture.class);    BulkResponse response = mock(BulkResponse.class);    when(bulkRequestBuilder.execute()).thenReturn(action);    when(action.actionGet()).thenReturn(response);    when(response.hasFailures()).thenReturn(true);    fixture.addEvent(event, nameBuilder, "bar_type", 10);    fixture.execute();}
public void flume_f5419_0() throws Exception
{    ElasticSearchDynamicSerializer fixture = new ElasticSearchDynamicSerializer();    Context context = new Context();    fixture.configure(context);    String message = "test body";    Map<String, String> headers = Maps.newHashMap();    headers.put("headerNameOne", "headerValueOne");    headers.put("headerNameTwo", "headerValueTwo");    headers.put("headerNameThree", "headerValueThree");    Event event = EventBuilder.withBody(message.getBytes(charset));    event.setHeaders(headers);    XContentBuilder expected = jsonBuilder().startObject();    expected.field("body", new String(message.getBytes(), charset));    for (String headerName : headers.keySet()) {        expected.field(headerName, new String(headers.get(headerName).getBytes(), charset));    }    expected.endObject();    XContentBuilder actual = fixture.getContentBuilder(event);    assertEquals(new String(expected.bytes().array()), new String(actual.bytes().array()));}
public void flume_f5420_0() throws Exception
{    serializer = new FakeEventSerializer();    factory = new EventSerializerIndexRequestBuilderFactory(serializer) {        @Override        IndexRequestBuilder prepareIndex(Client client) {            return new IndexRequestBuilder(FAKE_CLIENT);        }    };}
 IndexRequestBuilder flume_f5421_0(Client client)
{    return new IndexRequestBuilder(FAKE_CLIENT);}
public void flume_f5422_0()
{    assertEquals("Coordinated Universal Time", factory.fastDateFormat.getTimeZone().getDisplayName());}
public void flume_f5423_0()
{    long millis = 987654321L;    assertEquals("prefix-" + factory.fastDateFormat.format(millis), factory.getIndexName("prefix", millis));}
public void flume_f5424_0()
{    SimpleEvent base = new SimpleEvent();    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals(FIXED_TIME_MILLIS, timestampedEvent.getTimestamp());    assertEquals(String.valueOf(FIXED_TIME_MILLIS), timestampedEvent.getHeaders().get("timestamp"));}
public void flume_f5425_0()
{    SimpleEvent base = new SimpleEvent();    Map<String, String> headersWithTimestamp = Maps.newHashMap();    headersWithTimestamp.put("timestamp", "-321");    base.setHeaders(headersWithTimestamp);    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals(-321L, timestampedEvent.getTimestamp());    assertEquals("-321", timestampedEvent.getHeaders().get("timestamp"));}
public void flume_f5426_0()
{    SimpleEvent base = new SimpleEvent();    Map<String, String> headersWithTimestamp = Maps.newHashMap();    headersWithTimestamp.put("@timestamp", "-999");    base.setHeaders(headersWithTimestamp);    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals(-999L, timestampedEvent.getTimestamp());    assertEquals("-999", timestampedEvent.getHeaders().get("@timestamp"));    assertNull(timestampedEvent.getHeaders().get("timestamp"));}
public void flume_f5427_0()
{    SimpleEvent base = new SimpleEvent();    base.setBody(new byte[] { 1, 2, 3, 4 });    Map<String, String> headersWithTimestamp = Maps.newHashMap();    headersWithTimestamp.put("foo", "bar");    base.setHeaders(headersWithTimestamp);    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals("bar", timestampedEvent.getHeaders().get("foo"));    assertArrayEquals(base.getBody(), timestampedEvent.getBody());}
public void flume_f5428_0() throws Exception
{    String indexPrefix = "qwerty";    String indexType = "uiop";    Event event = new SimpleEvent();    IndexRequestBuilder indexRequestBuilder = factory.createIndexRequest(FAKE_CLIENT, indexPrefix, indexType, event);    assertEquals(indexPrefix + '-' + ElasticSearchIndexRequestBuilderFactory.df.format(FIXED_TIME_MILLIS), indexRequestBuilder.request().index());    assertEquals(indexType, indexRequestBuilder.request().type());    assertArrayEquals(FakeEventSerializer.FAKE_BYTES, indexRequestBuilder.request().source().array());}
public void flume_f5429_0() throws Exception
{    String indexPrefix = "qwerty";    String indexType = "uiop";    Event event = new SimpleEvent();    event.getHeaders().put("timestamp", "1213141516");    IndexRequestBuilder indexRequestBuilder = factory.createIndexRequest(null, indexPrefix, indexType, event);    assertEquals(indexPrefix + '-' + ElasticSearchIndexRequestBuilderFactory.df.format(1213141516L), indexRequestBuilder.request().index());}
public void flume_f5430_0() throws Exception
{    String indexPrefix = "%{index-name}";    String indexType = "%{index-type}";    String indexValue = "testing-index-name-from-headers";    String typeValue = "testing-index-type-from-headers";    Event event = new SimpleEvent();    event.getHeaders().put("index-name", indexValue);    event.getHeaders().put("index-type", typeValue);    IndexRequestBuilder indexRequestBuilder = factory.createIndexRequest(null, indexPrefix, indexType, event);    assertEquals(indexValue + '-' + ElasticSearchIndexRequestBuilderFactory.df.format(FIXED_TIME_MILLIS), indexRequestBuilder.request().index());    assertEquals(typeValue, indexRequestBuilder.request().type());}
public void flume_f5431_0() throws Exception
{    assertFalse(serializer.configuredWithContext);    factory.configure(new Context());    assertTrue(serializer.configuredWithContext);    assertFalse(serializer.configuredWithComponentConfiguration);    factory.configure(new SinkConfiguration("name"));    assertTrue(serializer.configuredWithComponentConfiguration);}
public BytesStream flume_f5432_0(Event event) throws IOException
{    FastByteArrayOutputStream fbaos = new FastByteArrayOutputStream(4);    fbaos.write(FAKE_BYTES);    return fbaos;}
public void flume_f5433_0(Context arg0)
{    configuredWithContext = true;}
public void flume_f5434_0(ComponentConfiguration arg0)
{    configuredWithComponentConfiguration = true;}
public void flume_f5435_0() throws Exception
{    ElasticSearchLogStashEventSerializer fixture = new ElasticSearchLogStashEventSerializer();    Context context = new Context();    fixture.configure(context);    String message = "test body";    Map<String, String> headers = Maps.newHashMap();    long timestamp = System.currentTimeMillis();    headers.put("timestamp", String.valueOf(timestamp));    headers.put("source", "flume_tail_src");    headers.put("host", "test@localhost");    headers.put("src_path", "/tmp/test");    headers.put("headerNameOne", "headerValueOne");    headers.put("headerNameTwo", "headerValueTwo");    headers.put("type", "sometype");    Event event = EventBuilder.withBody(message.getBytes(charset));    event.setHeaders(headers);    XContentBuilder expected = jsonBuilder().startObject();    expected.field("@message", new String(message.getBytes(), charset));    expected.field("@timestamp", new Date(timestamp));    expected.field("@source", "flume_tail_src");    expected.field("@type", "sometype");    expected.field("@source_host", "test@localhost");    expected.field("@source_path", "/tmp/test");    expected.startObject("@fields");    expected.field("timestamp", String.valueOf(timestamp));    expected.field("src_path", "/tmp/test");    expected.field("host", "test@localhost");    expected.field("headerNameTwo", "headerValueTwo");    expected.field("source", "flume_tail_src");    expected.field("headerNameOne", "headerValueOne");    expected.field("type", "sometype");    expected.endObject();    expected.endObject();    XContentBuilder actual = fixture.getContentBuilder(event);    JsonParser parser = new JsonParser();    assertEquals(parser.parse(expected.string()), parser.parse(actual.string()));}
public void flume_f5436_0() throws Exception
{    ElasticSearchLogStashEventSerializer fixture = new ElasticSearchLogStashEventSerializer();    Context context = new Context();    fixture.configure(context);    String message = "{flume: somethingnotvalid}";    Map<String, String> headers = Maps.newHashMap();    long timestamp = System.currentTimeMillis();    headers.put("timestamp", String.valueOf(timestamp));    headers.put("source", "flume_tail_src");    headers.put("host", "test@localhost");    headers.put("src_path", "/tmp/test");    headers.put("headerNameOne", "headerValueOne");    headers.put("headerNameTwo", "headerValueTwo");    headers.put("type", "sometype");    Event event = EventBuilder.withBody(message.getBytes(charset));    event.setHeaders(headers);    XContentBuilder expected = jsonBuilder().startObject();    expected.field("@message", new String(message.getBytes(), charset));    expected.field("@timestamp", new Date(timestamp));    expected.field("@source", "flume_tail_src");    expected.field("@type", "sometype");    expected.field("@source_host", "test@localhost");    expected.field("@source_path", "/tmp/test");    expected.startObject("@fields");    expected.field("timestamp", String.valueOf(timestamp));    expected.field("src_path", "/tmp/test");    expected.field("host", "test@localhost");    expected.field("headerNameTwo", "headerValueTwo");    expected.field("source", "flume_tail_src");    expected.field("headerNameOne", "headerValueOne");    expected.field("type", "sometype");    expected.endObject();    expected.endObject();    XContentBuilder actual = fixture.getContentBuilder(event);    JsonParser parser = new JsonParser();    assertEquals(parser.parse(expected.string()), parser.parse(actual.string()));}
public void flume_f5437_0() throws Exception
{    initDefaults();    createNodes();    fixture = new ElasticSearchSink(true);    fixture.setName("ElasticSearchSink-" + UUID.randomUUID().toString());}
public void flume_f5438_0() throws Exception
{    shutdownNodes();}
public void flume_f5439_0() throws Exception
{    Configurables.configure(fixture, new Context(parameters));    Channel channel = bindAndStartChannel(fixture);    Transaction tx = channel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody("event #1 or 1".getBytes());    channel.put(event);    tx.commit();    tx.close();    fixture.process();    fixture.stop();    client.admin().indices().refresh(Requests.refreshRequest(timestampedIndexName)).actionGet();    assertMatchAllQuery(1, event);    assertBodyQuery(1, event);}
public void flume_f5440_0() throws Exception
{    parameters.put(BATCH_SIZE, "3");    Configurables.configure(fixture, new Context(parameters));    Channel channel = bindAndStartChannel(fixture);    Transaction tx = channel.getTransaction();    tx.begin();    Event event1 = EventBuilder.withBody("TEST1 {test}".getBytes());    channel.put(event1);    Event event2 = EventBuilder.withBody("{test: TEST2 }".getBytes());    channel.put(event2);    Event event3 = EventBuilder.withBody("{\"test\":{ TEST3 {test} }}".getBytes());    channel.put(event3);    tx.commit();    tx.close();    fixture.process();    fixture.stop();    client.admin().indices().refresh(Requests.refreshRequest(timestampedIndexName)).actionGet();    assertMatchAllQuery(3);    assertSearch(1, performSearch(QueryBuilders.fieldQuery("@message", "TEST1")), null, event1);    assertSearch(1, performSearch(QueryBuilders.fieldQuery("@message", "TEST2")), null, event2);    assertSearch(1, performSearch(QueryBuilders.fieldQuery("@message", "TEST3")), null, event3);}
public void flume_f5441_0() throws Exception
{    Configurables.configure(fixture, new Context(parameters));    Channel channel = bindAndStartChannel(fixture);    Transaction tx = channel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody("{\"event\":\"json content\",\"num\":1}".getBytes());    channel.put(event);    tx.commit();    tx.close();    fixture.process();    fixture.stop();    client.admin().indices().refresh(Requests.refreshRequest(timestampedIndexName)).actionGet();    Map<String, Object> expectedBody = new HashMap<String, Object>();    expectedBody.put("event", "json content");    expectedBody.put("num", 1);    assertSearch(1, performSearch(QueryBuilders.matchAllQuery()), expectedBody, event);    assertSearch(1, performSearch(QueryBuilders.fieldQuery("@message.event", "json")), expectedBody, event);}
public void flume_f5442_0() throws Exception
{        parameters.put(BATCH_SIZE, "5");    Configurables.configure(fixture, new Context(parameters));    Channel channel = bindAndStartChannel(fixture);    int numberOfEvents = 5;    Event[] events = new Event[numberOfEvents];    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < numberOfEvents; i++) {        String body = "event #" + i + " of " + numberOfEvents;        Event event = EventBuilder.withBody(body.getBytes());        events[i] = event;        channel.put(event);    }    tx.commit();    tx.close();    fixture.process();    fixture.stop();    client.admin().indices().refresh(Requests.refreshRequest(timestampedIndexName)).actionGet();    assertMatchAllQuery(numberOfEvents, events);    assertBodyQuery(5, events);}
public void flume_f5443_0() throws Exception
{    parameters.put(BATCH_SIZE, "2");    Configurables.configure(fixture, new Context(parameters));    Channel channel = bindAndStartChannel(fixture);    int numberOfEvents = 5;    Event[] events = new Event[numberOfEvents];    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < numberOfEvents; i++) {        String body = "event #" + i + " of " + numberOfEvents;        Event event = EventBuilder.withBody(body.getBytes());        events[i] = event;        channel.put(event);    }    tx.commit();    tx.close();    int count = 0;    Status status = Status.READY;    while (status != Status.BACKOFF) {        count++;        status = fixture.process();    }    fixture.stop();    assertEquals(3, count);    client.admin().indices().refresh(Requests.refreshRequest(timestampedIndexName)).actionGet();    assertMatchAllQuery(numberOfEvents, events);    assertBodyQuery(5, events);}
public void flume_f5444_0()
{    parameters.put(HOSTNAMES, "10.5.5.27");    parameters.put(CLUSTER_NAME, "testing-cluster-name");    parameters.put(INDEX_NAME, "testing-index-name");    parameters.put(INDEX_TYPE, "testing-index-type");    parameters.put(TTL, "10");    fixture = new ElasticSearchSink();    fixture.configure(new Context(parameters));    String[] expected = { "10.5.5.27" };    assertEquals("testing-cluster-name", fixture.getClusterName());    assertEquals("testing-index-name", fixture.getIndexName());    assertEquals("testing-index-type", fixture.getIndexType());    assertEquals(TimeUnit.DAYS.toMillis(10), fixture.getTTLMs());    assertArrayEquals(expected, fixture.getServerAddresses());}
public void flume_f5445_0()
{    parameters.put(HOSTNAMES, "10.5.5.27");    parameters.remove(INDEX_NAME);    parameters.remove(INDEX_TYPE);    parameters.remove(CLUSTER_NAME);    fixture = new ElasticSearchSink();    fixture.configure(new Context(parameters));    String[] expected = { "10.5.5.27" };    assertEquals(DEFAULT_INDEX_NAME, fixture.getIndexName());    assertEquals(DEFAULT_INDEX_TYPE, fixture.getIndexType());    assertEquals(DEFAULT_CLUSTER_NAME, fixture.getClusterName());    assertArrayEquals(expected, fixture.getServerAddresses());}
public void flume_f5446_0()
{    parameters.put(HOSTNAMES, "10.5.5.27,10.5.5.28,10.5.5.29");    fixture = new ElasticSearchSink();    fixture.configure(new Context(parameters));    String[] expected = { "10.5.5.27", "10.5.5.28", "10.5.5.29" };    assertArrayEquals(expected, fixture.getServerAddresses());}
public void flume_f5447_0()
{    parameters.put(HOSTNAMES, " 10.5.5.27 , 10.5.5.28 , 10.5.5.29 ");    fixture = new ElasticSearchSink();    fixture.configure(new Context(parameters));    String[] expected = { "10.5.5.27", "10.5.5.28", "10.5.5.29" };    assertArrayEquals(expected, fixture.getServerAddresses());}
public void flume_f5448_0()
{    parameters.put(HOSTNAMES, "10.5.5.27:9300,10.5.5.28:9301,10.5.5.29:9302");    fixture = new ElasticSearchSink();    fixture.configure(new Context(parameters));    String[] expected = { "10.5.5.27:9300", "10.5.5.28:9301", "10.5.5.29:9302" };    assertArrayEquals(expected, fixture.getServerAddresses());}
public void flume_f5449_0()
{    parameters.put(HOSTNAMES, " 10.5.5.27 : 9300 , 10.5.5.28 : 9301 , 10.5.5.29 : 9302 ");    fixture = new ElasticSearchSink();    fixture.configure(new Context(parameters));    String[] expected = { "10.5.5.27:9300", "10.5.5.28:9301", "10.5.5.29:9302" };    assertArrayEquals(expected, fixture.getServerAddresses());}
public void flume_f5450_0() throws Exception
{    parameters.put(SERIALIZER, CustomElasticSearchIndexRequestBuilderFactory.class.getName());    fixture.configure(new Context(parameters));    Channel channel = bindAndStartChannel(fixture);    Transaction tx = channel.getTransaction();    tx.begin();    String body = "{ foo: \"bar\" }";    Event event = EventBuilder.withBody(body.getBytes());    channel.put(event);    tx.commit();    tx.close();    fixture.process();    fixture.stop();    assertEquals(fixture.getIndexName() + "-05_17_36_789", CustomElasticSearchIndexRequestBuilderFactory.actualIndexName);    assertEquals(fixture.getIndexType(), CustomElasticSearchIndexRequestBuilderFactory.actualIndexType);    assertArrayEquals(event.getBody(), CustomElasticSearchIndexRequestBuilderFactory.actualEventBody);    assertTrue(CustomElasticSearchIndexRequestBuilderFactory.hasContext);}
public void flume_f5451_0()
{    Map<String, Long> testTTLMap = new HashMap<String, Long>();    testTTLMap.put("1ms", Long.valueOf(1));    testTTLMap.put("1s", Long.valueOf(1000));    testTTLMap.put("1m", Long.valueOf(60000));    testTTLMap.put("1h", Long.valueOf(3600000));    testTTLMap.put("1d", Long.valueOf(86400000));    testTTLMap.put("1w", Long.valueOf(604800000));    testTTLMap.put("1", Long.valueOf(86400000));    parameters.put(HOSTNAMES, "10.5.5.27");    parameters.put(CLUSTER_NAME, "testing-cluster-name");    parameters.put(INDEX_NAME, "testing-index-name");    parameters.put(INDEX_TYPE, "testing-index-type");    for (String ttl : testTTLMap.keySet()) {        parameters.put(TTL, ttl);        fixture = new ElasticSearchSink();        fixture.configure(new Context(parameters));        String[] expected = { "10.5.5.27" };        assertEquals("testing-cluster-name", fixture.getClusterName());        assertEquals("testing-index-name", fixture.getIndexName());        assertEquals("testing-index-type", fixture.getIndexType());        assertEquals((long) testTTLMap.get(ttl), fixture.getTTLMs());        assertArrayEquals(expected, fixture.getServerAddresses());    }}
protected void flume_f5452_0(IndexRequestBuilder indexRequest, String indexName, String indexType, Event event) throws IOException
{    actualIndexName = indexName;    actualIndexType = indexType;    actualEventBody = event.getBody();    indexRequest.setIndex(indexName).setType(indexType).setSource(event.getBody());}
public void flume_f5453_0(Context arg0)
{    hasContext = true;}
public void flume_f5454_0(ComponentConfiguration arg0)
{}
public void flume_f5455_0() throws Exception
{    parameters.put(SERIALIZER, "java.lang.String");    try {        Configurables.configure(fixture, new Context(parameters));    } catch (ClassCastException e) {        }    parameters.put(SERIALIZER, FakeConfigurable.class.getName());    try {        Configurables.configure(fixture, new Context(parameters));    } catch (IllegalArgumentException e) {        }}
public void flume_f5456_0() throws Exception
{    Context context = new Context();    context.put(SERIALIZER, "org.apache.flume.sink.elasticsearch.FakeEventSerializer");    assertNull(fixture.getEventSerializer());    fixture.configure(context);    assertTrue(fixture.getEventSerializer() instanceof FakeEventSerializer);}
public void flume_f5457_0() throws Exception
{    Context context = new Context();    context.put(ElasticSearchSinkConstants.INDEX_NAME_BUILDER, "org.apache.flume.sink.elasticsearch.FakeIndexNameBuilder");    assertNull(fixture.getIndexNameBuilder());    fixture.configure(context);    assertTrue(fixture.getIndexNameBuilder() instanceof FakeIndexNameBuilder);}
public void flume_f5458_0(Context arg0)
{}
public BytesStream flume_f5459_0(Event event) throws IOException
{    FastByteArrayOutputStream fbaos = new FastByteArrayOutputStream(4);    fbaos.write(FAKE_BYTES);    return fbaos;}
public void flume_f5460_0(Context arg0)
{    configuredWithContext = true;}
public void flume_f5461_0(ComponentConfiguration arg0)
{    configuredWithComponentConfiguration = true;}
public String flume_f5462_0(Event event)
{    return INDEX_NAME;}
public String flume_f5463_0(Event event)
{    return INDEX_NAME;}
public void flume_f5464_0(Context context)
{}
public void flume_f5465_0(ComponentConfiguration conf)
{}
public void flume_f5466_0()
{    sinkFactory = new DefaultSinkFactory();}
private void flume_f5467_0(String name, String type, Class<?> typeClass) throws FlumeException
{    Sink sink = sinkFactory.create(name, type);    Assert.assertNotNull(sink);    Assert.assertTrue(typeClass.isInstance(sink));}
public void flume_f5468_0()
{    verifySinkCreation("elasticsearch-sink", "elasticsearch", ElasticSearchSink.class);}
public void flume_f5469_0() throws Exception
{    Context context = new Context();    context.put(ElasticSearchSinkConstants.INDEX_NAME, "prefix");    indexNameBuilder = new TimeBasedIndexNameBuilder();    indexNameBuilder.configure(context);}
public void flume_f5470_0()
{    assertEquals("Coordinated Universal Time", indexNameBuilder.getFastDateFormat().getTimeZone().getDisplayName());}
public void flume_f5471_0()
{    long time = 987654321L;    Event event = new SimpleEvent();    Map<String, String> headers = new HashMap<String, String>();    headers.put("timestamp", Long.toString(time));    event.setHeaders(headers);    assertEquals("prefix-" + indexNameBuilder.getFastDateFormat().format(time), indexNameBuilder.getIndexName(event));}
public void flume_f5472_0()
{    DateTimeUtils.setCurrentMillisFixed(FIXED_TIME_MILLIS);}
public void flume_f5473_0()
{    SimpleEvent base = new SimpleEvent();    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals(FIXED_TIME_MILLIS, timestampedEvent.getTimestamp());    assertEquals(String.valueOf(FIXED_TIME_MILLIS), timestampedEvent.getHeaders().get("timestamp"));}
public void flume_f5474_0()
{    SimpleEvent base = new SimpleEvent();    Map<String, String> headersWithTimestamp = Maps.newHashMap();    headersWithTimestamp.put("timestamp", "-321");    base.setHeaders(headersWithTimestamp);    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals(-321L, timestampedEvent.getTimestamp());    assertEquals("-321", timestampedEvent.getHeaders().get("timestamp"));}
public void flume_f5475_0()
{    SimpleEvent base = new SimpleEvent();    Map<String, String> headersWithTimestamp = Maps.newHashMap();    headersWithTimestamp.put("@timestamp", "-999");    base.setHeaders(headersWithTimestamp);    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals(-999L, timestampedEvent.getTimestamp());    assertEquals("-999", timestampedEvent.getHeaders().get("@timestamp"));    assertNull(timestampedEvent.getHeaders().get("timestamp"));}
public void flume_f5476_0()
{    SimpleEvent base = new SimpleEvent();    base.setBody(new byte[] { 1, 2, 3, 4 });    Map<String, String> headersWithTimestamp = Maps.newHashMap();    headersWithTimestamp.put("foo", "bar");    base.setHeaders(headersWithTimestamp);    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals("bar", timestampedEvent.getHeaders().get("foo"));    assertArrayEquals(base.getBody(), timestampedEvent.getBody());}
public Status flume_f5477_1() throws EventDeliveryException
{    /*     * Reference to the boolean representing failure of the current transaction.     * Since each txn gets a new boolean, failure of one txn will not affect     * the next even if errbacks for the current txn get called while     * the next one is being processed.     *     */    if (!open) {        throw new EventDeliveryException("Sink was never opened. " + "Please fix the configuration.");    }    if (client == null) {        client = initHBaseClient();        if (client == null) {            throw new EventDeliveryException("Could not establish connection to HBase!");        }    }    AtomicBoolean txnFail = new AtomicBoolean(false);    AtomicInteger callbacksReceived = new AtomicInteger(0);    AtomicInteger callbacksExpected = new AtomicInteger(0);    final Lock lock = new ReentrantLock();    final Condition condition = lock.newCondition();    if (incrementBuffer != null) {        incrementBuffer.clear();    }    /*     * Callbacks can be reused per transaction, since they share the same     * locks and conditions.     */    Callback<Object, Object> putSuccessCallback = new SuccessCallback<Object, Object>(lock, callbacksReceived, condition);    Callback<Object, Exception> putFailureCallback = new FailureCallback<Object, Exception>(lock, callbacksReceived, txnFail, condition);    Callback<Long, Long> incrementSuccessCallback = new SuccessCallback<Long, Long>(lock, callbacksReceived, condition);    Callback<Long, Exception> incrementFailureCallback = new FailureCallback<Long, Exception>(lock, callbacksReceived, txnFail, condition);    Status status = Status.READY;    Channel channel = getChannel();    txn = channel.getTransaction();    txn.begin();    int i = 0;    try {        for (; i < batchSize; i++) {            Event event = channel.take();            if (event == null) {                status = Status.BACKOFF;                if (i == 0) {                    sinkCounter.incrementBatchEmptyCount();                } else {                    sinkCounter.incrementBatchUnderflowCount();                }                break;            } else {                serializer.setEvent(event);                List<PutRequest> actions = serializer.getActions();                List<AtomicIncrementRequest> increments = serializer.getIncrements();                callbacksExpected.addAndGet(actions.size());                if (!batchIncrements) {                    callbacksExpected.addAndGet(increments.size());                }                for (PutRequest action : actions) {                    action.setDurable(enableWal);                    client.put(action).addCallbacks(putSuccessCallback, putFailureCallback);                }                for (AtomicIncrementRequest increment : increments) {                    if (batchIncrements) {                        CellIdentifier identifier = new CellIdentifier(increment.key(), increment.qualifier());                        AtomicIncrementRequest request = incrementBuffer.get(identifier);                        if (request == null) {                            incrementBuffer.put(identifier, increment);                        } else {                            request.setAmount(request.getAmount() + increment.getAmount());                        }                    } else {                        client.atomicIncrement(increment).addCallbacks(incrementSuccessCallback, incrementFailureCallback);                    }                }            }        }        if (batchIncrements) {            Collection<AtomicIncrementRequest> increments = incrementBuffer.values();            for (AtomicIncrementRequest increment : increments) {                client.atomicIncrement(increment).addCallbacks(incrementSuccessCallback, incrementFailureCallback);            }            callbacksExpected.addAndGet(increments.size());        }        client.flush();    } catch (Throwable e) {        this.handleTransactionFailure(txn);        this.checkIfChannelExceptionAndThrow(e);    }    if (i == batchSize) {        sinkCounter.incrementBatchCompleteCount();    }    sinkCounter.addToEventDrainAttemptCount(i);    lock.lock();    long startTime = System.nanoTime();    long timeRemaining;    try {        while ((callbacksReceived.get() < callbacksExpected.get()) && !txnFail.get()) {            timeRemaining = timeout - (System.nanoTime() - startTime);            timeRemaining = (timeRemaining >= 0) ? timeRemaining : 0;            try {                if (!condition.await(timeRemaining, TimeUnit.NANOSECONDS)) {                    txnFail.set(true);                                    }            } catch (Exception ex) {                                this.handleTransactionFailure(txn);                Throwables.propagate(ex);            }        }    } finally {        lock.unlock();    }    if (isCoalesceTest) {        totalCallbacksReceived += callbacksReceived.get();    }    /*     * At this point, either the txn has failed     * or all callbacks received and txn is successful.     *     * This need not be in the monitor, since all callbacks for this txn     * have been received. So txnFail will not be modified any more(even if     * it is, it is set from true to true only - false happens only     * in the next process call).     *     */    if (txnFail.get()) {                if (lastTxnFailed) {            consecutiveHBaseFailures++;        }        lastTxnFailed = true;        this.handleTransactionFailure(txn);        throw new EventDeliveryException("Could not write events to Hbase. " + "Transaction failed, and rolled back.");    } else {        try {            lastTxnFailed = false;            consecutiveHBaseFailures = 0;            txn.commit();            txn.close();            sinkCounter.addToEventDrainSuccessCount(i);        } catch (Throwable e) {            this.handleTransactionFailure(txn);            this.checkIfChannelExceptionAndThrow(e);        }    }    return status;}
public void flume_f5478_1(Context context)
{    if (!HBaseVersionCheck.hasVersionLessThan2(logger)) {        throw new ConfigurationException("HBase major version number must be less than 2 for asynchbase sink. ");    }    tableName = context.getString(HBaseSinkConfigurationConstants.CONFIG_TABLE);    String cf = context.getString(HBaseSinkConfigurationConstants.CONFIG_COLUMN_FAMILY);    batchSize = context.getLong(HBaseSinkConfigurationConstants.CONFIG_BATCHSIZE, new Long(100));    serializerContext = new Context();        eventSerializerType = context.getString(HBaseSinkConfigurationConstants.CONFIG_SERIALIZER);    Preconditions.checkNotNull(tableName, "Table name cannot be empty, please specify in configuration file");    Preconditions.checkNotNull(cf, "Column family cannot be empty, please specify in configuration file");        if (eventSerializerType == null || eventSerializerType.isEmpty()) {        eventSerializerType = "org.apache.flume.sink.hbase.SimpleAsyncHbaseEventSerializer";            }    serializerContext.putAll(context.getSubProperties(HBaseSinkConfigurationConstants.CONFIG_SERIALIZER_PREFIX));    columnFamily = cf.getBytes(Charsets.UTF_8);    try {        @SuppressWarnings("unchecked")        Class<? extends AsyncHbaseEventSerializer> clazz = (Class<? extends AsyncHbaseEventSerializer>) Class.forName(eventSerializerType);        serializer = clazz.newInstance();        serializer.configure(serializerContext);        serializer.initialize(tableName.getBytes(Charsets.UTF_8), columnFamily);    } catch (Exception e) {                Throwables.propagate(e);    }    if (sinkCounter == null) {        sinkCounter = new SinkCounter(this.getName());    }    timeout = context.getLong(HBaseSinkConfigurationConstants.CONFIG_TIMEOUT, HBaseSinkConfigurationConstants.DEFAULT_TIMEOUT);    if (timeout <= 0) {                timeout = HBaseSinkConfigurationConstants.DEFAULT_TIMEOUT;    }        timeout = TimeUnit.MILLISECONDS.toNanos(timeout);    zkQuorum = context.getString(HBaseSinkConfigurationConstants.ZK_QUORUM, "").trim();    if (!zkQuorum.isEmpty()) {        zkBaseDir = context.getString(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, HBaseSinkConfigurationConstants.DEFAULT_ZK_ZNODE_PARENT);    } else {        if (conf == null) {                        conf = HBaseConfiguration.create();        }        zkQuorum = ZKConfig.getZKQuorumServersString(conf);        zkBaseDir = conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT);    }    Preconditions.checkState(zkQuorum != null && !zkQuorum.isEmpty(), "The Zookeeper quorum cannot be null and should be specified.");    enableWal = context.getBoolean(HBaseSinkConfigurationConstants.CONFIG_ENABLE_WAL, HBaseSinkConfigurationConstants.DEFAULT_ENABLE_WAL);        if (!enableWal) {            }    batchIncrements = context.getBoolean(HBaseSinkConfigurationConstants.CONFIG_COALESCE_INCREMENTS, HBaseSinkConfigurationConstants.DEFAULT_COALESCE_INCREMENTS);    if (batchIncrements) {        incrementBuffer = Maps.newHashMap();            }    maxConsecutiveFails = context.getInteger(HBaseSinkConfigurationConstants.CONFIG_MAX_CONSECUTIVE_FAILS, HBaseSinkConfigurationConstants.DEFAULT_MAX_CONSECUTIVE_FAILS);    Map<String, String> asyncProperties = context.getSubProperties(HBaseSinkConfigurationConstants.ASYNC_PREFIX);    asyncClientConfig = new Config();    asyncClientConfig.overrideConfig(HBaseSinkConfigurationConstants.ASYNC_ZK_QUORUM_KEY, zkQuorum);    asyncClientConfig.overrideConfig(HBaseSinkConfigurationConstants.ASYNC_ZK_BASEPATH_KEY, zkBaseDir);    for (String property : asyncProperties.keySet()) {        asyncClientConfig.overrideConfig(property, asyncProperties.get(property));    }}
 int flume_f5479_0()
{    return totalCallbacksReceived;}
 boolean flume_f5480_0()
{    return conf == null;}
public long flume_f5481_0()
{    return batchSize;}
public void flume_f5482_0()
{    Preconditions.checkArgument(client == null, "Please call stop " + "before calling start on an old instance.");    sinkCounter.start();    sinkCounter.incrementConnectionCreatedCount();    client = initHBaseClient();    super.start();}
private HBaseClient flume_f5483_1()
{        sinkCallbackPool = Executors.newCachedThreadPool(new ThreadFactoryBuilder().setNameFormat(this.getName() + " HBase Call Pool").build());        client = new HBaseClient(asyncClientConfig, new NioClientSocketChannelFactory(sinkCallbackPool, sinkCallbackPool));    final CountDownLatch latch = new CountDownLatch(1);    final AtomicBoolean fail = new AtomicBoolean(false);    client.ensureTableFamilyExists(tableName.getBytes(Charsets.UTF_8), columnFamily).addCallbacks(new Callback<Object, Object>() {        @Override        public Object call(Object arg) throws Exception {            latch.countDown();                        return null;        }    }, new Callback<Object, Object>() {        @Override        public Object call(Object arg) throws Exception {            fail.set(true);            latch.countDown();            return null;        }    });    try {                latch.await();            } catch (InterruptedException e) {        sinkCounter.incrementConnectionFailedCount();        throw new FlumeException("Interrupted while waiting for Hbase Callbacks", e);    }    if (fail.get()) {        sinkCounter.incrementConnectionFailedCount();        if (client != null) {            shutdownHBaseClient();        }        throw new FlumeException("Could not start sink. " + "Table or column family does not exist in Hbase.");    } else {        open = true;    }    client.setFlushInterval((short) 0);    return client;}
public Object flume_f5484_1(Object arg) throws Exception
{    latch.countDown();        return null;}
public Object flume_f5485_0(Object arg) throws Exception
{    fail.set(true);    latch.countDown();    return null;}
public void flume_f5486_1()
{    serializer.cleanUp();    if (client != null) {        shutdownHBaseClient();    }    sinkCounter.incrementConnectionClosedCount();    sinkCounter.stop();    try {        if (sinkCallbackPool != null) {            sinkCallbackPool.shutdown();            if (!sinkCallbackPool.awaitTermination(5, TimeUnit.SECONDS)) {                sinkCallbackPool.shutdownNow();            }        }    } catch (InterruptedException e) {                if (sinkCallbackPool != null) {            sinkCallbackPool.shutdownNow();        }    }    sinkCallbackPool = null;    client = null;    conf = null;    open = false;    super.stop();}
private void flume_f5487_1()
{        final CountDownLatch waiter = new CountDownLatch(1);    try {        client.shutdown().addCallback(new Callback<Object, Object>() {            @Override            public Object call(Object arg) throws Exception {                waiter.countDown();                return null;            }        }).addErrback(new Callback<Object, Object>() {            @Override            public Object call(Object arg) throws Exception {                                waiter.countDown();                return null;            }        });        if (!waiter.await(timeout, TimeUnit.NANOSECONDS)) {                    }    } catch (Exception ex) {            } finally {                client = null;    }}
public Object flume_f5488_0(Object arg) throws Exception
{    waiter.countDown();    return null;}
public Object flume_f5489_1(Object arg) throws Exception
{        waiter.countDown();    return null;}
private void flume_f5490_1(Transaction txn) throws EventDeliveryException
{    if (maxConsecutiveFails > 0 && consecutiveHBaseFailures >= maxConsecutiveFails) {        if (client != null) {            shutdownHBaseClient();        }        consecutiveHBaseFailures = 0;    }    try {        txn.rollback();    } catch (Throwable e) {                if (e instanceof Error || e instanceof RuntimeException) {                        Throwables.propagate(e);        } else {                        throw new EventDeliveryException("Failed to commit transaction." + "Transaction rolled back.", e);        }    } finally {        txn.close();    }}
public R flume_f5491_0(T arg) throws Exception
{    if (isTimeoutTesting) {        try {                        TimeUnit.NANOSECONDS.sleep(TimeUnit.SECONDS.toNanos(4));        } catch (InterruptedException e) {                }    }    doCall();    return null;}
private void flume_f5492_0() throws Exception
{    callbacksReceived.incrementAndGet();    lock.lock();    try {        condition.signal();    } finally {        lock.unlock();    }}
public R flume_f5493_1(T arg) throws Exception
{        if (isTimeoutTesting) {                try {            TimeUnit.NANOSECONDS.sleep(TimeUnit.SECONDS.toNanos(4));        } catch (InterruptedException e) {                }    }    doCall();    return null;}
private void flume_f5494_0() throws Exception
{    callbacksReceived.incrementAndGet();    this.txnFail.set(true);    lock.lock();    try {        condition.signal();    } finally {        lock.unlock();    }}
private void flume_f5495_0(Throwable e) throws EventDeliveryException
{    if (e instanceof ChannelException) {        throw new EventDeliveryException("Error in processing transaction.", e);    } else if (e instanceof Error || e instanceof RuntimeException) {        Throwables.propagate(e);    }    throw new EventDeliveryException("Error in processing transaction.", e);}
public int flume_f5496_0()
{    return hashCode;}
public boolean flume_f5497_0(Object other)
{    CellIdentifier o = (CellIdentifier) other;    if (other == null) {        return false;    } else {        return (COMPARATOR.compare(row, o.row) == 0 && COMPARATOR.compare(column, o.column) == 0);    }}
public void flume_f5498_1()
{    Preconditions.checkArgument(table == null, "Please call stop " + "before calling start on an old instance.");    try {        privilegedExecutor = FlumeAuthenticationUtil.getAuthenticator(kerberosPrincipal, kerberosKeytab);    } catch (Exception ex) {        sinkCounter.incrementConnectionFailedCount();        throw new FlumeException("Failed to login to HBase using " + "provided credentials.", ex);    }    try {        table = privilegedExecutor.execute(new PrivilegedExceptionAction<HTable>() {            @Override            public HTable run() throws Exception {                HTable table = new HTable(config, tableName);                table.setAutoFlush(false);                                return table;            }        });    } catch (Exception e) {        sinkCounter.incrementConnectionFailedCount();                throw new FlumeException("Could not load table, " + tableName + " from HBase", e);    }    try {        if (!privilegedExecutor.execute(new PrivilegedExceptionAction<Boolean>() {            @Override            public Boolean run() throws IOException {                return table.getTableDescriptor().hasFamily(columnFamily);            }        })) {            throw new IOException("Table " + tableName + " has no such column family " + Bytes.toString(columnFamily));        }    } catch (Exception e) {                        sinkCounter.incrementConnectionFailedCount();        throw new FlumeException("Error getting column family from HBase." + "Please verify that the table " + tableName + " and Column Family, " + Bytes.toString(columnFamily) + " exists in HBase, and the" + " current user has permissions to access that table.", e);    }    super.start();    sinkCounter.incrementConnectionCreatedCount();    sinkCounter.start();}
public HTable flume_f5499_0() throws Exception
{    HTable table = new HTable(config, tableName);    table.setAutoFlush(false);        return table;}
public Boolean flume_f5500_0() throws IOException
{    return table.getTableDescriptor().hasFamily(columnFamily);}
public void flume_f5501_0()
{    try {        if (table != null) {            table.close();        }        table = null;    } catch (IOException e) {        throw new FlumeException("Error closing table.", e);    }    sinkCounter.incrementConnectionClosedCount();    sinkCounter.stop();}
public void flume_f5502_1(Context context)
{    if (!HBaseVersionCheck.hasVersionLessThan2(logger)) {        throw new ConfigurationException("HBase major version number must be less than 2 for hbase-sink.");    }    tableName = context.getString(HBaseSinkConfigurationConstants.CONFIG_TABLE);    String cf = context.getString(HBaseSinkConfigurationConstants.CONFIG_COLUMN_FAMILY);    batchSize = context.getLong(HBaseSinkConfigurationConstants.CONFIG_BATCHSIZE, new Long(100));    serializerContext = new Context();        eventSerializerType = context.getString(HBaseSinkConfigurationConstants.CONFIG_SERIALIZER);    Preconditions.checkNotNull(tableName, "Table name cannot be empty, please specify in configuration file");    Preconditions.checkNotNull(cf, "Column family cannot be empty, please specify in configuration file");        if (eventSerializerType == null || eventSerializerType.isEmpty()) {        eventSerializerType = "org.apache.flume.sink.hbase.SimpleHbaseEventSerializer";            }    serializerContext.putAll(context.getSubProperties(HBaseSinkConfigurationConstants.CONFIG_SERIALIZER_PREFIX));    columnFamily = cf.getBytes(Charsets.UTF_8);    try {        Class<? extends HbaseEventSerializer> clazz = (Class<? extends HbaseEventSerializer>) Class.forName(eventSerializerType);        serializer = clazz.newInstance();        serializer.configure(serializerContext);    } catch (Exception e) {                Throwables.propagate(e);    }    kerberosKeytab = context.getString(HBaseSinkConfigurationConstants.CONFIG_KEYTAB);    kerberosPrincipal = context.getString(HBaseSinkConfigurationConstants.CONFIG_PRINCIPAL);    enableWal = context.getBoolean(HBaseSinkConfigurationConstants.CONFIG_ENABLE_WAL, HBaseSinkConfigurationConstants.DEFAULT_ENABLE_WAL);        if (!enableWal) {            }    batchIncrements = context.getBoolean(HBaseSinkConfigurationConstants.CONFIG_COALESCE_INCREMENTS, HBaseSinkConfigurationConstants.DEFAULT_COALESCE_INCREMENTS);    if (batchIncrements) {                refGetFamilyMap = reflectLookupGetFamilyMap();    }    String zkQuorum = context.getString(HBaseSinkConfigurationConstants.ZK_QUORUM);    Integer port = null;    /**     * HBase allows multiple nodes in the quorum, but all need to use the     * same client port. So get the nodes in host:port format,     * and ignore the ports for all nodes except the first one. If no port is     * specified, use default.     */    if (zkQuorum != null && !zkQuorum.isEmpty()) {        StringBuilder zkBuilder = new StringBuilder();                String[] zkHosts = zkQuorum.split(",");        int length = zkHosts.length;        for (int i = 0; i < length; i++) {            String[] zkHostAndPort = zkHosts[i].split(":");            zkBuilder.append(zkHostAndPort[0].trim());            if (i != length - 1) {                zkBuilder.append(",");            } else {                zkQuorum = zkBuilder.toString();            }            if (zkHostAndPort[1] == null) {                throw new FlumeException("Expected client port for the ZK node!");            }            if (port == null) {                port = Integer.parseInt(zkHostAndPort[1].trim());            } else if (!port.equals(Integer.parseInt(zkHostAndPort[1].trim()))) {                throw new FlumeException("All Zookeeper nodes in the quorum must " + "use the same client port.");            }        }        if (port == null) {            port = HConstants.DEFAULT_ZOOKEPER_CLIENT_PORT;        }        this.config.set(HConstants.ZOOKEEPER_QUORUM, zkQuorum);        this.config.setInt(HConstants.ZOOKEEPER_CLIENT_PORT, port);    }    String hbaseZnode = context.getString(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT);    if (hbaseZnode != null && !hbaseZnode.isEmpty()) {        this.config.set(HConstants.ZOOKEEPER_ZNODE_PARENT, hbaseZnode);    }    sinkCounter = new SinkCounter(this.getName());}
public Configuration flume_f5503_0()
{    return config;}
public Status flume_f5504_1() throws EventDeliveryException
{    Status status = Status.READY;    Channel channel = getChannel();    Transaction txn = channel.getTransaction();    List<Row> actions = new LinkedList<Row>();    List<Increment> incs = new LinkedList<Increment>();    try {        txn.begin();        if (serializer instanceof BatchAware) {            ((BatchAware) serializer).onBatchStart();        }        long i = 0;        for (; i < batchSize; i++) {            Event event = channel.take();            if (event == null) {                if (i == 0) {                    status = Status.BACKOFF;                    sinkCounter.incrementBatchEmptyCount();                } else {                    sinkCounter.incrementBatchUnderflowCount();                }                break;            } else {                serializer.initialize(event, columnFamily);                actions.addAll(serializer.getActions());                incs.addAll(serializer.getIncrements());            }        }        if (i == batchSize) {            sinkCounter.incrementBatchCompleteCount();        }        sinkCounter.addToEventDrainAttemptCount(i);        putEventsAndCommit(actions, incs, txn);    } catch (Throwable e) {        try {            txn.rollback();        } catch (Exception e2) {                    }                if (e instanceof Error || e instanceof RuntimeException) {                        Throwables.propagate(e);        } else {                        throw new EventDeliveryException("Failed to commit transaction." + "Transaction rolled back.", e);        }    } finally {        txn.close();    }    return status;}
private void flume_f5505_0(final List<Row> actions, final List<Increment> incs, Transaction txn) throws Exception
{    privilegedExecutor.execute(new PrivilegedExceptionAction<Void>() {        @Override        public Void run() throws Exception {            for (Row r : actions) {                if (r instanceof Put) {                    ((Put) r).setWriteToWAL(enableWal);                }                                if (r instanceof Increment) {                    ((Increment) r).setWriteToWAL(enableWal);                }            }            table.batch(actions);            return null;        }    });    privilegedExecutor.execute(new PrivilegedExceptionAction<Void>() {        @Override        public Void run() throws Exception {            List<Increment> processedIncrements;            if (batchIncrements) {                processedIncrements = coalesceIncrements(incs);            } else {                processedIncrements = incs;            }                        if (debugIncrCallback != null) {                debugIncrCallback.onAfterCoalesce(processedIncrements);            }            for (final Increment i : processedIncrements) {                i.setWriteToWAL(enableWal);                table.increment(i);            }            return null;        }    });    txn.commit();    sinkCounter.addToEventDrainSuccessCount(actions.size());}
public Void flume_f5506_0() throws Exception
{    for (Row r : actions) {        if (r instanceof Put) {            ((Put) r).setWriteToWAL(enableWal);        }                if (r instanceof Increment) {            ((Increment) r).setWriteToWAL(enableWal);        }    }    table.batch(actions);    return null;}
public Void flume_f5507_0() throws Exception
{    List<Increment> processedIncrements;    if (batchIncrements) {        processedIncrements = coalesceIncrements(incs);    } else {        processedIncrements = incs;    }        if (debugIncrCallback != null) {        debugIncrCallback.onAfterCoalesce(processedIncrements);    }    for (final Increment i : processedIncrements) {        i.setWriteToWAL(enableWal);        table.increment(i);    }    return null;}
private Map<byte[], NavigableMap<byte[], Long>> flume_f5509_1(Increment inc)
{    Preconditions.checkNotNull(refGetFamilyMap, "Increment.getFamilymap() not found");    Preconditions.checkNotNull(inc, "Increment required");    Map<byte[], NavigableMap<byte[], Long>> familyMap = null;    try {        Object familyObj = refGetFamilyMap.invoke(inc);        familyMap = (Map<byte[], NavigableMap<byte[], Long>>) familyObj;    } catch (IllegalAccessException e) {                Throwables.propagate(e);    } catch (InvocationTargetException e) {                Throwables.propagate(e);    }    return familyMap;}
private List<Increment> flume_f5510_0(Iterable<Increment> incs)
{    Preconditions.checkNotNull(incs, "List of Increments must not be null");            Map<byte[], Map<byte[], NavigableMap<byte[], Long>>> counters = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);    for (Increment inc : incs) {        byte[] row = inc.getRow();        Map<byte[], NavigableMap<byte[], Long>> families = getFamilyMap(inc);        for (Map.Entry<byte[], NavigableMap<byte[], Long>> familyEntry : families.entrySet()) {            byte[] family = familyEntry.getKey();            NavigableMap<byte[], Long> qualifiers = familyEntry.getValue();            for (Map.Entry<byte[], Long> qualifierEntry : qualifiers.entrySet()) {                byte[] qualifier = qualifierEntry.getKey();                Long count = qualifierEntry.getValue();                incrementCounter(counters, row, family, qualifier, count);            }        }    }        List<Increment> coalesced = Lists.newLinkedList();    for (Map.Entry<byte[], Map<byte[], NavigableMap<byte[], Long>>> rowEntry : counters.entrySet()) {        byte[] row = rowEntry.getKey();        Map<byte[], NavigableMap<byte[], Long>> families = rowEntry.getValue();        Increment inc = new Increment(row);        for (Map.Entry<byte[], NavigableMap<byte[], Long>> familyEntry : families.entrySet()) {            byte[] family = familyEntry.getKey();            NavigableMap<byte[], Long> qualifiers = familyEntry.getValue();            for (Map.Entry<byte[], Long> qualifierEntry : qualifiers.entrySet()) {                byte[] qualifier = qualifierEntry.getKey();                long count = qualifierEntry.getValue();                inc.addColumn(family, qualifier, count);            }        }        coalesced.add(inc);    }    return coalesced;}
private void flume_f5511_0(Map<byte[], Map<byte[], NavigableMap<byte[], Long>>> counters, byte[] row, byte[] family, byte[] qualifier, Long count)
{    Map<byte[], NavigableMap<byte[], Long>> families = counters.get(row);    if (families == null) {        families = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);        counters.put(row, families);    }    NavigableMap<byte[], Long> qualifiers = families.get(family);    if (qualifiers == null) {        qualifiers = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);        families.put(family, qualifiers);    }    Long existingValue = qualifiers.get(qualifier);    if (existingValue == null) {        qualifiers.put(qualifier, count);    } else {        qualifiers.put(qualifier, existingValue + count);    }}
 HbaseEventSerializer flume_f5512_0()
{    return serializer;}
public long flume_f5513_0()
{    return batchSize;}
private static int flume_f5514_0(String version) throws NumberFormatException
{    return Integer.parseInt(version.split("\\.")[0]);}
 static boolean flume_f5515_1(Logger logger)
{    String version = VersionInfo.getVersion();    try {        if (getMajorVersion(version) < 2) {            return true;        }    } catch (NumberFormatException ex) {            }        return false;}
public void flume_f5516_0(Context context)
{    String regex = context.getString(REGEX_CONFIG, REGEX_DEFAULT);    regexIgnoreCase = context.getBoolean(IGNORE_CASE_CONFIG, IGNORE_CASE_DEFAULT);    depositHeaders = context.getBoolean(DEPOSIT_HEADERS_CONFIG, DEPOSIT_HEADERS_DEFAULT);    inputPattern = Pattern.compile(regex, Pattern.DOTALL + (regexIgnoreCase ? Pattern.CASE_INSENSITIVE : 0));    charset = Charset.forName(context.getString(CHARSET_CONFIG, CHARSET_DEFAULT));    String colNameStr = context.getString(COL_NAME_CONFIG, COLUMN_NAME_DEFAULT);    String[] columnNames = colNameStr.split(",");    for (String s : columnNames) {        colNames.add(s.getBytes(charset));    }        rowKeyIndex = context.getInteger(ROW_KEY_INDEX_CONFIG, -1);        if (rowKeyIndex >= 0) {        if (rowKeyIndex >= columnNames.length) {            throw new IllegalArgumentException(ROW_KEY_INDEX_CONFIG + " must be " + "less than num columns " + columnNames.length);        }        if (!ROW_KEY_NAME.equalsIgnoreCase(columnNames[rowKeyIndex])) {            throw new IllegalArgumentException("Column at " + rowKeyIndex + " must be " + ROW_KEY_NAME + " and is " + columnNames[rowKeyIndex]);        }    }}
public void flume_f5517_0(ComponentConfiguration conf)
{}
public void flume_f5518_0(Event event, byte[] columnFamily)
{    this.headers = event.getHeaders();    this.payload = event.getBody();    this.cf = columnFamily;}
protected byte[] flume_f5519_0(Calendar cal)
{    /* NOTE: This key generation strategy has the following properties:     *      * 1) Within a single JVM, the same row key will never be duplicated.     * 2) Amongst any two JVM's operating at different time periods (according     *    to their respective clocks), the same row key will never be      *    duplicated.     * 3) Amongst any two JVM's operating concurrently (according to their     *    respective clocks), the odds of duplicating a row-key are non-zero     *    but infinitesimal. This would require simultaneous collision in (a)      *    the timestamp (b) the respective nonce and (c) the random string.     *    The string is necessary since (a) and (b) could collide if a fleet     *    of Flume agents are restarted in tandem.     *         *  Row-key uniqueness is important because conflicting row-keys will cause     *  data loss. */    String rowKey = String.format("%s-%s-%s", cal.getTimeInMillis(), randomKey, nonce.getAndIncrement());    return rowKey.getBytes(charset);}
protected byte[] flume_f5520_0()
{    return getRowKey(Calendar.getInstance());}
public List<Row> flume_f5521_0() throws FlumeException
{    List<Row> actions = Lists.newArrayList();    byte[] rowKey;    Matcher m = inputPattern.matcher(new String(payload, charset));    if (!m.matches()) {        return Lists.newArrayList();    }    if (m.groupCount() != colNames.size()) {        return Lists.newArrayList();    }    try {        if (rowKeyIndex < 0) {            rowKey = getRowKey();        } else {            rowKey = m.group(rowKeyIndex + 1).getBytes(Charsets.UTF_8);        }        Put put = new Put(rowKey);        for (int i = 0; i < colNames.size(); i++) {            if (i != rowKeyIndex) {                put.add(cf, colNames.get(i), m.group(i + 1).getBytes(Charsets.UTF_8));            }        }        if (depositHeaders) {            for (Map.Entry<String, String> entry : headers.entrySet()) {                put.add(cf, entry.getKey().getBytes(charset), entry.getValue().getBytes(charset));            }        }        actions.add(put);    } catch (Exception e) {        throw new FlumeException("Could not get row key!", e);    }    return actions;}
public List<Increment> flume_f5522_0()
{    return Lists.newArrayList();}
public void flume_f5523_0()
{}
public void flume_f5524_0(byte[] table, byte[] cf)
{    this.table = table;    this.cf = cf;}
public List<PutRequest> flume_f5525_0()
{    List<PutRequest> actions = new ArrayList<PutRequest>();    if (payloadColumn != null) {        byte[] rowKey;        try {            switch(keyType) {                case TS:                    rowKey = SimpleRowKeyGenerator.getTimestampKey(rowPrefix);                    break;                case TSNANO:                    rowKey = SimpleRowKeyGenerator.getNanoTimestampKey(rowPrefix);                    break;                case RANDOM:                    rowKey = SimpleRowKeyGenerator.getRandomKey(rowPrefix);                    break;                default:                    rowKey = SimpleRowKeyGenerator.getUUIDKey(rowPrefix);                    break;            }            PutRequest putRequest = new PutRequest(table, rowKey, cf, payloadColumn, payload);            actions.add(putRequest);        } catch (Exception e) {            throw new FlumeException("Could not get row key!", e);        }    }    return actions;}
public List<AtomicIncrementRequest> flume_f5526_0()
{    List<AtomicIncrementRequest> actions = new ArrayList<AtomicIncrementRequest>();    if (incrementColumn != null) {        AtomicIncrementRequest inc = new AtomicIncrementRequest(table, incrementRow, cf, incrementColumn);        actions.add(inc);    }    return actions;}
public void flume_f5527_0()
{}
public void flume_f5528_0(Context context)
{    String pCol = context.getString("payloadColumn", "pCol");    String iCol = context.getString("incrementColumn", "iCol");    rowPrefix = context.getString("rowPrefix", "default");    String suffix = context.getString("suffix", "uuid");    if (pCol != null && !pCol.isEmpty()) {        if (suffix.equals("timestamp")) {            keyType = KeyType.TS;        } else if (suffix.equals("random")) {            keyType = KeyType.RANDOM;        } else if (suffix.equals("nano")) {            keyType = KeyType.TSNANO;        } else {            keyType = KeyType.UUID;        }        payloadColumn = pCol.getBytes(Charsets.UTF_8);    }    if (iCol != null && !iCol.isEmpty()) {        incrementColumn = iCol.getBytes(Charsets.UTF_8);    }    incrementRow = context.getString("incrementRow", "incRow").getBytes(Charsets.UTF_8);}
public void flume_f5529_0(Event event)
{    this.payload = event.getBody();}
public void flume_f5530_0(ComponentConfiguration conf)
{}
public void flume_f5531_0(Context context)
{    rowPrefix = context.getString("rowPrefix", "default");    incrementRow = context.getString("incrementRow", "incRow").getBytes(Charsets.UTF_8);    String suffix = context.getString("suffix", "uuid");    String payloadColumn = context.getString("payloadColumn", "pCol");    String incColumn = context.getString("incrementColumn", "iCol");    if (payloadColumn != null && !payloadColumn.isEmpty()) {        if (suffix.equals("timestamp")) {            keyType = KeyType.TS;        } else if (suffix.equals("random")) {            keyType = KeyType.RANDOM;        } else if (suffix.equals("nano")) {            keyType = KeyType.TSNANO;        } else {            keyType = KeyType.UUID;        }        plCol = payloadColumn.getBytes(Charsets.UTF_8);    }    if (incColumn != null && !incColumn.isEmpty()) {        incCol = incColumn.getBytes(Charsets.UTF_8);    }}
public void flume_f5532_0(ComponentConfiguration conf)
{}
public void flume_f5533_0(Event event, byte[] cf)
{    this.payload = event.getBody();    this.cf = cf;}
public List<Row> flume_f5534_0() throws FlumeException
{    List<Row> actions = new LinkedList<Row>();    if (plCol != null) {        byte[] rowKey;        try {            if (keyType == KeyType.TS) {                rowKey = SimpleRowKeyGenerator.getTimestampKey(rowPrefix);            } else if (keyType == KeyType.RANDOM) {                rowKey = SimpleRowKeyGenerator.getRandomKey(rowPrefix);            } else if (keyType == KeyType.TSNANO) {                rowKey = SimpleRowKeyGenerator.getNanoTimestampKey(rowPrefix);            } else {                rowKey = SimpleRowKeyGenerator.getUUIDKey(rowPrefix);            }            Put put = new Put(rowKey);            put.add(cf, plCol, payload);            actions.add(put);        } catch (Exception e) {            throw new FlumeException("Could not get row key!", e);        }    }    return actions;}
public List<Increment> flume_f5535_0()
{    List<Increment> incs = new LinkedList<Increment>();    if (incCol != null) {        Increment inc = new Increment(incrementRow);        inc.addColumn(cf, incCol, 1);        incs.add(inc);    }    return incs;}
public void flume_f5536_0()
{}
public static byte[] flume_f5537_0(String prefix) throws UnsupportedEncodingException
{    return (prefix + UUID.randomUUID().toString()).getBytes("UTF8");}
public static byte[] flume_f5538_0(String prefix) throws UnsupportedEncodingException
{    return (prefix + String.valueOf(new Random().nextLong())).getBytes("UTF8");}
public static byte[] flume_f5539_0(String prefix) throws UnsupportedEncodingException
{    return (prefix + String.valueOf(System.currentTimeMillis())).getBytes("UTF8");}
public static byte[] flume_f5540_0(String prefix) throws UnsupportedEncodingException
{    return (prefix + String.valueOf(System.nanoTime())).getBytes("UTF8");}
public void flume_f5541_0(byte[] table, byte[] cf)
{    this.table = table;    this.cf = cf;}
public void flume_f5542_0(Event event)
{    this.currentEvent = event;}
public List<PutRequest> flume_f5543_0()
{    return Collections.emptyList();}
public List<AtomicIncrementRequest> flume_f5544_0()
{    List<AtomicIncrementRequest> incrs = new ArrayList<AtomicIncrementRequest>();    AtomicIncrementRequest incr = new AtomicIncrementRequest(table, currentEvent.getBody(), cf, column, 1);    incrs.add(incr);    return incrs;}
public void flume_f5545_0()
{}
public void flume_f5546_0(Context context)
{    column = context.getString("column", "col").getBytes();}
public void flume_f5547_0(ComponentConfiguration conf)
{}
public void flume_f5548_0(Context context)
{}
public void flume_f5549_0(ComponentConfiguration conf)
{}
public void flume_f5550_0()
{}
public void flume_f5551_0(Event event, byte[] columnFamily)
{    this.event = event;    this.family = columnFamily;}
public List<Row> flume_f5552_0()
{    return Collections.emptyList();}
public List<Increment> flume_f5553_0()
{    List<Increment> increments = Lists.newArrayList();    String body = new String(event.getBody(), Charsets.UTF_8);    String[] pieces = body.split(":");    String row = pieces[0];    String qualifier = pieces[1];    Increment inc = new Increment(row.getBytes(Charsets.UTF_8));    inc.addColumn(family, qualifier.getBytes(Charsets.UTF_8), 1L);    increments.add(inc);    return increments;}
public void flume_f5554_0()
{    numBatchesStarted++;}
public int flume_f5555_0()
{    return numBatchesStarted;}
public List<Row> flume_f5556_0() throws FlumeException
{    if (throwException) {        throw new FlumeException("Exception for testing");    }    return super.getActions();}
public static void flume_f5557_0() throws Exception
{    testUtility.startMiniCluster();    Map<String, String> ctxMap = new HashMap<String, String>();    ctxMap.put("table", tableName);    ctxMap.put("columnFamily", columnFamily);    ctxMap.put("serializer", "org.apache.flume.sink.hbase.SimpleAsyncHbaseEventSerializer");    ctxMap.put("serializer.payloadColumn", plCol);    ctxMap.put("serializer.incrementColumn", inColumn);    ctxMap.put("keep-alive", "0");    ctxMap.put("timeout", "10000");    ctx.putAll(ctxMap);    os = ManagementFactory.getOperatingSystemMXBean();}
public static void flume_f5558_0() throws Exception
{    testUtility.shutdownMiniCluster();}
public void flume_f5559_0() throws Exception
{    if (deleteTable) {        testUtility.deleteTable(tableName.getBytes());    }}
public void flume_f5560_0() throws Exception
{    Map<String, String> ctxMap = new HashMap<String, String>();    ctxMap.put("table", tableName);    ctxMap.put("columnFamily", columnFamily);    ctxMap.put("serializer", "org.apache.flume.sink.hbase.SimpleAsyncHbaseEventSerializer");    ctxMap.put("keep-alive", "0");    ctxMap.put("timeout", "10000");    Context tmpctx = new Context();    tmpctx.putAll(ctxMap);    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration());    Configurables.configure(sink, tmpctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, tmpctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase));    channel.put(e);    tx.commit();    tx.close();    Assert.assertFalse(sink.isConfNull());    sink.process();    sink.stop();    HTable table = new HTable(testUtility.getConfiguration(), tableName);    byte[][] results = getResults(table, 1);    byte[] out = results[0];    Assert.assertArrayEquals(e.getBody(), out);    out = results[1];    Assert.assertArrayEquals(Longs.toByteArray(1), out);}
public void flume_f5561_0() throws Exception
{    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration());    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase));    channel.put(e);    tx.commit();    tx.close();    Assert.assertFalse(sink.isConfNull());    sink.process();    sink.stop();    HTable table = new HTable(testUtility.getConfiguration(), tableName);    byte[][] results = getResults(table, 1);    byte[] out = results[0];    Assert.assertArrayEquals(e.getBody(), out);    out = results[1];    Assert.assertArrayEquals(Longs.toByteArray(1), out);}
public void flume_f5562_0() throws Exception
{    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration());    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    Assert.assertFalse(sink.isConfNull());    sink.process();    sink.stop();    HTable table = new HTable(testUtility.getConfiguration(), tableName);    byte[][] results = getResults(table, 3);    byte[] out;    int found = 0;    for (int i = 0; i < 3; i++) {        for (int j = 0; j < 3; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(3, found);    out = results[3];    Assert.assertArrayEquals(Longs.toByteArray(3), out);}
public void flume_f5563_0() throws Exception
{    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration(), true, false);    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    channel.start();    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    Assert.assertFalse(sink.isConfNull());    sink.process();    Assert.fail();}
public void flume_f5564_0() throws Exception
{    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    ctx.put("batchSize", "2");    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration());    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    int count = 0;    Status status = Status.READY;    while (status != Status.BACKOFF) {        count++;        status = sink.process();    }    Assert.assertFalse(sink.isConfNull());    sink.stop();    Assert.assertEquals(2, count);    HTable table = new HTable(testUtility.getConfiguration(), tableName);    byte[][] results = getResults(table, 3);    byte[] out;    int found = 0;    for (int i = 0; i < 3; i++) {        for (int j = 0; j < 3; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(3, found);    out = results[3];    Assert.assertArrayEquals(Longs.toByteArray(3), out);}
public void flume_f5565_0() throws Exception
{    doTestMultipleBatchesBatchIncrements(true);}
public void flume_f5566_0() throws Exception
{    doTestMultipleBatchesBatchIncrements(false);}
public void flume_f5567_0(boolean coalesce) throws Exception
{    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration(), false, true);    if (coalesce) {        ctx.put(HBaseSinkConfigurationConstants.CONFIG_COALESCE_INCREMENTS, "true");    }    ctx.put("batchSize", "2");    ctx.put("serializer", IncrementAsyncHBaseSerializer.class.getName());    ctx.put("serializer.column", "test");    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");        ctx.put("serializer", SimpleAsyncHbaseEventSerializer.class.getName());        ctx.put(HBaseSinkConfigurationConstants.CONFIG_COALESCE_INCREMENTS, "false");    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 4; i++) {        for (int j = 0; j < 3; j++) {            Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));            channel.put(e);        }    }    tx.commit();    tx.close();    int count = 0;    Status status = Status.READY;    while (status != Status.BACKOFF) {        count++;        status = sink.process();    }    Assert.assertFalse(sink.isConfNull());    sink.stop();    Assert.assertEquals(7, count);    HTable table = new HTable(testUtility.getConfiguration(), tableName);    Scan scan = new Scan();    scan.addColumn(columnFamily.getBytes(), "test".getBytes());    scan.setStartRow(Bytes.toBytes(valBase));    ResultScanner rs = table.getScanner(scan);    int i = 0;    try {        for (Result r = rs.next(); r != null; r = rs.next()) {            byte[] out = r.getValue(columnFamily.getBytes(), "test".getBytes());            Assert.assertArrayEquals(Longs.toByteArray(3), out);            Assert.assertTrue(new String(r.getRow()).startsWith(valBase));            i++;        }    } finally {        rs.close();    }    Assert.assertEquals(4, i);    if (coalesce) {        Assert.assertEquals(8, sink.getTotalCallbacksReceived());    } else {        Assert.assertEquals(12, sink.getTotalCallbacksReceived());    }}
public void flume_f5568_0() throws Exception
{    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    ctx.put("batchSize", "2");    ctx.put(HBaseSinkConfigurationConstants.ZK_QUORUM, ZKConfig.getZKQuorumServersString(testUtility.getConfiguration()));    ctx.put(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, testUtility.getConfiguration().get(HConstants.ZOOKEEPER_ZNODE_PARENT));    AsyncHBaseSink sink = new AsyncHBaseSink();    Configurables.configure(sink, ctx);        ctx.put(HBaseSinkConfigurationConstants.ZK_QUORUM, null);    ctx.put(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, null);    ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    int count = 0;    Status status = Status.READY;    while (status != Status.BACKOFF) {        count++;        status = sink.process();    }    /*     * Make sure that the configuration was picked up from the context itself     * and not from a configuration object which was created by the sink.     */    Assert.assertTrue(sink.isConfNull());    sink.stop();    Assert.assertEquals(2, count);    HTable table = new HTable(testUtility.getConfiguration(), tableName);    byte[][] results = getResults(table, 3);    byte[] out;    int found = 0;    for (int i = 0; i < 3; i++) {        for (int j = 0; j < 3; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(3, found);    out = results[3];    Assert.assertArrayEquals(Longs.toByteArray(3), out);}
public void flume_f5569_0() throws Exception
{    deleteTable = false;    ctx.put("batchSize", "2");    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration());    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    sink.process();    Assert.assertFalse(sink.isConfNull());    HTable table = new HTable(testUtility.getConfiguration(), tableName);    byte[][] results = getResults(table, 2);    byte[] out;    int found = 0;    for (int i = 0; i < 2; i++) {        for (int j = 0; j < 2; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(2, found);    out = results[2];    Assert.assertArrayEquals(Longs.toByteArray(2), out);    sink.process();    sink.stop();}
private long flume_f5570_0()
{    if (os instanceof UnixOperatingSystemMXBean) {        return ((UnixOperatingSystemMXBean) os).getOpenFileDescriptorCount();    } else {        return -1;    }}
public void flume_f5571_0() throws Exception
{    if (getOpenFileDescriptorCount() < 0) {        return;    }    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration(), true, false);    ctx.put("maxConsecutiveFails", "1");    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    channel.start();    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    Assert.assertFalse(sink.isConfNull());    long initialFDCount = getOpenFileDescriptorCount();        for (int i = 0; i < 10; i++) {        try {            sink.process();        } catch (EventDeliveryException ex) {        }    }    long increaseInFD = getOpenFileDescriptorCount() - initialFDCount;    Assert.assertTrue("File Descriptor leak detected. FDs have increased by " + increaseInFD + " from an initial FD count of " + initialFDCount, increaseInFD < 50);}
public void flume_f5572_0() throws Exception
{    ctx.put("batchSize", "2");    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = false;    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration());    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    sink.process();    Assert.assertFalse(sink.isConfNull());    HTable table = new HTable(testUtility.getConfiguration(), tableName);    byte[][] results = getResults(table, 2);    byte[] out;    int found = 0;    for (int i = 0; i < 2; i++) {        for (int j = 0; j < 2; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(2, found);    out = results[2];    Assert.assertArrayEquals(Longs.toByteArray(2), out);    testUtility.shutdownMiniCluster();    sink.process();    sink.stop();}
private byte[][] flume_f5573_0(HTable table, int numEvents) throws IOException
{    byte[][] results = new byte[numEvents + 1][];    Scan scan = new Scan();    scan.addColumn(columnFamily.getBytes(), plCol.getBytes());    scan.setStartRow(Bytes.toBytes("default"));    ResultScanner rs = table.getScanner(scan);    byte[] out = null;    int i = 0;    try {        for (Result r = rs.next(); r != null; r = rs.next()) {            out = r.getValue(columnFamily.getBytes(), plCol.getBytes());            if (i >= results.length - 1) {                rs.close();                throw new FlumeException("More results than expected in the table." + "Expected = " + numEvents + ". Found = " + i);            }            results[i++] = out;            System.out.println(out);        }    } finally {        rs.close();    }    Assert.assertEquals(i, results.length - 1);    scan = new Scan();    scan.addColumn(columnFamily.getBytes(), inColumn.getBytes());    scan.setStartRow(Bytes.toBytes("incRow"));    rs = table.getScanner(scan);    out = null;    try {        for (Result r = rs.next(); r != null; r = rs.next()) {            out = r.getValue(columnFamily.getBytes(), inColumn.getBytes());            results[i++] = out;            System.out.println(out);        }    } finally {        rs.close();    }    return results;}
public void flume_f5574_0() throws Exception
{    Map<String, String> ctxMap = new HashMap<>();    ctxMap.put("table", tableName);    ctxMap.put("columnFamily", columnFamily);    ctx = new Context();    ctx.putAll(ctxMap);}
public void flume_f5575_0() throws Exception
{        String oldZkQuorumTestValue = "old_zookeeper_quorum_test_value";    String oldZkZnodeParentValue = "old_zookeeper_znode_parent_test_value";    ctx.put(HBaseSinkConfigurationConstants.ZK_QUORUM, oldZkQuorumTestValue);    ctx.put(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, oldZkZnodeParentValue);    AsyncHBaseSink sink = new AsyncHBaseSink();    Configurables.configure(sink, ctx);    Assert.assertEquals(oldZkQuorumTestValue, sink.asyncClientConfig.getString(HBaseSinkConfigurationConstants.ASYNC_ZK_QUORUM_KEY));    Assert.assertEquals(oldZkZnodeParentValue, sink.asyncClientConfig.getString(HBaseSinkConfigurationConstants.ASYNC_ZK_BASEPATH_KEY));}
public void flume_f5576_0() throws Exception
{        String oldZkQuorumTestValue = "old_zookeeper_quorum_test_value";    String oldZkZnodeParentValue = "old_zookeeper_znode_parent_test_value";    ctx.put(HBaseSinkConfigurationConstants.ZK_QUORUM, oldZkQuorumTestValue);    ctx.put(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, oldZkZnodeParentValue);    String newZkQuorumTestValue = "new_zookeeper_quorum_test_value";    String newZkZnodeParentValue = "new_zookeeper_znode_parent_test_value";    ctx.put(HBaseSinkConfigurationConstants.ASYNC_PREFIX + HBaseSinkConfigurationConstants.ASYNC_ZK_QUORUM_KEY, newZkQuorumTestValue);    ctx.put(HBaseSinkConfigurationConstants.ASYNC_PREFIX + HBaseSinkConfigurationConstants.ASYNC_ZK_BASEPATH_KEY, newZkZnodeParentValue);    AsyncHBaseSink sink = new AsyncHBaseSink();    Configurables.configure(sink, ctx);    Assert.assertEquals(newZkQuorumTestValue, sink.asyncClientConfig.getString(HBaseSinkConfigurationConstants.ASYNC_ZK_QUORUM_KEY));    Assert.assertEquals(newZkZnodeParentValue, sink.asyncClientConfig.getString(HBaseSinkConfigurationConstants.ASYNC_ZK_BASEPATH_KEY));}
public void flume_f5577_0() throws Exception
{    String valueOfANewProp = "vale of the new property";    String keyOfANewProp = "some.key.to.be.passed";    ctx.put(HBaseSinkConfigurationConstants.ASYNC_PREFIX + keyOfANewProp, valueOfANewProp);    AsyncHBaseSink sink = new AsyncHBaseSink();    Configurables.configure(sink, ctx);    Assert.assertEquals(valueOfANewProp, sink.asyncClientConfig.getString(keyOfANewProp));}
public static void flume_f5578_0() throws Exception
{    testUtility.startMiniCluster();}
public static void flume_f5579_0() throws Exception
{    testUtility.shutdownMiniCluster();}
public void flume_f5580_0() throws IOException
{    conf = new Configuration(testUtility.getConfiguration());    ctx = new Context();    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());}
public void flume_f5581_0() throws IOException
{    testUtility.deleteTable(tableName.getBytes());}
private void flume_f5582_0()
{    ctx = new Context();    ctx.put("table", tableName);    ctx.put("columnFamily", columnFamily);    ctx.put("serializer", SimpleHbaseEventSerializer.class.getName());    ctx.put("serializer.payloadColumn", plCol);    ctx.put("serializer.incrementColumn", inColumn);}
private void flume_f5583_0()
{    ctx = new Context();    ctx.put("table", tableName);    ctx.put("columnFamily", columnFamily);    ctx.put("serializer", IncrementHBaseSerializer.class.getName());}
public void flume_f5584_0() throws Exception
{        ctx = new Context();    ctx.put("table", tableName);    ctx.put("columnFamily", columnFamily);    ctx.put("serializer", SimpleHbaseEventSerializer.class.getName());    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase));    channel.put(e);    tx.commit();    tx.close();    sink.process();    sink.stop();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 1);    byte[] out = results[0];    Assert.assertArrayEquals(e.getBody(), out);    out = results[1];    Assert.assertArrayEquals(Longs.toByteArray(1), out);}
public void flume_f5585_0() throws Exception
{    initContextForSimpleHbaseEventSerializer();    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase));    channel.put(e);    tx.commit();    tx.close();    sink.process();    sink.stop();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 1);    byte[] out = results[0];    Assert.assertArrayEquals(e.getBody(), out);    out = results[1];    Assert.assertArrayEquals(Longs.toByteArray(1), out);}
public void flume_f5586_0() throws Exception
{    initContextForSimpleHbaseEventSerializer();    ctx.put("batchSize", "3");    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    sink.process();    sink.stop();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 3);    byte[] out;    int found = 0;    for (int i = 0; i < 3; i++) {        for (int j = 0; j < 3; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(3, found);    out = results[3];    Assert.assertArrayEquals(Longs.toByteArray(3), out);}
public void flume_f5587_0() throws Exception
{    initContextForSimpleHbaseEventSerializer();    ctx.put("batchSize", "2");    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    int count = 0;    while (sink.process() != Status.BACKOFF) {        count++;    }    sink.stop();    Assert.assertEquals(2, count);    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 3);    byte[] out;    int found = 0;    for (int i = 0; i < 3; i++) {        for (int j = 0; j < 3; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(3, found);    out = results[3];    Assert.assertArrayEquals(Longs.toByteArray(3), out);}
public void flume_f5588_1() throws Exception
{        initContextForSimpleHbaseEventSerializer();            testUtility.deleteTable(tableName.getBytes());    ctx.put("batchSize", "2");    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);        Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();        try {                        sink.start();                        sink.process();                sink.stop();    } finally {                testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    }        Assert.fail();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 2);    byte[] out;    int found = 0;    for (int i = 0; i < 2; i++) {        for (int j = 0; j < 2; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(2, found);    out = results[2];    Assert.assertArrayEquals(Longs.toByteArray(2), out);    sink.process();}
public void flume_f5589_0() throws Exception
{    initContextForSimpleHbaseEventSerializer();    ctx.put("batchSize", "2");    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    sink.process();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 2);    byte[] out;    int found = 0;    for (int i = 0; i < 2; i++) {        for (int j = 0; j < 2; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(2, found);    out = results[2];    Assert.assertArrayEquals(Longs.toByteArray(2), out);    testUtility.shutdownMiniCluster();    sink.process();    sink.stop();}
private byte[][] flume_f5590_0(HTable table, int numEvents) throws IOException
{    byte[][] results = new byte[numEvents + 1][];    Scan scan = new Scan();    scan.addColumn(columnFamily.getBytes(), plCol.getBytes());    scan.setStartRow(Bytes.toBytes("default"));    ResultScanner rs = table.getScanner(scan);    byte[] out = null;    int i = 0;    try {        for (Result r = rs.next(); r != null; r = rs.next()) {            out = r.getValue(columnFamily.getBytes(), plCol.getBytes());            if (i >= results.length - 1) {                rs.close();                throw new FlumeException("More results than expected in the table." + "Expected = " + numEvents + ". Found = " + i);            }            results[i++] = out;            System.out.println(out);        }    } finally {        rs.close();    }    Assert.assertEquals(i, results.length - 1);    scan = new Scan();    scan.addColumn(columnFamily.getBytes(), inColumn.getBytes());    scan.setStartRow(Bytes.toBytes("incRow"));    rs = table.getScanner(scan);    out = null;    try {        for (Result r = rs.next(); r != null; r = rs.next()) {            out = r.getValue(columnFamily.getBytes(), inColumn.getBytes());            results[i++] = out;            System.out.println(out);        }    } finally {        rs.close();    }    return results;}
public void flume_f5591_0() throws Exception
{    initContextForSimpleHbaseEventSerializer();    ctx.put("batchSize", "1");    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);        Channel channel = spy(new MemoryChannel());    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + 0));    channel.put(e);    tx.commit();    tx.close();    doThrow(new ChannelException("Mock Exception")).when(channel).take();    try {        sink.process();        Assert.fail("take() method should throw exception");    } catch (ChannelException ex) {        Assert.assertEquals("Mock Exception", ex.getMessage());    }    doReturn(e).when(channel).take();    sink.process();    sink.stop();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 1);    byte[] out = results[0];    Assert.assertArrayEquals(e.getBody(), out);    out = results[1];    Assert.assertArrayEquals(Longs.toByteArray(1), out);}
public void flume_f5592_0() throws Exception
{    initContextForSimpleHbaseEventSerializer();    ctx.put("batchSize", "1");    ctx.put(HBaseSinkConfigurationConstants.CONFIG_SERIALIZER, "org.apache.flume.sink.hbase.MockSimpleHbaseEventSerializer");    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + 0));    channel.put(e);    tx.commit();    tx.close();    try {        MockSimpleHbaseEventSerializer.throwException = true;        sink.process();        Assert.fail("FlumeException expected from serilazer");    } catch (FlumeException ex) {        Assert.assertEquals("Exception for testing", ex.getMessage());    }    MockSimpleHbaseEventSerializer.throwException = false;    sink.process();    sink.stop();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 1);    byte[] out = results[0];    Assert.assertArrayEquals(e.getBody(), out);    out = results[1];    Assert.assertArrayEquals(Longs.toByteArray(1), out);}
public void flume_f5593_0() throws Exception
{    initContextForSimpleHbaseEventSerializer();    Context tmpContext = new Context(ctx.getParameters());    tmpContext.put("batchSize", "2");    tmpContext.put(HBaseSinkConfigurationConstants.ZK_QUORUM, ZKConfig.getZKQuorumServersString(conf));    System.out.print(ctx.getString(HBaseSinkConfigurationConstants.ZK_QUORUM));    tmpContext.put(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT));    HBaseSink sink = new HBaseSink();    Configurables.configure(sink, tmpContext);    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    Status status = Status.READY;    while (status != Status.BACKOFF) {        status = sink.process();    }    sink.stop();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 3);    byte[] out;    int found = 0;    for (int i = 0; i < 3; i++) {        for (int j = 0; j < 3; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(3, found);    out = results[3];    Assert.assertArrayEquals(Longs.toByteArray(3), out);}
public void flume_f5594_0() throws Exception
{    initContextForSimpleHbaseEventSerializer();    Context tmpContext = new Context(ctx.getParameters());    String zkQuorum = "zk1.flume.apache.org:3342, zk2.flume.apache.org:3342, " + "zk3.flume.apache.org:3342";    tmpContext.put("batchSize", "2");    tmpContext.put(HBaseSinkConfigurationConstants.ZK_QUORUM, zkQuorum);    tmpContext.put(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT));    HBaseSink sink = new HBaseSink();    Configurables.configure(sink, tmpContext);    Assert.assertEquals("zk1.flume.apache.org,zk2.flume.apache.org," + "zk3.flume.apache.org", sink.getConfig().get(HConstants.ZOOKEEPER_QUORUM));    Assert.assertEquals(String.valueOf(3342), sink.getConfig().get(HConstants.ZOOKEEPER_CLIENT_PORT));}
public void flume_f5595_0() throws Exception
{    initContextForSimpleHbaseEventSerializer();    Context tmpContext = new Context(ctx.getParameters());    String zkQuorum = "zk1.flume.apache.org:3345, zk2.flume.apache.org:3342, " + "zk3.flume.apache.org:3342";    tmpContext.put("batchSize", "2");    tmpContext.put(HBaseSinkConfigurationConstants.ZK_QUORUM, zkQuorum);    tmpContext.put(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT));    HBaseSink sink = new HBaseSink();    Configurables.configure(sink, tmpContext);    Assert.fail();}
public void flume_f5596_0() throws EventDeliveryException
{    initContextForIncrementHBaseSerializer();    ctx.put("batchSize", "100");    ctx.put(HBaseSinkConfigurationConstants.CONFIG_COALESCE_INCREMENTS, String.valueOf(true));    final Map<String, Long> expectedCounts = Maps.newHashMap();    expectedCounts.put("r1:c1", 10L);    expectedCounts.put("r1:c2", 20L);    expectedCounts.put("r2:c1", 7L);    expectedCounts.put("r2:c3", 63L);    HBaseSink.DebugIncrementsCallback cb = new CoalesceValidator(expectedCounts);    HBaseSink sink = new HBaseSink(testUtility.getConfiguration(), cb);    Configurables.configure(sink, ctx);    Channel channel = createAndConfigureMemoryChannel(sink);    List<Event> events = Lists.newLinkedList();    generateEvents(events, expectedCounts);    putEvents(channel, events);    sink.start();        sink.process();    sink.stop();}
public void flume_f5597_0() throws EventDeliveryException
{    initContextForIncrementHBaseSerializer();    ctx.put("batchSize", "10");    final Map<String, Long> expectedCounts = Maps.newHashMap();    expectedCounts.put("r1:c1", 10L);    HBaseSink.DebugIncrementsCallback cb = new CoalesceValidator(expectedCounts);    HBaseSink sink = new HBaseSink(testUtility.getConfiguration(), cb);    Configurables.configure(sink, ctx);    Channel channel = createAndConfigureMemoryChannel(sink);    List<Event> events = Lists.newLinkedList();    generateEvents(events, expectedCounts);    putEvents(channel, events);    sink.start();        sink.process();    sink.stop();}
public void flume_f5598_1() throws EventDeliveryException
{        initContextForIncrementHBaseSerializer();    HBaseSink sink = new HBaseSink(testUtility.getConfiguration());    Configurables.configure(sink, ctx);    Channel channel = createAndConfigureMemoryChannel(sink);    sink.start();    int batchCount = 3;    for (int i = 0; i < batchCount; i++) {        sink.process();    }    sink.stop();    Assert.assertEquals(batchCount, ((IncrementHBaseSerializer) sink.getSerializer()).getNumBatchesStarted());}
public void flume_f5599_0(Iterable<Increment> increments)
{    for (Increment inc : increments) {        byte[] row = inc.getRow();        Map<byte[], NavigableMap<byte[], Long>> families = null;        try {            families = (Map<byte[], NavigableMap<byte[], Long>>) refGetFamilyMap.invoke(inc);        } catch (Exception e) {            Throwables.propagate(e);        }        for (byte[] family : families.keySet()) {            NavigableMap<byte[], Long> qualifiers = families.get(family);            for (Map.Entry<byte[], Long> entry : qualifiers.entrySet()) {                byte[] qualifier = entry.getKey();                Long count = entry.getValue();                StringBuilder b = new StringBuilder(20);                b.append(new String(row, Charsets.UTF_8));                b.append(':');                b.append(new String(qualifier, Charsets.UTF_8));                String key = b.toString();                Assert.assertEquals("Expected counts don't match observed for " + key, expectedCounts.get(key), count);            }        }    }}
private void flume_f5600_0(List<Event> events, Map<String, Long> counts)
{    for (String key : counts.keySet()) {        long count = counts.get(key);        for (long i = 0; i < count; i++) {            events.add(EventBuilder.withBody(key, Charsets.UTF_8));        }    }}
private Channel flume_f5601_0(HBaseSink sink)
{    Channel channel = new MemoryChannel();    Context channelCtx = new Context();    channelCtx.put("capacity", String.valueOf(1000L));    channelCtx.put("transactionCapacity", String.valueOf(1000L));    Configurables.configure(channel, channelCtx);    sink.setChannel(channel);    channel.start();    return channel;}
private void flume_f5602_0(Channel channel, Iterable<Event> events)
{    Transaction tx = channel.getTransaction();    tx.begin();    for (Event event : events) {        channel.put(event);    }    tx.commit();    tx.close();}
public void flume_f5603_0()
{    sinkFactory = new DefaultSinkFactory();}
private void flume_f5604_0(String name, String type, Class<?> typeClass) throws FlumeException
{    Sink sink = sinkFactory.create(name, type);    Assert.assertNotNull(sink);    Assert.assertTrue(typeClass.isInstance(sink));}
public void flume_f5605_0()
{    verifySinkCreation("hbase-sink", "hbase", HBaseSink.class);    verifySinkCreation("asynchbase-sink", "asynchbase", AsyncHBaseSink.class);}
public void flume_f5606_0() throws Exception
{    RegexHbaseEventSerializer s = new RegexHbaseEventSerializer();    Context context = new Context();    s.configure(context);    String logMsg = "The sky is falling!";    Event e = EventBuilder.withBody(Bytes.toBytes(logMsg));    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    assertTrue(actions.size() == 1);    assertTrue(actions.get(0) instanceof Put);    Put put = (Put) actions.get(0);    assertTrue(put.getFamilyMap().containsKey(s.cf));    List<KeyValue> kvPairs = put.getFamilyMap().get(s.cf);    assertTrue(kvPairs.size() == 1);    Map<String, String> resultMap = Maps.newHashMap();    for (KeyValue kv : kvPairs) {        resultMap.put(new String(kv.getQualifier()), new String(kv.getValue()));    }    assertTrue(resultMap.containsKey(RegexHbaseEventSerializer.COLUMN_NAME_DEFAULT));    assertEquals("The sky is falling!", resultMap.get(RegexHbaseEventSerializer.COLUMN_NAME_DEFAULT));}
public void flume_f5607_0() throws Exception
{    RegexHbaseEventSerializer s = new RegexHbaseEventSerializer();    Context context = new Context();    context.put(RegexHbaseEventSerializer.REGEX_CONFIG, "^([^\t]+)\t([^\t]+)\t" + "([^\t]+)$");    context.put(RegexHbaseEventSerializer.COL_NAME_CONFIG, "col1,col2,ROW_KEY");    context.put("rowKeyIndex", "2");    s.configure(context);    String body = "val1\tval2\trow1";    Event e = EventBuilder.withBody(Bytes.toBytes(body));    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    Put put = (Put) actions.get(0);    List<KeyValue> kvPairs = put.getFamilyMap().get(s.cf);    assertTrue(kvPairs.size() == 2);    Map<String, String> resultMap = Maps.newHashMap();    for (KeyValue kv : kvPairs) {        resultMap.put(new String(kv.getQualifier()), new String(kv.getValue()));    }    assertEquals("val1", resultMap.get("col1"));    assertEquals("val2", resultMap.get("col2"));    assertEquals("row1", Bytes.toString(put.getRow()));}
public void flume_f5608_0() throws Exception
{    RegexHbaseEventSerializer s = new RegexHbaseEventSerializer();    Context context = new Context();    context.put(RegexHbaseEventSerializer.REGEX_CONFIG, "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) \"([^ ]+) ([^ ]+)" + " ([^\"]+)\" (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\"[^\"]*\")" + " ([^ \"]*|\"[^\"]*\"))?");    context.put(RegexHbaseEventSerializer.COL_NAME_CONFIG, "host,identity,user,time,method,request,protocol,status,size," + "referer,agent");    s.configure(context);    String logMsg = "33.22.11.00 - - [20/May/2011:07:01:19 +0000] " + "\"GET /wp-admin/css/install.css HTTP/1.0\" 200 813 " + "\"http://www.cloudera.com/wp-admin/install.php\" \"Mozilla/5.0 (comp" + "atible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)\"";    Event e = EventBuilder.withBody(Bytes.toBytes(logMsg));    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    assertEquals(1, s.getActions().size());    assertTrue(actions.get(0) instanceof Put);    Put put = (Put) actions.get(0);    assertTrue(put.getFamilyMap().containsKey(s.cf));    List<KeyValue> kvPairs = put.getFamilyMap().get(s.cf);    assertTrue(kvPairs.size() == 11);    Map<String, String> resultMap = Maps.newHashMap();    for (KeyValue kv : kvPairs) {        resultMap.put(new String(kv.getQualifier()), new String(kv.getValue()));    }    assertEquals("33.22.11.00", resultMap.get("host"));    assertEquals("-", resultMap.get("identity"));    assertEquals("-", resultMap.get("user"));    assertEquals("[20/May/2011:07:01:19 +0000]", resultMap.get("time"));    assertEquals("GET", resultMap.get("method"));    assertEquals("/wp-admin/css/install.css", resultMap.get("request"));    assertEquals("HTTP/1.0", resultMap.get("protocol"));    assertEquals("200", resultMap.get("status"));    assertEquals("813", resultMap.get("size"));    assertEquals("\"http://www.cloudera.com/wp-admin/install.php\"", resultMap.get("referer"));    assertEquals("\"Mozilla/5.0 (compatible; Yahoo! Slurp; " + "http://help.yahoo.com/help/us/ysearch/slurp)\"", resultMap.get("agent"));    List<Increment> increments = s.getIncrements();    assertEquals(0, increments.size());}
public void flume_f5609_0()
{    Context context = new Context();    RegexHbaseEventSerializer s1 = new RegexHbaseEventSerializer();    s1.configure(context);    RegexHbaseEventSerializer s2 = new RegexHbaseEventSerializer();    s2.configure(context);        RegexHbaseEventSerializer.nonce.set(0);    String randomString = RegexHbaseEventSerializer.randomKey;    Event e1 = EventBuilder.withBody(Bytes.toBytes("body"));    Event e2 = EventBuilder.withBody(Bytes.toBytes("body"));    Event e3 = EventBuilder.withBody(Bytes.toBytes("body"));    Calendar cal = mock(Calendar.class);    when(cal.getTimeInMillis()).thenReturn(1L);    s1.initialize(e1, "CF".getBytes());    String rk1 = new String(s1.getRowKey(cal));    assertEquals("1-" + randomString + "-0", rk1);    when(cal.getTimeInMillis()).thenReturn(10L);    s1.initialize(e2, "CF".getBytes());    String rk2 = new String(s1.getRowKey(cal));    assertEquals("10-" + randomString + "-1", rk2);    when(cal.getTimeInMillis()).thenReturn(100L);    s2.initialize(e3, "CF".getBytes());    String rk3 = new String(s2.getRowKey(cal));    assertEquals("100-" + randomString + "-2", rk3);}
public void flume_f5610_0() throws Exception
{    Charset charset = Charset.forName("KOI8-R");    RegexHbaseEventSerializer s = new RegexHbaseEventSerializer();    Context context = new Context();    context.put(RegexHbaseEventSerializer.DEPOSIT_HEADERS_CONFIG, "true");    context.put(RegexHbaseEventSerializer.CHARSET_CONFIG, charset.toString());    s.configure(context);    String body = "body";    Map<String, String> headers = Maps.newHashMap();    headers.put("header1", "value1");    headers.put("заголовок2", "значение2");    Event e = EventBuilder.withBody(Bytes.toBytes(body), headers);    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    assertEquals(1, s.getActions().size());    assertTrue(actions.get(0) instanceof Put);    Put put = (Put) actions.get(0);    assertTrue(put.getFamilyMap().containsKey(s.cf));    List<KeyValue> kvPairs = put.getFamilyMap().get(s.cf);    assertTrue(kvPairs.size() == 3);    Map<String, byte[]> resultMap = Maps.newHashMap();    for (KeyValue kv : kvPairs) {        resultMap.put(new String(kv.getQualifier(), charset), kv.getValue());    }    assertEquals(body, new String(resultMap.get(RegexHbaseEventSerializer.COLUMN_NAME_DEFAULT), charset));    assertEquals("value1", new String(resultMap.get("header1"), charset));    assertArrayEquals("значение2".getBytes(charset), resultMap.get("заголовок2"));    assertEquals("значение2".length(), resultMap.get("заголовок2").length);    List<Increment> increments = s.getIncrements();    assertEquals(0, increments.size());}
public void flume_f5611_1()
{    Preconditions.checkArgument(table == null, "Please call stop " + "before calling start on an old instance.");    try {        privilegedExecutor = FlumeAuthenticationUtil.getAuthenticator(kerberosPrincipal, kerberosKeytab);    } catch (Exception ex) {        sinkCounter.incrementConnectionFailedCount();        throw new FlumeException("Failed to login to HBase using " + "provided credentials.", ex);    }    try {        conn = privilegedExecutor.execute((PrivilegedExceptionAction<Connection>) () -> {            conn = ConnectionFactory.createConnection(config);            return conn;        });                        table = conn.getBufferedMutator(TableName.valueOf(tableName));    } catch (Exception e) {        sinkCounter.incrementConnectionFailedCount();                throw new FlumeException("Could not load table, " + tableName + " from HBase", e);    }    try {        if (!privilegedExecutor.execute((PrivilegedExceptionAction<Boolean>) () -> {            Table t = null;            try {                t = conn.getTable(TableName.valueOf(tableName));                return t.getTableDescriptor().hasFamily(columnFamily);            } finally {                if (t != null) {                    t.close();                }            }        })) {            throw new IOException("Table " + tableName + " has no such column family " + Bytes.toString(columnFamily));        }    } catch (Exception e) {                        sinkCounter.incrementConnectionFailedCount();        throw new FlumeException("Error getting column family from HBase." + "Please verify that the table " + tableName + " and Column Family, " + Bytes.toString(columnFamily) + " exists in HBase, and the" + " current user has permissions to access that table.", e);    }    super.start();    sinkCounter.incrementConnectionCreatedCount();    sinkCounter.start();}
public void flume_f5612_0()
{    try {        if (table != null) {            table.close();        }        table = null;    } catch (IOException e) {        throw new FlumeException("Error closing table.", e);    }    try {        if (conn != null) {            conn.close();        }        conn = null;    } catch (IOException e) {        throw new FlumeException("Error closing connection.", e);    }    sinkCounter.incrementConnectionClosedCount();    sinkCounter.stop();}
public void flume_f5613_1(Context context)
{    if (!this.hasVersionAtLeast2()) {        throw new ConfigurationException("HBase major version number must be at least 2 for hbase2sink");    }    tableName = context.getString(HBase2SinkConfigurationConstants.CONFIG_TABLE);    String cf = context.getString(HBase2SinkConfigurationConstants.CONFIG_COLUMN_FAMILY);    batchSize = context.getLong(HBase2SinkConfigurationConstants.CONFIG_BATCHSIZE, 100L);    Context serializerContext = new Context();        String eventSerializerType = context.getString(HBase2SinkConfigurationConstants.CONFIG_SERIALIZER);    Preconditions.checkNotNull(tableName, "Table name cannot be empty, please specify in configuration file");    Preconditions.checkNotNull(cf, "Column family cannot be empty, please specify in configuration file");        if (eventSerializerType == null || eventSerializerType.isEmpty()) {        eventSerializerType = "org.apache.flume.sink.hbase2.SimpleHBase2EventSerializer";            }    serializerContext.putAll(context.getSubProperties(HBase2SinkConfigurationConstants.CONFIG_SERIALIZER_PREFIX));    columnFamily = cf.getBytes(Charsets.UTF_8);    try {        Class<? extends HBase2EventSerializer> clazz = (Class<? extends HBase2EventSerializer>) Class.forName(eventSerializerType);        serializer = clazz.newInstance();        serializer.configure(serializerContext);    } catch (Exception e) {                Throwables.propagate(e);    }    kerberosKeytab = context.getString(HBase2SinkConfigurationConstants.CONFIG_KEYTAB);    kerberosPrincipal = context.getString(HBase2SinkConfigurationConstants.CONFIG_PRINCIPAL);    enableWal = context.getBoolean(HBase2SinkConfigurationConstants.CONFIG_ENABLE_WAL, HBase2SinkConfigurationConstants.DEFAULT_ENABLE_WAL);        if (!enableWal) {            }    batchIncrements = context.getBoolean(HBase2SinkConfigurationConstants.CONFIG_COALESCE_INCREMENTS, HBase2SinkConfigurationConstants.DEFAULT_COALESCE_INCREMENTS);    if (batchIncrements) {            }    String zkQuorum = context.getString(HBase2SinkConfigurationConstants.ZK_QUORUM);    Integer port = null;    /*     * HBase allows multiple nodes in the quorum, but all need to use the     * same client port. So get the nodes in host:port format,     * and ignore the ports for all nodes except the first one. If no port is     * specified, use default.     */    if (zkQuorum != null && !zkQuorum.isEmpty()) {        StringBuilder zkBuilder = new StringBuilder();                String[] zkHosts = zkQuorum.split(",");        int length = zkHosts.length;        for (int i = 0; i < length; i++) {            String[] zkHostAndPort = zkHosts[i].split(":");            zkBuilder.append(zkHostAndPort[0].trim());            if (i != length - 1) {                zkBuilder.append(",");            } else {                zkQuorum = zkBuilder.toString();            }            if (zkHostAndPort[1] == null) {                throw new FlumeException("Expected client port for the ZK node!");            }            if (port == null) {                port = Integer.parseInt(zkHostAndPort[1].trim());            } else if (!port.equals(Integer.parseInt(zkHostAndPort[1].trim()))) {                throw new FlumeException("All Zookeeper nodes in the quorum must " + "use the same client port.");            }        }        if (port == null) {            port = HConstants.DEFAULT_ZOOKEPER_CLIENT_PORT;        }        this.config.set(HConstants.ZOOKEEPER_QUORUM, zkQuorum);        this.config.setInt(HConstants.ZOOKEEPER_CLIENT_PORT, port);    }    String hbaseZnode = context.getString(HBase2SinkConfigurationConstants.ZK_ZNODE_PARENT);    if (hbaseZnode != null && !hbaseZnode.isEmpty()) {        this.config.set(HConstants.ZOOKEEPER_ZNODE_PARENT, hbaseZnode);    }    sinkCounter = new SinkCounter(this.getName());}
public Configuration flume_f5614_0()
{    return config;}
public Status flume_f5615_1() throws EventDeliveryException
{    Status status = Status.READY;    Channel channel = getChannel();    Transaction txn = channel.getTransaction();    List<Row> actions = new LinkedList<>();    List<Increment> incs = new LinkedList<>();    try {        txn.begin();        if (serializer instanceof BatchAware) {            ((BatchAware) serializer).onBatchStart();        }        long i = 0;        for (; i < batchSize; i++) {            Event event = channel.take();            if (event == null) {                if (i == 0) {                    status = Status.BACKOFF;                    sinkCounter.incrementBatchEmptyCount();                } else {                    sinkCounter.incrementBatchUnderflowCount();                }                break;            } else {                serializer.initialize(event, columnFamily);                actions.addAll(serializer.getActions());                incs.addAll(serializer.getIncrements());            }        }        if (i == batchSize) {            sinkCounter.incrementBatchCompleteCount();        }        sinkCounter.addToEventDrainAttemptCount(i);        putEventsAndCommit(actions, incs, txn);    } catch (Throwable e) {        try {            txn.rollback();        } catch (Exception e2) {                    }                sinkCounter.incrementEventWriteOrChannelFail(e);        if (e instanceof Error || e instanceof RuntimeException) {                        Throwables.propagate(e);        } else {                        throw new EventDeliveryException("Failed to commit transaction." + "Transaction rolled back.", e);        }    } finally {        txn.close();    }    return status;}
private void flume_f5616_1(final List<Row> actions, final List<Increment> incs, Transaction txn) throws Exception
{    privilegedExecutor.execute((PrivilegedExceptionAction<Void>) () -> {        final List<Mutation> mutations = new ArrayList<>(actions.size());        for (Row r : actions) {            if (r instanceof Put) {                ((Put) r).setDurability(enableWal ? Durability.USE_DEFAULT : Durability.SKIP_WAL);            }                        if (r instanceof Increment) {                ((Increment) r).setDurability(enableWal ? Durability.USE_DEFAULT : Durability.SKIP_WAL);            }            if (r instanceof Mutation) {                mutations.add((Mutation) r);            } else {                            }        }        table.mutate(mutations);        table.flush();        return null;    });    privilegedExecutor.execute((PrivilegedExceptionAction<Void>) () -> {        List<Increment> processedIncrements;        if (batchIncrements) {            processedIncrements = coalesceIncrements(incs);        } else {            processedIncrements = incs;        }                if (debugIncrCallback != null) {            debugIncrCallback.onAfterCoalesce(processedIncrements);        }        for (final Increment i : processedIncrements) {            i.setDurability(enableWal ? Durability.USE_DEFAULT : Durability.SKIP_WAL);            table.mutate(i);        }        table.flush();        return null;    });    txn.commit();    sinkCounter.addToEventDrainSuccessCount(actions.size());}
private Map<byte[], NavigableMap<byte[], Long>> flume_f5617_0(Increment inc)
{    Preconditions.checkNotNull(inc, "Increment required");    return inc.getFamilyMapOfLongs();}
private List<Increment> flume_f5618_0(Iterable<Increment> incs)
{    Preconditions.checkNotNull(incs, "List of Increments must not be null");            Map<byte[], Map<byte[], NavigableMap<byte[], Long>>> counters = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);    for (Increment inc : incs) {        byte[] row = inc.getRow();        Map<byte[], NavigableMap<byte[], Long>> families = getFamilyMap(inc);        for (Map.Entry<byte[], NavigableMap<byte[], Long>> familyEntry : families.entrySet()) {            byte[] family = familyEntry.getKey();            NavigableMap<byte[], Long> qualifiers = familyEntry.getValue();            for (Map.Entry<byte[], Long> qualifierEntry : qualifiers.entrySet()) {                byte[] qualifier = qualifierEntry.getKey();                Long count = qualifierEntry.getValue();                incrementCounter(counters, row, family, qualifier, count);            }        }    }        List<Increment> coalesced = Lists.newLinkedList();    for (Map.Entry<byte[], Map<byte[], NavigableMap<byte[], Long>>> rowEntry : counters.entrySet()) {        byte[] row = rowEntry.getKey();        Map<byte[], NavigableMap<byte[], Long>> families = rowEntry.getValue();        Increment inc = new Increment(row);        for (Map.Entry<byte[], NavigableMap<byte[], Long>> familyEntry : families.entrySet()) {            byte[] family = familyEntry.getKey();            NavigableMap<byte[], Long> qualifiers = familyEntry.getValue();            for (Map.Entry<byte[], Long> qualifierEntry : qualifiers.entrySet()) {                byte[] qualifier = qualifierEntry.getKey();                long count = qualifierEntry.getValue();                inc.addColumn(family, qualifier, count);            }        }        coalesced.add(inc);    }    return coalesced;}
private void flume_f5619_0(Map<byte[], Map<byte[], NavigableMap<byte[], Long>>> counters, byte[] row, byte[] family, byte[] qualifier, Long count)
{    Map<byte[], NavigableMap<byte[], Long>> families = counters.computeIfAbsent(row, k -> Maps.newTreeMap(Bytes.BYTES_COMPARATOR));    NavigableMap<byte[], Long> qualifiers = families.computeIfAbsent(family, k -> Maps.newTreeMap(Bytes.BYTES_COMPARATOR));    qualifiers.merge(qualifier, count, (a, b) -> a + b);}
 String flume_f5620_0()
{    return VersionInfo.getVersion();}
private int flume_f5621_0(String version) throws NumberFormatException
{    return Integer.parseInt(version.split("\\.")[0]);}
private boolean flume_f5622_1()
{    String version = getHBbaseVersionString();    try {        if (this.getMajorVersion(version) >= 2) {            return true;        }    } catch (NumberFormatException ex) {            }        return false;}
 HBase2EventSerializer flume_f5623_0()
{    return serializer;}
public long flume_f5624_0()
{    return batchSize;}
public void flume_f5625_0(Context context)
{    String regex = context.getString(REGEX_CONFIG, REGEX_DEFAULT);    boolean regexIgnoreCase = context.getBoolean(IGNORE_CASE_CONFIG, IGNORE_CASE_DEFAULT);    depositHeaders = context.getBoolean(DEPOSIT_HEADERS_CONFIG, DEPOSIT_HEADERS_DEFAULT);    inputPattern = Pattern.compile(regex, Pattern.DOTALL + (regexIgnoreCase ? Pattern.CASE_INSENSITIVE : 0));    charset = Charset.forName(context.getString(CHARSET_CONFIG, CHARSET_DEFAULT));    String colNameStr = context.getString(COL_NAME_CONFIG, COLUMN_NAME_DEFAULT);    String[] columnNames = colNameStr.split(",");    for (String s : columnNames) {        colNames.add(s.getBytes(charset));    }        rowKeyIndex = context.getInteger(ROW_KEY_INDEX_CONFIG, -1);        if (rowKeyIndex >= 0) {        if (rowKeyIndex >= columnNames.length) {            throw new IllegalArgumentException(ROW_KEY_INDEX_CONFIG + " must be " + "less than num columns " + columnNames.length);        }        if (!ROW_KEY_NAME.equalsIgnoreCase(columnNames[rowKeyIndex])) {            throw new IllegalArgumentException("Column at " + rowKeyIndex + " must be " + ROW_KEY_NAME + " and is " + columnNames[rowKeyIndex]);        }    }}
public void flume_f5626_0(ComponentConfiguration conf)
{}
public void flume_f5627_0(Event event, byte[] columnFamily)
{    this.headers = event.getHeaders();    this.payload = event.getBody();    this.cf = columnFamily;}
protected byte[] flume_f5628_0(Calendar cal)
{    /* NOTE: This key generation strategy has the following properties:     *      * 1) Within a single JVM, the same row key will never be duplicated.     * 2) Amongst any two JVM's operating at different time periods (according     *    to their respective clocks), the same row key will never be      *    duplicated.     * 3) Amongst any two JVM's operating concurrently (according to their     *    respective clocks), the odds of duplicating a row-key are non-zero     *    but infinitesimal. This would require simultaneous collision in (a)      *    the timestamp (b) the respective nonce and (c) the random string.     *    The string is necessary since (a) and (b) could collide if a fleet     *    of Flume agents are restarted in tandem.     *         *  Row-key uniqueness is important because conflicting row-keys will cause     *  data loss. */    String rowKey = String.format("%s-%s-%s", cal.getTimeInMillis(), randomKey, nonce.getAndIncrement());    return rowKey.getBytes(charset);}
protected byte[] flume_f5629_0()
{    return getRowKey(Calendar.getInstance());}
public List<Row> flume_f5630_0() throws FlumeException
{    List<Row> actions = Lists.newArrayList();    byte[] rowKey;    Matcher m = inputPattern.matcher(new String(payload, charset));    if (!m.matches()) {        return Lists.newArrayList();    }    if (m.groupCount() != colNames.size()) {        return Lists.newArrayList();    }    try {        if (rowKeyIndex < 0) {            rowKey = getRowKey();        } else {            rowKey = m.group(rowKeyIndex + 1).getBytes(Charsets.UTF_8);        }        Put put = new Put(rowKey);        for (int i = 0; i < colNames.size(); i++) {            if (i != rowKeyIndex) {                put.addColumn(cf, colNames.get(i), m.group(i + 1).getBytes(Charsets.UTF_8));            }        }        if (depositHeaders) {            for (Map.Entry<String, String> entry : headers.entrySet()) {                put.addColumn(cf, entry.getKey().getBytes(charset), entry.getValue().getBytes(charset));            }        }        actions.add(put);    } catch (Exception e) {        throw new FlumeException("Could not get row key!", e);    }    return actions;}
public List<Increment> flume_f5631_0()
{    return Lists.newArrayList();}
public void flume_f5632_0()
{}
public void flume_f5633_0(Context context)
{    rowPrefix = context.getString("rowPrefix", "default");    incrementRow = context.getString("incrementRow", "incRow").getBytes(Charsets.UTF_8);    String suffix = context.getString("suffix", "uuid");    String payloadColumn = context.getString("payloadColumn", "pCol");    String incColumn = context.getString("incrementColumn", "iCol");    if (payloadColumn != null && !payloadColumn.isEmpty()) {        switch(suffix) {            case "timestamp":                keyType = KeyType.TS;                break;            case "random":                keyType = KeyType.RANDOM;                break;            case "nano":                keyType = KeyType.TSNANO;                break;            default:                keyType = KeyType.UUID;                break;        }        plCol = payloadColumn.getBytes(Charsets.UTF_8);    }    if (incColumn != null && !incColumn.isEmpty()) {        incCol = incColumn.getBytes(Charsets.UTF_8);    }}
public void flume_f5634_0(ComponentConfiguration conf)
{}
public void flume_f5635_0(Event event, byte[] cf)
{    this.payload = event.getBody();    this.cf = cf;}
public List<Row> flume_f5636_0() throws FlumeException
{    List<Row> actions = new LinkedList<>();    if (plCol != null) {        byte[] rowKey;        try {            if (keyType == KeyType.TS) {                rowKey = SimpleRowKeyGenerator.getTimestampKey(rowPrefix);            } else if (keyType == KeyType.RANDOM) {                rowKey = SimpleRowKeyGenerator.getRandomKey(rowPrefix);            } else if (keyType == KeyType.TSNANO) {                rowKey = SimpleRowKeyGenerator.getNanoTimestampKey(rowPrefix);            } else {                rowKey = SimpleRowKeyGenerator.getUUIDKey(rowPrefix);            }            Put put = new Put(rowKey);            put.addColumn(cf, plCol, payload);            actions.add(put);        } catch (Exception e) {            throw new FlumeException("Could not get row key!", e);        }    }    return actions;}
public List<Increment> flume_f5637_0()
{    List<Increment> incs = new LinkedList<>();    if (incCol != null) {        Increment inc = new Increment(incrementRow);        inc.addColumn(cf, incCol, 1);        incs.add(inc);    }    return incs;}
public void flume_f5638_0()
{}
public static byte[] flume_f5639_0(String prefix) throws UnsupportedEncodingException
{    return (prefix + UUID.randomUUID().toString()).getBytes("UTF8");}
public static byte[] flume_f5640_0(String prefix) throws UnsupportedEncodingException
{    return (prefix + String.valueOf(new Random().nextLong())).getBytes("UTF8");}
public static byte[] flume_f5641_0(String prefix) throws UnsupportedEncodingException
{    return (prefix + String.valueOf(System.currentTimeMillis())).getBytes("UTF8");}
public static byte[] flume_f5642_0(String prefix) throws UnsupportedEncodingException
{    return (prefix + String.valueOf(System.nanoTime())).getBytes("UTF8");}
public void flume_f5643_0(Context context)
{}
public void flume_f5644_0(ComponentConfiguration conf)
{}
public void flume_f5645_0()
{}
public void flume_f5646_0(Event event, byte[] columnFamily)
{    this.event = event;    this.family = columnFamily;}
public List<Row> flume_f5647_0()
{    return Collections.emptyList();}
public List<Increment> flume_f5648_0()
{    List<Increment> increments = Lists.newArrayList();    String body = new String(event.getBody(), Charsets.UTF_8);    String[] pieces = body.split(":");    String row = pieces[0];    String qualifier = pieces[1];    Increment inc = new Increment(row.getBytes(Charsets.UTF_8));    inc.addColumn(family, qualifier.getBytes(Charsets.UTF_8), 1L);    increments.add(inc);    return increments;}
public void flume_f5649_0()
{    numBatchesStarted++;}
public int flume_f5650_0()
{    return numBatchesStarted;}
public List<Row> flume_f5651_0() throws FlumeException
{    if (throwException) {        throw new FlumeException("Exception for testing");    }    return super.getActions();}
public static void flume_f5652_0() throws Exception
{    String hbaseVer = org.apache.hadoop.hbase.util.VersionInfo.getVersion();    System.out.println("HBASE VERSION:" + hbaseVer);    Configuration conf = HBaseConfiguration.create();    conf.setBoolean("hbase.localcluster.assign.random.ports", true);    testUtility = new HBaseTestingUtility(conf);    testUtility.startMiniCluster();}
public static void flume_f5653_0() throws Exception
{    testUtility.shutdownMiniCluster();}
public void flume_f5654_0() throws IOException
{    conf = new Configuration(testUtility.getConfiguration());    testUtility.createTable(TableName.valueOf(tableName), columnFamily.getBytes());}
public void flume_f5655_0() throws IOException
{    testUtility.deleteTable(TableName.valueOf(tableName));}
private Context flume_f5656_0()
{    Context ctx = new Context();    ctx.put("table", tableName);    ctx.put("columnFamily", columnFamily);    ctx.put("serializer", SimpleHBase2EventSerializer.class.getName());    ctx.put("serializer.payloadColumn", plCol);    ctx.put("serializer.incrementColumn", inColumn);    return ctx;}
private Context flume_f5657_0()
{    Context ctx = new Context();    ctx.put("table", tableName);    ctx.put("columnFamily", columnFamily);    ctx.put("serializer", IncrementHBase2Serializer.class.getName());    return ctx;}
private Context flume_f5658_0()
{        Context ctx = new Context();    ctx.put("table", tableName);    ctx.put("columnFamily", columnFamily);    ctx.put("serializer", SimpleHBase2EventSerializer.class.getName());    return ctx;}
public void flume_f5659_0() throws Exception
{    Context ctx = getContextWithoutIncrementHBaseSerializer();    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase));    channel.put(e);    tx.commit();    tx.close();    sink.process();    sink.stop();    try (Connection connection = ConnectionFactory.createConnection(conf);        Table table = connection.getTable(TableName.valueOf(tableName))) {        byte[][] results = getResults(table, 1);        byte[] out = results[0];        Assert.assertArrayEquals(e.getBody(), out);        out = results[1];        Assert.assertArrayEquals(Longs.toByteArray(1), out);    }}
public void flume_f5660_0() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase));    channel.put(e);    tx.commit();    tx.close();    sink.process();    sink.stop();    try (Connection connection = ConnectionFactory.createConnection(conf);        Table table = connection.getTable(TableName.valueOf(tableName))) {        byte[][] results = getResults(table, 1);        byte[] out = results[0];        Assert.assertArrayEquals(e.getBody(), out);        out = results[1];        Assert.assertArrayEquals(Longs.toByteArray(1), out);    }}
public void flume_f5661_0() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    ctx.put("batchSize", "3");    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    sink.process();    sink.stop();    try (Connection connection = ConnectionFactory.createConnection(conf);        Table table = connection.getTable(TableName.valueOf(tableName))) {        byte[][] results = getResults(table, 3);        byte[] out;        int found = 0;        for (int i = 0; i < 3; i++) {            for (int j = 0; j < 3; j++) {                if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                    found++;                    break;                }            }        }        Assert.assertEquals(3, found);        out = results[3];        Assert.assertArrayEquals(Longs.toByteArray(3), out);    }}
public void flume_f5662_0() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    ctx.put("batchSize", "2");    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    int count = 0;    while (sink.process() != Status.BACKOFF) {        count++;    }    sink.stop();    Assert.assertEquals(2, count);    try (Connection connection = ConnectionFactory.createConnection(conf)) {        Table table = connection.getTable(TableName.valueOf(tableName));        byte[][] results = getResults(table, 3);        byte[] out;        int found = 0;        for (int i = 0; i < 3; i++) {            for (int j = 0; j < 3; j++) {                if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                    found++;                    break;                }            }        }        Assert.assertEquals(3, found);        out = results[3];        Assert.assertArrayEquals(Longs.toByteArray(3), out);    }}
public void flume_f5663_1() throws Exception
{        Context ctx = getContextForSimpleHBase2EventSerializer();            testUtility.deleteTable(TableName.valueOf(tableName));    ctx.put("batchSize", "2");    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);        Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();        try {                        sink.start();                        sink.process();                sink.stop();    } finally {                testUtility.createTable(TableName.valueOf(tableName), columnFamily.getBytes());    }        Assert.fail();}
public void flume_f5664_0() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    ctx.put("batchSize", "2");    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    sink.process();    try (Connection connection = ConnectionFactory.createConnection(conf);        Table table = connection.getTable(TableName.valueOf(tableName))) {        byte[][] results = getResults(table, 2);        byte[] out;        int found = 0;        for (int i = 0; i < 2; i++) {            for (int j = 0; j < 2; j++) {                if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                    found++;                    break;                }            }        }        Assert.assertEquals(2, found);        out = results[2];        Assert.assertArrayEquals(Longs.toByteArray(2), out);    }    testUtility.shutdownMiniCluster();    sink.process();    sink.stop();}
private byte[][] flume_f5665_0(Table table, int numEvents) throws IOException
{    byte[][] results = new byte[numEvents + 1][];    Scan scan = new Scan();    scan.addColumn(columnFamily.getBytes(), plCol.getBytes());    scan.withStartRow(Bytes.toBytes("default"));    ResultScanner rs = table.getScanner(scan);    byte[] out;    int i = 0;    try {        for (Result r = rs.next(); r != null; r = rs.next()) {            out = r.getValue(columnFamily.getBytes(), plCol.getBytes());            if (i >= results.length - 1) {                rs.close();                throw new FlumeException("More results than expected in the table." + "Expected = " + numEvents + ". Found = " + i);            }            results[i++] = out;            System.out.println(out);        }    } finally {        rs.close();    }    Assert.assertEquals(i, results.length - 1);    scan = new Scan();    scan.addColumn(columnFamily.getBytes(), inColumn.getBytes());    scan.withStartRow(Bytes.toBytes("incRow"));    rs = table.getScanner(scan);    try {        for (Result r = rs.next(); r != null; r = rs.next()) {            out = r.getValue(columnFamily.getBytes(), inColumn.getBytes());            results[i++] = out;            System.out.println(out);        }    } finally {        rs.close();    }    return results;}
public void flume_f5666_0() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    ctx.put("batchSize", "1");    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);        Channel channel = spy(new MemoryChannel());    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + 0));    channel.put(e);    tx.commit();    tx.close();    doThrow(new ChannelException("Mock Exception")).when(channel).take();    try {        sink.process();        Assert.fail("take() method should throw exception");    } catch (ChannelException ex) {        Assert.assertEquals("Mock Exception", ex.getMessage());        SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");        Assert.assertEquals(1, sinkCounter.getChannelReadFail());    }    doReturn(e).when(channel).take();    sink.process();    sink.stop();    try (Connection connection = ConnectionFactory.createConnection(conf);        Table table = connection.getTable(TableName.valueOf(tableName))) {        byte[][] results = getResults(table, 1);        byte[] out = results[0];        Assert.assertArrayEquals(e.getBody(), out);        out = results[1];        Assert.assertArrayEquals(Longs.toByteArray(1), out);    }}
public void flume_f5667_0() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    ctx.put("batchSize", "1");    ctx.put(HBase2SinkConfigurationConstants.CONFIG_SERIALIZER, "org.apache.flume.sink.hbase2.MockSimpleHBase2EventSerializer");    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + 0));    channel.put(e);    tx.commit();    tx.close();    try {        MockSimpleHBase2EventSerializer.throwException = true;        sink.process();        Assert.fail("FlumeException expected from serializer");    } catch (FlumeException ex) {        Assert.assertEquals("Exception for testing", ex.getMessage());        SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");        Assert.assertEquals(1, sinkCounter.getEventWriteFail());    }    MockSimpleHBase2EventSerializer.throwException = false;    sink.process();    sink.stop();    try (Connection connection = ConnectionFactory.createConnection(conf);        Table table = connection.getTable(TableName.valueOf(tableName))) {        byte[][] results = getResults(table, 1);        byte[] out = results[0];        Assert.assertArrayEquals(e.getBody(), out);        out = results[1];        Assert.assertArrayEquals(Longs.toByteArray(1), out);    }}
public void flume_f5668_0() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    Context tmpContext = new Context(ctx.getParameters());    tmpContext.put("batchSize", "2");    tmpContext.put(HBase2SinkConfigurationConstants.ZK_QUORUM, ZKConfig.getZKQuorumServersString(conf));    System.out.print(ctx.getString(HBase2SinkConfigurationConstants.ZK_QUORUM));    tmpContext.put(HBase2SinkConfigurationConstants.ZK_ZNODE_PARENT, conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT));    HBase2Sink sink = new HBase2Sink();    Configurables.configure(sink, tmpContext);    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    Status status = Status.READY;    while (status != Status.BACKOFF) {        status = sink.process();    }    sink.stop();    try (Connection connection = ConnectionFactory.createConnection(conf);        Table table = connection.getTable(TableName.valueOf(tableName))) {        byte[][] results = getResults(table, 3);        byte[] out;        int found = 0;        for (int i = 0; i < 3; i++) {            for (int j = 0; j < 3; j++) {                if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                    found++;                    break;                }            }        }        Assert.assertEquals(3, found);        out = results[3];        Assert.assertArrayEquals(Longs.toByteArray(3), out);    }}
public void flume_f5669_0() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    Context tmpContext = new Context(ctx.getParameters());    String zkQuorum = "zk1.flume.apache.org:3342, zk2.flume.apache.org:3342, " + "zk3.flume.apache.org:3342";    tmpContext.put("batchSize", "2");    tmpContext.put(HBase2SinkConfigurationConstants.ZK_QUORUM, zkQuorum);    tmpContext.put(HBase2SinkConfigurationConstants.ZK_ZNODE_PARENT, conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT));    HBase2Sink sink = new HBase2Sink();    Configurables.configure(sink, tmpContext);    Assert.assertEquals("zk1.flume.apache.org,zk2.flume.apache.org," + "zk3.flume.apache.org", sink.getConfig().get(HConstants.ZOOKEEPER_QUORUM));    Assert.assertEquals(String.valueOf(3342), sink.getConfig().get(HConstants.ZOOKEEPER_CLIENT_PORT));}
public void flume_f5670_0() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    Context tmpContext = new Context(ctx.getParameters());    String zkQuorum = "zk1.flume.apache.org:3345, zk2.flume.apache.org:3342, " + "zk3.flume.apache.org:3342";    tmpContext.put("batchSize", "2");    tmpContext.put(HBase2SinkConfigurationConstants.ZK_QUORUM, zkQuorum);    tmpContext.put(HBase2SinkConfigurationConstants.ZK_ZNODE_PARENT, conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT));    HBase2Sink sink = new HBase2Sink();    Configurables.configure(sink, tmpContext);    Assert.fail();}
public void flume_f5671_0() throws EventDeliveryException
{    Context ctx = getContextForIncrementHBaseSerializer();    ctx.put("batchSize", "100");    ctx.put(HBase2SinkConfigurationConstants.CONFIG_COALESCE_INCREMENTS, String.valueOf(true));    final Map<String, Long> expectedCounts = Maps.newHashMap();    expectedCounts.put("r1:c1", 10L);    expectedCounts.put("r1:c2", 20L);    expectedCounts.put("r2:c1", 7L);    expectedCounts.put("r2:c3", 63L);    HBase2Sink.DebugIncrementsCallback cb = new CoalesceValidator(expectedCounts);    HBase2Sink sink = new HBase2Sink(testUtility.getConfiguration(), cb);    Configurables.configure(sink, ctx);    Channel channel = createAndConfigureMemoryChannel(sink);    List<Event> events = Lists.newLinkedList();    generateEvents(events, expectedCounts);    putEvents(channel, events);    sink.start();        sink.process();    sink.stop();}
public void flume_f5672_0() throws EventDeliveryException
{    Context ctx = getContextForIncrementHBaseSerializer();    ctx.put("batchSize", "10");    final Map<String, Long> expectedCounts = Maps.newHashMap();    expectedCounts.put("r1:c1", 10L);    HBase2Sink.DebugIncrementsCallback cb = new CoalesceValidator(expectedCounts);    HBase2Sink sink = new HBase2Sink(testUtility.getConfiguration(), cb);    Configurables.configure(sink, ctx);    Channel channel = createAndConfigureMemoryChannel(sink);    List<Event> events = Lists.newLinkedList();    generateEvents(events, expectedCounts);    putEvents(channel, events);    sink.start();        sink.process();    sink.stop();}
public void flume_f5673_1() throws EventDeliveryException
{        Context ctx = getContextForIncrementHBaseSerializer();    HBase2Sink sink = new HBase2Sink(testUtility.getConfiguration());    Configurables.configure(sink, ctx);    Channel channel = createAndConfigureMemoryChannel(sink);    sink.start();    int batchCount = 3;    for (int i = 0; i < batchCount; i++) {        sink.process();    }    sink.stop();    Assert.assertEquals(batchCount, ((IncrementHBase2Serializer) sink.getSerializer()).getNumBatchesStarted());}
public void flume_f5674_0() throws Exception
{    Context ctx = getContextWithoutIncrementHBaseSerializer();    HBase2Sink sink = mock(HBase2Sink.class);    doCallRealMethod().when(sink).configure(any());    when(sink.getHBbaseVersionString()).thenReturn("1.0.0");    Configurables.configure(sink, ctx);}
public void flume_f5675_0() throws Exception
{    Context ctx = getContextWithoutIncrementHBaseSerializer();    HBase2Sink sink = mock(HBase2Sink.class);    doCallRealMethod().when(sink).configure(any());    when(sink.getHBbaseVersionString()).thenReturn("Dummy text");    Configurables.configure(sink, ctx);}
public void flume_f5676_0(Iterable<Increment> increments)
{    for (Increment inc : increments) {        byte[] row = inc.getRow();        Map<byte[], NavigableMap<byte[], Long>> families = null;        try {            families = inc.getFamilyMapOfLongs();        } catch (Exception e) {            Throwables.propagate(e);        }        assert families != null;        for (byte[] family : families.keySet()) {            NavigableMap<byte[], Long> qualifiers = families.get(family);            for (Map.Entry<byte[], Long> entry : qualifiers.entrySet()) {                byte[] qualifier = entry.getKey();                Long count = entry.getValue();                String key = new String(row, Charsets.UTF_8) + ':' + new String(qualifier, Charsets.UTF_8);                Assert.assertEquals("Expected counts don't match observed for " + key, expectedCounts.get(key), count);            }        }    }}
private void flume_f5677_0(List<Event> events, Map<String, Long> counts)
{    for (String key : counts.keySet()) {        long count = counts.get(key);        for (long i = 0; i < count; i++) {            events.add(EventBuilder.withBody(key, Charsets.UTF_8));        }    }}
private Channel flume_f5678_0(HBase2Sink sink)
{    Channel channel = new MemoryChannel();    Context channelCtx = new Context();    channelCtx.put("capacity", String.valueOf(1000L));    channelCtx.put("transactionCapacity", String.valueOf(1000L));    Configurables.configure(channel, channelCtx);    sink.setChannel(channel);    channel.start();    return channel;}
private void flume_f5679_0(Channel channel, Iterable<Event> events)
{    Transaction tx = channel.getTransaction();    tx.begin();    for (Event event : events) {        channel.put(event);    }    tx.commit();    tx.close();}
public void flume_f5680_0()
{    sinkFactory = new DefaultSinkFactory();}
private void flume_f5681_0(Class<?> typeClass) throws FlumeException
{    Sink sink = sinkFactory.create("hbase2-sink", "hbase2");    Assert.assertNotNull(sink);    Assert.assertTrue(typeClass.isInstance(sink));}
public void flume_f5682_0()
{    verifySinkCreation(HBase2Sink.class);}
public void flume_f5683_0() throws Exception
{    RegexHBase2EventSerializer s = new RegexHBase2EventSerializer();    Context context = new Context();    s.configure(context);    String logMsg = "The sky is falling!";    Event e = EventBuilder.withBody(Bytes.toBytes(logMsg));    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    assertTrue(actions.size() == 1);    assertTrue(actions.get(0) instanceof Put);    Put put = (Put) actions.get(0);    assertTrue(put.getFamilyCellMap().containsKey(s.cf));    List<Cell> cells = put.getFamilyCellMap().get(s.cf);    assertTrue(cells.size() == 1);    Map<String, String> resultMap = Maps.newHashMap();    for (Cell cell : cells) {        resultMap.put(new String(CellUtil.cloneQualifier(cell)), new String(CellUtil.cloneValue(cell)));    }    assertTrue(resultMap.containsKey(RegexHBase2EventSerializer.COLUMN_NAME_DEFAULT));    assertEquals("The sky is falling!", resultMap.get(RegexHBase2EventSerializer.COLUMN_NAME_DEFAULT));}
public void flume_f5684_0() throws Exception
{    RegexHBase2EventSerializer s = new RegexHBase2EventSerializer();    Context context = new Context();    context.put(RegexHBase2EventSerializer.REGEX_CONFIG, "^([^\t]+)\t([^\t]+)\t" + "([^\t]+)$");    context.put(RegexHBase2EventSerializer.COL_NAME_CONFIG, "col1,col2,ROW_KEY");    context.put("rowKeyIndex", "2");    s.configure(context);    String body = "val1\tval2\trow1";    Event e = EventBuilder.withBody(Bytes.toBytes(body));    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    Put put = (Put) actions.get(0);    List<Cell> cells = put.getFamilyCellMap().get(s.cf);    assertTrue(cells.size() == 2);    Map<String, String> resultMap = Maps.newHashMap();    for (Cell cell : cells) {        resultMap.put(new String(CellUtil.cloneQualifier(cell)), new String(CellUtil.cloneValue(cell)));    }    assertEquals("val1", resultMap.get("col1"));    assertEquals("val2", resultMap.get("col2"));    assertEquals("row1", Bytes.toString(put.getRow()));}
public void flume_f5685_0() throws Exception
{    RegexHBase2EventSerializer s = new RegexHBase2EventSerializer();    Context context = new Context();    context.put(RegexHBase2EventSerializer.REGEX_CONFIG, "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) \"([^ ]+) ([^ ]+)" + " ([^\"]+)\" (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\"[^\"]*\")" + " ([^ \"]*|\"[^\"]*\"))?");    context.put(RegexHBase2EventSerializer.COL_NAME_CONFIG, "host,identity,user,time,method,request,protocol,status,size," + "referer,agent");    s.configure(context);    String logMsg = "33.22.11.00 - - [20/May/2011:07:01:19 +0000] " + "\"GET /wp-admin/css/install.css HTTP/1.0\" 200 813 " + "\"http://www.cloudera.com/wp-admin/install.php\" \"Mozilla/5.0 (comp" + "atible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)\"";    Event e = EventBuilder.withBody(Bytes.toBytes(logMsg));    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    assertEquals(1, s.getActions().size());    assertTrue(actions.get(0) instanceof Put);    Put put = (Put) actions.get(0);    assertTrue(put.getFamilyCellMap().containsKey(s.cf));    List<Cell> cells = put.getFamilyCellMap().get(s.cf);    assertTrue(cells.size() == 11);    Map<String, String> resultMap = Maps.newHashMap();    for (Cell cell : cells) {        resultMap.put(new String(CellUtil.cloneQualifier(cell)), new String(CellUtil.cloneValue(cell)));    }    assertEquals("33.22.11.00", resultMap.get("host"));    assertEquals("-", resultMap.get("identity"));    assertEquals("-", resultMap.get("user"));    assertEquals("[20/May/2011:07:01:19 +0000]", resultMap.get("time"));    assertEquals("GET", resultMap.get("method"));    assertEquals("/wp-admin/css/install.css", resultMap.get("request"));    assertEquals("HTTP/1.0", resultMap.get("protocol"));    assertEquals("200", resultMap.get("status"));    assertEquals("813", resultMap.get("size"));    assertEquals("\"http://www.cloudera.com/wp-admin/install.php\"", resultMap.get("referer"));    assertEquals("\"Mozilla/5.0 (compatible; Yahoo! Slurp; " + "http://help.yahoo.com/help/us/ysearch/slurp)\"", resultMap.get("agent"));    List<Increment> increments = s.getIncrements();    assertEquals(0, increments.size());}
public void flume_f5686_0()
{    Context context = new Context();    RegexHBase2EventSerializer s1 = new RegexHBase2EventSerializer();    s1.configure(context);    RegexHBase2EventSerializer s2 = new RegexHBase2EventSerializer();    s2.configure(context);        RegexHBase2EventSerializer.nonce.set(0);    String randomString = RegexHBase2EventSerializer.randomKey;    Event e1 = EventBuilder.withBody(Bytes.toBytes("body"));    Event e2 = EventBuilder.withBody(Bytes.toBytes("body"));    Event e3 = EventBuilder.withBody(Bytes.toBytes("body"));    Calendar cal = mock(Calendar.class);    when(cal.getTimeInMillis()).thenReturn(1L);    s1.initialize(e1, "CF".getBytes());    String rk1 = new String(s1.getRowKey(cal));    assertEquals("1-" + randomString + "-0", rk1);    when(cal.getTimeInMillis()).thenReturn(10L);    s1.initialize(e2, "CF".getBytes());    String rk2 = new String(s1.getRowKey(cal));    assertEquals("10-" + randomString + "-1", rk2);    when(cal.getTimeInMillis()).thenReturn(100L);    s2.initialize(e3, "CF".getBytes());    String rk3 = new String(s2.getRowKey(cal));    assertEquals("100-" + randomString + "-2", rk3);}
public void flume_f5687_0() throws Exception
{    Charset charset = Charset.forName("KOI8-R");    RegexHBase2EventSerializer s = new RegexHBase2EventSerializer();    Context context = new Context();    context.put(RegexHBase2EventSerializer.DEPOSIT_HEADERS_CONFIG, "true");    context.put(RegexHBase2EventSerializer.CHARSET_CONFIG, charset.toString());    s.configure(context);    String body = "body";    Map<String, String> headers = Maps.newHashMap();    headers.put("header1", "value1");    headers.put("заголовок2", "значение2");    Event e = EventBuilder.withBody(Bytes.toBytes(body), headers);    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    assertEquals(1, s.getActions().size());    assertTrue(actions.get(0) instanceof Put);    Put put = (Put) actions.get(0);    assertTrue(put.getFamilyCellMap().containsKey(s.cf));    List<Cell> cells = put.getFamilyCellMap().get(s.cf);    assertTrue(cells.size() == 3);    Map<String, byte[]> resultMap = Maps.newHashMap();    for (Cell cell : cells) {        resultMap.put(new String(CellUtil.cloneQualifier(cell), charset), CellUtil.cloneValue(cell));    }    assertEquals(body, new String(resultMap.get(RegexHBase2EventSerializer.COLUMN_NAME_DEFAULT), charset));    assertEquals("value1", new String(resultMap.get("header1"), charset));    assertArrayEquals("значение2".getBytes(charset), resultMap.get("заголовок2"));    assertEquals("значение2".length(), resultMap.get("заголовок2").length);    List<Increment> increments = s.getIncrements();    assertEquals(0, increments.size());}
public String flume_f5688_0()
{    return topic;}
public long flume_f5689_0()
{    return batchSize;}
public Status flume_f5690_1() throws EventDeliveryException
{    Status result = Status.READY;    Channel channel = getChannel();    Transaction transaction = null;    Event event = null;    String eventTopic = null;    String eventKey = null;    try {        long processedEvents = 0;        transaction = channel.getTransaction();        transaction.begin();        kafkaFutures.clear();        long batchStartTime = System.nanoTime();        for (; processedEvents < batchSize; processedEvents += 1) {            event = channel.take();            if (event == null) {                                if (processedEvents == 0) {                    result = Status.BACKOFF;                    counter.incrementBatchEmptyCount();                } else {                    counter.incrementBatchUnderflowCount();                }                break;            }            counter.incrementEventDrainAttemptCount();            byte[] eventBody = event.getBody();            Map<String, String> headers = event.getHeaders();            if (allowTopicOverride) {                eventTopic = headers.get(topicHeader);                if (eventTopic == null) {                    eventTopic = BucketPath.escapeString(topic, event.getHeaders());                                    }            } else {                eventTopic = topic;            }            eventKey = headers.get(KEY_HEADER);            if (logger.isTraceEnabled()) {                if (LogPrivacyUtil.allowLogRawData()) {                    logger.trace("{Event} " + eventTopic + " : " + eventKey + " : " + new String(eventBody, "UTF-8"));                } else {                    logger.trace("{Event} " + eventTopic + " : " + eventKey);                }            }                                    long startTime = System.currentTimeMillis();            Integer partitionId = null;            try {                ProducerRecord<String, byte[]> record;                if (staticPartitionId != null) {                    partitionId = staticPartitionId;                }                                if (partitionHeader != null) {                    String headerVal = event.getHeaders().get(partitionHeader);                    if (headerVal != null) {                        partitionId = Integer.parseInt(headerVal);                    }                }                if (partitionId != null) {                    record = new ProducerRecord<String, byte[]>(eventTopic, partitionId, eventKey, serializeEvent(event, useAvroEventFormat));                } else {                    record = new ProducerRecord<String, byte[]>(eventTopic, eventKey, serializeEvent(event, useAvroEventFormat));                }                kafkaFutures.add(producer.send(record, new SinkCallback(startTime)));            } catch (NumberFormatException ex) {                throw new EventDeliveryException("Non integer partition id specified", ex);            } catch (Exception ex) {                                throw new EventDeliveryException("Could not send event", ex);            }        }                producer.flush();                if (processedEvents > 0) {            for (Future<RecordMetadata> future : kafkaFutures) {                future.get();            }            long endTime = System.nanoTime();            counter.addToKafkaEventSendTimer((endTime - batchStartTime) / (1000 * 1000));            counter.addToEventDrainSuccessCount(Long.valueOf(kafkaFutures.size()));        }        transaction.commit();    } catch (Exception ex) {        String errorMsg = "Failed to publish events";                counter.incrementEventWriteOrChannelFail(ex);        result = Status.BACKOFF;        if (transaction != null) {            try {                kafkaFutures.clear();                transaction.rollback();                counter.incrementRollbackCount();            } catch (Exception e) {                                throw Throwables.propagate(e);            }        }        throw new EventDeliveryException(errorMsg, ex);    } finally {        if (transaction != null) {            transaction.close();        }    }    return result;}
public synchronized void flume_f5691_0()
{        producer = new KafkaProducer<String, byte[]>(kafkaProps);    counter.start();    super.start();}
public synchronized void flume_f5692_1()
{    producer.close();    counter.stop();        super.stop();}
public void flume_f5693_1(Context context)
{    translateOldProps(context);    String topicStr = context.getString(TOPIC_CONFIG);    if (topicStr == null || topicStr.isEmpty()) {        topicStr = DEFAULT_TOPIC;            } else {            }    topic = topicStr;    batchSize = context.getInteger(BATCH_SIZE, DEFAULT_BATCH_SIZE);    if (logger.isDebugEnabled()) {            }    useAvroEventFormat = context.getBoolean(KafkaSinkConstants.AVRO_EVENT, KafkaSinkConstants.DEFAULT_AVRO_EVENT);    partitionHeader = context.getString(KafkaSinkConstants.PARTITION_HEADER_NAME);    staticPartitionId = context.getInteger(KafkaSinkConstants.STATIC_PARTITION_CONF);    allowTopicOverride = context.getBoolean(KafkaSinkConstants.ALLOW_TOPIC_OVERRIDE_HEADER, KafkaSinkConstants.DEFAULT_ALLOW_TOPIC_OVERRIDE_HEADER);    topicHeader = context.getString(KafkaSinkConstants.TOPIC_OVERRIDE_HEADER, KafkaSinkConstants.DEFAULT_TOPIC_OVERRIDE_HEADER);    if (logger.isDebugEnabled()) {            }    kafkaFutures = new LinkedList<Future<RecordMetadata>>();    String bootStrapServers = context.getString(BOOTSTRAP_SERVERS_CONFIG);    if (bootStrapServers == null || bootStrapServers.isEmpty()) {        throw new ConfigurationException("Bootstrap Servers must be specified");    }    setProducerProps(context, bootStrapServers);    if (logger.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {            }    if (counter == null) {        counter = new KafkaSinkCounter(getName());    }}
private void flume_f5694_1(Context ctx)
{    if (!(ctx.containsKey(TOPIC_CONFIG))) {        ctx.put(TOPIC_CONFIG, ctx.getString("topic"));            }        if (!(ctx.containsKey(BOOTSTRAP_SERVERS_CONFIG))) {        String brokerList = ctx.getString(BROKER_LIST_FLUME_KEY);        if (brokerList == null || brokerList.isEmpty()) {            throw new ConfigurationException("Bootstrap Servers must be specified");        } else {            ctx.put(BOOTSTRAP_SERVERS_CONFIG, brokerList);                    }    }        if (!(ctx.containsKey(BATCH_SIZE))) {        String oldBatchSize = ctx.getString(OLD_BATCH_SIZE);        if (oldBatchSize != null && !oldBatchSize.isEmpty()) {            ctx.put(BATCH_SIZE, oldBatchSize);                    }    }        if (!(ctx.containsKey(KAFKA_PRODUCER_PREFIX + ProducerConfig.ACKS_CONFIG))) {        String requiredKey = ctx.getString(KafkaSinkConstants.REQUIRED_ACKS_FLUME_KEY);        if (!(requiredKey == null) && !(requiredKey.isEmpty())) {            ctx.put(KAFKA_PRODUCER_PREFIX + ProducerConfig.ACKS_CONFIG, requiredKey);                    }    }    if (ctx.containsKey(KEY_SERIALIZER_KEY)) {            }    if (ctx.containsKey(MESSAGE_SERIALIZER_KEY)) {            }}
private void flume_f5695_0(Context context, String bootStrapServers)
{    kafkaProps.clear();    kafkaProps.put(ProducerConfig.ACKS_CONFIG, DEFAULT_ACKS);        kafkaProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, DEFAULT_KEY_SERIALIZER);    kafkaProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, DEFAULT_VALUE_SERIAIZER);    kafkaProps.putAll(context.getSubProperties(KAFKA_PRODUCER_PREFIX));    kafkaProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootStrapServers);    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);}
protected Properties flume_f5696_0()
{    return kafkaProps;}
private byte[] flume_f5697_0(Event event, boolean useAvroEventFormat) throws IOException
{    byte[] bytes;    if (useAvroEventFormat) {        if (!tempOutStream.isPresent()) {            tempOutStream = Optional.of(new ByteArrayOutputStream());        }        if (!writer.isPresent()) {            writer = Optional.of(new SpecificDatumWriter<AvroFlumeEvent>(AvroFlumeEvent.class));        }        tempOutStream.get().reset();        AvroFlumeEvent e = new AvroFlumeEvent(toCharSeqMap(event.getHeaders()), ByteBuffer.wrap(event.getBody()));        encoder = EncoderFactory.get().directBinaryEncoder(tempOutStream.get(), encoder);        writer.get().write(e, encoder);        encoder.flush();        bytes = tempOutStream.get().toByteArray();    } else {        bytes = event.getBody();    }    return bytes;}
private static Map<CharSequence, CharSequence> flume_f5698_0(Map<String, String> stringMap)
{    Map<CharSequence, CharSequence> charSeqMap = new HashMap<CharSequence, CharSequence>();    for (Map.Entry<String, String> entry : stringMap.entrySet()) {        charSeqMap.put(entry.getKey(), entry.getValue());    }    return charSeqMap;}
public void flume_f5699_1(RecordMetadata metadata, Exception exception)
{    if (exception != null) {            }    if (logger.isDebugEnabled()) {        long eventElapsedTime = System.currentTimeMillis() - startTime;        if (metadata != null) {                    }            }}
public static void flume_f5700_0()
{    testUtil.prepare();    List<String> topics = new ArrayList<String>(3);    topics.add(DEFAULT_TOPIC);    topics.add(TestConstants.STATIC_TOPIC);    topics.add(TestConstants.CUSTOM_TOPIC);    topics.add(TestConstants.HEADER_1_VALUE + "-topic");    testUtil.initTopicList(topics);}
public static void flume_f5701_0()
{    testUtil.tearDown();}
public void flume_f5702_0()
{    KafkaSink kafkaSink = new KafkaSink();    Context context = new Context();    context.put(KAFKA_PREFIX + TOPIC_CONFIG, "");    context.put(KAFKA_PRODUCER_PREFIX + ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "override.default.serializer");    context.put("kafka.producer.fake.property", "kafka.property.value");    context.put("kafka.bootstrap.servers", "localhost:9092,localhost:9092");    context.put("brokerList", "real-broker-list");    Configurables.configure(kafkaSink, context);    Properties kafkaProps = kafkaSink.getKafkaProps();        assertEquals(kafkaProps.getProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG), DEFAULT_KEY_SERIALIZER);        assertEquals(kafkaProps.getProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG), "override.default.serializer");        assertEquals(kafkaProps.getProperty("fake.property"), "kafka.property.value");        assertEquals(kafkaProps.getProperty("bootstrap.servers"), "localhost:9092,localhost:9092");}
public void flume_f5703_0()
{    KafkaSink kafkaSink = new KafkaSink();    Context context = new Context();    context.put("topic", "test-topic");    context.put(OLD_BATCH_SIZE, "300");    context.put(BROKER_LIST_FLUME_KEY, "localhost:9092,localhost:9092");    context.put(REQUIRED_ACKS_FLUME_KEY, "all");    Configurables.configure(kafkaSink, context);    Properties kafkaProps = kafkaSink.getKafkaProps();    assertEquals(kafkaSink.getTopic(), "test-topic");    assertEquals(kafkaSink.getBatchSize(), 300);    assertEquals(kafkaProps.getProperty(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG), "localhost:9092,localhost:9092");    assertEquals(kafkaProps.getProperty(ProducerConfig.ACKS_CONFIG), "all");}
public void flume_f5704_0()
{    Sink kafkaSink = new KafkaSink();    Context context = prepareDefaultContext();    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    String msg = "default-topic-test";    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody(msg.getBytes());    memoryChannel.put(event);    tx.commit();    tx.close();    try {        Sink.Status status = kafkaSink.process();        if (status == Sink.Status.BACKOFF) {            fail("Error Occurred");        }    } catch (EventDeliveryException ex) {        }    checkMessageArrived(msg, DEFAULT_TOPIC);}
private void flume_f5705_0(String msg, String topic)
{    ConsumerRecords recs = pollConsumerRecords(topic);    assertNotNull(recs);    assertTrue(recs.count() > 0);    ConsumerRecord consumerRecord = (ConsumerRecord) recs.iterator().next();    assertEquals(msg, consumerRecord.value());}
public void flume_f5706_0()
{    Context context = prepareDefaultContext();        context.put(TOPIC_CONFIG, TestConstants.STATIC_TOPIC);    String msg = "static-topic-test";    try {        Sink.Status status = prepareAndSend(context, msg);        if (status == Sink.Status.BACKOFF) {            fail("Error Occurred");        }    } catch (EventDeliveryException ex) {        }    checkMessageArrived(msg, TestConstants.STATIC_TOPIC);}
public void flume_f5707_0()
{    Sink kafkaSink = new KafkaSink();    Context context = prepareDefaultContext();    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    String msg = "test-topic-and-key-from-header";    Map<String, String> headers = new HashMap<String, String>();    headers.put("topic", TestConstants.CUSTOM_TOPIC);    headers.put("key", TestConstants.CUSTOM_KEY);    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody(msg.getBytes(), headers);    memoryChannel.put(event);    tx.commit();    tx.close();    try {        Sink.Status status = kafkaSink.process();        if (status == Sink.Status.BACKOFF) {            fail("Error Occurred");        }    } catch (EventDeliveryException ex) {        }    checkMessageArrived(msg, TestConstants.CUSTOM_TOPIC);}
public void flume_f5708_0()
{    String customTopicHeader = "customTopicHeader";    Sink kafkaSink = new KafkaSink();    Context context = prepareDefaultContext();    context.put(KafkaSinkConstants.TOPIC_OVERRIDE_HEADER, customTopicHeader);    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    String msg = "test-topic-from-config-header";    Map<String, String> headers = new HashMap<String, String>();    headers.put(customTopicHeader, TestConstants.CUSTOM_TOPIC);    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody(msg.getBytes(), headers);    memoryChannel.put(event);    tx.commit();    tx.close();    try {        Sink.Status status = kafkaSink.process();        if (status == Sink.Status.BACKOFF) {            fail("Error Occurred");        }    } catch (EventDeliveryException ex) {        }    checkMessageArrived(msg, TestConstants.CUSTOM_TOPIC);}
public void flume_f5709_0()
{    Sink kafkaSink = new KafkaSink();    Context context = prepareDefaultContext();    context.put(KafkaSinkConstants.ALLOW_TOPIC_OVERRIDE_HEADER, "false");    context.put(KafkaSinkConstants.TOPIC_OVERRIDE_HEADER, "foo");    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    String msg = "test-topic-from-config-header";    Map<String, String> headers = new HashMap<String, String>();    headers.put(KafkaSinkConstants.DEFAULT_TOPIC_OVERRIDE_HEADER, TestConstants.CUSTOM_TOPIC);    headers.put("foo", TestConstants.CUSTOM_TOPIC);    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody(msg.getBytes(), headers);    memoryChannel.put(event);    tx.commit();    tx.close();    try {        Sink.Status status = kafkaSink.process();        if (status == Sink.Status.BACKOFF) {            fail("Error Occurred");        }    } catch (EventDeliveryException ex) {        }    checkMessageArrived(msg, DEFAULT_TOPIC);}
public void flume_f5710_0()
{    String topic = TestConstants.HEADER_1_VALUE + "-topic";    Sink kafkaSink = new KafkaSink();    Context context = prepareDefaultContext();    context.put(TOPIC_CONFIG, TestConstants.HEADER_TOPIC);    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    String msg = "test-replace-substring-of-topic-with-headers";    Map<String, String> headers = new HashMap<>();    headers.put(TestConstants.HEADER_1_KEY, TestConstants.HEADER_1_VALUE);    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody(msg.getBytes(), headers);    memoryChannel.put(event);    tx.commit();    tx.close();    try {        Sink.Status status = kafkaSink.process();        if (status == Sink.Status.BACKOFF) {            fail("Error Occurred");        }    } catch (EventDeliveryException ex) {        }    checkMessageArrived(msg, topic);}
public void flume_f5711_0() throws IOException
{    Sink kafkaSink = new KafkaSink();    Context context = prepareDefaultContext();    context.put(AVRO_EVENT, "true");    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    String msg = "test-avro-event";    Map<String, String> headers = new HashMap<String, String>();    headers.put("topic", TestConstants.CUSTOM_TOPIC);    headers.put("key", TestConstants.CUSTOM_KEY);    headers.put(TestConstants.HEADER_1_KEY, TestConstants.HEADER_1_VALUE);    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody(msg.getBytes(), headers);    memoryChannel.put(event);    tx.commit();    tx.close();    try {        Sink.Status status = kafkaSink.process();        if (status == Sink.Status.BACKOFF) {            fail("Error Occurred");        }    } catch (EventDeliveryException ex) {        }    String topic = TestConstants.CUSTOM_TOPIC;    ConsumerRecords<String, String> recs = pollConsumerRecords(topic);    assertNotNull(recs);    assertTrue(recs.count() > 0);    ConsumerRecord<String, String> consumerRecord = recs.iterator().next();    ByteArrayInputStream in = new ByteArrayInputStream(consumerRecord.value().getBytes());    BinaryDecoder decoder = DecoderFactory.get().directBinaryDecoder(in, null);    SpecificDatumReader<AvroFlumeEvent> reader = new SpecificDatumReader<>(AvroFlumeEvent.class);    AvroFlumeEvent avroevent = reader.read(null, decoder);    String eventBody = new String(avroevent.getBody().array(), Charsets.UTF_8);    Map<CharSequence, CharSequence> eventHeaders = avroevent.getHeaders();    assertEquals(msg, eventBody);    assertEquals(TestConstants.CUSTOM_KEY, consumerRecord.key());    assertEquals(TestConstants.HEADER_1_VALUE, eventHeaders.get(new Utf8(TestConstants.HEADER_1_KEY)).toString());    assertEquals(TestConstants.CUSTOM_KEY, eventHeaders.get(new Utf8("key")).toString());}
private ConsumerRecords<String, String> flume_f5712_0(String topic)
{    return pollConsumerRecords(topic, 20);}
private ConsumerRecords<String, String> flume_f5713_0(String topic, int maxIter)
{    ConsumerRecords<String, String> recs = null;    for (int i = 0; i < maxIter; i++) {        recs = testUtil.getNextMessageFromConsumer(topic);        if (recs.count() > 0)            break;        try {            Thread.sleep(1000L);        } catch (InterruptedException e) {                }    }    return recs;}
public void flume_f5714_0() throws EventDeliveryException
{    Sink kafkaSink = new KafkaSink();    Context context = prepareDefaultContext();    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    Sink.Status status = kafkaSink.process();    if (status != Sink.Status.BACKOFF) {        fail("Error Occurred");    }    ConsumerRecords recs = pollConsumerRecords(DEFAULT_TOPIC, 2);    assertNotNull(recs);    assertEquals(recs.count(), 0);}
public void flume_f5715_0() throws Exception
{    doPartitionHeader(PartitionTestScenario.PARTITION_ID_HEADER_ONLY);}
public void flume_f5716_0() throws Exception
{    doPartitionHeader(PartitionTestScenario.NO_PARTITION_HEADERS);}
public void flume_f5717_0() throws Exception
{    doPartitionHeader(PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID);}
public void flume_f5718_0() throws Exception
{    doPartitionHeader(PartitionTestScenario.STATIC_HEADER_ONLY);}
public void flume_f5719_0() throws Exception
{    doPartitionErrors(PartitionOption.NOTSET);}
public void flume_f5720_0() throws Exception
{    Sink kafkaSink = new KafkaSink();    try {        doPartitionErrors(PartitionOption.VALIDBUTOUTOFRANGE, kafkaSink);        fail();    } catch (EventDeliveryException e) {        }    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(kafkaSink, "counter");    assertEquals(1, sinkCounter.getEventWriteFail());}
public void flume_f5721_0() throws Exception
{    doPartitionErrors(PartitionOption.NOTANUMBER);}
public void flume_f5722_0()
{    String sampleProducerProp = "compression.type";    String sampleProducerVal = "snappy";    Context context = prepareDefaultContext();    context.put(KafkaSinkConstants.KAFKA_PRODUCER_PREFIX + sampleProducerProp, sampleProducerVal);    KafkaSink kafkaSink = new KafkaSink();    Configurables.configure(kafkaSink, context);    Assert.assertEquals(sampleProducerVal, kafkaSink.getKafkaProps().getProperty(sampleProducerProp));    context = prepareDefaultContext();    Configurables.configure(kafkaSink, context);    Assert.assertNull(kafkaSink.getKafkaProps().getProperty(sampleProducerProp));}
private void flume_f5723_0(PartitionOption option) throws Exception
{    doPartitionErrors(option, new KafkaSink());}
private void flume_f5724_0(PartitionOption option, Sink kafkaSink) throws Exception
{    Context context = prepareDefaultContext();    context.put(KafkaSinkConstants.PARTITION_HEADER_NAME, "partition-header");    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    String topic = findUnusedTopic();    createTopic(topic, 5);    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Map<String, String> headers = new HashMap<String, String>();    headers.put("topic", topic);    switch(option) {        case VALIDBUTOUTOFRANGE:            headers.put("partition-header", "9");            break;        case NOTSET:            headers.put("wrong-header", "2");            break;        case NOTANUMBER:            headers.put("partition-header", "not-a-number");            break;        default:            break;    }    Event event = EventBuilder.withBody(String.valueOf(9).getBytes(), headers);    memoryChannel.put(event);    tx.commit();    tx.close();    Sink.Status status = kafkaSink.process();    assertEquals(Sink.Status.READY, status);    deleteTopic(topic);}
private void flume_f5725_0(PartitionTestScenario scenario) throws Exception
{    final int numPtns = 5;    final int numMsgs = numPtns * 10;    final Integer staticPtn = 3;    String topic = findUnusedTopic();    createTopic(topic, numPtns);    Context context = prepareDefaultContext();    context.put(BATCH_SIZE, "100");    if (scenario == PartitionTestScenario.PARTITION_ID_HEADER_ONLY || scenario == PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID) {        context.put(KafkaSinkConstants.PARTITION_HEADER_NAME, KafkaPartitionTestUtil.PARTITION_HEADER);    }    if (scenario == PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID || scenario == PartitionTestScenario.STATIC_HEADER_ONLY) {        context.put(KafkaSinkConstants.STATIC_PARTITION_CONF, staticPtn.toString());    }    Sink kafkaSink = new KafkaSink();    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();        Map<Integer, List<Event>> partitionMap = new HashMap<Integer, List<Event>>(numPtns);    for (int i = 0; i < numPtns; i++) {        partitionMap.put(i, new ArrayList<Event>());    }    Transaction tx = memoryChannel.getTransaction();    tx.begin();    List<Event> orderedEvents = KafkaPartitionTestUtil.generateSkewedMessageList(scenario, numMsgs, partitionMap, numPtns, staticPtn);    for (Event event : orderedEvents) {        event.getHeaders().put("topic", topic);        memoryChannel.put(event);    }    tx.commit();    tx.close();    Sink.Status status = kafkaSink.process();    assertEquals(Sink.Status.READY, status);    Properties props = new Properties();    props.put("bootstrap.servers", testUtil.getKafkaServerUrl());    props.put("group.id", "group_1");    props.put("enable.auto.commit", "true");    props.put("auto.commit.interval.ms", "1000");    props.put("session.timeout.ms", "30000");    props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");    props.put("value.deserializer", "org.apache.kafka.common.serialization.ByteArrayDeserializer");    props.put("auto.offset.reset", "earliest");    Map<Integer, List<byte[]>> resultsMap = KafkaPartitionTestUtil.retrieveRecordsFromPartitions(topic, numPtns, props);    KafkaPartitionTestUtil.checkResultsAgainstSkew(scenario, partitionMap, resultsMap, staticPtn, numMsgs);    memoryChannel.stop();    kafkaSink.stop();    deleteTopic(topic);}
private Context flume_f5726_0()
{        Context context = new Context();    context.put(BOOTSTRAP_SERVERS_CONFIG, testUtil.getKafkaServerUrl());    context.put(BATCH_SIZE, "1");    return context;}
private Sink.Status flume_f5727_0(Context context, String msg) throws EventDeliveryException
{    Sink kafkaSink = new KafkaSink();    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody(msg.getBytes());    memoryChannel.put(event);    tx.commit();    tx.close();    return kafkaSink.process();}
private void flume_f5728_0(String topicName, int numPartitions)
{    testUtil.createTopics(Collections.singletonList(topicName), numPartitions);}
private void flume_f5729_0(String topicName)
{    testUtil.deleteTopic(topicName);}
private String flume_f5730_0()
{    String newTopic = null;    boolean topicFound = false;    while (!topicFound) {        newTopic = RandomStringUtils.randomAlphabetic(8);        if (!usedTopics.contains(newTopic)) {            usedTopics.add(newTopic);            topicFound = true;        }    }    return newTopic;}
public void flume_f5731_0() throws Exception
{    kafka.startup();}
public void flume_f5732_0()
{    kafka.shutdown();}
public static TestUtil flume_f5733_0()
{    return instance;}
private void flume_f5734_1()
{    try {        Properties settings = new Properties();        InputStream in = Class.class.getResourceAsStream("/testutil.properties");        if (in != null) {            settings.load(in);        }        externalServers = "true".equalsIgnoreCase(settings.getProperty("external-servers"));        if (externalServers) {            kafkaServerUrl = settings.getProperty("kafka-server-url");            zkServerUrl = settings.getProperty("zk-server-url");        } else {            String hostname = InetAddress.getLocalHost().getHostName();            zkLocalPort = getNextPort();            kafkaLocalPort = getNextPort();            kafkaServerUrl = hostname + ":" + kafkaLocalPort;            zkServerUrl = hostname + ":" + zkLocalPort;        }        clientProps = createClientProperties();    } catch (Exception e) {                throw new RuntimeException("Unexpected error", e);    }}
private boolean flume_f5735_1()
{    Properties kafkaProperties = new Properties();    Properties zkProperties = new Properties();        try {                zkProperties.load(Class.class.getResourceAsStream("/zookeeper.properties"));                        zkProperties.setProperty("clientPort", Integer.toString(zkLocalPort));        new ZooKeeperLocal(zkProperties);                kafkaProperties.load(Class.class.getResourceAsStream("/kafka-server.properties"));                kafkaProperties.setProperty("zookeeper.connect", getZkUrl());                kafkaProperties.setProperty("port", Integer.toString(kafkaLocalPort));        kafkaServer = new KafkaLocal(kafkaProperties);        kafkaServer.start();                return true;    } catch (Exception e) {                return false;    }}
private AdminClient flume_f5736_0()
{    if (adminClient == null) {        Properties adminClientProps = createAdminClientProperties();        adminClient = AdminClient.create(adminClientProps);    }    return adminClient;}
private Properties flume_f5737_0()
{    final Properties props = createAdminClientProperties();    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());    props.put("auto.commit.interval.ms", "1000");    props.put("auto.offset.reset", "earliest");    props.put("consumer.timeout.ms", "10000");    props.put("max.poll.interval.ms", "10000");        return props;}
private Properties flume_f5738_0()
{    final Properties props = new Properties();    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, getKafkaServerUrl());    props.put(ConsumerConfig.GROUP_ID_CONFIG, "group_1");    return props;}
public void flume_f5739_0(List<String> topics)
{    consumer = new KafkaConsumer<>(clientProps);    consumer.subscribe(topics);}
public void flume_f5740_0(List<String> topicNames, int numPartitions)
{    List<NewTopic> newTopics = new ArrayList<>();    for (String topicName : topicNames) {        NewTopic newTopic = new NewTopic(topicName, numPartitions, (short) 1);        newTopics.add(newTopic);    }    getAdminClient().createTopics(newTopics);        DescribeTopicsResult dtr = getAdminClient().describeTopics(topicNames);    try {        dtr.all().get(10, TimeUnit.SECONDS);    } catch (Exception e) {        throw new RuntimeException("Error getting topic info", e);    }}
public void flume_f5741_0(String topicName)
{    getAdminClient().deleteTopics(Collections.singletonList(topicName));}
public ConsumerRecords<String, String> flume_f5742_0(String topic)
{    return consumer.poll(Duration.ofMillis(1000L));}
public void flume_f5743_1()
{    if (externalServers) {        return;    }    boolean startStatus = startEmbeddedKafkaServer();    if (!startStatus) {        throw new RuntimeException("Error starting the server!");    }    try {                Thread.sleep(3 * 1000);        } catch (InterruptedException e) {        }    }
public void flume_f5744_1()
{        if (consumer != null) {        consumer.close();    }    if (adminClient != null) {        adminClient.close();        adminClient = null;    }    try {                Thread.sleep(3 * 1000);        } catch (InterruptedException e) {        }    if (kafkaServer != null) {                kafkaServer.stop();    }    }
private synchronized int flume_f5745_0() throws IOException
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
public String flume_f5746_0()
{    return zkServerUrl;}
public String flume_f5747_0()
{    return kafkaServerUrl;}
public void flume_f5748_1()
{    try {        zooKeeperServer.runFromConfig(configuration);    } catch (IOException e) {            }}
public Event flume_f5749_1() throws IOException
{    ensureOpen();    ByteArrayOutputStream blob = null;    byte[] buf = new byte[Math.min(maxBlobLength, DEFAULT_BUFFER_SIZE)];    int blobLength = 0;    int n = 0;    while ((n = in.read(buf, 0, Math.min(buf.length, maxBlobLength - blobLength))) != -1) {        if (blob == null) {            blob = new ByteArrayOutputStream(n);        }        blob.write(buf, 0, n);        blobLength += n;        if (blobLength >= maxBlobLength) {                        break;        }    }    if (blob == null) {        return null;    } else {        return EventBuilder.withBody(blob.toByteArray());    }}
public List<Event> flume_f5750_0(int numEvents) throws IOException
{    ensureOpen();    List<Event> events = Lists.newLinkedList();    for (int i = 0; i < numEvents; i++) {        Event event = readEvent();        if (event != null) {            events.add(event);        } else {            break;        }    }    return events;}
public void flume_f5751_0() throws IOException
{    ensureOpen();    in.mark();}
public void flume_f5752_0() throws IOException
{    ensureOpen();    in.reset();}
public void flume_f5753_0() throws IOException
{    if (isOpen) {        reset();        in.close();        isOpen = false;    }}
private void flume_f5754_0()
{    if (!isOpen) {        throw new IllegalStateException("Serializer has been closed");    }}
public BlobDeserializer flume_f5755_0(Context context, ResettableInputStream in)
{    return new BlobDeserializer(context, in);}
public void flume_f5756_0(Context context)
{    this.maxBlobLength = context.getInteger(MAX_BLOB_LENGTH_KEY, MAX_BLOB_LENGTH_DEFAULT);    if (this.maxBlobLength <= 0) {        throw new ConfigurationException("Configuration parameter " + MAX_BLOB_LENGTH_KEY + " must be greater than zero: " + maxBlobLength);    }}
public List<Event> flume_f5757_1(HttpServletRequest request) throws Exception
{    Map<String, String> headers = getHeaders(request);    InputStream in = request.getInputStream();    try {        ByteArrayOutputStream blob = null;        byte[] buf = new byte[Math.min(maxBlobLength, DEFAULT_BUFFER_SIZE)];        int blobLength = 0;        int n = 0;        while ((n = in.read(buf, 0, Math.min(buf.length, maxBlobLength - blobLength))) != -1) {            if (blob == null) {                blob = new ByteArrayOutputStream(n);            }            blob.write(buf, 0, n);            blobLength += n;            if (blobLength >= maxBlobLength) {                                break;            }        }        byte[] array = blob != null ? blob.toByteArray() : new byte[0];        Event event = EventBuilder.withBody(array, headers);        if (LOGGER.isDebugEnabled() && LogPrivacyUtil.allowLogRawData()) {                    }        return Collections.singletonList(event);    } finally {        in.close();    }}
private Map<String, String> flume_f5758_1(HttpServletRequest request)
{    if (LOGGER.isDebugEnabled() && LogPrivacyUtil.allowLogRawData()) {        Map requestHeaders = new HashMap();        Enumeration iter = request.getHeaderNames();        while (iter.hasMoreElements()) {            String name = (String) iter.nextElement();            requestHeaders.put(name, request.getHeader(name));        }            }    Map<String, String> headers = new HashMap();    if (request.getContentType() != null) {        headers.put(Metadata.CONTENT_TYPE, request.getContentType());    }    Enumeration iter = request.getParameterNames();    while (iter.hasMoreElements()) {        String name = (String) iter.nextElement();        headers.put(name, request.getParameter(name));    }    return headers;}
 void flume_f5759_0(MorphlineContext morphlineContext)
{    this.morphlineContext = morphlineContext;}
 void flume_f5760_0(Command finalChild)
{    this.finalChild = finalChild;}
public void flume_f5761_0(Context context)
{    String morphlineFile = context.getString(MORPHLINE_FILE_PARAM);    String morphlineId = context.getString(MORPHLINE_ID_PARAM);    if (morphlineFile == null || morphlineFile.trim().length() == 0) {        throw new MorphlineCompilationException("Missing parameter: " + MORPHLINE_FILE_PARAM, null);    }    morphlineFileAndId = morphlineFile + "@" + morphlineId;    if (morphlineContext == null) {        FaultTolerance faultTolerance = new FaultTolerance(context.getBoolean(FaultTolerance.IS_PRODUCTION_MODE, false), context.getBoolean(FaultTolerance.IS_IGNORING_RECOVERABLE_EXCEPTIONS, false), context.getString(FaultTolerance.RECOVERABLE_EXCEPTION_CLASSES));        morphlineContext = new MorphlineContext.Builder().setExceptionHandler(faultTolerance).setMetricRegistry(SharedMetricRegistries.getOrCreate(morphlineFileAndId)).build();    }    Config override = ConfigFactory.parseMap(context.getSubProperties(MORPHLINE_VARIABLE_PARAM + "."));    morphline = new Compiler().compile(new File(morphlineFile), morphlineId, morphlineContext, finalChild, override);    this.mappingTimer = morphlineContext.getMetricRegistry().timer(MetricRegistry.name("morphline.app", Metrics.ELAPSED_TIME));    this.numRecords = morphlineContext.getMetricRegistry().meter(MetricRegistry.name("morphline.app", Metrics.NUM_RECORDS));    this.numFailedRecords = morphlineContext.getMetricRegistry().meter(MetricRegistry.name("morphline.app", "numFailedRecords"));    this.numExceptionRecords = morphlineContext.getMetricRegistry().meter(MetricRegistry.name("morphline.app", "numExceptionRecords"));}
public void flume_f5762_1(Event event)
{    numRecords.mark();    Timer.Context timerContext = mappingTimer.time();    try {        Record record = new Record();        for (Entry<String, String> entry : event.getHeaders().entrySet()) {            record.put(entry.getKey(), entry.getValue());        }        byte[] bytes = event.getBody();        if (bytes != null && bytes.length > 0) {            record.put(Fields.ATTACHMENT_BODY, bytes);        }        try {            Notifications.notifyStartSession(morphline);            if (!morphline.process(record)) {                numFailedRecords.mark();                            }        } catch (RuntimeException t) {            numExceptionRecords.mark();            morphlineContext.getExceptionHandler().handleException(t, record);        }    } finally {        timerContext.stop();    }}
public void flume_f5763_0()
{    Notifications.notifyBeginTransaction(morphline);}
public void flume_f5764_0()
{    Notifications.notifyCommitTransaction(morphline);}
public void flume_f5765_0()
{    Notifications.notifyRollbackTransaction(morphline);}
public void flume_f5766_0()
{    Notifications.notifyShutdown(morphline);}
public void flume_f5767_0()
{}
public void flume_f5768_0()
{    LocalMorphlineInterceptor interceptor;    while ((interceptor = pool.poll()) != null) {        interceptor.close();    }}
public List<Event> flume_f5769_0(List<Event> events)
{    LocalMorphlineInterceptor interceptor = borrowFromPool();    List<Event> results = interceptor.intercept(events);    returnToPool(interceptor);    return results;}
public Event flume_f5770_0(Event event)
{    LocalMorphlineInterceptor interceptor = borrowFromPool();    Event result = interceptor.intercept(event);    returnToPool(interceptor);    return result;}
private void flume_f5771_0(LocalMorphlineInterceptor interceptor)
{    pool.add(interceptor);}
private LocalMorphlineInterceptor flume_f5772_0()
{    LocalMorphlineInterceptor interceptor = pool.poll();    if (interceptor == null) {        interceptor = new LocalMorphlineInterceptor(context);    }    return interceptor;}
public MorphlineInterceptor flume_f5773_0()
{    return new MorphlineInterceptor(context);}
public void flume_f5774_0(Context context)
{    this.context = context;}
public void flume_f5775_0()
{}
public void flume_f5776_0()
{    morphline.stop();}
public List<Event> flume_f5777_0(List<Event> events)
{    List results = new ArrayList(events.size());    for (Event event : events) {        event = intercept(event);        if (event != null) {            results.add(event);        }    }    return results;}
public Event flume_f5778_0(Event event)
{    collector.reset();    morphline.process(event);    List<Record> results = collector.getRecords();    if (results.size() == 0) {        return null;    }    if (results.size() > 1) {        throw new FlumeException(getClass().getName() + " must not generate more than one output record per input event");    }    Event result = toEvent(results.get(0));    return result;}
private Event flume_f5779_0(Record record)
{    Map<String, String> headers = new HashMap();    Map<String, Collection<Object>> recordMap = record.getFields().asMap();    byte[] body = null;    for (Map.Entry<String, Collection<Object>> entry : recordMap.entrySet()) {        if (entry.getValue().size() > 1) {            throw new FlumeException(getClass().getName() + " must not generate more than one output value per record field");        }                assert entry.getValue().size() != 0;        Object firstValue = entry.getValue().iterator().next();        if (Fields.ATTACHMENT_BODY.equals(entry.getKey())) {            if (firstValue instanceof byte[]) {                body = (byte[]) firstValue;            } else if (firstValue instanceof InputStream) {                try {                    body = ByteStreams.toByteArray((InputStream) firstValue);                } catch (IOException e) {                    throw new FlumeException(e);                }            } else {                throw new FlumeException(getClass().getName() + " must non generate attachments that are not a byte[] or InputStream");            }        } else {            headers.put(entry.getKey(), firstValue.toString());        }    }    return EventBuilder.withBody(body, headers);}
public List<Record> flume_f5780_0()
{    return results;}
public void flume_f5781_0()
{    results.clear();}
public Command flume_f5782_0()
{    return null;}
public void flume_f5783_0(Record notification)
{}
public boolean flume_f5784_0(Record record)
{    Preconditions.checkNotNull(record);    results.add(record);    return true;}
public void flume_f5785_0(Context context)
{    this.context = context;    maxBatchSize = context.getInteger(BATCH_SIZE, maxBatchSize);    maxBatchDurationMillis = context.getLong(BATCH_DURATION_MILLIS, maxBatchDurationMillis);    handlerClass = context.getString(HANDLER_CLASS, MorphlineHandlerImpl.class.getName());    if (sinkCounter == null) {        sinkCounter = new SinkCounter(getName());    }}
private int flume_f5786_0()
{    return maxBatchSize;}
private long flume_f5787_0()
{    return maxBatchDurationMillis;}
public synchronized void flume_f5788_1()
{        sinkCounter.start();    if (handler == null) {        MorphlineHandler tmpHandler;        try {            tmpHandler = (MorphlineHandler) Class.forName(handlerClass).newInstance();        } catch (Exception e) {            throw new ConfigurationException(e);        }        tmpHandler.configure(context);        handler = tmpHandler;    }    super.start();    }
public synchronized void flume_f5789_1()
{        try {        if (handler != null) {            handler.stop();        }        sinkCounter.stop();            } finally {        super.stop();    }}
public Status flume_f5790_1() throws EventDeliveryException
{    int batchSize = getMaxBatchSize();    long batchEndTime = System.currentTimeMillis() + getMaxBatchDurationMillis();    Channel myChannel = getChannel();    Transaction txn = myChannel.getTransaction();    txn.begin();    boolean isMorphlineTransactionCommitted = true;    try {        int numEventsTaken = 0;        handler.beginTransaction();        isMorphlineTransactionCommitted = false;                for (int i = 0; i < batchSize; i++) {            Event event = myChannel.take();            if (event == null) {                break;            }            sinkCounter.incrementEventDrainAttemptCount();            numEventsTaken++;            if (LOGGER.isTraceEnabled() && LogPrivacyUtil.allowLogRawData()) {                LOGGER.trace("Flume event arrived {}", event);            }                        handler.process(event);            if (System.currentTimeMillis() >= batchEndTime) {                break;            }        }                if (numEventsTaken == 0) {            sinkCounter.incrementBatchEmptyCount();        }        if (numEventsTaken < batchSize) {            sinkCounter.incrementBatchUnderflowCount();        } else {            sinkCounter.incrementBatchCompleteCount();        }        handler.commitTransaction();        isMorphlineTransactionCommitted = true;        txn.commit();        sinkCounter.addToEventDrainSuccessCount(numEventsTaken);        return numEventsTaken == 0 ? Status.BACKOFF : Status.READY;    } catch (Throwable t) {                        sinkCounter.incrementEventWriteOrChannelFail(t);        try {            if (!isMorphlineTransactionCommitted) {                handler.rollbackTransaction();            }        } catch (Throwable t2) {                    } finally {            try {                txn.rollback();            } catch (Throwable t4) {                            }        }        if (t instanceof Error) {                        throw (Error) t;        } else if (t instanceof ChannelException) {            return Status.BACKOFF;        } else {                        throw new EventDeliveryException("Failed to send events", t);        }    } finally {        txn.close();    }}
public long flume_f5791_0()
{    return getMaxBatchSize();}
public String flume_f5792_0()
{    int i = getClass().getName().lastIndexOf('.') + 1;    String shortClassName = getClass().getName().substring(i);    return getName() + " (" + shortClassName + ")";}
public void flume_f5793_0(Context context)
{    if (context.getString(FaultTolerance.RECOVERABLE_EXCEPTION_CLASSES) == null) {        context.put(FaultTolerance.RECOVERABLE_EXCEPTION_CLASSES, "org.apache.solr.client.solrj.SolrServerException");    }    super.configure(context);}
public void flume_f5794_0()
{}
protected String flume_f5795_0()
{    return prefix;}
protected String flume_f5796_0()
{    return getPrefix() + UUID.randomUUID().toString();}
protected boolean flume_f5797_0(Event event)
{    return true;}
public Event flume_f5798_0(Event event)
{    Map<String, String> headers = event.getHeaders();    if (preserveExisting && headers.containsKey(headerName)) {        } else if (isMatch(event)) {        headers.put(headerName, generateUUID());    }    return event;}
public List<Event> flume_f5799_0(List<Event> events)
{    List results = new ArrayList(events.size());    for (Event event : events) {        event = intercept(event);        if (event != null) {            results.add(event);        }    }    return results;}
public void flume_f5800_0()
{}
public UUIDInterceptor flume_f5801_0()
{    return new UUIDInterceptor(context);}
public void flume_f5802_0(Context context)
{    this.context = context;}
public void flume_f5803_0(Event event) throws EventDeliveryException
{    getChannelProcessor().processEvent(event);    sink.process();}
public void flume_f5804_0(List<Event> events) throws EventDeliveryException
{    getChannelProcessor().processEventBatch(events);    sink.process();}
public boolean flume_f5805_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f5806_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public void flume_f5807_0(ReadListener readListener)
{    throw new UnsupportedOperationException("Not supported yet.");}
public int flume_f5808_0() throws IOException
{    return in.read();}
public String flume_f5809_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public Cookie[] flume_f5810_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public long flume_f5811_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5812_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public Enumeration flume_f5813_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public Enumeration flume_f5814_0()
{    return Collections.enumeration(Collections.EMPTY_LIST);}
public int flume_f5815_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5816_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5817_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5818_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5819_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5820_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5821_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f5822_0(String role)
{    throw new UnsupportedOperationException("Not supported yet.");}
public Principal flume_f5823_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5824_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5825_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public StringBuffer flume_f5826_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5827_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public HttpSession flume_f5828_0(boolean create)
{    throw new UnsupportedOperationException("Not supported yet.");}
public HttpSession flume_f5829_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5830_0()
{    return null;}
public boolean flume_f5831_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f5832_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f5833_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f5834_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f5835_0(HttpServletResponse response) throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
public void flume_f5836_0(String username, String password) throws ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
public void flume_f5837_0() throws ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
public Collection<Part> flume_f5838_0() throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
public Part flume_f5839_0(String name) throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
public T flume_f5840_0(Class<T> handlerClass) throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
public Object flume_f5841_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public Enumeration<String> flume_f5842_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5843_0()
{    return charset;}
public void flume_f5844_0(String env) throws UnsupportedEncodingException
{    this.charset = env;}
public int flume_f5845_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public long flume_f5846_0()
{    return 0;}
public String flume_f5847_0()
{    return null;}
public ServletInputStream flume_f5848_0() throws IOException
{    return stream;}
public String flume_f5849_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public Enumeration flume_f5850_0()
{    return Collections.enumeration(Collections.EMPTY_LIST);}
public String[] flume_f5851_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public Map flume_f5852_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5853_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5854_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5855_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public int flume_f5856_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public BufferedReader flume_f5857_0() throws IOException
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5858_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5859_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public void flume_f5860_0(String name, Object o)
{    throw new UnsupportedOperationException("Not supported yet.");}
public void flume_f5861_0(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
public Locale flume_f5862_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public Enumeration<Locale> flume_f5863_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f5864_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public RequestDispatcher flume_f5865_0(String path)
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5866_0(String path)
{    throw new UnsupportedOperationException("Not supported yet.");}
public int flume_f5867_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5868_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public String flume_f5869_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public int flume_f5870_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public ServletContext flume_f5871_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public AsyncContext flume_f5872_0() throws IllegalStateException
{    throw new UnsupportedOperationException("Not supported yet.");}
public AsyncContext flume_f5873_0(ServletRequest servletRequest, ServletResponse servletResponse) throws IllegalStateException
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f5874_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public boolean flume_f5875_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public AsyncContext flume_f5876_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public DispatcherType flume_f5877_0()
{    throw new UnsupportedOperationException("Not supported yet.");}
public int flume_f5878_0() throws IOException
{    throw new UnsupportedOperationException("This test class doesn't return " + "strings!");}
public void flume_f5879_0() throws IOException
{    markPos = curPos;}
public void flume_f5880_0() throws IOException
{    curPos = markPos;}
public void flume_f5881_0(long position) throws IOException
{    throw new UnsupportedOperationException("Unimplemented in test class");}
public long flume_f5882_0() throws IOException
{    throw new UnsupportedOperationException("Unimplemented in test class");}
public int flume_f5883_0() throws IOException
{    if (curPos >= str.length()) {        return -1;    }    return str.charAt(curPos++);}
public int flume_f5884_0(byte[] b, int off, int len) throws IOException
{    if (curPos >= str.length()) {        return -1;    }    int n = 0;    while (len > 0 && curPos < str.length()) {        b[off++] = (byte) str.charAt(curPos++);        n++;        len--;    }    return n;}
public void flume_f5885_0() throws IOException
{}
public void flume_f5886_0()
{    StringBuilder sb = new StringBuilder();    sb.append("line 1\n");    sb.append("line 2\n");    mini = sb.toString();}
public void flume_f5887_0() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer des = new BlobDeserializer(new Context(), in);    validateMiniParse(des);}
public void flume_f5888_0() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer.Builder builder = new BlobDeserializer.Builder();    EventDeserializer des = builder.build(new Context(), in);    validateMiniParse(des);}
public void flume_f5889_0() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer des;    des = EventDeserializerFactory.getInstance(BlobDeserializer.Builder.class.getName(), new Context(), in);    validateMiniParse(des);}
public void flume_f5890_0() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer des = new BlobDeserializer(new Context(), in);    List<Event> events;        events = des.readEvents(10);    assertEquals(1, events.size());    assertEventBodyEquals(mini, events.get(0));    des.mark();    des.close();}
public void flume_f5891_0() throws IOException
{    String longLine = "abcdefghijklmnopqrstuvwxyz\n";    Context ctx = new Context();    ctx.put(BlobDeserializer.MAX_BLOB_LENGTH_KEY, "10");    ResettableInputStream in = new ResettableTestStringInputStream(longLine);    EventDeserializer des = new BlobDeserializer(ctx, in);    assertEventBodyEquals("abcdefghij", des.readEvent());    assertEventBodyEquals("klmnopqrst", des.readEvent());    assertEventBodyEquals("uvwxyz\n", des.readEvent());    assertNull(des.readEvent());}
private void flume_f5892_0(String expected, Event event)
{    String bodyStr = new String(event.getBody(), Charsets.UTF_8);    assertEquals(expected, bodyStr);}
private void flume_f5893_0(EventDeserializer des) throws IOException
{    Event evt;    des.mark();    evt = des.readEvent();    assertEquals(new String(evt.getBody()), mini);        des.reset();    evt = des.readEvent();    assertEquals("data should be repeated, " + "because we reset() the stream", new String(evt.getBody()), mini);    evt = des.readEvent();    assertNull("Event should be null because there are no lines " + "left to read", evt);    des.mark();    des.close();}
public void flume_f5894_0()
{    handler = new BlobHandler();}
public void flume_f5895_0() throws Exception
{    byte[] json = "foo".getBytes("UTF-8");    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    assertEquals(1, deserialized.size());    Event e = deserialized.get(0);    assertEquals(0, e.getHeaders().size());    assertEquals("foo", new String(e.getBody(), "UTF-8"));}
public void flume_f5896_0() throws Exception
{    byte[] json = "".getBytes("UTF-8");    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    assertEquals(1, deserialized.size());    Event e = deserialized.get(0);    assertEquals(0, e.getHeaders().size());    assertEquals("", new String(e.getBody(), "UTF-8"));}
public void flume_f5897_0() throws UnknownHostException
{    super.testEnvironment();}
public void flume_f5898_0() throws Exception
{    Context context = new Context();    context.put(MorphlineHandlerImpl.MORPHLINE_FILE_PARAM, RESOURCES_DIR + "/test-morphlines/noOperation.conf");    Event input = EventBuilder.withBody("foo", Charsets.UTF_8);    input.getHeaders().put("name", "nadja");    MorphlineInterceptor interceptor = build(context);    Event actual = interceptor.intercept(input);    interceptor.close();    Event expected = EventBuilder.withBody("foo".getBytes("UTF-8"), ImmutableMap.of("name", "nadja"));    assertEqualsEvent(expected, actual);    List<Event> actualList = build(context).intercept(Collections.singletonList(input));    List<Event> expectedList = Collections.singletonList(expected);    assertEqualsEventList(expectedList, actualList);}
public void flume_f5899_0() throws Exception
{    Context context = new Context();    context.put(MorphlineHandlerImpl.MORPHLINE_FILE_PARAM, RESOURCES_DIR + "/test-morphlines/readClob.conf");    Event input = EventBuilder.withBody("foo", Charsets.UTF_8);    input.getHeaders().put("name", "nadja");    Event actual = build(context).intercept(input);    Event expected = EventBuilder.withBody(null, ImmutableMap.of("name", "nadja", Fields.MESSAGE, "foo"));    assertEqualsEvent(expected, actual);    List<Event> actualList = build(context).intercept(Collections.singletonList(input));    List<Event> expectedList = Collections.singletonList(expected);    assertEqualsEventList(expectedList, actualList);}
public void flume_f5900_0() throws Exception
{    Context context = new Context();    context.put(MorphlineHandlerImpl.MORPHLINE_FILE_PARAM, RESOURCES_DIR + "/test-morphlines/grokIfNotMatchDropRecord.conf");    String msg = "<164>Feb  4 10:46:14 syslog sshd[607]: Server listening on 0.0.0.0 port 22.";    Event input = EventBuilder.withBody(null, ImmutableMap.of(Fields.MESSAGE, msg));    Event actual = build(context).intercept(input);    Map<String, String> expected = new HashMap();    expected.put(Fields.MESSAGE, msg);    expected.put("syslog_pri", "164");    expected.put("syslog_timestamp", "Feb  4 10:46:14");    expected.put("syslog_hostname", "syslog");    expected.put("syslog_program", "sshd");    expected.put("syslog_pid", "607");    expected.put("syslog_message", "Server listening on 0.0.0.0 port 22.");    Event expectedEvent = EventBuilder.withBody(null, expected);    assertEqualsEvent(expectedEvent, actual);}
public void flume_f5901_0() throws Exception
{    Context context = new Context();    context.put(MorphlineHandlerImpl.MORPHLINE_FILE_PARAM, RESOURCES_DIR + "/test-morphlines/grokIfNotMatchDropRecord.conf");    String msg = "<XXXXXXXXXXXXX164>Feb  4 10:46:14 syslog sshd[607]: Server listening on 0.0.0.0" + " port 22.";    Event input = EventBuilder.withBody(null, ImmutableMap.of(Fields.MESSAGE, msg));    Event actual = build(context).intercept(input);    assertNull(actual);}
public void flume_f5902_0() throws Exception
{    Context context = new Context();    context.put(MorphlineHandlerImpl.MORPHLINE_FILE_PARAM, RESOURCES_DIR + "/test-morphlines/ifDetectMimeType.conf");    context.put(MorphlineHandlerImpl.MORPHLINE_VARIABLE_PARAM + ".MY.MIME_TYPE", "avro/binary");    Event input = EventBuilder.withBody(Files.toByteArray(new File(RESOURCES_DIR + "/test-documents/sample-statuses-20120906-141433.avro")));    Event actual = build(context).intercept(input);    Map<String, String> expected = new HashMap();    expected.put(Fields.ATTACHMENT_MIME_TYPE, "avro/binary");    expected.put("flume.selector.header", "goToSouthPole");    Event expectedEvent = EventBuilder.withBody(input.getBody(), expected);    assertEqualsEvent(expectedEvent, actual);}
public void flume_f5903_0() throws Exception
{    Context context = new Context();    context.put(MorphlineHandlerImpl.MORPHLINE_FILE_PARAM, RESOURCES_DIR + "/test-morphlines/ifDetectMimeType.conf");    context.put(MorphlineHandlerImpl.MORPHLINE_VARIABLE_PARAM + ".MY.MIME_TYPE", "avro/binary");    Event input = EventBuilder.withBody(Files.toByteArray(new File(RESOURCES_DIR + "/test-documents/testPDF.pdf")));    Event actual = build(context).intercept(input);    Map<String, String> expected = new HashMap();    expected.put(Fields.ATTACHMENT_MIME_TYPE, "application/pdf");    expected.put("flume.selector.header", "goToNorthPole");    Event expectedEvent = EventBuilder.withBody(input.getBody(), expected);    assertEqualsEvent(expectedEvent, actual);}
private MorphlineInterceptor flume_f5904_0(Context context)
{    MorphlineInterceptor.Builder builder = new MorphlineInterceptor.Builder();    builder.configure(context);    return builder.build();}
private void flume_f5905_0(Event x, Event y)
{    assertEquals(x.getHeaders(), y.getHeaders());    assertArrayEquals(x.getBody(), y.getBody());}
private void flume_f5906_0(List<Event> x, List<Event> y)
{    assertEquals(x.size(), y.size());    for (int i = 0; i < x.size(); i++) {        assertEqualsEvent(x.get(i), y.get(i));    }}
public static void flume_f5907_0() throws Exception
{    initCore(RESOURCES_DIR + "/solr/collection1/conf/solrconfig.xml", RESOURCES_DIR + "/solr/collection1/conf/schema.xml", RESOURCES_DIR + "/solr");}
public void flume_f5908_0() throws Exception
{    super.setUp();    String path = RESOURCES_DIR + "/test-documents";    expectedRecords = new HashMap();    expectedRecords.put(path + "/sample-statuses-20120906-141433.avro", 2);    expectedRecords.put(path + "/sample-statuses-20120906-141433", 2);    expectedRecords.put(path + "/sample-statuses-20120906-141433.gz", 2);    expectedRecords.put(path + "/sample-statuses-20120906-141433.bz2", 2);    expectedRecords.put(path + "/cars.csv", 5);    expectedRecords.put(path + "/cars.csv.gz", 5);    expectedRecords.put(path + "/cars.tar.gz", 4);    expectedRecords.put(path + "/cars.tsv", 5);    expectedRecords.put(path + "/cars.ssv", 5);    final Map<String, String> context = new HashMap();    if (EXTERNAL_SOLR_SERVER_URL != null) {        throw new UnsupportedOperationException();                } else {        if (TEST_WITH_EMBEDDED_SOLR_SERVER) {            solrServer = new TestEmbeddedSolrServer(h.getCoreContainer(), "");        } else {            throw new RuntimeException("Not yet implemented");                }    }    Map<String, String> channelContext = new HashMap();    channelContext.put("capacity", "1000000");        channelContext.put("keep-alive", "0");    Channel channel = new MemoryChannel();    channel.setName(channel.getClass().getName() + SEQ_NUM.getAndIncrement());    Configurables.configure(channel, new Context(channelContext));    class MySolrSink extends MorphlineSolrSink {        public MySolrSink(MorphlineHandlerImpl indexer) {            super(indexer);        }    }    int batchSize = SEQ_NUM2.incrementAndGet() % 2 == 0 ? 100 : 1;    DocumentLoader testServer = new SolrServerDocumentLoader(solrServer, batchSize);    MorphlineContext solrMorphlineContext = new SolrMorphlineContext.Builder().setDocumentLoader(testServer).setExceptionHandler(new FaultTolerance(false, false, SolrServerException.class.getName())).setMetricRegistry(new MetricRegistry()).build();    MorphlineHandlerImpl impl = new MorphlineHandlerImpl();    impl.setMorphlineContext(solrMorphlineContext);    class MySolrLocator extends     SolrLocator {        public MySolrLocator(MorphlineContext indexer) {            super(indexer);        }    }    SolrLocator locator = new MySolrLocator(solrMorphlineContext);    locator.setSolrHomeDir(testSolrHome + "/collection1");    String str1 = "SOLR_LOCATOR : " + locator.toString();            File morphlineFile = new File("target/test-classes/test-morphlines/solrCellDocumentTypes.conf");    String str2 = Files.toString(morphlineFile, Charsets.UTF_8);    tmpFile = File.createTempFile("morphline", ".conf");    tmpFile.deleteOnExit();    Files.write(str1 + "\n" + str2, tmpFile, Charsets.UTF_8);    context.put("morphlineFile", tmpFile.getPath());    impl.configure(new Context(context));    sink = new MySolrSink(impl);    sink.setName(sink.getClass().getName() + SEQ_NUM.getAndIncrement());    sink.configure(new Context(context));    sink.setChannel(channel);    sink.start();    source = new EmbeddedSource(sink);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(Collections.singletonList(channel));    ChannelProcessor chp = new ChannelProcessor(rcs);    Context chpContext = new Context();    chpContext.put("interceptors", "uuidinterceptor");    chpContext.put("interceptors.uuidinterceptor.type", UUIDInterceptor.Builder.class.getName());    chp.configure(chpContext);    source.setChannelProcessor(chp);    deleteAllDocuments();}
private void flume_f5909_0() throws SolrServerException, IOException
{    SolrServer s = solrServer;        s.deleteByQuery("*:*");    s.commit();}
public void flume_f5910_0() throws Exception
{    try {        if (source != null) {            source.stop();            source = null;        }        if (sink != null) {            sink.stop();            sink = null;        }        if (tmpFile != null) {            tmpFile.delete();        }    } finally {        solrServer = null;        expectedRecords = null;        super.tearDown();    }}
public void flume_f5911_0() throws Exception
{    String path = RESOURCES_DIR + "/test-documents";    String[] files = new String[] { path + "/testBMPfp.txt", path + "/boilerplate.html", path + "/NullHeader.docx", path + "/testWORD_various.doc", path + "/testPDF.pdf", path + "/testJPEG_EXIF.jpg", path + "/testXML.xml",     path + "/sample-statuses-20120906-141433.avro", path + "/sample-statuses-20120906-141433", path + "/sample-statuses-20120906-141433.gz", path + "/sample-statuses-20120906-141433.bz2" };    testDocumentTypesInternal(files);}
public void flume_f5912_0() throws Exception
{    String path = RESOURCES_DIR + "/test-documents";    String[] files = new String[] { path + "/testPPT_various.ppt", path + "/testPPT_various.pptx", path + "/testEXCEL.xlsx", path + "/testEXCEL.xls", path + "/testPages.pages", path + "/testNumbers.numbers", path + "/testKeynote.key", path + "/testRTFVarious.rtf", path + "/complex.mbox", path + "/test-outlook.msg", path + "/testEMLX.emlx",     path + "/rsstest.rss", path + "/testMP3i18n.mp3", path + "/testAIFF.aif", path + "/testFLAC.flac",     path + "/testMP4.m4a", path + "/testWAV.wav", path + "/testFLV.flv", path + "/testBMP.bmp", path + "/testPNG.png", path + "/testPSD.psd", path + "/testSVG.svg", path + "/testTIFF.tif",     path + "/testTrueType.ttf", path + "/testVISIO.vsd"                 };    testDocumentTypesInternal(files);}
public void flume_f5913_0() throws Exception
{    Channel channel = Mockito.mock(Channel.class);    Mockito.when(channel.take()).thenThrow(new ChannelException("dummy"));    Transaction transaction = Mockito.mock(BasicTransactionSemantics.class);    Mockito.when(channel.getTransaction()).thenReturn(transaction);    sink.setChannel(channel);    sink.process();    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    assertEquals(1, sinkCounter.getChannelReadFail());}
public void flume_f5914_0() throws Exception
{    String file = RESOURCES_DIR + "/test-documents" + "/sample-statuses-20120906-141433.avro";    testDocumentTypesInternal(file);    QueryResponse rsp = query("*:*");    Iterator<SolrDocument> iter = rsp.getResults().iterator();    ListMultimap<String, String> expectedFieldValues;    expectedFieldValues = ImmutableListMultimap.of("id", "1234567890", "text", "sample tweet one", "user_screen_name", "fake_user1");    assertEquals(expectedFieldValues, next(iter));    expectedFieldValues = ImmutableListMultimap.of("id", "2345678901", "text", "sample tweet two", "user_screen_name", "fake_user2");    assertEquals(expectedFieldValues, next(iter));    assertFalse(iter.hasNext());}
private ListMultimap<String, Object> flume_f5915_0(Iterator<SolrDocument> iter)
{    SolrDocument doc = iter.next();    Record record = toRecord(doc);        record.removeAll("_version_");    return record.getFields();}
private Record flume_f5916_0(SolrDocument doc)
{    Record record = new Record();    for (String key : doc.keySet()) {        record.getFields().replaceValues(key, doc.getFieldValues(key));    }    return record;}
private void flume_f5917_0(String... files) throws Exception
{    int numDocs = 0;    long startTime = System.currentTimeMillis();    assertEquals(numDocs, queryResultSetSize("*:*"));        for (int i = 0; i < 1; i++) {        for (String file : files) {            File f = new File(file);            byte[] body = Files.toByteArray(f);            Event event = EventBuilder.withBody(body);            event.getHeaders().put(Fields.ATTACHMENT_NAME, f.getName());            load(event);            Integer count = expectedRecords.get(file);            if (count != null) {                numDocs += count;            } else {                numDocs++;            }            assertEquals(numDocs, queryResultSetSize("*:*"));        }        LOGGER.trace("iter: {}", i);    }    LOGGER.trace("all done with put at {}", System.currentTimeMillis() - startTime);    assertEquals(numDocs, queryResultSetSize("*:*"));    LOGGER.trace("sink: ", sink);}
public void flume_f5918_1() throws Exception
{    int iters = 200;        assertEquals(0, queryResultSetSize("*:*"));    String path = RESOURCES_DIR + "/test-documents";    String[] files = new String[] {     path + "/sample-statuses-20120906-141433-medium.avro" };    List<Event> events = new ArrayList();    for (String file : files) {        File f = new File(file);        byte[] body = Files.toByteArray(f);        Event event = EventBuilder.withBody(body);                events.add(event);    }    long startTime = System.currentTimeMillis();    for (int i = 0; i < iters; i++) {        if (i % 10000 == 0) {                    }        for (Event event : events) {            event = EventBuilder.withBody(event.getBody(), new HashMap(event.getHeaders()));            event.getHeaders().put("id", UUID.randomUUID().toString());            load(event);        }    }    float secs = (System.currentTimeMillis() - startTime) / 1000.0f;    long numDocs = queryResultSetSize("*:*");                }
private void flume_f5919_0(Event event) throws EventDeliveryException
{    source.load(event);}
private void flume_f5920_0() throws SolrServerException, IOException
{    solrServer.commit(false, true, true);}
private int flume_f5921_1(String query) throws SolrServerException, IOException
{    commit();    QueryResponse rsp = query(query);        int size = rsp.getResults().size();    return size;}
private QueryResponse flume_f5922_1(String query) throws SolrServerException, IOException
{    commit();    QueryResponse rsp = solrServer.query(new SolrQuery(query).setRows(Integer.MAX_VALUE));        return rsp;}
public void flume_f5923_0() throws Exception
{    Context context = new Context();    context.put(UUIDInterceptor.HEADER_NAME, ID);    context.put(UUIDInterceptor.PRESERVE_EXISTING_NAME, "true");    Event event = new SimpleEvent();    assertTrue(build(context).intercept(event).getHeaders().get(ID).length() > 0);}
public void flume_f5924_0() throws Exception
{    Context context = new Context();    context.put(UUIDInterceptor.HEADER_NAME, ID);    context.put(UUIDInterceptor.PRESERVE_EXISTING_NAME, "true");    Event event = new SimpleEvent();    event.getHeaders().put(ID, "foo");    assertEquals("foo", build(context).intercept(event).getHeaders().get(ID));}
public void flume_f5925_0() throws Exception
{    Context context = new Context();    context.put(UUIDInterceptor.HEADER_NAME, ID);    context.put(UUIDInterceptor.PREFIX_NAME, "bar#");    Event event = new SimpleEvent();    assertTrue(build(context).intercept(event).getHeaders().get(ID).startsWith("bar#"));}
private UUIDInterceptor flume_f5926_0(Context context)
{    UUIDInterceptor.Builder builder = new UUIDInterceptor.Builder();    builder.configure(context);    return builder.build();}
public JMSMessageConverter flume_f5927_0(Context context)
{    return new DefaultJMSMessageConverter(context.getString(JMSSourceConfiguration.CONVERTER_CHARSET, JMSSourceConfiguration.CONVERTER_CHARSET_DEFAULT).trim());}
public List<Event> flume_f5928_0(Message message) throws JMSException
{    Event event = new SimpleEvent();    Map<String, String> headers = event.getHeaders();    @SuppressWarnings("rawtypes")    Enumeration propertyNames = message.getPropertyNames();    while (propertyNames.hasMoreElements()) {        String name = propertyNames.nextElement().toString();        String value = message.getStringProperty(name);        headers.put(name, value);    }    if (message instanceof BytesMessage) {        BytesMessage bytesMessage = (BytesMessage) message;        long length = bytesMessage.getBodyLength();        if (length > 0L) {            if (length > Integer.MAX_VALUE) {                throw new JMSException("Unable to process message " + "of size " + length);            }            byte[] body = new byte[(int) length];            int count = bytesMessage.readBytes(body);            if (count != length) {                throw new JMSException("Unable to read full message. " + "Read " + count + " of total " + length);            }            event.setBody(body);        }    } else if (message instanceof TextMessage) {        TextMessage textMessage = (TextMessage) message;        String text = textMessage.getText();        if (text != null) {            event.setBody(text.getBytes(charset));        }    } else if (message instanceof ObjectMessage) {        ObjectMessage objectMessage = (ObjectMessage) message;        Object object = objectMessage.getObject();        if (object != null) {            ByteArrayOutputStream bos = new ByteArrayOutputStream();            ObjectOutput out = null;            try {                out = new ObjectOutputStream(bos);                out.writeObject(object);                event.setBody(bos.toByteArray());            } catch (IOException e) {                throw new FlumeException("Error serializing object", e);            } finally {                try {                    if (out != null) {                        out.close();                    }                } catch (IOException e) {                    throw new FlumeException("Error closing ObjectOutputStream", e);                }                try {                    if (bos != null) {                        bos.close();                    }                } catch (IOException e) {                    throw new FlumeException("Error closing ByteArrayOutputStream", e);                }            }        }    }    List<Event> events = new ArrayList<Event>(1);    events.add(event);    return events;}
public InitialContext flume_f5929_0(Properties properties) throws NamingException
{    return new InitialContext(properties);}
 List<Event> flume_f5930_1() throws JMSException
{    List<Event> result = new ArrayList<Event>(batchSize);    Message message;    message = receive();    if (message != null) {        result.addAll(messageConverter.convert(message));        int max = batchSize - 1;        for (int i = 0; i < max; i++) {            message = receiveNoWait();            if (message == null) {                break;            }            result.addAll(messageConverter.convert(message));        }    }    if (logger.isDebugEnabled()) {            }    return result;}
private Message flume_f5931_0() throws JMSException
{    try {        return messageConsumer.receive(pollTimeout);    } catch (RuntimeException runtimeException) {        JMSException jmsException = new JMSException("JMS provider has thrown runtime exception: " + runtimeException.getMessage());        jmsException.setLinkedException(runtimeException);        throw jmsException;    }}
private Message flume_f5932_0() throws JMSException
{    try {        return messageConsumer.receiveNoWait();    } catch (RuntimeException runtimeException) {        JMSException jmsException = new JMSException("JMS provider has thrown runtime exception: " + runtimeException.getMessage());        jmsException.setLinkedException(runtimeException);        throw jmsException;    }}
 void flume_f5933_1()
{    try {        session.commit();    } catch (JMSException jmsException) {            } catch (RuntimeException runtimeException) {            }}
 void flume_f5934_1()
{    try {        session.rollback();    } catch (JMSException jmsException) {            } catch (RuntimeException runtimeException) {            }}
 void flume_f5935_1()
{    try {        if (session != null) {            session.close();        }    } catch (JMSException e) {            }    try {        if (connection != null) {            connection.close();        }    } catch (JMSException e) {            }}
protected void flume_f5936_1(Context context) throws FlumeException
{    sourceCounter = new SourceCounter(getName());    initialContextFactoryName = context.getString(JMSSourceConfiguration.INITIAL_CONTEXT_FACTORY, "").trim();    providerUrl = context.getString(JMSSourceConfiguration.PROVIDER_URL, "").trim();    destinationName = context.getString(JMSSourceConfiguration.DESTINATION_NAME, "").trim();    String destinationTypeName = context.getString(JMSSourceConfiguration.DESTINATION_TYPE, "").trim().toUpperCase(Locale.ENGLISH);    String destinationLocatorName = context.getString(JMSSourceConfiguration.DESTINATION_LOCATOR, JMSSourceConfiguration.DESTINATION_LOCATOR_DEFAULT).trim().toUpperCase(Locale.ENGLISH);    messageSelector = context.getString(JMSSourceConfiguration.MESSAGE_SELECTOR, "").trim();    batchSize = context.getInteger(JMSSourceConfiguration.BATCH_SIZE, JMSSourceConfiguration.BATCH_SIZE_DEFAULT);    errorThreshold = context.getInteger(JMSSourceConfiguration.ERROR_THRESHOLD, JMSSourceConfiguration.ERROR_THRESHOLD_DEFAULT);    userName = Optional.fromNullable(context.getString(JMSSourceConfiguration.USERNAME));    pollTimeout = context.getLong(JMSSourceConfiguration.POLL_TIMEOUT, JMSSourceConfiguration.POLL_TIMEOUT_DEFAULT);    clientId = Optional.fromNullable(context.getString(JMSSourceConfiguration.CLIENT_ID));    createDurableSubscription = context.getBoolean(JMSSourceConfiguration.CREATE_DURABLE_SUBSCRIPTION, JMSSourceConfiguration.DEFAULT_CREATE_DURABLE_SUBSCRIPTION);    durableSubscriptionName = context.getString(JMSSourceConfiguration.DURABLE_SUBSCRIPTION_NAME, JMSSourceConfiguration.DEFAULT_DURABLE_SUBSCRIPTION_NAME);    String passwordFile = context.getString(JMSSourceConfiguration.PASSWORD_FILE, "").trim();    if (passwordFile.isEmpty()) {        password = Optional.absent();    } else {        try {            password = Optional.of(Files.toString(new File(passwordFile), Charsets.UTF_8).trim());        } catch (IOException e) {            throw new FlumeException(String.format("Could not read password file %s", passwordFile), e);        }    }    String converterClassName = context.getString(JMSSourceConfiguration.CONVERTER_TYPE, JMSSourceConfiguration.CONVERTER_TYPE_DEFAULT).trim();    if (JMSSourceConfiguration.CONVERTER_TYPE_DEFAULT.equalsIgnoreCase(converterClassName)) {        converterClassName = DefaultJMSMessageConverter.Builder.class.getName();    }    Context converterContext = new Context(context.getSubProperties(JMSSourceConfiguration.CONVERTER + "."));    try {        @SuppressWarnings("rawtypes")        Class clazz = Class.forName(converterClassName);        boolean isBuilder = JMSMessageConverter.Builder.class.isAssignableFrom(clazz);        if (isBuilder) {            JMSMessageConverter.Builder builder = (JMSMessageConverter.Builder) clazz.newInstance();            converter = builder.build(converterContext);        } else {            Preconditions.checkState(JMSMessageConverter.class.isAssignableFrom(clazz), String.format("Class %s is not a subclass of JMSMessageConverter", clazz.getName()));            converter = (JMSMessageConverter) clazz.newInstance();            boolean configured = Configurables.configure(converter, converterContext);            if (logger.isDebugEnabled()) {                            }        }    } catch (Exception e) {        throw new FlumeException(String.format("Unable to create instance of converter %s", converterClassName), e);    }    String connectionFactoryName = context.getString(JMSSourceConfiguration.CONNECTION_FACTORY, JMSSourceConfiguration.CONNECTION_FACTORY_DEFAULT).trim();    assertNotEmpty(initialContextFactoryName, String.format("Initial Context Factory is empty. This is specified by %s", JMSSourceConfiguration.INITIAL_CONTEXT_FACTORY));    assertNotEmpty(providerUrl, String.format("Provider URL is empty. This is specified by %s", JMSSourceConfiguration.PROVIDER_URL));    assertNotEmpty(destinationName, String.format("Destination Name is empty. This is specified by %s", JMSSourceConfiguration.DESTINATION_NAME));    assertNotEmpty(destinationTypeName, String.format("Destination Type is empty. This is specified by %s", JMSSourceConfiguration.DESTINATION_TYPE));    try {        destinationType = JMSDestinationType.valueOf(destinationTypeName);    } catch (IllegalArgumentException e) {        throw new FlumeException(String.format("Destination type '%s' is " + "invalid.", destinationTypeName), e);    }    if (createDurableSubscription) {        if (JMSDestinationType.TOPIC != destinationType) {            throw new FlumeException(String.format("Only Destination type '%s' supports durable subscriptions.", JMSDestinationType.TOPIC.toString()));        }        if (!clientId.isPresent()) {            throw new FlumeException(String.format("You have to specify '%s' when using durable subscriptions.", JMSSourceConfiguration.CLIENT_ID));        }        if (StringUtils.isEmpty(durableSubscriptionName)) {            throw new FlumeException(String.format("If '%s' is set to true, '%s' has to be specified.", JMSSourceConfiguration.CREATE_DURABLE_SUBSCRIPTION, JMSSourceConfiguration.DURABLE_SUBSCRIPTION_NAME));        }    } else if (!StringUtils.isEmpty(durableSubscriptionName)) {            }    try {        destinationLocator = JMSDestinationLocator.valueOf(destinationLocatorName);    } catch (IllegalArgumentException e) {        throw new FlumeException(String.format("Destination locator '%s' is " + "invalid.", destinationLocatorName), e);    }    Preconditions.checkArgument(batchSize > 0, "Batch size must be greater than 0");    try {        Properties contextProperties = new Properties();        contextProperties.setProperty(javax.naming.Context.INITIAL_CONTEXT_FACTORY, initialContextFactoryName);        contextProperties.setProperty(javax.naming.Context.PROVIDER_URL, providerUrl);                if (this.userName.isPresent()) {            contextProperties.setProperty(javax.naming.Context.SECURITY_PRINCIPAL, this.userName.get());        }        if (this.password.isPresent()) {            contextProperties.setProperty(javax.naming.Context.SECURITY_CREDENTIALS, this.password.get());        }        initialContext = initialContextFactory.create(contextProperties);    } catch (NamingException e) {        throw new FlumeException(String.format("Could not create initial context %s provider %s", initialContextFactoryName, providerUrl), e);    }    try {        connectionFactory = (ConnectionFactory) initialContext.lookup(connectionFactoryName);    } catch (NamingException e) {        throw new FlumeException("Could not lookup ConnectionFactory", e);    }}
private void flume_f5937_0(String arg, String msg)
{    Preconditions.checkArgument(!arg.isEmpty(), msg);}
protected synchronized Status flume_f5938_1() throws EventDeliveryException
{    boolean error = true;    try {        if (consumer == null) {            consumer = createConsumer();        }        List<Event> events = consumer.take();        int size = events.size();        if (size == 0) {            error = false;            return Status.BACKOFF;        }        sourceCounter.incrementAppendBatchReceivedCount();        sourceCounter.addToEventReceivedCount(size);        getChannelProcessor().processEventBatch(events);        error = false;        sourceCounter.addToEventAcceptedCount(size);        sourceCounter.incrementAppendBatchAcceptedCount();        return Status.READY;    } catch (ChannelException channelException) {                sourceCounter.incrementChannelWriteFail();    } catch (JMSException jmsException) {                if (++jmsExceptionCounter > errorThreshold) {            if (consumer != null) {                                sourceCounter.incrementEventReadFail();                consumer.rollback();                consumer.close();                consumer = null;            }        }    } catch (Throwable throwable) {                sourceCounter.incrementEventReadFail();        if (throwable instanceof Error) {            throw (Error) throwable;        }    } finally {        if (error) {            if (consumer != null) {                consumer.rollback();            }        } else {            if (consumer != null) {                consumer.commit();                jmsExceptionCounter = 0;            }        }    }    return Status.BACKOFF;}
protected synchronized void flume_f5939_0()
{    try {        consumer = createConsumer();        jmsExceptionCounter = 0;        sourceCounter.start();    } catch (JMSException e) {        throw new FlumeException("Unable to create consumer", e);    }}
protected synchronized void flume_f5940_0()
{    if (consumer != null) {        consumer.close();        consumer = null;    }    sourceCounter.stop();}
 JMSMessageConsumer flume_f5941_1() throws JMSException
{        JMSMessageConsumer consumer = new JMSMessageConsumer(initialContext, connectionFactory, destinationName, destinationLocator, destinationType, messageSelector, batchSize, pollTimeout, converter, userName, password, clientId, createDurableSubscription, durableSubscriptionName);    jmsExceptionCounter = 0;    return consumer;}
public long flume_f5942_0()
{    return batchSize;}
public void flume_f5943_0() throws Exception
{    beforeSetup();    connectionFactory = mock(ConnectionFactory.class);    connection = mock(Connection.class);    session = mock(Session.class);    queue = mock(Queue.class);    topic = mock(Topic.class);    messageConsumer = mock(MessageConsumer.class);    message = mock(TextMessage.class);    when(message.getPropertyNames()).thenReturn(new Enumeration<Object>() {        @Override        public boolean hasMoreElements() {            return false;        }        @Override        public Object nextElement() {            throw new UnsupportedOperationException();        }    });    when(message.getText()).thenReturn(TEXT);    when(connectionFactory.createConnection(USERNAME, PASSWORD)).thenReturn(connection);    when(connection.createSession(true, Session.SESSION_TRANSACTED)).thenReturn(session);    when(session.createQueue(destinationName)).thenReturn(queue);    when(session.createConsumer(any(Destination.class), anyString())).thenReturn(messageConsumer);    when(messageConsumer.receiveNoWait()).thenReturn(message);    when(messageConsumer.receive(anyLong())).thenReturn(message);    destinationName = DESTINATION_NAME;    destinationType = JMSDestinationType.QUEUE;    destinationLocator = JMSDestinationLocator.CDI;    messageSelector = SELECTOR;    batchSize = 10;    pollTimeout = 500L;    context = new Context();    converter = new DefaultJMSMessageConverter.Builder().build(context);    event = converter.convert(message).iterator().next();    userName = Optional.of(USERNAME);    password = Optional.of(PASSWORD);    afterSetup();}
public boolean flume_f5944_0()
{    return false;}
public Object flume_f5945_0()
{    throw new UnsupportedOperationException();}
 void flume_f5946_0() throws Exception
{}
 void flume_f5947_0() throws Exception
{}
 void flume_f5948_0() throws Exception
{}
 void flume_f5949_0() throws Exception
{}
 void flume_f5950_0(List<Event> events)
{    for (Event event : events) {        assertEquals(TEXT, new String(event.getBody(), Charsets.UTF_8));    }}
 JMSMessageConsumer flume_f5951_0()
{    return new JMSMessageConsumer(WONT_USE, connectionFactory, destinationName, destinationLocator, destinationType, messageSelector, batchSize, pollTimeout, converter, userName, password, Optional.<String>absent(), false, "");}
public void flume_f5952_0() throws Exception
{    beforeTearDown();    if (consumer != null) {        consumer.close();    }    afterTearDown();}
public void flume_f5953_0() throws Exception
{    headers = Maps.newHashMap();    context = new Context();    converter = new DefaultJMSMessageConverter.Builder().build(context);}
 void flume_f5954_0() throws Exception
{    TextMessage message = mock(TextMessage.class);    when(message.getText()).thenReturn(TEXT);    this.message = message;}
 void flume_f5955_0() throws Exception
{    TextMessage message = mock(TextMessage.class);    when(message.getText()).thenReturn(null);    this.message = message;}
 void flume_f5956_0() throws Exception
{    BytesMessage message = mock(BytesMessage.class);    when(message.getBodyLength()).thenReturn((long) BYTES.length);    when(message.readBytes(any(byte[].class))).then(new Answer<Integer>() {        @Override        public Integer answer(InvocationOnMock invocation) throws Throwable {            byte[] buffer = (byte[]) invocation.getArguments()[0];            if (buffer != null) {                assertEquals(buffer.length, BYTES.length);                System.arraycopy(BYTES, 0, buffer, 0, BYTES.length);            }            return BYTES.length;        }    });    this.message = message;}
public Integer flume_f5957_0(InvocationOnMock invocation) throws Throwable
{    byte[] buffer = (byte[]) invocation.getArguments()[0];    if (buffer != null) {        assertEquals(buffer.length, BYTES.length);        System.arraycopy(BYTES, 0, buffer, 0, BYTES.length);    }    return BYTES.length;}
 void flume_f5958_0() throws Exception
{    ObjectMessage message = mock(ObjectMessage.class);    when(message.getObject()).thenReturn(TEXT);    this.message = message;}
 void flume_f5959_0() throws Exception
{    final Iterator<String> keys = headers.keySet().iterator();    when(message.getPropertyNames()).thenReturn(new Enumeration<Object>() {        @Override        public boolean hasMoreElements() {            return keys.hasNext();        }        @Override        public Object nextElement() {            return keys.next();        }    });    when(message.getStringProperty(anyString())).then(new Answer<String>() {        @Override        public String answer(InvocationOnMock invocation) throws Throwable {            return headers.get(invocation.getArguments()[0]);        }    });}
public boolean flume_f5960_0()
{    return keys.hasNext();}
public Object flume_f5961_0()
{    return keys.next();}
public String flume_f5962_0(InvocationOnMock invocation) throws Throwable
{    return headers.get(invocation.getArguments()[0]);}
public void flume_f5963_0() throws Exception
{    createTextMessage();    headers.put("key1", "value1");    headers.put("key2", "value2");    createHeaders();    Event event = converter.convert(message).iterator().next();    assertEquals(headers, event.getHeaders());    assertEquals(TEXT, new String(event.getBody(), Charsets.UTF_8));}
public void flume_f5964_0() throws Exception
{    createNullTextMessage();    headers.put("key1", "value1");    headers.put("key2", "value2");    createHeaders();    Event event = converter.convert(message).iterator().next();    assertEquals(headers, event.getHeaders());            assertEquals(event.getBody().length, 0);}
public void flume_f5965_0() throws Exception
{    createBytesMessage();    headers.put("key1", "value1");    headers.put("key2", "value2");    createHeaders();    Event event = converter.convert(message).iterator().next();    assertEquals(headers, event.getHeaders());    assertArrayEquals(BYTES, event.getBody());}
public void flume_f5966_0() throws Exception
{    createBytesMessage();    when(((BytesMessage) message).getBodyLength()).thenReturn(Long.MAX_VALUE);    createHeaders();    converter.convert(message);}
public void flume_f5967_0() throws Exception
{    createBytesMessage();    when(((BytesMessage) message).readBytes(any(byte[].class))).thenReturn(BYTES.length + 1);    createHeaders();    converter.convert(message);}
public void flume_f5968_0() throws Exception
{    createObjectMessage();    headers.put("key1", "value1");    headers.put("key2", "value2");    createHeaders();    Event event = converter.convert(message).iterator().next();    assertEquals(headers, event.getHeaders());    ByteArrayOutputStream bos = new ByteArrayOutputStream();    ObjectOutput out = new ObjectOutputStream(bos);    out.writeObject(TEXT);    assertArrayEquals(bos.toByteArray(), event.getBody());}
public void flume_f5969_0() throws Exception
{    createTextMessage();    createHeaders();    Event event = converter.convert(message).iterator().next();    assertEquals(Collections.EMPTY_MAP, event.getHeaders());    assertEquals(TEXT, new String(event.getBody(), Charsets.UTF_8));}
public static Collection<Object[]> flume_f5970_0()
{    return Arrays.asList(new Object[][] { { TestMode.WITH_AUTHENTICATION }, { TestMode.WITHOUT_AUTHENTICATION } });}
public void flume_f5971_0() throws Exception
{    baseDir = Files.createTempDir();    tmpDir = new File(baseDir, "tmp");    dataDir = new File(baseDir, "data");    Assert.assertTrue(tmpDir.mkdir());    broker = new BrokerService();    broker.addConnector(BROKER_BIND_URL);    broker.setTmpDataDirectory(tmpDir);    broker.setDataDirectoryFile(dataDir);    context = new Context();    context.put(JMSSourceConfiguration.INITIAL_CONTEXT_FACTORY, INITIAL_CONTEXT_FACTORY);    context.put(JMSSourceConfiguration.PROVIDER_URL, BROKER_BIND_URL);    context.put(JMSSourceConfiguration.DESTINATION_NAME, DESTINATION_NAME);    if (jmsUserName != null) {        File passwordFile = new File(baseDir, "password");        Files.write(jmsPassword.getBytes(Charsets.UTF_8), passwordFile);        AuthenticationUser jmsUser = new AuthenticationUser(jmsUserName, jmsPassword, "");        List<AuthenticationUser> users = Collections.singletonList(jmsUser);        SimpleAuthenticationPlugin authentication = new SimpleAuthenticationPlugin(users);        broker.setPlugins(new BrokerPlugin[] { authentication });        context.put(JMSSourceConfiguration.USERNAME, jmsUserName);        context.put(JMSSourceConfiguration.PASSWORD_FILE, passwordFile.getAbsolutePath());    }    broker.start();    events = Lists.newArrayList();    source = new JMSSource();    source.setName("JMSSource-" + UUID.randomUUID());    ChannelProcessor channelProcessor = mock(ChannelProcessor.class);    doAnswer(new Answer<Void>() {        @Override        public Void answer(InvocationOnMock invocation) throws Throwable {            events.addAll((List<Event>) invocation.getArguments()[0]);            return null;        }    }).when(channelProcessor).processEventBatch(any(List.class));    source.setChannelProcessor(channelProcessor);}
public Void flume_f5972_0(InvocationOnMock invocation) throws Throwable
{    events.addAll((List<Event>) invocation.getArguments()[0]);    return null;}
public void flume_f5973_0() throws Exception
{    if (source != null) {        source.stop();    }    if (broker != null) {        broker.stop();    }    FileUtils.deleteDirectory(baseDir);}
private void flume_f5974_0(List<String> events) throws Exception
{    ConnectionFactory factory = new ActiveMQConnectionFactory(jmsUserName, jmsPassword, BROKER_BIND_URL);    Connection connection = factory.createConnection();    connection.start();    Session session = connection.createSession(true, Session.AUTO_ACKNOWLEDGE);    Destination destination = session.createQueue(DESTINATION_NAME);    MessageProducer producer = session.createProducer(destination);    for (String event : events) {        TextMessage message = session.createTextMessage();        message.setText(event);        producer.send(message);    }    session.commit();    session.close();    connection.close();}
private void flume_f5975_0(List<String> events) throws Exception
{    ConnectionFactory factory = new ActiveMQConnectionFactory(jmsUserName, jmsPassword, BROKER_BIND_URL);    Connection connection = factory.createConnection();    connection.start();    Session session = connection.createSession(true, Session.AUTO_ACKNOWLEDGE);    Destination destination = session.createTopic(DESTINATION_NAME);    MessageProducer producer = session.createProducer(destination);    for (String event : events) {        TextMessage message = session.createTextMessage();        message.setText(event);        producer.send(message);    }    session.commit();    session.close();    connection.close();}
public void flume_f5976_0() throws Exception
{    context.put(JMSSourceConfiguration.DESTINATION_NAME, JNDI_PREFIX + DESTINATION_NAME);    context.put(JMSSourceConfiguration.DESTINATION_LOCATOR, JMSDestinationLocator.JNDI.name());    testQueue();}
public void flume_f5977_0() throws Exception
{    context.put(JMSSourceConfiguration.DESTINATION_TYPE, JMSSourceConfiguration.DESTINATION_TYPE_QUEUE);    source.configure(context);    source.start();    Thread.sleep(500L);    List<String> expected = Lists.newArrayList();    for (int i = 0; i < 10; i++) {        expected.add(String.valueOf(i));    }    putQueue(expected);    Thread.sleep(500L);    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(Status.BACKOFF, source.process());    Assert.assertEquals(expected.size(), events.size());    List<String> actual = Lists.newArrayList();    for (Event event : events) {        actual.add(new String(event.getBody(), Charsets.UTF_8));    }    Collections.sort(expected);    Collections.sort(actual);    Assert.assertEquals(expected, actual);}
public void flume_f5978_0() throws Exception
{    context.put(JMSSourceConfiguration.DESTINATION_TYPE, JMSSourceConfiguration.DESTINATION_TYPE_TOPIC);    source.configure(context);    source.start();    Thread.sleep(500L);    List<String> expected = Lists.newArrayList();    for (int i = 0; i < 10; i++) {        expected.add(String.valueOf(i));    }    putTopic(expected);    Thread.sleep(500L);    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(Status.BACKOFF, source.process());    Assert.assertEquals(expected.size(), events.size());    List<String> actual = Lists.newArrayList();    for (Event event : events) {        actual.add(new String(event.getBody(), Charsets.UTF_8));    }    Collections.sort(expected);    Collections.sort(actual);    Assert.assertEquals(expected, actual);}
public void flume_f5979_0() throws Exception
{    context.put(JMSSourceConfiguration.DESTINATION_TYPE, JMSSourceConfiguration.DESTINATION_TYPE_TOPIC);    context.put(JMSSourceConfiguration.CLIENT_ID, "FLUME");    context.put(JMSSourceConfiguration.DURABLE_SUBSCRIPTION_NAME, "SOURCE1");    context.put(JMSSourceConfiguration.CREATE_DURABLE_SUBSCRIPTION, "true");    context.put(JMSSourceConfiguration.BATCH_SIZE, "10");    source.configure(context);    source.start();    Thread.sleep(5000L);    List<String> expected = Lists.newArrayList();    List<String> input = Lists.newArrayList();    for (int i = 0; i < 10; i++) {        input.add("before " + String.valueOf(i));    }    expected.addAll(input);    putTopic(input);    Thread.sleep(500L);    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(Status.BACKOFF, source.process());    source.stop();    Thread.sleep(500L);    input = Lists.newArrayList();    for (int i = 0; i < 10; i++) {        input.add("during " + String.valueOf(i));    }    expected.addAll(input);    putTopic(input);    source.start();    Thread.sleep(500L);    input = Lists.newArrayList();    for (int i = 0; i < 10; i++) {        input.add("after " + String.valueOf(i));    }    expected.addAll(input);    putTopic(input);    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(Status.BACKOFF, source.process());    Assert.assertEquals(expected.size(), events.size());    List<String> actual = Lists.newArrayList();    for (Event event : events) {        actual.add(new String(event.getBody(), Charsets.UTF_8));    }    Collections.sort(expected);    Collections.sort(actual);    Assert.assertEquals(expected, actual);}
public void flume_f5980_0() throws Exception
{    when(connectionFactory.createConnection(USERNAME, PASSWORD)).thenThrow(new JMSException(""));    create();}
public void flume_f5981_0() throws Exception
{    when(connection.createSession(true, Session.SESSION_TRANSACTED)).thenThrow(new JMSException(""));    try {        create();        fail("Expected exception: org.apache.flume.FlumeException");    } catch (FlumeException e) {        verify(connection).close();    }}
public void flume_f5982_0() throws Exception
{    when(session.createQueue(destinationName)).thenThrow(new JMSException(""));    try {        create();        fail("Expected exception: org.apache.flume.FlumeException");    } catch (FlumeException e) {        verify(session).close();        verify(connection).close();    }}
public void flume_f5983_0() throws Exception
{    destinationType = JMSDestinationType.TOPIC;    when(session.createTopic(destinationName)).thenThrow(new JMSException(""));    try {        create();        fail("Expected exception: org.apache.flume.FlumeException");    } catch (FlumeException e) {        verify(session).close();        verify(connection).close();    }}
public void flume_f5984_0() throws Exception
{    when(session.createConsumer(any(Destination.class), anyString())).thenThrow(new JMSException(""));    try {        create();        fail("Expected exception: org.apache.flume.FlumeException");    } catch (FlumeException e) {        verify(session).close();        verify(connection).close();    }}
public void flume_f5985_0() throws Exception
{    batchSize = 0;    create();}
public void flume_f5986_0() throws Exception
{    pollTimeout = -1L;    create();}
public void flume_f5987_0() throws Exception
{    batchSize = -1;    create();}
public void flume_f5988_0() throws Exception
{    destinationType = JMSDestinationType.QUEUE;    when(session.createQueue(destinationName)).thenReturn(queue);    consumer = create();    List<Event> events = consumer.take();    assertEquals(batchSize, events.size());    assertBodyIsExpected(events);    verify(session, never()).createTopic(anyString());}
public void flume_f5989_0() throws Exception
{    destinationType = JMSDestinationType.TOPIC;    when(session.createTopic(destinationName)).thenReturn(topic);    consumer = create();    List<Event> events = consumer.take();    assertEquals(batchSize, events.size());    assertBodyIsExpected(events);    verify(session, never()).createQueue(anyString());}
public void flume_f5990_0() throws Exception
{    consumer = create();    List<Event> events = consumer.take();    assertEquals(batchSize, events.size());    assertBodyIsExpected(events);}
public void flume_f5991_0() throws Exception
{    userName = Optional.absent();    when(connectionFactory.createConnection(USERNAME, PASSWORD)).thenThrow(new AssertionError());    when(connectionFactory.createConnection()).thenReturn(connection);    consumer = create();    List<Event> events = consumer.take();    assertEquals(batchSize, events.size());    assertBodyIsExpected(events);}
public void flume_f5992_0() throws Exception
{    when(messageConsumer.receive(anyLong())).thenReturn(null);    consumer = create();    List<Event> events = consumer.take();    assertEquals(0, events.size());    verify(messageConsumer, times(1)).receive(anyLong());    verifyNoMoreInteractions(messageConsumer);}
public void flume_f5993_0() throws Exception
{    when(messageConsumer.receiveNoWait()).thenReturn(null);    consumer = create();    List<Event> events = consumer.take();    assertEquals(1, events.size());    assertBodyIsExpected(events);}
public void flume_f5994_0() throws Exception
{    when(messageConsumer.receiveNoWait()).thenReturn(message, (Message) null);    consumer = create();    List<Event> events = consumer.take();    assertEquals(2, events.size());    assertBodyIsExpected(events);}
public void flume_f5995_0() throws Exception
{    consumer = create();    consumer.commit();    verify(session, times(1)).commit();}
public void flume_f5996_0() throws Exception
{    consumer = create();    consumer.rollback();    verify(session, times(1)).rollback();}
public void flume_f5997_0() throws Exception
{    doThrow(new JMSException("")).when(session).close();    consumer = create();    consumer.close();    verify(session, times(1)).close();    verify(connection, times(1)).close();}
public void flume_f5998_0() throws Exception
{    String name = "SUBSCRIPTION_NAME";    String clientID = "CLIENT_ID";    TopicSubscriber mockTopicSubscriber = mock(TopicSubscriber.class);    when(session.createDurableSubscriber(any(Topic.class), anyString(), anyString(), anyBoolean())).thenReturn(mockTopicSubscriber);    when(session.createTopic(destinationName)).thenReturn(topic);    new JMSMessageConsumer(WONT_USE, connectionFactory, destinationName, destinationLocator, JMSDestinationType.TOPIC, messageSelector, batchSize, pollTimeout, converter, userName, password, Optional.of(clientID), true, name);    verify(connection, times(1)).setClientID(clientID);    verify(session, times(1)).createDurableSubscriber(topic, name, messageSelector, true);}
public void flume_f5999_0() throws JMSException
{    when(messageConsumer.receive(anyLong())).thenThrow(new JMSException(""));    consumer = create();    consumer.take();}
public void flume_f6000_0() throws JMSException
{    when(messageConsumer.receive(anyLong())).thenThrow(new RuntimeException());    consumer = create();    consumer.take();}
public void flume_f6001_0() throws JMSException
{    when(messageConsumer.receiveNoWait()).thenThrow(new JMSException(""));    consumer = create();    consumer.take();}
public void flume_f6002_0() throws JMSException
{    when(messageConsumer.receiveNoWait()).thenThrow(new RuntimeException());    consumer = create();    consumer.take();}
public void flume_f6003_0() throws JMSException
{    doThrow(new JMSException("")).when(session).commit();    consumer = create();    consumer.commit();}
public void flume_f6004_0() throws JMSException
{    doThrow(new RuntimeException()).when(session).commit();    consumer = create();    consumer.commit();}
public void flume_f6005_0() throws JMSException
{    doThrow(new JMSException("")).when(session).rollback();    consumer = create();    consumer.rollback();}
public void flume_f6006_0() throws JMSException
{    doThrow(new RuntimeException()).when(session).rollback();    consumer = create();    consumer.rollback();}
 void flume_f6007_0() throws Exception
{    baseDir = Files.createTempDir();    passwordFile = new File(baseDir, "password");    Assert.assertTrue(passwordFile.createNewFile());    initialContext = mock(InitialContext.class);    channelProcessor = mock(ChannelProcessor.class);    events = Lists.newArrayList();    doAnswer(new Answer<Void>() {        @Override        public Void answer(InvocationOnMock invocation) throws Throwable {            events.addAll((List<Event>) invocation.getArguments()[0]);            return null;        }    }).when(channelProcessor).processEventBatch(any(List.class));    consumer = spy(create());    when(initialContext.lookup(anyString())).thenReturn(connectionFactory);    contextFactory = mock(InitialContextFactory.class);    when(contextFactory.create(any(Properties.class))).thenReturn(initialContext);    source = spy(new JMSSource(contextFactory));    doReturn(consumer).when(source).createConsumer();    source.setName("JMSSource-" + UUID.randomUUID());    source.setChannelProcessor(channelProcessor);    context = new Context();    context.put(JMSSourceConfiguration.BATCH_SIZE, String.valueOf(batchSize));    context.put(JMSSourceConfiguration.DESTINATION_NAME, "INBOUND");    context.put(JMSSourceConfiguration.DESTINATION_TYPE, JMSSourceConfiguration.DESTINATION_TYPE_QUEUE);    context.put(JMSSourceConfiguration.PROVIDER_URL, "dummy:1414");    context.put(JMSSourceConfiguration.INITIAL_CONTEXT_FACTORY, "ldap://dummy:389");}
public Void flume_f6008_0(InvocationOnMock invocation) throws Throwable
{    events.addAll((List<Event>) invocation.getArguments()[0]);    return null;}
 void flume_f6009_0() throws Exception
{    FileUtils.deleteDirectory(baseDir);}
public void flume_f6010_0() throws Exception
{    source.configure(context);    source.start();    source.stop();    verify(consumer).close();}
public void flume_f6011_0() throws Exception
{    context.put(JMSSourceConfiguration.INITIAL_CONTEXT_FACTORY, "");    source.configure(context);}
public void flume_f6012_0() throws Exception
{    context.put(JMSSourceConfiguration.PROVIDER_URL, "");    source.configure(context);}
public void flume_f6013_0() throws Exception
{    context.put(JMSSourceConfiguration.DESTINATION_NAME, "");    source.configure(context);}
public void flume_f6014_0() throws Exception
{    context.put(JMSSourceConfiguration.DESTINATION_TYPE, "DUMMY");    source.configure(context);}
public void flume_f6015_0() throws Exception
{    context.put(JMSSourceConfiguration.DESTINATION_TYPE, "");    source.configure(context);}
public void flume_f6016_0() throws Exception
{    doThrow(new RuntimeException("Expected")).when(source).createConsumer();    source.configure(context);    source.start();    try {        source.process();        Assert.fail();    } catch (FlumeException expected) {    }}
public void flume_f6017_0() throws Exception
{    when(initialContext.lookup(anyString())).thenThrow(new NamingException());    source.configure(context);}
public void flume_f6018_0() throws Exception
{    when(contextFactory.create(any(Properties.class))).thenThrow(new NamingException());    source.configure(context);}
public void flume_f6019_0() throws Exception
{    context.put(JMSSourceConfiguration.BATCH_SIZE, "0");    source.configure(context);}
public void flume_f6020_0() throws Exception
{    context.put(JMSSourceConfiguration.PASSWORD_FILE, "/dev/does/not/exist/nor/will/ever/exist");    source.configure(context);}
public void flume_f6021_0() throws Exception
{    context.put(JMSSourceConfiguration.USERNAME, "dummy");    source.configure(context);    source.start();    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(batchSize, events.size());    assertBodyIsExpected(events);}
public void flume_f6022_0() throws Exception
{    context.put(JMSSourceConfiguration.USERNAME, "dummy");    context.put(JMSSourceConfiguration.PASSWORD_FILE, passwordFile.getAbsolutePath());    source.configure(context);    source.start();    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(batchSize, events.size());    assertBodyIsExpected(events);}
public void flume_f6023_0() throws Exception
{    context.put(JMSSourceConfiguration.CONVERTER_TYPE, "not a valid classname");    source.configure(context);}
public void flume_f6024_0() throws Exception
{    try {        source.process();        Assert.fail();    } catch (EventDeliveryException expected) {    }}
public void flume_f6025_0() throws Exception
{        context.put(JMSSourceConfiguration.CONVERTER_TYPE, DefaultJMSMessageConverter.Builder.class.getName());    source.configure(context);    source.start();    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(batchSize, events.size());    assertBodyIsExpected(events);    verify(consumer).commit();}
public List<Event> flume_f6026_0(Message message) throws JMSException
{    throw new UnsupportedOperationException();}
public List<Event> flume_f6027_0(Message message) throws JMSException
{    throw new UnsupportedOperationException();}
public void flume_f6028_0(Context context)
{}
public void flume_f6029_0() throws Exception
{        context.put(JMSSourceConfiguration.CONVERTER_TYPE, NonBuilderConfigurableConverter.class.getName());    source.configure(context);    source.start();    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(batchSize, events.size());    assertBodyIsExpected(events);    verify(consumer).commit();}
public void flume_f6030_0() throws Exception
{        context.put(JMSSourceConfiguration.CONVERTER_TYPE, NonBuilderNonConfigurableConverter.class.getName());    source.configure(context);    source.start();    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(batchSize, events.size());    assertBodyIsExpected(events);    verify(consumer).commit();}
public void flume_f6031_0() throws Exception
{    source.configure(context);    source.start();    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(batchSize, events.size());    assertBodyIsExpected(events);    verify(consumer).commit();}
public void flume_f6032_0() throws Exception
{    when(messageConsumer.receive(anyLong())).thenReturn(null);    source.configure(context);    source.start();    Assert.assertEquals(Status.BACKOFF, source.process());    Assert.assertEquals(0, events.size());    verify(consumer).commit();}
public void flume_f6033_0() throws Exception
{    when(messageConsumer.receiveNoWait()).thenReturn(message, (Message) null);    source.configure(context);    source.start();    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(2, events.size());    assertBodyIsExpected(events);    verify(consumer).commit();}
public void flume_f6034_0() throws Exception
{    doThrow(new ChannelException("dummy")).when(channelProcessor).processEventBatch(any(List.class));    source.configure(context);    source.start();    Assert.assertEquals(Status.BACKOFF, source.process());    verify(consumer).rollback();}
public void flume_f6035_0() throws Exception
{    doThrow(new Error()).when(channelProcessor).processEventBatch(any(List.class));    source.configure(context);    source.start();    try {        source.process();        Assert.fail();    } catch (Error ignores) {    }    verify(consumer).rollback();}
public void flume_f6036_0() throws Exception
{    source.configure(context);    source.start();    when(consumer.take()).thenThrow(new JMSException("dummy"));    int attempts = JMSSourceConfiguration.ERROR_THRESHOLD_DEFAULT;    for (int i = 0; i < attempts; i++) {        Assert.assertEquals(Status.BACKOFF, source.process());    }    Assert.assertEquals(Status.BACKOFF, source.process());    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(source, "sourceCounter");    Assert.assertEquals(1, sc.getEventReadFail());    verify(consumer, times(attempts + 1)).rollback();    verify(consumer, times(1)).close();}
public void flume_f6037_0() throws Exception
{    source.configure(context);    source.start();    when(consumer.take()).thenThrow(new RuntimeException("dummy"));    source.process();    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(source, "sourceCounter");    Assert.assertEquals(1, sc.getEventReadFail());}
public void flume_f6038_0() throws Exception
{    source.configure(context);    source.start();    when(source.getChannelProcessor()).thenThrow(new ChannelException("dummy"));    source.process();    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(source, "sourceCounter");    Assert.assertEquals(1, sc.getChannelWriteFail());}
public void flume_f6039_0()
{    sourceFactory = new DefaultSourceFactory();}
private void flume_f6040_0(String name, String type, Class<?> typeClass) throws FlumeException
{    Source src = sourceFactory.create(name, type);    Assert.assertNotNull(src);    Assert.assertTrue(typeClass.isInstance(src));}
public void flume_f6041_0()
{    verifySourceCreation("jms-src", "jms", JMSSource.class);}
public long flume_f6042_0()
{    return batchUpperLimit;}
public T flume_f6043_0()
{    return null;}
public void flume_f6044_0(KafkaConsumer<?, ?> consumer, SourceRebalanceListener listener)
{    consumer.subscribe(topicList, listener);}
public List<String> flume_f6045_0()
{    return topicList;}
public void flume_f6046_0(KafkaConsumer<?, ?> consumer, SourceRebalanceListener listener)
{    consumer.subscribe(pattern, listener);}
public Pattern flume_f6047_0()
{    return pattern;}
protected Status flume_f6048_1() throws EventDeliveryException
{    final String batchUUID = UUID.randomUUID().toString();    String kafkaKey;    Event event;    byte[] eventBody;    try {                final long nanoBatchStartTime = System.nanoTime();        final long batchStartTime = System.currentTimeMillis();        final long maxBatchEndTime = System.currentTimeMillis() + maxBatchDurationMillis;        while (eventList.size() < batchUpperLimit && System.currentTimeMillis() < maxBatchEndTime) {            if (it == null || !it.hasNext()) {                                                long durMs = Math.max(0L, maxBatchEndTime - System.currentTimeMillis());                Duration duration = Duration.ofMillis(durMs);                ConsumerRecords<String, byte[]> records = consumer.poll(duration);                it = records.iterator();                                if (rebalanceFlag.compareAndSet(true, false)) {                    break;                }                                if (!it.hasNext()) {                    counter.incrementKafkaEmptyCount();                                                            break;                }            }                        ConsumerRecord<String, byte[]> message = it.next();            kafkaKey = message.key();            if (useAvroEventFormat) {                                                ByteArrayInputStream in = new ByteArrayInputStream(message.value());                decoder = DecoderFactory.get().directBinaryDecoder(in, decoder);                if (!reader.isPresent()) {                    reader = Optional.of(new SpecificDatumReader<AvroFlumeEvent>(AvroFlumeEvent.class));                }                                                AvroFlumeEvent avroevent = reader.get().read(null, decoder);                eventBody = avroevent.getBody().array();                headers = toStringMap(avroevent.getHeaders());            } else {                eventBody = message.value();                headers.clear();                headers = new HashMap<String, String>(4);            }                        if (!headers.containsKey(KafkaSourceConstants.TIMESTAMP_HEADER)) {                headers.put(KafkaSourceConstants.TIMESTAMP_HEADER, String.valueOf(System.currentTimeMillis()));            }                        if (setTopicHeader && !headers.containsKey(topicHeader)) {                headers.put(topicHeader, message.topic());            }            if (!headers.containsKey(KafkaSourceConstants.PARTITION_HEADER)) {                headers.put(KafkaSourceConstants.PARTITION_HEADER, String.valueOf(message.partition()));            }            if (!headers.containsKey(OFFSET_HEADER)) {                headers.put(OFFSET_HEADER, String.valueOf(message.offset()));            }            if (kafkaKey != null) {                headers.put(KafkaSourceConstants.KEY_HEADER, kafkaKey);            }            if (log.isTraceEnabled()) {                if (LogPrivacyUtil.allowLogRawData()) {                    log.trace("Topic: {} Partition: {} Message: {}", new String[] { message.topic(), String.valueOf(message.partition()), new String(eventBody) });                } else {                    log.trace("Topic: {} Partition: {} Message arrived.", message.topic(), String.valueOf(message.partition()));                }            }            event = EventBuilder.withBody(eventBody, headers);            eventList.add(event);            if (log.isDebugEnabled()) {                                            }                        tpAndOffsetMetadata.put(new TopicPartition(message.topic(), message.partition()), new OffsetAndMetadata(message.offset() + 1, batchUUID));        }        if (eventList.size() > 0) {            counter.addToKafkaEventGetTimer((System.nanoTime() - nanoBatchStartTime) / (1000 * 1000));            counter.addToEventReceivedCount((long) eventList.size());            getChannelProcessor().processEventBatch(eventList);            counter.addToEventAcceptedCount(eventList.size());            if (log.isDebugEnabled()) {                            }            eventList.clear();            if (!tpAndOffsetMetadata.isEmpty()) {                long commitStartTime = System.nanoTime();                consumer.commitSync(tpAndOffsetMetadata);                long commitEndTime = System.nanoTime();                counter.addToKafkaCommitTimer((commitEndTime - commitStartTime) / (1000 * 1000));                tpAndOffsetMetadata.clear();            }            return Status.READY;        }        return Status.BACKOFF;    } catch (Exception e) {                counter.incrementEventReadOrChannelFail(e);        return Status.BACKOFF;    }}
protected void flume_f6049_1(Context context) throws FlumeException
{    this.context = context;    headers = new HashMap<String, String>(4);    tpAndOffsetMetadata = new HashMap<TopicPartition, OffsetAndMetadata>();    rebalanceFlag = new AtomicBoolean(false);    kafkaProps = new Properties();            translateOldProperties(context);    String topicProperty = context.getString(KafkaSourceConstants.TOPICS_REGEX);    if (topicProperty != null && !topicProperty.isEmpty()) {                subscriber = new PatternSubscriber(topicProperty);    } else if ((topicProperty = context.getString(KafkaSourceConstants.TOPICS)) != null && !topicProperty.isEmpty()) {                subscriber = new TopicListSubscriber(topicProperty);    } else if (subscriber == null) {        throw new ConfigurationException("At least one Kafka topic must be specified.");    }    batchUpperLimit = context.getInteger(KafkaSourceConstants.BATCH_SIZE, KafkaSourceConstants.DEFAULT_BATCH_SIZE);    maxBatchDurationMillis = context.getInteger(KafkaSourceConstants.BATCH_DURATION_MS, KafkaSourceConstants.DEFAULT_BATCH_DURATION);    useAvroEventFormat = context.getBoolean(KafkaSourceConstants.AVRO_EVENT, KafkaSourceConstants.DEFAULT_AVRO_EVENT);    if (log.isDebugEnabled()) {            }    zookeeperConnect = context.getString(ZOOKEEPER_CONNECT_FLUME_KEY);    migrateZookeeperOffsets = context.getBoolean(MIGRATE_ZOOKEEPER_OFFSETS, DEFAULT_MIGRATE_ZOOKEEPER_OFFSETS);    bootstrapServers = context.getString(KafkaSourceConstants.BOOTSTRAP_SERVERS);    if (bootstrapServers == null || bootstrapServers.isEmpty()) {        if (zookeeperConnect == null || zookeeperConnect.isEmpty()) {            throw new ConfigurationException("Bootstrap Servers must be specified");        } else {                                                String securityProtocolStr = context.getSubProperties(KafkaSourceConstants.KAFKA_CONSUMER_PREFIX).get(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG);            if (securityProtocolStr == null || securityProtocolStr.isEmpty()) {                securityProtocolStr = CommonClientConfigs.DEFAULT_SECURITY_PROTOCOL;            }            bootstrapServers = lookupBootstrap(zookeeperConnect, SecurityProtocol.valueOf(securityProtocolStr));        }    }    String groupIdProperty = context.getString(KAFKA_CONSUMER_PREFIX + ConsumerConfig.GROUP_ID_CONFIG);    if (groupIdProperty != null && !groupIdProperty.isEmpty()) {                groupId = groupIdProperty;    }    if (groupId == null || groupId.isEmpty()) {        groupId = DEFAULT_GROUP_ID;            }    setTopicHeader = context.getBoolean(KafkaSourceConstants.SET_TOPIC_HEADER, KafkaSourceConstants.DEFAULT_SET_TOPIC_HEADER);    topicHeader = context.getString(KafkaSourceConstants.TOPIC_HEADER, KafkaSourceConstants.DEFAULT_TOPIC_HEADER);    setConsumerProps(context);    if (log.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {            }    if (counter == null) {        counter = new KafkaSourceCounter(getName());    }}
private void flume_f6050_1(Context ctx)
{        String topic = context.getString(KafkaSourceConstants.TOPIC);    if (topic != null && !topic.isEmpty()) {        subscriber = new TopicListSubscriber(topic);            }        groupId = ctx.getString(KafkaSourceConstants.OLD_GROUP_ID);    if (groupId != null && !groupId.isEmpty()) {            }}
private void flume_f6051_0(Context ctx)
{    kafkaProps.clear();    kafkaProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, KafkaSourceConstants.DEFAULT_KEY_DESERIALIZER);    kafkaProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaSourceConstants.DEFAULT_VALUE_DESERIALIZER);        kafkaProps.putAll(ctx.getSubProperties(KafkaSourceConstants.KAFKA_CONSUMER_PREFIX));        kafkaProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);    if (groupId != null) {        kafkaProps.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);    }    kafkaProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, KafkaSourceConstants.DEFAULT_AUTO_COMMIT);    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);}
private String flume_f6052_0(String zookeeperConnect, SecurityProtocol securityProtocol)
{    try (KafkaZkClient zkClient = KafkaZkClient.apply(zookeeperConnect, JaasUtils.isZkSecurityEnabled(), ZK_SESSION_TIMEOUT, ZK_CONNECTION_TIMEOUT, 10, Time.SYSTEM, "kafka.server", "SessionExpireListener")) {        List<Broker> brokerList = JavaConverters.seqAsJavaListConverter(zkClient.getAllBrokersInCluster()).asJava();        List<BrokerEndPoint> endPoints = brokerList.stream().map(broker -> broker.brokerEndPoint(ListenerName.forSecurityProtocol(securityProtocol))).collect(Collectors.toList());        List<String> connections = new ArrayList<>();        for (BrokerEndPoint endPoint : endPoints) {            connections.add(endPoint.connectionString());        }        return StringUtils.join(connections, ',');    }}
 String flume_f6053_0()
{    return bootstrapServers;}
 Properties flume_f6054_0()
{    return kafkaProps;}
private static Map<String, String> flume_f6055_0(Map<CharSequence, CharSequence> charSeqMap)
{    Map<String, String> stringMap = new HashMap<String, String>();    for (Map.Entry<CharSequence, CharSequence> entry : charSeqMap.entrySet()) {        stringMap.put(entry.getKey().toString(), entry.getValue().toString());    }    return stringMap;}
 Subscriber<T> flume_f6056_0()
{    return subscriber;}
protected void flume_f6057_1() throws FlumeException
{            if (migrateZookeeperOffsets && zookeeperConnect != null && !zookeeperConnect.isEmpty()) {                if (subscriber instanceof TopicListSubscriber && ((TopicListSubscriber) subscriber).get().size() == 1) {            String topicStr = ((TopicListSubscriber) subscriber).get().get(0);            migrateOffsets(topicStr);        } else {                    }    }        consumer = new KafkaConsumer<String, byte[]>(kafkaProps);        subscriber.subscribe(consumer, new SourceRebalanceListener(rebalanceFlag));        counter.start();}
protected void flume_f6058_1() throws FlumeException
{    if (consumer != null) {        consumer.wakeup();        consumer.close();    }    if (counter != null) {        counter.stop();    }    }
private void flume_f6059_1(String topicStr)
{    try (KafkaZkClient zkClient = KafkaZkClient.apply(zookeeperConnect, JaasUtils.isZkSecurityEnabled(), ZK_SESSION_TIMEOUT, ZK_CONNECTION_TIMEOUT, 10, Time.SYSTEM, "kafka.server", "SessionExpireListener");        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<>(kafkaProps)) {        Map<TopicPartition, OffsetAndMetadata> kafkaOffsets = getKafkaOffsets(consumer, topicStr);        if (kafkaOffsets == null) {                        return;        }        if (!kafkaOffsets.isEmpty()) {                                    return;        }                Map<TopicPartition, OffsetAndMetadata> zookeeperOffsets = getZookeeperOffsets(zkClient, consumer, topicStr);        if (zookeeperOffsets.isEmpty()) {                        return;        }                        consumer.commitSync(zookeeperOffsets);                Map<TopicPartition, OffsetAndMetadata> newKafkaOffsets = getKafkaOffsets(consumer, topicStr);                if (newKafkaOffsets == null || !newKafkaOffsets.keySet().containsAll(zookeeperOffsets.keySet())) {            throw new FlumeException("Offsets could not be committed");        }    }}
private Map<TopicPartition, OffsetAndMetadata> flume_f6060_0(KafkaConsumer<String, byte[]> client, String topicStr)
{    Map<TopicPartition, OffsetAndMetadata> offsets = null;    List<PartitionInfo> partitions = client.partitionsFor(topicStr);    if (partitions != null) {        offsets = new HashMap<>();        for (PartitionInfo partition : partitions) {            TopicPartition key = new TopicPartition(topicStr, partition.partition());            OffsetAndMetadata offsetAndMetadata = client.committed(key);            if (offsetAndMetadata != null) {                offsets.put(key, offsetAndMetadata);            }        }    }    return offsets;}
private Map<TopicPartition, OffsetAndMetadata> flume_f6061_0(KafkaZkClient zkClient, KafkaConsumer<String, byte[]> consumer, String topicStr)
{    Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();    List<PartitionInfo> partitions = consumer.partitionsFor(topicStr);    for (PartitionInfo partition : partitions) {        TopicPartition topicPartition = new TopicPartition(topicStr, partition.partition());        Option<Object> optionOffset = zkClient.getConsumerOffset(groupId, topicPartition);        if (optionOffset.nonEmpty()) {            Long offset = (Long) optionOffset.get();            OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(offset);            offsets.put(topicPartition, offsetAndMetadata);        }    }    return offsets;}
public void flume_f6062_1(Collection<TopicPartition> partitions)
{    for (TopicPartition partition : partitions) {                rebalanceFlag.set(true);    }}
public void flume_f6063_1(Collection<TopicPartition> partitions)
{    for (TopicPartition partition : partitions) {            }}
private static int flume_f6064_0()
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    } catch (IOException e) {        throw new AssertionError("Can not find free port.", e);    }}
public void flume_f6065_0() throws IOException
{    producer.close();    kafkaServer.shutdown();    zookeeper.stopZookeeper();}
public String flume_f6066_0()
{    return zookeeper.getConnectString();}
public String flume_f6067_0()
{    return HOST + ":" + serverPort;}
private void flume_f6068_0()
{    Properties props = new Properties();    props.put("bootstrap.servers", HOST + ":" + serverPort);    props.put("acks", "1");    producer = new KafkaProducer<String, byte[]>(props, new StringSerializer(), new ByteArraySerializer());}
public void flume_f6069_0(String topic, String k, String v)
{    produce(topic, k, v.getBytes());}
public void flume_f6070_0(String topic, String k, byte[] v)
{    ProducerRecord<String, byte[]> rec = new ProducerRecord<String, byte[]>(topic, k, v);    try {        producer.send(rec).get();    } catch (InterruptedException e) {        e.printStackTrace();    } catch (ExecutionException e) {        e.printStackTrace();    }}
public void flume_f6071_0(String topic, int partition, String k, String v)
{    produce(topic, partition, k, v.getBytes());}
public void flume_f6072_0(String topic, int partition, String k, byte[] v)
{    ProducerRecord<String, byte[]> rec = new ProducerRecord<String, byte[]>(topic, partition, k, v);    try {        producer.send(rec).get();    } catch (InterruptedException e) {        e.printStackTrace();    } catch (ExecutionException e) {        e.printStackTrace();    }}
public void flume_f6073_0(String topicName, int numPartitions)
{    AdminClient adminClient = getAdminClient();    NewTopic newTopic = new NewTopic(topicName, numPartitions, (short) 1);    adminClient.createTopics(Collections.singletonList(newTopic));        DescribeTopicsResult dtr = adminClient.describeTopics(Collections.singletonList(topicName));    try {        dtr.all().get(10, TimeUnit.SECONDS);    } catch (Exception e) {        throw new RuntimeException("Error getting topic info", e);    }}
private AdminClient flume_f6074_0()
{    if (adminClient == null) {        final Properties props = new Properties();        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, HOST + ":" + serverPort);        props.put(ConsumerConfig.GROUP_ID_CONFIG, "group_1");        adminClient = AdminClient.create(props);    }    return adminClient;}
public void flume_f6075_0(List<String> topic)
{    getAdminClient().deleteTopics(topic);}
public void flume_f6076_0() throws IOException
{    zookeeper.shutdown();    factory.shutdown();    FileUtils.deleteDirectory(dir);}
public String flume_f6077_0()
{    return KafkaSourceEmbeddedKafka.HOST + ":" + zkPort;}
public static void flume_f6078_0()
{    kafkaServer = new KafkaSourceEmbeddedKafka(null);    startupCheck();}
public void flume_f6079_0() throws Exception
{    kafkaSource = new KafkaSource();    try {        topic0 = findUnusedTopic();        kafkaServer.createTopic(topic0, 1);        usedTopics.add(topic0);        topic1 = findUnusedTopic();        kafkaServer.createTopic(topic1, 3);        usedTopics.add(topic1);    } catch (TopicExistsException e) {                e.printStackTrace();    }    context = prepareDefaultContext("flume-group");    kafkaSource.setChannelProcessor(createGoodChannel());}
private static void flume_f6080_1()
{    String startupTopic = "startupCheck";    KafkaConsumer<String, String> startupConsumer;    kafkaServer.createTopic(startupTopic, 1);    final Properties props = new Properties();    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaServer.getBootstrapServers());    props.put(ConsumerConfig.GROUP_ID_CONFIG, "group_1");    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);    consumer.subscribe(Collections.singletonList(startupTopic));        boolean success = false;    for (int i = 0; i < 20; i++) {        kafkaServer.produce(startupTopic, "", "record");        ConsumerRecords recs = consumer.poll(Duration.ofMillis(1000L));        if (!recs.isEmpty()) {            success = true;            break;        }    }    if (!success) {        fail("Kafka server startup failed");    }        consumer.close();    kafkaServer.deleteTopics(Collections.singletonList(startupTopic));}
private Context flume_f6081_0(String groupId)
{    Context context = new Context();    context.put(BOOTSTRAP_SERVERS, kafkaServer.getBootstrapServers());    context.put(KAFKA_CONSUMER_PREFIX + "group.id", groupId);    return context;}
public void flume_f6082_1() throws Exception
{    try {        kafkaSource.stop();    } catch (Exception e) {            }    topic0 = null;    topic1 = null;    kafkaServer.deleteTopics(usedTopics);    usedTopics.clear();}
public static void flume_f6083_0() throws Exception
{    kafkaServer.stop();}
private void flume_f6084_0() throws EventDeliveryException, InterruptedException
{    kafkaSource.start();    /* Timing magic: We call the process method, that executes a consumer.poll()      A thread.sleep(10000L) does not work even though it takes longer */    for (int i = 0; i < 3; i++) {        kafkaSource.process();        Thread.sleep(1000);    }}
public void flume_f6085_0() throws InterruptedException, EventDeliveryException
{    long batchDuration = 2000;    context.put(TOPICS, topic1);    context.put(BATCH_DURATION_MS, String.valueOf(batchDuration));    context.put(BATCH_SIZE, "3");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    Status status = kafkaSource.process();    assertEquals(Status.BACKOFF, status);    assertEquals(0, events.size());    kafkaServer.produce(topic1, "", "record1");    kafkaServer.produce(topic1, "", "record2");    Thread.sleep(500L);    status = kafkaSource.process();    assertEquals(Status.READY, status);    assertEquals(2, events.size());    events.clear();    kafkaServer.produce(topic1, "", "record3");    kafkaServer.produce(topic1, "", "record4");    kafkaServer.produce(topic1, "", "record5");    Thread.sleep(500L);    assertEquals(Status.READY, kafkaSource.process());    assertEquals(3, events.size());    assertEquals("record3", new String(events.get(0).getBody(), Charsets.UTF_8));    assertEquals("record4", new String(events.get(1).getBody(), Charsets.UTF_8));    assertEquals("record5", new String(events.get(2).getBody(), Charsets.UTF_8));    events.clear();    kafkaServer.produce(topic1, "", "record6");    kafkaServer.produce(topic1, "", "record7");    kafkaServer.produce(topic1, "", "record8");    kafkaServer.produce(topic1, "", "record9");    kafkaServer.produce(topic1, "", "record10");    Thread.sleep(500L);    assertEquals(Status.READY, kafkaSource.process());    assertEquals(3, events.size());    assertEquals("record6", new String(events.get(0).getBody(), Charsets.UTF_8));    assertEquals("record7", new String(events.get(1).getBody(), Charsets.UTF_8));    assertEquals("record8", new String(events.get(2).getBody(), Charsets.UTF_8));    events.clear();    kafkaServer.produce(topic1, "", "record11");        assertEquals(Status.READY, kafkaSource.process());    assertEquals(3, events.size());    assertEquals("record9", new String(events.get(0).getBody(), Charsets.UTF_8));    assertEquals("record10", new String(events.get(1).getBody(), Charsets.UTF_8));    assertEquals("record11", new String(events.get(2).getBody(), Charsets.UTF_8));    events.clear();    kafkaServer.produce(topic1, "", "record12");    kafkaServer.produce(topic1, "", "record13");        kafkaSource.stop();        kafkaSource = new KafkaSource();    kafkaSource.setChannelProcessor(createGoodChannel());    kafkaSource.configure(context);    startKafkaSource();    kafkaServer.produce(topic1, "", "record14");    Thread.sleep(1000L);    assertEquals(Status.READY, kafkaSource.process());    assertEquals(3, events.size());    assertEquals("record12", new String(events.get(0).getBody(), Charsets.UTF_8));    assertEquals("record13", new String(events.get(1).getBody(), Charsets.UTF_8));    assertEquals("record14", new String(events.get(2).getBody(), Charsets.UTF_8));    events.clear();}
public void flume_f6086_0() throws EventDeliveryException, SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException, InterruptedException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world");    Thread.sleep(500L);    Assert.assertEquals(Status.READY, kafkaSource.process());    Assert.assertEquals(Status.BACKOFF, kafkaSource.process());    Assert.assertEquals(1, events.size());    Assert.assertEquals("hello, world", new String(events.get(0).getBody(), Charsets.UTF_8));}
public void flume_f6087_0() throws EventDeliveryException, SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException, InterruptedException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "2");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world");    kafkaServer.produce(topic0, "", "foo, bar");    Thread.sleep(500L);    Status status = kafkaSource.process();    assertEquals(Status.READY, status);    Assert.assertEquals("hello, world", new String(events.get(0).getBody(), Charsets.UTF_8));    Assert.assertEquals("foo, bar", new String(events.get(1).getBody(), Charsets.UTF_8));}
public void flume_f6088_0() throws EventDeliveryException, SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException, InterruptedException
{    context.put(TOPICS, topic0);    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    Status status = kafkaSource.process();    assertEquals(Status.BACKOFF, status);}
public void flume_f6089_0() throws EventDeliveryException, SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException, InterruptedException
{    context.put(TOPICS, "faketopic");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    assertEquals(LifecycleState.START, kafkaSource.getLifecycleState());    Status status = kafkaSource.process();    assertEquals(Status.BACKOFF, status);}
public void flume_f6090_0() throws EventDeliveryException, SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException, InterruptedException
{    context.put(TOPICS, topic0);    context.put(BOOTSTRAP_SERVERS, "blabla:666");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    Status status = kafkaSource.process();    assertEquals(Status.BACKOFF, status);}
public void flume_f6091_0() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    context.put(BATCH_DURATION_MS, "250");    kafkaSource.configure(context);    startKafkaSource();        kafkaSource.process();    Thread.sleep(500L);    for (int i = 1; i < 5000; i++) {        kafkaServer.produce(topic0, "", "hello, world " + i);    }    Thread.sleep(500L);    long error = 50;    long startTime = System.currentTimeMillis();    Status status = kafkaSource.process();    long endTime = System.currentTimeMillis();    assertEquals(Status.READY, status);    assertTrue(endTime - startTime < (context.getLong(BATCH_DURATION_MS) + error));}
public void flume_f6092_0() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world");    Thread.sleep(500L);    Assert.assertEquals(Status.READY, kafkaSource.process());    kafkaSource.stop();    Thread.sleep(500L);    startKafkaSource();    Thread.sleep(500L);    Assert.assertEquals(Status.BACKOFF, kafkaSource.process());}
public void flume_f6093_1() throws EventDeliveryException, InterruptedException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    context.put(BATCH_DURATION_MS, "30000");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world");    Thread.sleep(500L);    kafkaSource.setChannelProcessor(createBadChannel());        Assert.assertEquals(Status.BACKOFF, kafkaSource.process());        kafkaSource.setChannelProcessor(createGoodChannel());        kafkaSource.process();    Assert.assertEquals("hello, world", new String(events.get(0).getBody(), Charsets.UTF_8));}
public void flume_f6094_0() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    context.put(BATCH_DURATION_MS, "30000");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "event 1");    Thread.sleep(500L);    kafkaSource.process();    Assert.assertEquals("event 1", new String(events.get(0).getBody(), Charsets.UTF_8));    events.clear();    kafkaServer.produce(topic0, "", "event 2");    Thread.sleep(500L);    kafkaSource.process();    Assert.assertEquals("event 2", new String(events.get(0).getBody(), Charsets.UTF_8));}
public void flume_f6095_0() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    context.put(BATCH_DURATION_MS, "30000");    context.put(KAFKA_CONSUMER_PREFIX + "enable.auto.commit", "true");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "event 1");    Thread.sleep(500L);    kafkaSource.process();    Assert.assertEquals("event 1", new String(events.get(0).getBody(), Charsets.UTF_8));    events.clear();    kafkaServer.produce(topic0, "", "event 2");    Thread.sleep(500L);    kafkaSource.process();    Assert.assertEquals("event 2", new String(events.get(0).getBody(), Charsets.UTF_8));}
public void flume_f6096_0() throws EventDeliveryException, SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException, InterruptedException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, null, "hello, world");    Thread.sleep(500L);    Assert.assertEquals(Status.READY, kafkaSource.process());    Assert.assertEquals(Status.BACKOFF, kafkaSource.process());    Assert.assertEquals(1, events.size());    Assert.assertEquals("hello, world", new String(events.get(0).getBody(), Charsets.UTF_8));}
public void flume_f6097_0() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    kafkaSource.configure(context);    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(new ChannelException("dummy")).doThrow(new RuntimeException("dummy")).when(cp).processEventBatch(any(List.class));    kafkaSource.setChannelProcessor(cp);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world");    Thread.sleep(500L);    kafkaSource.doProcess();    kafkaSource.doProcess();    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(kafkaSource, "counter");    Assert.assertEquals(1, sc.getChannelWriteFail());    Assert.assertEquals(1, sc.getEventReadFail());    kafkaSource.stop();}
public void flume_f6098_0()
{    Context context = new Context();    context.put(TOPICS, "test1, test2");    context.put(TOPICS_REGEX, "^stream[0-9]$");    context.put(BOOTSTRAP_SERVERS, "bootstrap-servers-list");    KafkaSource source = new KafkaSource();    source.doConfigure(context);            KafkaSource.Subscriber<Pattern> subscriber = source.getSubscriber();    Pattern pattern = subscriber.get();    Assert.assertTrue(pattern.matcher("stream1").find());}
public void flume_f6099_0()
{    Context context = new Context();    context.put(TOPICS, "test1, test2");    context.put(KAFKA_CONSUMER_PREFIX + ConsumerConfig.GROUP_ID_CONFIG, "override.default.group.id");    context.put(KAFKA_CONSUMER_PREFIX + "fake.property", "kafka.property.value");    context.put(BOOTSTRAP_SERVERS, "real-bootstrap-servers-list");    context.put(KAFKA_CONSUMER_PREFIX + "bootstrap.servers", "bad-bootstrap-servers-list");    KafkaSource source = new KafkaSource();    source.doConfigure(context);    Properties kafkaProps = source.getConsumerProps();        assertEquals(String.valueOf(DEFAULT_AUTO_COMMIT), kafkaProps.getProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));        assertEquals("override.default.group.id", kafkaProps.getProperty(ConsumerConfig.GROUP_ID_CONFIG));        assertEquals("kafka.property.value", kafkaProps.getProperty("fake.property"));        assertEquals("real-bootstrap-servers-list", kafkaProps.getProperty("bootstrap.servers"));}
public void flume_f6100_0()
{    Context context = new Context();    context.put(TOPIC, "old.topic");    context.put(OLD_GROUP_ID, "old.groupId");    context.put(BOOTSTRAP_SERVERS, "real-bootstrap-servers-list");    KafkaSource source = new KafkaSource();    source.doConfigure(context);    Properties kafkaProps = source.getConsumerProps();    KafkaSource.Subscriber<List<String>> subscriber = source.getSubscriber();        assertEquals("old.topic", subscriber.get().get(0));        assertEquals("old.groupId", kafkaProps.getProperty(ConsumerConfig.GROUP_ID_CONFIG));    source = new KafkaSource();    context.put(KAFKA_CONSUMER_PREFIX + ConsumerConfig.GROUP_ID_CONFIG, "override.old.group.id");    source.doConfigure(context);    kafkaProps = source.getConsumerProps();        assertEquals("override.old.group.id", kafkaProps.getProperty(ConsumerConfig.GROUP_ID_CONFIG));    context.clear();    context.put(BOOTSTRAP_SERVERS, "real-bootstrap-servers-list");    context.put(TOPIC, "old.topic");    source = new KafkaSource();    source.doConfigure(context);    kafkaProps = source.getConsumerProps();        assertEquals(KafkaSourceConstants.DEFAULT_GROUP_ID, kafkaProps.getProperty(ConsumerConfig.GROUP_ID_CONFIG));}
public void flume_f6101_0()
{    Context context = new Context();    context.put(TOPICS_REGEX, "^topic[0-9]$");    context.put(OLD_GROUP_ID, "old.groupId");    context.put(BOOTSTRAP_SERVERS, "real-bootstrap-servers-list");    KafkaSource source = new KafkaSource();    source.doConfigure(context);    KafkaSource.Subscriber<Pattern> subscriber = source.getSubscriber();    for (int i = 0; i < 10; i++) {        Assert.assertTrue(subscriber.get().matcher("topic" + i).find());    }    Assert.assertFalse(subscriber.get().matcher("topic").find());}
public void flume_f6102_0() throws InterruptedException, EventDeliveryException, IOException
{    SpecificDatumWriter<AvroFlumeEvent> writer;    ByteArrayOutputStream tempOutStream;    BinaryEncoder encoder;    byte[] bytes;    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    context.put(AVRO_EVENT, "true");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    tempOutStream = new ByteArrayOutputStream();    writer = new SpecificDatumWriter<AvroFlumeEvent>(AvroFlumeEvent.class);    Map<CharSequence, CharSequence> headers = new HashMap<CharSequence, CharSequence>();    headers.put("header1", "value1");    headers.put("header2", "value2");    AvroFlumeEvent e = new AvroFlumeEvent(headers, ByteBuffer.wrap("hello, world".getBytes()));    encoder = EncoderFactory.get().directBinaryEncoder(tempOutStream, null);    writer.write(e, encoder);    encoder.flush();    bytes = tempOutStream.toByteArray();    kafkaServer.produce(topic0, "", bytes);    String currentTimestamp = Long.toString(System.currentTimeMillis());    headers.put(TIMESTAMP_HEADER, currentTimestamp);    headers.put(PARTITION_HEADER, "1");    headers.put(DEFAULT_TOPIC_HEADER, "topic0");    e = new AvroFlumeEvent(headers, ByteBuffer.wrap("hello, world2".getBytes()));    tempOutStream.reset();    encoder = EncoderFactory.get().directBinaryEncoder(tempOutStream, null);    writer.write(e, encoder);    encoder.flush();    bytes = tempOutStream.toByteArray();    kafkaServer.produce(topic0, "", bytes);    Thread.sleep(500L);    Assert.assertEquals(Status.READY, kafkaSource.process());    Assert.assertEquals(Status.READY, kafkaSource.process());    Assert.assertEquals(Status.BACKOFF, kafkaSource.process());    Assert.assertEquals(2, events.size());    Event event = events.get(0);    Assert.assertEquals("hello, world", new String(event.getBody(), Charsets.UTF_8));    Assert.assertEquals("value1", e.getHeaders().get("header1"));    Assert.assertEquals("value2", e.getHeaders().get("header2"));    event = events.get(1);    Assert.assertEquals("hello, world2", new String(event.getBody(), Charsets.UTF_8));    Assert.assertEquals("value1", e.getHeaders().get("header1"));    Assert.assertEquals("value2", e.getHeaders().get("header2"));    Assert.assertEquals(currentTimestamp, e.getHeaders().get(TIMESTAMP_HEADER));    Assert.assertEquals(e.getHeaders().get(PARTITION_HEADER), "1");    Assert.assertEquals(e.getHeaders().get(DEFAULT_TOPIC_HEADER), "topic0");}
public void flume_f6103_0()
{    Context context = new Context();    context.put(ZOOKEEPER_CONNECT_FLUME_KEY, kafkaServer.getZkConnectString());    context.put(TOPIC, "old.topic");    context.put(OLD_GROUP_ID, "old.groupId");    KafkaSource source = new KafkaSource();    source.doConfigure(context);    String bootstrapServers = source.getBootstrapServers();    Assert.assertEquals(kafkaServer.getBootstrapServers(), bootstrapServers);}
public void flume_f6104_0() throws Exception
{    doTestMigrateZookeeperOffsets(false, false, "testMigrateOffsets-none");}
public void flume_f6105_0() throws Exception
{    doTestMigrateZookeeperOffsets(true, false, "testMigrateOffsets-zookeeper");}
public void flume_f6106_0() throws Exception
{    doTestMigrateZookeeperOffsets(false, true, "testMigrateOffsets-kafka");}
public void flume_f6107_0() throws Exception
{    doTestMigrateZookeeperOffsets(true, true, "testMigrateOffsets-both");}
public void flume_f6108_0() throws Exception
{    String sampleConsumerProp = "auto.offset.reset";    String sampleConsumerVal = "earliest";    String group = "group";    Context context = prepareDefaultContext(group);    context.put(KafkaSourceConstants.KAFKA_CONSUMER_PREFIX + sampleConsumerProp, sampleConsumerVal);    context.put(TOPIC, "random-topic");    kafkaSource.configure(context);    Assert.assertEquals(sampleConsumerVal, kafkaSource.getConsumerProps().getProperty(sampleConsumerProp));    context = prepareDefaultContext(group);    context.put(TOPIC, "random-topic");    kafkaSource.configure(context);    Assert.assertNull(kafkaSource.getConsumerProps().getProperty(sampleConsumerProp));}
public void flume_f6109_0() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world");    Thread.sleep(500L);    Status status = kafkaSource.process();    assertEquals(Status.READY, status);    Assert.assertEquals("hello, world", new String(events.get(0).getBody(), Charsets.UTF_8));    Assert.assertEquals(topic0, events.get(0).getHeaders().get("topic"));    kafkaSource.stop();    events.clear();}
public void flume_f6110_0() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    context.put(KafkaSourceConstants.TOPIC_HEADER, "customTopicHeader");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world2");    Thread.sleep(500L);    Status status = kafkaSource.process();    assertEquals(Status.READY, status);    Assert.assertEquals("hello, world2", new String(events.get(0).getBody(), Charsets.UTF_8));    Assert.assertEquals(topic0, events.get(0).getHeaders().get("customTopicHeader"));    kafkaSource.stop();    events.clear();}
public void flume_f6111_0() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    context.put(KafkaSourceConstants.SET_TOPIC_HEADER, "false");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world3");    Thread.sleep(500L);    Status status = kafkaSource.process();    assertEquals(Status.READY, status);    Assert.assertEquals("hello, world3", new String(events.get(0).getBody(), Charsets.UTF_8));    Assert.assertNull(events.get(0).getHeaders().get("customTopicHeader"));    kafkaSource.stop();}
private void flume_f6112_0(boolean hasZookeeperOffsets, boolean hasKafkaOffsets, String group) throws Exception
{        String topic = findUnusedTopic();    kafkaServer.createTopic(topic, 1);    Context context = prepareDefaultContext(group);    context.put(ZOOKEEPER_CONNECT_FLUME_KEY, kafkaServer.getZkConnectString());    context.put(TOPIC, topic);    KafkaSource source = new KafkaSource();    source.doConfigure(context);        Long fifthOffset = 0L;    Long tenthOffset = 0L;    Properties props = createProducerProps(kafkaServer.getBootstrapServers());    KafkaProducer<String, byte[]> producer = new KafkaProducer<>(props);    for (int i = 1; i <= 50; i++) {        ProducerRecord<String, byte[]> data = new ProducerRecord<>(topic, null, String.valueOf(i).getBytes());        RecordMetadata recordMetadata = producer.send(data).get();        if (i == 5) {            fifthOffset = recordMetadata.offset();        }        if (i == 10) {            tenthOffset = recordMetadata.offset();        }    }        if (hasZookeeperOffsets) {        KafkaZkClient zkClient = KafkaZkClient.apply(kafkaServer.getZkConnectString(), JaasUtils.isZkSecurityEnabled(), 30000, 30000, 10, Time.SYSTEM, "kafka.server", "SessionExpireListener");        zkClient.getConsumerOffset(group, new TopicPartition(topic, 0));        Long offset = tenthOffset + 1;        zkClient.setOrCreateConsumerOffset(group, new TopicPartition(topic, 0), offset);        zkClient.close();    }        if (hasKafkaOffsets) {        Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();        offsets.put(new TopicPartition(topic, 0), new OffsetAndMetadata(fifthOffset + 1));        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<>(source.getConsumerProps());        consumer.commitSync(offsets);        consumer.close();    }        source.setChannelProcessor(createGoodChannel());    source.start();    for (int i = 0; i < 3; i++) {        source.process();        Thread.sleep(1000);    }    Thread.sleep(500L);    source.process();    List<Integer> finals = new ArrayList<Integer>(40);    for (Event event : events) {        finals.add(Integer.parseInt(new String(event.getBody())));    }    source.stop();    if (!hasKafkaOffsets && !hasZookeeperOffsets) {                org.junit.Assert.assertTrue("Source should read no messages", finals.isEmpty());    } else if (hasKafkaOffsets && hasZookeeperOffsets) {                org.junit.Assert.assertFalse("Source should not read the 5th message", finals.contains(5));        org.junit.Assert.assertTrue("Source should read the 6th message", finals.contains(6));    } else if (hasKafkaOffsets) {                org.junit.Assert.assertFalse("Source should not read the 5th message", finals.contains(5));        org.junit.Assert.assertTrue("Source should read the 6th message", finals.contains(6));    } else {                org.junit.Assert.assertFalse("Source should not read the 10th message", finals.contains(10));        org.junit.Assert.assertTrue("Source should read the 11th message", finals.contains(11));    }}
public void flume_f6113_0() throws Exception
{    String topic = findUnusedTopic();    Context context = prepareDefaultContext("testMigrateOffsets-nonExistingTopic");    context.put(ZOOKEEPER_CONNECT_FLUME_KEY, kafkaServer.getZkConnectString());    context.put(TOPIC, topic);    KafkaSource source = new KafkaSource();    source.doConfigure(context);    source.setChannelProcessor(createGoodChannel());    source.start();    assertEquals(LifecycleState.START, source.getLifecycleState());    Status status = source.process();    assertEquals(Status.BACKOFF, status);    source.stop();}
 ChannelProcessor flume_f6114_0()
{    ChannelProcessor channelProcessor = mock(ChannelProcessor.class);    events = Lists.newArrayList();    doAnswer(new Answer<Void>() {        @Override        public Void answer(InvocationOnMock invocation) throws Throwable {            events.addAll((List<Event>) invocation.getArguments()[0]);            return null;        }    }).when(channelProcessor).processEventBatch(any(List.class));    return channelProcessor;}
public Void flume_f6115_0(InvocationOnMock invocation) throws Throwable
{    events.addAll((List<Event>) invocation.getArguments()[0]);    return null;}
 ChannelProcessor flume_f6116_0()
{    ChannelProcessor channelProcessor = mock(ChannelProcessor.class);    doAnswer(new Answer<Void>() {        @Override        public Void answer(InvocationOnMock invocation) throws Throwable {            throw new ChannelException("channel intentional broken");        }    }).when(channelProcessor).processEventBatch(any(List.class));    return channelProcessor;}
public Void flume_f6117_0(InvocationOnMock invocation) throws Throwable
{    throw new ChannelException("channel intentional broken");}
public String flume_f6118_0()
{    String newTopic = null;    boolean topicFound = false;    while (!topicFound) {        newTopic = RandomStringUtils.randomAlphabetic(8);        if (!usedTopics.contains(newTopic)) {            usedTopics.add(newTopic);            topicFound = true;        }    }    return newTopic;}
private Properties flume_f6119_0(String bootStrapServers)
{    Properties props = new Properties();    props.put(ProducerConfig.ACKS_CONFIG, "-1");    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.ByteArraySerializer");    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootStrapServers);    return props;}
public static _Fields flume_f6120_0(int fieldId)
{    switch(fieldId) {        case         1:            return CATEGORY;        case         2:            return MESSAGE;        default:            return null;    }}
public static _Fields flume_f6121_0(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
public static _Fields flume_f6122_0(String name)
{    return byName.get(name);}
public short flume_f6123_0()
{    return _thriftId;}
public String flume_f6124_0()
{    return _fieldName;}
public LogEntry flume_f6125_0()
{    return new LogEntry(this);}
public void flume_f6126_0()
{    this.category = null;    this.message = null;}
public String flume_f6127_0()
{    return this.category;}
public LogEntry flume_f6128_0(String category)
{    this.category = category;    return this;}
public void flume_f6129_0()
{    this.category = null;}
public boolean flume_f6130_0()
{    return this.category != null;}
public void flume_f6131_0(boolean value)
{    if (!value) {        this.category = null;    }}
public String flume_f6132_0()
{    return this.message;}
public LogEntry flume_f6133_0(String message)
{    this.message = message;    return this;}
public void flume_f6134_0()
{    this.message = null;}
public boolean flume_f6135_0()
{    return this.message != null;}
public void flume_f6136_0(boolean value)
{    if (!value) {        this.message = null;    }}
public void flume_f6137_0(_Fields field, Object value)
{    switch(field) {        case CATEGORY:            if (value == null) {                unsetCategory();            } else {                setCategory((String) value);            }            break;        case MESSAGE:            if (value == null) {                unsetMessage();            } else {                setMessage((String) value);            }            break;    }}
public Object flume_f6138_0(_Fields field)
{    switch(field) {        case CATEGORY:            return getCategory();        case MESSAGE:            return getMessage();    }    throw new IllegalStateException();}
public boolean flume_f6139_0(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case CATEGORY:            return isSetCategory();        case MESSAGE:            return isSetMessage();    }    throw new IllegalStateException();}
public boolean flume_f6140_0(Object that)
{    if (that == null)        return false;    if (that instanceof LogEntry)        return this.equals((LogEntry) that);    return false;}
public boolean flume_f6141_0(LogEntry that)
{    if (that == null)        return false;    boolean this_present_category = true && this.isSetCategory();    boolean that_present_category = true && that.isSetCategory();    if (this_present_category || that_present_category) {        if (!(this_present_category && that_present_category))            return false;        if (!this.category.equals(that.category))            return false;    }    boolean this_present_message = true && this.isSetMessage();    boolean that_present_message = true && that.isSetMessage();    if (this_present_message || that_present_message) {        if (!(this_present_message && that_present_message))            return false;        if (!this.message.equals(that.message))            return false;    }    return true;}
public int flume_f6142_0()
{    List<Object> list = new ArrayList<Object>();    boolean present_category = true && (isSetCategory());    list.add(present_category);    if (present_category)        list.add(category);    boolean present_message = true && (isSetMessage());    list.add(present_message);    if (present_message)        list.add(message);    return list.hashCode();}
public int flume_f6143_0(LogEntry other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetCategory()).compareTo(other.isSetCategory());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetCategory()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.category, other.category);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetMessage()).compareTo(other.isSetMessage());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetMessage()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.message, other.message);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
public _Fields flume_f6144_0(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
public void flume_f6145_0(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
public void flume_f6146_0(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
public String flume_f6147_0()
{    StringBuilder sb = new StringBuilder("LogEntry(");    boolean first = true;    sb.append("category:");    if (this.category == null) {        sb.append("null");    } else {        sb.append(this.category);    }    first = false;    if (!first)        sb.append(", ");    sb.append("message:");    if (this.message == null) {        sb.append("null");    } else {        sb.append(this.message);    }    first = false;    sb.append(")");    return sb.toString();}
public void flume_f6148_0() throws org.apache.thrift.TException
{}
private void flume_f6149_0(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
private void flume_f6150_0(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
public LogEntryStandardScheme flume_f6151_0()
{    return new LogEntryStandardScheme();}
public void flume_f6152_0(org.apache.thrift.protocol.TProtocol iprot, LogEntry struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {                    struct.category = iprot.readString();                    struct.setCategoryIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             2:                if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {                    struct.message = iprot.readString();                    struct.setMessageIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
public void flume_f6153_0(org.apache.thrift.protocol.TProtocol oprot, LogEntry struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.category != null) {        oprot.writeFieldBegin(CATEGORY_FIELD_DESC);        oprot.writeString(struct.category);        oprot.writeFieldEnd();    }    if (struct.message != null) {        oprot.writeFieldBegin(MESSAGE_FIELD_DESC);        oprot.writeString(struct.message);        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
public LogEntryTupleScheme flume_f6154_0()
{    return new LogEntryTupleScheme();}
public void flume_f6155_0(org.apache.thrift.protocol.TProtocol prot, LogEntry struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetCategory()) {        optionals.set(0);    }    if (struct.isSetMessage()) {        optionals.set(1);    }    oprot.writeBitSet(optionals, 2);    if (struct.isSetCategory()) {        oprot.writeString(struct.category);    }    if (struct.isSetMessage()) {        oprot.writeString(struct.message);    }}
public void flume_f6156_0(org.apache.thrift.protocol.TProtocol prot, LogEntry struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(2);    if (incoming.get(0)) {        struct.category = iprot.readString();        struct.setCategoryIsSet(true);    }    if (incoming.get(1)) {        struct.message = iprot.readString();        struct.setMessageIsSet(true);    }}
public int flume_f6157_0()
{    return value;}
public static ResultCode flume_f6158_0(int value)
{    switch(value) {        case 0:            return OK;        case 1:            return TRY_LATER;        default:            return null;    }}
public Client flume_f6159_0(org.apache.thrift.protocol.TProtocol prot)
{    return new Client(prot);}
public Client flume_f6160_0(org.apache.thrift.protocol.TProtocol iprot, org.apache.thrift.protocol.TProtocol oprot)
{    return new Client(iprot, oprot);}
public ResultCode flume_f6161_0(List<LogEntry> messages) throws org.apache.thrift.TException
{    send_Log(messages);    return recv_Log();}
public void flume_f6162_0(List<LogEntry> messages) throws org.apache.thrift.TException
{    Log_args args = new Log_args();    args.setMessages(messages);    sendBase("Log", args);}
public ResultCode flume_f6163_0() throws org.apache.thrift.TException
{    Log_result result = new Log_result();    receiveBase(result, "Log");    if (result.isSetSuccess()) {        return result.success;    }    throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.MISSING_RESULT, "Log failed: unknown result");}
public AsyncClient flume_f6164_0(org.apache.thrift.transport.TNonblockingTransport transport)
{    return new AsyncClient(protocolFactory, clientManager, transport);}
public void flume_f6165_0(List<LogEntry> messages, org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException
{    checkReady();    Log_call method_call = new Log_call(messages, resultHandler, this, ___protocolFactory, ___transport);    this.___currentMethod = method_call;    ___manager.call(method_call);}
public void flume_f6166_0(org.apache.thrift.protocol.TProtocol prot) throws org.apache.thrift.TException
{    prot.writeMessageBegin(new org.apache.thrift.protocol.TMessage("Log", org.apache.thrift.protocol.TMessageType.CALL, 0));    Log_args args = new Log_args();    args.setMessages(messages);    args.write(prot);    prot.writeMessageEnd();}
public ResultCode flume_f6167_0() throws org.apache.thrift.TException
{    if (getState() != org.apache.thrift.async.TAsyncMethodCall.State.RESPONSE_READ) {        throw new IllegalStateException("Method call not finished!");    }    org.apache.thrift.transport.TMemoryInputTransport memoryTransport = new org.apache.thrift.transport.TMemoryInputTransport(getFrameBuffer().array());    org.apache.thrift.protocol.TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);    return (new Client(prot)).recv_Log();}
private static Map<String, org.apache.thrift.ProcessFunction<I, ? extends org.apache.thrift.TBase>> flume_f6168_0(Map<String, org.apache.thrift.ProcessFunction<I, ? extends org.apache.thrift.TBase>> processMap)
{    processMap.put("Log", new Log());    return processMap;}
public Log_args flume_f6169_0()
{    return new Log_args();}
protected boolean flume_f6170_0()
{    return false;}
public Log_result flume_f6171_0(I iface, Log_args args) throws org.apache.thrift.TException
{    Log_result result = new Log_result();    result.success = iface.Log(args.messages);    return result;}
private static Map<String, org.apache.thrift.AsyncProcessFunction<I, ? extends org.apache.thrift.TBase, ?>> flume_f6172_0(Map<String, org.apache.thrift.AsyncProcessFunction<I, ? extends org.apache.thrift.TBase, ?>> processMap)
{    processMap.put("Log", new Log());    return processMap;}
public Log_args flume_f6173_0()
{    return new Log_args();}
public AsyncMethodCallback<ResultCode> flume_f6174_1(final AsyncFrameBuffer fb, final int seqid)
{    final org.apache.thrift.AsyncProcessFunction fcall = this;    return new AsyncMethodCallback<ResultCode>() {        public void onComplete(ResultCode o) {            Log_result result = new Log_result();            result.success = o;            try {                fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);                return;            } catch (Exception e) {                            }            fb.close();        }        public void onError(Exception e) {            byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;            org.apache.thrift.TBase msg;            Log_result result = new Log_result();            {                msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;                msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());            }            try {                fcall.sendResponse(fb, msg, msgType, seqid);                return;            } catch (Exception ex) {                            }            fb.close();        }    };}
public void flume_f6175_1(ResultCode o)
{    Log_result result = new Log_result();    result.success = o;    try {        fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);        return;    } catch (Exception e) {            }    fb.close();}
public void flume_f6176_1(Exception e)
{    byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;    org.apache.thrift.TBase msg;    Log_result result = new Log_result();    {        msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;        msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());    }    try {        fcall.sendResponse(fb, msg, msgType, seqid);        return;    } catch (Exception ex) {            }    fb.close();}
protected boolean flume_f6177_0()
{    return false;}
public void flume_f6178_0(I iface, Log_args args, org.apache.thrift.async.AsyncMethodCallback<ResultCode> resultHandler) throws TException
{    iface.Log(args.messages, resultHandler);}
public static _Fields flume_f6179_0(int fieldId)
{    switch(fieldId) {        case         1:            return MESSAGES;        default:            return null;    }}
public static _Fields flume_f6180_0(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
public static _Fields flume_f6181_0(String name)
{    return byName.get(name);}
public short flume_f6182_0()
{    return _thriftId;}
public String flume_f6183_0()
{    return _fieldName;}
public Log_args flume_f6184_0()
{    return new Log_args(this);}
public void flume_f6185_0()
{    this.messages = null;}
public int flume_f6186_0()
{    return (this.messages == null) ? 0 : this.messages.size();}
public java.util.Iterator<LogEntry> flume_f6187_0()
{    return (this.messages == null) ? null : this.messages.iterator();}
public void flume_f6188_0(LogEntry elem)
{    if (this.messages == null) {        this.messages = new ArrayList<LogEntry>();    }    this.messages.add(elem);}
public List<LogEntry> flume_f6189_0()
{    return this.messages;}
public Log_args flume_f6190_0(List<LogEntry> messages)
{    this.messages = messages;    return this;}
public void flume_f6191_0()
{    this.messages = null;}
public boolean flume_f6192_0()
{    return this.messages != null;}
public void flume_f6193_0(boolean value)
{    if (!value) {        this.messages = null;    }}
public void flume_f6194_0(_Fields field, Object value)
{    switch(field) {        case MESSAGES:            if (value == null) {                unsetMessages();            } else {                setMessages((List<LogEntry>) value);            }            break;    }}
public Object flume_f6195_0(_Fields field)
{    switch(field) {        case MESSAGES:            return getMessages();    }    throw new IllegalStateException();}
public boolean flume_f6196_0(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case MESSAGES:            return isSetMessages();    }    throw new IllegalStateException();}
public boolean flume_f6197_0(Object that)
{    if (that == null)        return false;    if (that instanceof Log_args)        return this.equals((Log_args) that);    return false;}
public boolean flume_f6198_0(Log_args that)
{    if (that == null)        return false;    boolean this_present_messages = true && this.isSetMessages();    boolean that_present_messages = true && that.isSetMessages();    if (this_present_messages || that_present_messages) {        if (!(this_present_messages && that_present_messages))            return false;        if (!this.messages.equals(that.messages))            return false;    }    return true;}
public int flume_f6199_0()
{    List<Object> list = new ArrayList<Object>();    boolean present_messages = true && (isSetMessages());    list.add(present_messages);    if (present_messages)        list.add(messages);    return list.hashCode();}
public int flume_f6200_0(Log_args other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetMessages()).compareTo(other.isSetMessages());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetMessages()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.messages, other.messages);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
public _Fields flume_f6201_0(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
public void flume_f6202_0(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
public void flume_f6203_0(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
public String flume_f6204_0()
{    StringBuilder sb = new StringBuilder("Log_args(");    boolean first = true;    sb.append("messages:");    if (this.messages == null) {        sb.append("null");    } else {        sb.append(this.messages);    }    first = false;    sb.append(")");    return sb.toString();}
public void flume_f6205_0() throws org.apache.thrift.TException
{}
private void flume_f6206_0(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
private void flume_f6207_0(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
public Log_argsStandardScheme flume_f6208_0()
{    return new Log_argsStandardScheme();}
public void flume_f6209_0(org.apache.thrift.protocol.TProtocol iprot, Log_args struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.thrift.protocol.TType.LIST) {                    {                        org.apache.thrift.protocol.TList _list0 = iprot.readListBegin();                        struct.messages = new ArrayList<LogEntry>(_list0.size);                        LogEntry _elem1;                        for (int _i2 = 0; _i2 < _list0.size; ++_i2) {                            _elem1 = new LogEntry();                            _elem1.read(iprot);                            struct.messages.add(_elem1);                        }                        iprot.readListEnd();                    }                    struct.setMessagesIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
public void flume_f6210_0(org.apache.thrift.protocol.TProtocol oprot, Log_args struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.messages != null) {        oprot.writeFieldBegin(MESSAGES_FIELD_DESC);        {            oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.messages.size()));            for (LogEntry _iter3 : struct.messages) {                _iter3.write(oprot);            }            oprot.writeListEnd();        }        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
public Log_argsTupleScheme flume_f6211_0()
{    return new Log_argsTupleScheme();}
public void flume_f6212_0(org.apache.thrift.protocol.TProtocol prot, Log_args struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetMessages()) {        optionals.set(0);    }    oprot.writeBitSet(optionals, 1);    if (struct.isSetMessages()) {        {            oprot.writeI32(struct.messages.size());            for (LogEntry _iter4 : struct.messages) {                _iter4.write(oprot);            }        }    }}
public void flume_f6213_0(org.apache.thrift.protocol.TProtocol prot, Log_args struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(1);    if (incoming.get(0)) {        {            org.apache.thrift.protocol.TList _list5 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());            struct.messages = new ArrayList<LogEntry>(_list5.size);            LogEntry _elem6;            for (int _i7 = 0; _i7 < _list5.size; ++_i7) {                _elem6 = new LogEntry();                _elem6.read(iprot);                struct.messages.add(_elem6);            }        }        struct.setMessagesIsSet(true);    }}
public static _Fields flume_f6214_0(int fieldId)
{    switch(fieldId) {        case         0:            return SUCCESS;        default:            return null;    }}
public static _Fields flume_f6215_0(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
public static _Fields flume_f6216_0(String name)
{    return byName.get(name);}
public short flume_f6217_0()
{    return _thriftId;}
public String flume_f6218_0()
{    return _fieldName;}
public Log_result flume_f6219_0()
{    return new Log_result(this);}
public void flume_f6220_0()
{    this.success = null;}
public ResultCode flume_f6221_0()
{    return this.success;}
public Log_result flume_f6222_0(ResultCode success)
{    this.success = success;    return this;}
public void flume_f6223_0()
{    this.success = null;}
public boolean flume_f6224_0()
{    return this.success != null;}
public void flume_f6225_0(boolean value)
{    if (!value) {        this.success = null;    }}
public void flume_f6226_0(_Fields field, Object value)
{    switch(field) {        case SUCCESS:            if (value == null) {                unsetSuccess();            } else {                setSuccess((ResultCode) value);            }            break;    }}
public Object flume_f6227_0(_Fields field)
{    switch(field) {        case SUCCESS:            return getSuccess();    }    throw new IllegalStateException();}
public boolean flume_f6228_0(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case SUCCESS:            return isSetSuccess();    }    throw new IllegalStateException();}
public boolean flume_f6229_0(Object that)
{    if (that == null)        return false;    if (that instanceof Log_result)        return this.equals((Log_result) that);    return false;}
public boolean flume_f6230_0(Log_result that)
{    if (that == null)        return false;    boolean this_present_success = true && this.isSetSuccess();    boolean that_present_success = true && that.isSetSuccess();    if (this_present_success || that_present_success) {        if (!(this_present_success && that_present_success))            return false;        if (!this.success.equals(that.success))            return false;    }    return true;}
public int flume_f6231_0()
{    List<Object> list = new ArrayList<Object>();    boolean present_success = true && (isSetSuccess());    list.add(present_success);    if (present_success)        list.add(success.getValue());    return list.hashCode();}
public int flume_f6232_0(Log_result other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(other.isSetSuccess());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetSuccess()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.success, other.success);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
public _Fields flume_f6233_0(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
public void flume_f6234_0(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
public void flume_f6235_0(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
public String flume_f6236_0()
{    StringBuilder sb = new StringBuilder("Log_result(");    boolean first = true;    sb.append("success:");    if (this.success == null) {        sb.append("null");    } else {        sb.append(this.success);    }    first = false;    sb.append(")");    return sb.toString();}
public void flume_f6237_0() throws org.apache.thrift.TException
{}
private void flume_f6238_0(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
private void flume_f6239_0(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
public Log_resultStandardScheme flume_f6240_0()
{    return new Log_resultStandardScheme();}
public void flume_f6241_0(org.apache.thrift.protocol.TProtocol iprot, Log_result struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             0:                if (schemeField.type == org.apache.thrift.protocol.TType.I32) {                    struct.success = org.apache.flume.source.scribe.ResultCode.findByValue(iprot.readI32());                    struct.setSuccessIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
public void flume_f6242_0(org.apache.thrift.protocol.TProtocol oprot, Log_result struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.success != null) {        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);        oprot.writeI32(struct.success.getValue());        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
public Log_resultTupleScheme flume_f6243_0()
{    return new Log_resultTupleScheme();}
public void flume_f6244_0(org.apache.thrift.protocol.TProtocol prot, Log_result struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetSuccess()) {        optionals.set(0);    }    oprot.writeBitSet(optionals, 1);    if (struct.isSetSuccess()) {        oprot.writeI32(struct.success.getValue());    }}
public void flume_f6245_0(org.apache.thrift.protocol.TProtocol prot, Log_result struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(1);    if (incoming.get(0)) {        struct.success = org.apache.flume.source.scribe.ResultCode.findByValue(iprot.readI32());        struct.setSuccessIsSet(true);    }}
public void flume_f6246_0(Context context)
{    port = context.getInteger("port", DEFAULT_PORT);    maxReadBufferBytes = context.getInteger("maxReadBufferBytes", DEFAULT_MAX_READ_BUFFER_BYTES);    if (maxReadBufferBytes <= 0) {        maxReadBufferBytes = DEFAULT_MAX_READ_BUFFER_BYTES;    }    workers = context.getInteger("workerThreads", DEFAULT_WORKERS);    if (workers <= 0) {        workers = DEFAULT_WORKERS;    }    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
public void flume_f6247_1()
{    try {        Scribe.Processor processor = new Scribe.Processor(new Receiver());        TNonblockingServerTransport transport = new TNonblockingServerSocket(port);        THsHaServer.Args args = new THsHaServer.Args(transport);        args.minWorkerThreads(workers);        args.maxWorkerThreads(workers);        args.processor(processor);        args.transportFactory(new TFramedTransport.Factory(maxReadBufferBytes));        args.protocolFactory(new TBinaryProtocol.Factory(false, false));        args.maxReadBufferBytes = maxReadBufferBytes;        server = new THsHaServer(args);                server.serve();    } catch (Exception e) {            }}
public void flume_f6248_0()
{    Startup startupThread = new Startup();    startupThread.start();    try {        Thread.sleep(3000);    } catch (InterruptedException e) {    }    if (!server.isServing()) {        throw new IllegalStateException("Failed initialization of ScribeSource");    }    sourceCounter.start();    super.start();}
public void flume_f6249_1()
{        if (server != null) {        server.stop();    }    sourceCounter.stop();    super.stop();    }
public ResultCode flume_f6250_1(List<LogEntry> list) throws TException
{    if (list != null) {        sourceCounter.addToEventReceivedCount(list.size());        try {            List<Event> events = new ArrayList<Event>(list.size());            for (LogEntry entry : list) {                Map<String, String> headers = new HashMap<String, String>(1, 1);                String category = entry.getCategory();                if (category != null) {                    headers.put(SCRIBE_CATEGORY, category);                }                Event event = EventBuilder.withBody(entry.getMessage().getBytes(), headers);                events.add(event);            }            if (events.size() > 0) {                getChannelProcessor().processEventBatch(events);            }            sourceCounter.addToEventAcceptedCount(list.size());            return ResultCode.OK;        } catch (Exception e) {                        sourceCounter.incrementEventReadOrChannelFail(e);        }    }    return ResultCode.TRY_LATER;}
private static int flume_f6251_0() throws IOException
{    ServerSocket socket = new ServerSocket(0);    int port = socket.getLocalPort();    socket.close();    return port;}
public static void flume_f6252_0() throws Exception
{    port = findFreePort();    Context context = new Context();    context.put("port", String.valueOf(port));    scribeSource = new ScribeSource();    scribeSource.setName("Scribe Source");    Configurables.configure(scribeSource, context);    memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    List<Channel> channels = new ArrayList<Channel>(1);    channels.add(memoryChannel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    memoryChannel.start();    scribeSource.setChannelProcessor(new ChannelProcessor(rcs));    scribeSource.start();}
private void flume_f6253_0() throws org.apache.thrift.TException
{    TTransport transport = new TFramedTransport(new TSocket("localhost", port));    TProtocol protocol = new TBinaryProtocol(transport);    Scribe.Client client = new Scribe.Client(protocol);    transport.open();    LogEntry logEntry = new LogEntry("INFO", "Sending info msg to scribe source");    List<LogEntry> logEntries = new ArrayList<LogEntry>(1);    logEntries.add(logEntry);    client.Log(logEntries);}
public void flume_f6254_0() throws Exception
{    sendSingle();        Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event e = memoryChannel.take();    Assert.assertNotNull(e);    Assert.assertEquals("Sending info msg to scribe source", new String(e.getBody()));    tx.commit();    tx.close();}
public void flume_f6255_0() throws Exception
{    TTransport transport = new TFramedTransport(new TSocket("localhost", port));    TProtocol protocol = new TBinaryProtocol(transport);    Scribe.Client client = new Scribe.Client(protocol);    transport.open();    List<LogEntry> logEntries = new ArrayList<LogEntry>(10);    for (int i = 0; i < 10; i++) {        LogEntry logEntry = new LogEntry("INFO", String.format("Sending info msg# %d to scribe source", i));        logEntries.add(logEntry);    }    client.Log(logEntries);        Transaction tx = memoryChannel.getTransaction();    tx.begin();    for (int i = 0; i < 10; i++) {        Event e = memoryChannel.take();        Assert.assertNotNull(e);        Assert.assertEquals(String.format("Sending info msg# %d to scribe source", i), new String(e.getBody()));    }    tx.commit();    tx.close();}
public void flume_f6256_0() throws Exception
{    ChannelProcessor cp = mock(ChannelProcessor.class);    doThrow(new ChannelException("dummy")).when(cp).processEventBatch(anyListOf(Event.class));    ChannelProcessor origCp = scribeSource.getChannelProcessor();    scribeSource.setChannelProcessor(cp);    sendSingle();    scribeSource.setChannelProcessor(origCp);    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(scribeSource, "sourceCounter");    org.junit.Assert.assertEquals(1, sc.getChannelWriteFail());}
public static void flume_f6257_0()
{    memoryChannel.stop();    scribeSource.stop();}
public void flume_f6258_1(String filePath)
{    Long inode, pos;    String path;    FileReader fr = null;    JsonReader jr = null;    try {        fr = new FileReader(filePath);        jr = new JsonReader(fr);        jr.beginArray();        while (jr.hasNext()) {            inode = null;            pos = null;            path = null;            jr.beginObject();            while (jr.hasNext()) {                switch(jr.nextName()) {                    case "inode":                        inode = jr.nextLong();                        break;                    case "pos":                        pos = jr.nextLong();                        break;                    case "file":                        path = jr.nextString();                        break;                }            }            jr.endObject();            for (Object v : Arrays.asList(inode, pos, path)) {                Preconditions.checkNotNull(v, "Detected missing value in position file. " + "inode: " + inode + ", pos: " + pos + ", path: " + path);            }            TailFile tf = tailFiles.get(inode);            if (tf != null && tf.updatePos(path, inode, pos)) {                tailFiles.put(inode, tf);            } else {                            }        }        jr.endArray();    } catch (FileNotFoundException e) {            } catch (IOException e) {            } finally {        try {            if (fr != null)                fr.close();            if (jr != null)                jr.close();        } catch (IOException e) {                    }    }}
public Map<Long, TailFile> flume_f6259_0()
{    return tailFiles;}
public void flume_f6260_0(TailFile currentFile)
{    this.currentFile = currentFile;}
public Event flume_f6261_0() throws IOException
{    List<Event> events = readEvents(1);    if (events.isEmpty()) {        return null;    }    return events.get(0);}
public List<Event> flume_f6262_0(int numEvents) throws IOException
{    return readEvents(numEvents, false);}
public List<Event> flume_f6263_0(TailFile tf, int numEvents) throws IOException
{    setCurrentFile(tf);    return readEvents(numEvents, true);}
public List<Event> flume_f6264_1(int numEvents, boolean backoffWithoutNL) throws IOException
{    if (!committed) {        if (currentFile == null) {            throw new IllegalStateException("current file does not exist. " + currentFile.getPath());        }                long lastPos = currentFile.getPos();        currentFile.updateFilePos(lastPos);    }    List<Event> events = currentFile.readEvents(numEvents, backoffWithoutNL, addByteOffset);    if (events.isEmpty()) {        return events;    }    Map<String, String> headers = currentFile.getHeaders();    if (annotateFileName || (headers != null && !headers.isEmpty())) {        for (Event event : events) {            if (headers != null && !headers.isEmpty()) {                event.getHeaders().putAll(headers);            }            if (annotateFileName) {                event.getHeaders().put(fileNameHeader, currentFile.getPath());            }        }    }    committed = false;    return events;}
public void flume_f6265_0() throws IOException
{    for (TailFile tf : tailFiles.values()) {        if (tf.getRaf() != null)            tf.getRaf().close();    }}
public void flume_f6266_0() throws IOException
{    if (!committed && currentFile != null) {        long pos = currentFile.getLineReadPos();        currentFile.setPos(pos);        currentFile.setLastUpdated(updateTime);        committed = true;    }}
public List<Long> flume_f6267_1(boolean skipToEnd) throws IOException
{    updateTime = System.currentTimeMillis();    List<Long> updatedInodes = Lists.newArrayList();    for (TaildirMatcher taildir : taildirCache) {        Map<String, String> headers = headerTable.row(taildir.getFileGroup());        for (File f : taildir.getMatchingFiles()) {            long inode;            try {                inode = getInode(f);            } catch (NoSuchFileException e) {                                continue;            }            TailFile tf = tailFiles.get(inode);            if (tf == null || !tf.getPath().equals(f.getAbsolutePath())) {                long startPos = skipToEnd ? f.length() : 0;                tf = openFile(f, headers, inode, startPos);            } else {                boolean updated = tf.getLastUpdated() < f.lastModified() || tf.getPos() != f.length();                if (updated) {                    if (tf.getRaf() == null) {                        tf = openFile(f, headers, inode, tf.getPos());                    }                    if (f.length() < tf.getPos()) {                                                tf.updatePos(tf.getPath(), inode, 0);                    }                }                tf.setNeedTail(updated);            }            tailFiles.put(inode, tf);            updatedInodes.add(inode);        }    }    return updatedInodes;}
public List<Long> flume_f6268_0() throws IOException
{    return updateTailFiles(false);}
private long flume_f6269_0(File file) throws IOException
{    long inode = (long) Files.getAttribute(file.toPath(), "unix:ino");    return inode;}
private TailFile flume_f6270_1(File file, Map<String, String> headers, long inode, long pos)
{    try {                return new TailFile(file, headers, inode, pos);    } catch (IOException e) {        throw new FlumeException("Failed opening file: " + file, e);    }}
public Builder flume_f6271_0(Map<String, String> filePaths)
{    this.filePaths = filePaths;    return this;}
public Builder flume_f6272_0(Table<String, String, String> headerTable)
{    this.headerTable = headerTable;    return this;}
public Builder flume_f6273_0(String positionFilePath)
{    this.positionFilePath = positionFilePath;    return this;}
public Builder flume_f6274_0(boolean skipToEnd)
{    this.skipToEnd = skipToEnd;    return this;}
public Builder flume_f6275_0(boolean addByteOffset)
{    this.addByteOffset = addByteOffset;    return this;}
public Builder flume_f6276_0(boolean cachePatternMatching)
{    this.cachePatternMatching = cachePatternMatching;    return this;}
public Builder flume_f6277_0(boolean annotateFileName)
{    this.annotateFileName = annotateFileName;    return this;}
public Builder flume_f6278_0(String fileNameHeader)
{    this.fileNameHeader = fileNameHeader;    return this;}
public ReliableTaildirEventReader flume_f6279_0() throws IOException
{    return new ReliableTaildirEventReader(filePaths, headerTable, positionFilePath, skipToEnd, addByteOffset, cachePatternMatching, annotateFileName, fileNameHeader);}
public boolean flume_f6280_0(Path entry) throws IOException
{    return matcher.matches(entry.getFileName()) && !Files.isDirectory(entry);}
 List<File> flume_f6281_0()
{    long now = TimeUnit.SECONDS.toMillis(TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()));    long currentParentDirMTime = parentDir.lastModified();    List<File> result;        if (!cachePatternMatching || lastSeenParentDirMTime < currentParentDirMTime || !(currentParentDirMTime < lastCheckedTime)) {        lastMatchedFiles = sortByLastModifiedTime(getMatchingFilesNoCache());        lastSeenParentDirMTime = currentParentDirMTime;        lastCheckedTime = now;    }    return lastMatchedFiles;}
private List<File> flume_f6282_1()
{    List<File> result = Lists.newArrayList();    try (DirectoryStream<Path> stream = Files.newDirectoryStream(parentDir.toPath(), fileFilter)) {        for (Path entry : stream) {            result.add(entry.toFile());        }    } catch (IOException e) {            }    return result;}
private static List<File> flume_f6283_0(List<File> files)
{    final HashMap<File, Long> lastModificationTimes = new HashMap<File, Long>(files.size());    for (File f : files) {        lastModificationTimes.put(f, f.lastModified());    }    Collections.sort(files, new Comparator<File>() {        @Override        public int compare(File o1, File o2) {            return lastModificationTimes.get(o1).compareTo(lastModificationTimes.get(o2));        }    });    return files;}
public int flume_f6284_0(File o1, File o2)
{    return lastModificationTimes.get(o1).compareTo(lastModificationTimes.get(o2));}
public String flume_f6285_0()
{    return "{" + "filegroup='" + fileGroup + '\'' + ", filePattern='" + filePattern + '\'' + ", cached=" + cachePatternMatching + '}';}
public boolean flume_f6286_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    TaildirMatcher that = (TaildirMatcher) o;    return fileGroup.equals(that.fileGroup);}
public int flume_f6287_0()
{    return fileGroup.hashCode();}
public String flume_f6288_0()
{    return fileGroup;}
public synchronized void flume_f6289_1()
{        try {        reader = new ReliableTaildirEventReader.Builder().filePaths(filePaths).headerTable(headerTable).positionFilePath(positionFilePath).skipToEnd(skipToEnd).addByteOffset(byteOffsetHeader).cachePatternMatching(cachePatternMatching).annotateFileName(fileHeader).fileNameHeader(fileHeaderKey).build();    } catch (IOException e) {        throw new FlumeException("Error instantiating ReliableTaildirEventReader", e);    }    idleFileChecker = Executors.newSingleThreadScheduledExecutor(new ThreadFactoryBuilder().setNameFormat("idleFileChecker").build());    idleFileChecker.scheduleWithFixedDelay(new idleFileCheckerRunnable(), idleTimeout, checkIdleInterval, TimeUnit.MILLISECONDS);    positionWriter = Executors.newSingleThreadScheduledExecutor(new ThreadFactoryBuilder().setNameFormat("positionWriter").build());    positionWriter.scheduleWithFixedDelay(new PositionWriterRunnable(), writePosInitDelay, writePosInterval, TimeUnit.MILLISECONDS);    super.start();        sourceCounter.start();}
public synchronized void flume_f6290_1()
{    try {        super.stop();        ExecutorService[] services = { idleFileChecker, positionWriter };        for (ExecutorService service : services) {            service.shutdown();            if (!service.awaitTermination(1, TimeUnit.SECONDS)) {                service.shutdownNow();            }        }                writePosition();        reader.close();    } catch (InterruptedException e) {            } catch (IOException e) {            }    sourceCounter.stop();    }
public String flume_f6291_0()
{    return String.format("Taildir source: { positionFile: %s, skipToEnd: %s, " + "byteOffsetHeader: %s, idleTimeout: %s, writePosInterval: %s }", positionFilePath, skipToEnd, byteOffsetHeader, idleTimeout, writePosInterval);}
public synchronized void flume_f6292_1(Context context)
{    String fileGroups = context.getString(FILE_GROUPS);    Preconditions.checkState(fileGroups != null, "Missing param: " + FILE_GROUPS);    filePaths = selectByKeys(context.getSubProperties(FILE_GROUPS_PREFIX), fileGroups.split("\\s+"));    Preconditions.checkState(!filePaths.isEmpty(), "Mapping for tailing files is empty or invalid: '" + FILE_GROUPS_PREFIX + "'");    String homePath = System.getProperty("user.home").replace('\\', '/');    positionFilePath = context.getString(POSITION_FILE, homePath + DEFAULT_POSITION_FILE);    Path positionFile = Paths.get(positionFilePath);    try {        Files.createDirectories(positionFile.getParent());    } catch (IOException e) {        throw new FlumeException("Error creating positionFile parent directories", e);    }    headerTable = getTable(context, HEADERS_PREFIX);    batchSize = context.getInteger(BATCH_SIZE, DEFAULT_BATCH_SIZE);    skipToEnd = context.getBoolean(SKIP_TO_END, DEFAULT_SKIP_TO_END);    byteOffsetHeader = context.getBoolean(BYTE_OFFSET_HEADER, DEFAULT_BYTE_OFFSET_HEADER);    idleTimeout = context.getInteger(IDLE_TIMEOUT, DEFAULT_IDLE_TIMEOUT);    writePosInterval = context.getInteger(WRITE_POS_INTERVAL, DEFAULT_WRITE_POS_INTERVAL);    cachePatternMatching = context.getBoolean(CACHE_PATTERN_MATCHING, DEFAULT_CACHE_PATTERN_MATCHING);    backoffSleepIncrement = context.getLong(PollableSourceConstants.BACKOFF_SLEEP_INCREMENT, PollableSourceConstants.DEFAULT_BACKOFF_SLEEP_INCREMENT);    maxBackOffSleepInterval = context.getLong(PollableSourceConstants.MAX_BACKOFF_SLEEP, PollableSourceConstants.DEFAULT_MAX_BACKOFF_SLEEP);    fileHeader = context.getBoolean(FILENAME_HEADER, DEFAULT_FILE_HEADER);    fileHeaderKey = context.getString(FILENAME_HEADER_KEY, DEFAULT_FILENAME_HEADER_KEY);    maxBatchCount = context.getLong(MAX_BATCH_COUNT, DEFAULT_MAX_BATCH_COUNT);    if (maxBatchCount <= 0) {        maxBatchCount = DEFAULT_MAX_BATCH_COUNT;            }    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
public long flume_f6293_0()
{    return batchSize;}
private Map<String, String> flume_f6294_0(Map<String, String> map, String[] keys)
{    Map<String, String> result = Maps.newHashMap();    for (String key : keys) {        if (map.containsKey(key)) {            result.put(key, map.get(key));        }    }    return result;}
private Table<String, String, String> flume_f6295_0(Context context, String prefix)
{    Table<String, String, String> table = HashBasedTable.create();    for (Entry<String, String> e : context.getSubProperties(prefix).entrySet()) {        String[] parts = e.getKey().split("\\.", 2);        table.put(parts[0], parts[1], e.getValue());    }    return table;}
protected SourceCounter flume_f6296_0()
{    return sourceCounter;}
public Status flume_f6297_1()
{    Status status = Status.BACKOFF;    try {        existingInodes.clear();        existingInodes.addAll(reader.updateTailFiles());        for (long inode : existingInodes) {            TailFile tf = reader.getTailFiles().get(inode);            if (tf.needTail()) {                boolean hasMoreLines = tailFileProcess(tf, true);                if (hasMoreLines) {                    status = Status.READY;                }            }        }        closeTailFiles();    } catch (Throwable t) {                sourceCounter.incrementEventReadFail();        status = Status.BACKOFF;    }    return status;}
public long flume_f6298_0()
{    return backoffSleepIncrement;}
public long flume_f6299_0()
{    return maxBackOffSleepInterval;}
private boolean flume_f6300_1(TailFile tf, boolean backoffWithoutNL) throws IOException, InterruptedException
{    long batchCount = 0;    while (true) {        reader.setCurrentFile(tf);        List<Event> events = reader.readEvents(batchSize, backoffWithoutNL);        if (events.isEmpty()) {            return false;        }        sourceCounter.addToEventReceivedCount(events.size());        sourceCounter.incrementAppendBatchReceivedCount();        try {            getChannelProcessor().processEventBatch(events);            reader.commit();        } catch (ChannelException ex) {                        sourceCounter.incrementChannelWriteFail();            TimeUnit.MILLISECONDS.sleep(retryInterval);            retryInterval = retryInterval << 1;            retryInterval = Math.min(retryInterval, maxRetryInterval);            continue;        }        retryInterval = 1000;        sourceCounter.addToEventAcceptedCount(events.size());        sourceCounter.incrementAppendBatchAcceptedCount();        if (events.size() < batchSize) {                        return false;        }        if (++batchCount >= maxBatchCount) {                        return true;        }    }}
private void flume_f6301_1() throws IOException, InterruptedException
{    for (long inode : idleInodes) {        TailFile tf = reader.getTailFiles().get(inode);        if (tf.getRaf() != null) {                        tailFileProcess(tf, false);            tf.close();                    }    }    idleInodes.clear();}
public void flume_f6302_1()
{    try {        long now = System.currentTimeMillis();        for (TailFile tf : reader.getTailFiles().values()) {            if (tf.getLastUpdated() + idleTimeout < now && tf.getRaf() != null) {                idleInodes.add(tf.getInode());            }        }    } catch (Throwable t) {                sourceCounter.incrementGenericProcessingFail();    }}
public void flume_f6303_0()
{    writePosition();}
private void flume_f6304_1()
{    File file = new File(positionFilePath);    FileWriter writer = null;    try {        writer = new FileWriter(file);        if (!existingInodes.isEmpty()) {            String json = toPosInfoJson();            writer.write(json);        }    } catch (Throwable t) {                sourceCounter.incrementGenericProcessingFail();    } finally {        try {            if (writer != null)                writer.close();        } catch (IOException e) {                        sourceCounter.incrementGenericProcessingFail();        }    }}
private String flume_f6305_0()
{    @SuppressWarnings("rawtypes")    List<Map> posInfos = Lists.newArrayList();    for (Long inode : existingInodes) {        TailFile tf = reader.getTailFiles().get(inode);        posInfos.add(ImmutableMap.of("inode", inode, "pos", tf.getPos(), "file", tf.getPath()));    }    return new Gson().toJson(posInfos);}
public RandomAccessFile flume_f6306_0()
{    return raf;}
public String flume_f6307_0()
{    return path;}
public long flume_f6308_0()
{    return inode;}
public long flume_f6309_0()
{    return pos;}
public long flume_f6310_0()
{    return lastUpdated;}
public boolean flume_f6311_0()
{    return needTail;}
public Map<String, String> flume_f6312_0()
{    return headers;}
public long flume_f6313_0()
{    return lineReadPos;}
public void flume_f6314_0(long pos)
{    this.pos = pos;}
public void flume_f6315_0(long lastUpdated)
{    this.lastUpdated = lastUpdated;}
public void flume_f6316_0(boolean needTail)
{    this.needTail = needTail;}
public void flume_f6317_0(long lineReadPos)
{    this.lineReadPos = lineReadPos;}
public boolean flume_f6318_1(String path, long inode, long pos) throws IOException
{    if (this.inode == inode && this.path.equals(path)) {        setPos(pos);        updateFilePos(pos);                return true;    }    return false;}
public void flume_f6319_0(long pos) throws IOException
{    raf.seek(pos);    lineReadPos = pos;    bufferPos = NEED_READING;    oldBuffer = new byte[0];}
public List<Event> flume_f6320_0(int numEvents, boolean backoffWithoutNL, boolean addByteOffset) throws IOException
{    List<Event> events = Lists.newLinkedList();    for (int i = 0; i < numEvents; i++) {        Event event = readEvent(backoffWithoutNL, addByteOffset);        if (event == null) {            break;        }        events.add(event);    }    return events;}
private Event flume_f6321_1(boolean backoffWithoutNL, boolean addByteOffset) throws IOException
{    Long posTmp = getLineReadPos();    LineResult line = readLine();    if (line == null) {        return null;    }    if (backoffWithoutNL && !line.lineSepInclude) {                updateFilePos(posTmp);        return null;    }    Event event = EventBuilder.withBody(line.line);    if (addByteOffset == true) {        event.getHeaders().put(BYTE_OFFSET_HEADER_KEY, posTmp.toString());    }    return event;}
private void flume_f6322_0() throws IOException
{    if ((raf.length() - raf.getFilePointer()) < BUFFER_SIZE) {        buffer = new byte[(int) (raf.length() - raf.getFilePointer())];    } else {        buffer = new byte[BUFFER_SIZE];    }    raf.read(buffer, 0, buffer.length);    bufferPos = 0;}
private byte[] flume_f6323_0(byte[] a, int startIdxA, int lenA, byte[] b, int startIdxB, int lenB)
{    byte[] c = new byte[lenA + lenB];    System.arraycopy(a, startIdxA, c, 0, lenA);    System.arraycopy(b, startIdxB, c, lenA, lenB);    return c;}
public LineResult flume_f6324_0() throws IOException
{    LineResult lineResult = null;    while (true) {        if (bufferPos == NEED_READING) {            if (raf.getFilePointer() < raf.length()) {                readFile();            } else {                if (oldBuffer.length > 0) {                    lineResult = new LineResult(false, oldBuffer);                    oldBuffer = new byte[0];                    setLineReadPos(lineReadPos + lineResult.line.length);                }                break;            }        }        for (int i = bufferPos; i < buffer.length; i++) {            if (buffer[i] == BYTE_NL) {                int oldLen = oldBuffer.length;                                int lineLen = i - bufferPos;                                if (i > 0 && buffer[i - 1] == BYTE_CR) {                    lineLen -= 1;                } else if (oldBuffer.length > 0 && oldBuffer[oldBuffer.length - 1] == BYTE_CR) {                    oldLen -= 1;                }                lineResult = new LineResult(true, concatByteArrays(oldBuffer, 0, oldLen, buffer, bufferPos, lineLen));                setLineReadPos(lineReadPos + (oldBuffer.length + (i - bufferPos + 1)));                oldBuffer = new byte[0];                if (i + 1 < buffer.length) {                    bufferPos = i + 1;                } else {                    bufferPos = NEED_READING;                }                break;            }        }        if (lineResult != null) {            break;        }                oldBuffer = concatByteArrays(oldBuffer, 0, oldBuffer.length, buffer, bufferPos, buffer.length - bufferPos);        bufferPos = NEED_READING;    }    return lineResult;}
public void flume_f6325_1()
{    try {        raf.close();        raf = null;        long now = System.currentTimeMillis();        setLastUpdated(now);    } catch (IOException e) {            }}
public static String flume_f6326_0(Event event)
{    return new String(event.getBody());}
 static List<String> flume_f6327_0(List<Event> events)
{    List<String> bodies = Lists.newArrayListWithCapacity(events.size());    for (Event event : events) {        bodies.add(new String(event.getBody()));    }    return bodies;}
 static List<String> flume_f6328_0(List<Event> events, String headerKey)
{    List<String> headers = Lists.newArrayListWithCapacity(events.size());    for (Event event : events) {        headers.add(new String(event.getHeaders().get(headerKey)));    }    return headers;}
private ReliableTaildirEventReader flume_f6329_0(Map<String, String> filePaths, Table<String, String, String> headerTable, boolean addByteOffset, boolean cachedPatternMatching)
{    ReliableTaildirEventReader reader;    try {        reader = new ReliableTaildirEventReader.Builder().filePaths(filePaths).headerTable(headerTable).positionFilePath(posFilePath).skipToEnd(false).addByteOffset(addByteOffset).cachePatternMatching(cachedPatternMatching).build();        reader.updateTailFiles();    } catch (IOException ioe) {        throw Throwables.propagate(ioe);    }    return reader;}
private ReliableTaildirEventReader flume_f6330_0(boolean addByteOffset, boolean cachedPatternMatching)
{    Map<String, String> filePaths = ImmutableMap.of("testFiles", tmpDir.getAbsolutePath() + "/file.*");    Table<String, String, String> headerTable = HashBasedTable.create();    return getReader(filePaths, headerTable, addByteOffset, cachedPatternMatching);}
private ReliableTaildirEventReader flume_f6331_0()
{    return getReader(false, false);}
public void flume_f6332_0()
{    tmpDir = Files.createTempDir();    posFilePath = tmpDir.getAbsolutePath() + "/taildir_position_test.json";}
public void flume_f6333_0()
{    for (File f : tmpDir.listFiles()) {        if (f.isDirectory()) {            for (File sdf : f.listFiles()) {                sdf.delete();            }        }        f.delete();    }    tmpDir.delete();}
public void flume_f6334_0() throws IOException
{    File f1 = new File(tmpDir, "file1");    File f2 = new File(tmpDir, "file2");    File f3 = new File(tmpDir, "file3");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);    Files.write("file3line1\nfile3line2\n", f3, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    List<String> out = Lists.newArrayList();    for (TailFile tf : reader.getTailFiles().values()) {        List<String> bodies = bodiesAsStrings(reader.readEvents(tf, 2));        out.addAll(bodies);        reader.commit();    }    assertEquals(6, out.size());        assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file2line1"));    assertTrue(out.contains("file2line2"));    assertTrue(out.contains("file3line1"));    assertTrue(out.contains("file3line2"));    Files.append("file3line3\nfile3line4\n", f3, Charsets.UTF_8);    reader.updateTailFiles();    for (TailFile tf : reader.getTailFiles().values()) {        List<String> bodies = bodiesAsStrings(reader.readEvents(tf, 2));        out.addAll(bodies);        reader.commit();    }    assertEquals(8, out.size());    assertTrue(out.contains("file3line3"));    assertTrue(out.contains("file3line4"));}
public void flume_f6335_0() throws IOException
{    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);            ReliableTaildirEventReader reader = getReader(false, true);    File dir = f1.getParentFile();    long lastModified = dir.lastModified();    f1.delete();        dir.setLastModified(lastModified - 1000);    reader.updateTailFiles();}
public void flume_f6336_0() throws IOException
{    ReliableTaildirEventReader reader = getReader();    List<Long> fileInodes = reader.updateTailFiles();    assertEquals(0, fileInodes.size());    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    reader.updateTailFiles();    List<String> out = null;    for (TailFile tf : reader.getTailFiles().values()) {        out = bodiesAsStrings(reader.readEvents(tf, 2));        reader.commit();    }    assertEquals(2, out.size());        assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    reader.updateTailFiles();    List<String> empty = null;    for (TailFile tf : reader.getTailFiles().values()) {        empty = bodiesAsStrings(reader.readEvents(tf, 15));        reader.commit();    }    assertEquals(0, empty.size());}
public void flume_f6337_0() throws IOException
{    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n" + "file1line9\nfile1line10\nfile1line11\nfile1line12\n", f1, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    List<String> out1 = null;    for (TailFile tf : reader.getTailFiles().values()) {        out1 = bodiesAsStrings(reader.readEvents(tf, 4));    }    assertTrue(out1.contains("file1line1"));    assertTrue(out1.contains("file1line2"));    assertTrue(out1.contains("file1line3"));    assertTrue(out1.contains("file1line4"));    List<String> out2 = bodiesAsStrings(reader.readEvents(4));    assertTrue(out2.contains("file1line1"));    assertTrue(out2.contains("file1line2"));    assertTrue(out2.contains("file1line3"));    assertTrue(out2.contains("file1line4"));    reader.commit();    List<String> out3 = bodiesAsStrings(reader.readEvents(4));    assertTrue(out3.contains("file1line5"));    assertTrue(out3.contains("file1line6"));    assertTrue(out3.contains("file1line7"));    assertTrue(out3.contains("file1line8"));    reader.commit();    List<String> out4 = bodiesAsStrings(reader.readEvents(4));    assertEquals(4, out4.size());    assertTrue(out4.contains("file1line9"));    assertTrue(out4.contains("file1line10"));    assertTrue(out4.contains("file1line11"));    assertTrue(out4.contains("file1line12"));}
public void flume_f6338_0() throws IOException
{    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    List<String> out1 = null;    for (TailFile tf : reader.getTailFiles().values()) {        out1 = bodiesAsStrings(reader.readEvents(tf, 5));    }    assertTrue(out1.contains("file1line1"));    assertTrue(out1.contains("file1line2"));    assertTrue(out1.contains("file1line3"));    assertTrue(out1.contains("file1line4"));    assertTrue(out1.contains("file1line5"));    List<String> out2 = bodiesAsStrings(reader.readEvents(2));    assertTrue(out2.contains("file1line1"));    assertTrue(out2.contains("file1line2"));    reader.commit();    List<String> out3 = bodiesAsStrings(reader.readEvents(2));    assertTrue(out3.contains("file1line3"));    assertTrue(out3.contains("file1line4"));    reader.commit();    List<String> out4 = bodiesAsStrings(reader.readEvents(15));    assertTrue(out4.contains("file1line5"));    assertTrue(out4.contains("file1line6"));    assertTrue(out4.contains("file1line7"));    assertTrue(out4.contains("file1line8"));}
public void flume_f6339_0() throws IOException
{    File f1 = new File(tmpDir, "file1");    File f2 = new File(tmpDir, "file2");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Files.touch(f2);    ReliableTaildirEventReader reader = getReader();        List<String> out = Lists.newArrayList();    for (TailFile tf : reader.getTailFiles().values()) {        out.addAll(bodiesAsStrings(reader.readEvents(tf, 5)));        reader.commit();    }    assertEquals(2, out.size());    assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertNull(reader.readEvent());}
public void flume_f6340_0() throws IOException
{    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1", f1, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    List<String> out = Lists.newArrayList();        for (TailFile tf : reader.getTailFiles().values()) {        out.addAll(bodiesAsStrings(reader.readEvents(tf, 5)));        reader.commit();    }    assertEquals(1, out.size());    assertTrue(out.contains("file1line1"));    Files.append("line2\nfile1line3\nfile1line4", f1, Charsets.UTF_8);    for (TailFile tf : reader.getTailFiles().values()) {        out.addAll(bodiesAsStrings(reader.readEvents(tf, 5)));        reader.commit();    }    assertEquals(3, out.size());    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file1line3"));        out.addAll(bodiesAsStrings(reader.readEvents(5, false)));    reader.commit();    assertEquals(4, out.size());    assertTrue(out.contains("file1line4"));}
public void flume_f6341_0() throws IOException
{    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    List<String> out1 = Lists.newArrayList();    for (TailFile tf : reader.getTailFiles().values()) {        out1.addAll(bodiesAsStrings(reader.readEvents(tf, 5)));        reader.commit();    }    File f2 = new File(tmpDir, "file2");    Files.write("file2line1\nfile2line2\nfile2line3\nfile2line4\n" + "file2line5\nfile2line6\nfile2line7\nfile2line8\n", f2, Charsets.UTF_8);    List<String> out2 = bodiesAsStrings(reader.readEvents(5));    reader.commit();    reader.updateTailFiles();    List<String> out3 = Lists.newArrayList();    for (TailFile tf : reader.getTailFiles().values()) {        out3.addAll(bodiesAsStrings(reader.readEvents(tf, 5)));        reader.commit();    }        assertEquals(5, out1.size());    assertTrue(out1.contains("file1line1"));    assertTrue(out1.contains("file1line2"));    assertTrue(out1.contains("file1line3"));    assertTrue(out1.contains("file1line4"));    assertTrue(out1.contains("file1line5"));        assertEquals(3, out2.size());    assertTrue(out2.contains("file1line6"));    assertTrue(out2.contains("file1line7"));    assertTrue(out2.contains("file1line8"));        assertEquals(5, out3.size());    assertTrue(out3.contains("file2line1"));    assertTrue(out3.contains("file2line2"));    assertTrue(out3.contains("file2line3"));    assertTrue(out3.contains("file2line4"));    assertTrue(out3.contains("file2line5"));}
public void flume_f6342_0() throws IOException
{    int fileNum = 1000;    Set<String> expected = Sets.newHashSet();    for (int i = 0; i < fileNum; i++) {        String data = "data" + i;        File f = new File(tmpDir, "file" + i);        Files.write(data + "\n", f, Charsets.UTF_8);        expected.add(data);    }    ReliableTaildirEventReader reader = getReader();    for (TailFile tf : reader.getTailFiles().values()) {        List<Event> events = reader.readEvents(tf, 10);        for (Event e : events) {            expected.remove(new String(e.getBody()));        }        reader.commit();    }    assertEquals(0, expected.size());}
public void flume_f6343_0() throws IOException
{    File f1 = new File(tmpDir, "file1");    File f2 = new File(tmpDir, "file2");    File f3 = new File(tmpDir, "file3");    Files.write("file1line1\nfile1line2\nfile1line3\n", f1, Charsets.UTF_8);    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);    Files.write("file3line1\n", f3, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    Map<Long, TailFile> tailFiles = reader.getTailFiles();    long pos = f2.length();    int i = 1;    File posFile = new File(posFilePath);    for (TailFile tf : tailFiles.values()) {        Files.append(i == 1 ? "[" : "", posFile, Charsets.UTF_8);        Files.append(String.format("{\"inode\":%s,\"pos\":%s,\"file\":\"%s\"}", tf.getInode(), pos, tf.getPath()), posFile, Charsets.UTF_8);        Files.append(i == 3 ? "]" : ",", posFile, Charsets.UTF_8);        i++;    }    reader.loadPositionFile(posFilePath);    for (TailFile tf : tailFiles.values()) {        if (tf.getPath().equals(tmpDir + "file3")) {                        assertEquals(0, tf.getPos());        } else {            assertEquals(pos, tf.getPos());        }    }}
public void flume_f6344_0() throws IOException
{    ReliableTaildirEventReader reader = getReader();    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    reader.updateTailFiles();    for (TailFile tf : reader.getTailFiles().values()) {        if (tf.getPath().equals(tmpDir + "file1")) {            assertEquals(0, tf.getPos());        }    }    File f2 = new File(tmpDir, "file2");    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);        reader.updateTailFiles(true);    for (TailFile tf : reader.getTailFiles().values()) {        if (tf.getPath().equals(tmpDir + "file2")) {            assertEquals(f2.length(), tf.getPos());        }    }}
public void flume_f6345_0() throws IOException
{    File f1 = new File(tmpDir, "file1");    String line1 = "file1line1\n";    String line2 = "file1line2\n";    String line3 = "file1line3\n";    Files.write(line1 + line2 + line3, f1, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader(true, false);    List<String> headers = null;    for (TailFile tf : reader.getTailFiles().values()) {        headers = headersAsStrings(reader.readEvents(tf, 5), BYTE_OFFSET_HEADER_KEY);        reader.commit();    }    assertEquals(3, headers.size());        assertTrue(headers.contains(String.valueOf(0)));    assertTrue(headers.contains(String.valueOf(line1.length())));    assertTrue(headers.contains(String.valueOf((line1 + line2).length())));}
public void flume_f6346_0() throws IOException
{    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\rfile1line2\nfile1line3\r\nfile1line4\n", f1, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    List<String> out = Lists.newArrayList();    for (TailFile tf : reader.getTailFiles().values()) {        out.addAll(bodiesAsStrings(reader.readEvents(tf, 5)));        reader.commit();    }    assertEquals(4, out.size());        assertTrue(out.contains("file1line1"));        assertTrue(out.contains("file1line2\rfile1line2"));        assertTrue(out.contains("file1line3"));    assertTrue(out.contains("file1line4"));}
public void flume_f6347_0() throws IOException
{    File file = new File(tmpDir, "file");    Files.write("line1\n", file, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    for (TailFile tf : reader.getTailFiles().values()) {        reader.readEvents(tf, 1);        reader.commit();    }    Files.append("line2\n", file, Charsets.UTF_8);    for (TailFile tf : reader.getTailFiles().values()) {        tf.setLastUpdated(file.lastModified());    }    reader.updateTailFiles();    for (TailFile tf : reader.getTailFiles().values()) {        assertEquals(true, tf.needTail());    }}
private void flume_f6348_0(String fileName) throws IOException
{    File f;    if (!files.containsKey(fileName)) {        f = new File(tmpDir, fileName);        files.put(fileName, f);    } else {        f = files.get(fileName);    }    Files.append(fileName + "line\n", f, Charsets.UTF_8);}
private static List<String> flume_f6349_0(List<File> origList)
{    Function<File, String> file2nameFn = new Function<File, String>() {        @Override        public String apply(File input) {            return input.getName();        }    };    return Lists.transform(origList, file2nameFn);}
public String flume_f6350_0(File input)
{    return input.getName();}
public void flume_f6351_0() throws Exception
{    files = Maps.newHashMap();    tmpDir = Files.createTempDir();}
public void flume_f6352_0() throws Exception
{    for (File f : tmpDir.listFiles()) {        if (f.isDirectory()) {            for (File sdf : f.listFiles()) {                sdf.delete();            }        }        f.delete();    }    tmpDir.delete();    files = null;}
public void flume_f6353_0() throws Exception
{    append("file0");    append("file1");    TaildirMatcher tm = new TaildirMatcher("f1", tmpDir.getAbsolutePath() + File.separator + "file.*", isCachingNeeded);    List<String> files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAlreadyExistingFile, 2, files.size());    assertTrue(msgAlreadyExistingFile, files.contains("file1"));    append("file1");    files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAfterNewFileCreated, 2, files.size());    assertTrue(msgAfterNewFileCreated, files.contains("file0"));    assertTrue(msgAfterNewFileCreated, files.contains("file1"));    append("file2");    append("file3");    files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAfterAppend, 4, files.size());    assertTrue(msgAfterAppend, files.contains("file0"));    assertTrue(msgAfterAppend, files.contains("file1"));    assertTrue(msgAfterAppend, files.contains("file2"));    assertTrue(msgAfterAppend, files.contains("file3"));    this.files.get("file0").delete();    files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAfterDelete, 3, files.size());    assertFalse(msgAfterDelete, files.contains("file0"));    assertTrue(msgNoChange, files.contains("file1"));    assertTrue(msgNoChange, files.contains("file2"));    assertTrue(msgNoChange, files.contains("file3"));}
public void flume_f6354_0() throws Exception
{    append("file0");    append("file1");    TaildirMatcher tm = new TaildirMatcher("f1", tmpDir.getAbsolutePath() + File.separator + "file.*", false);    List<String> files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAlreadyExistingFile, 2, files.size());    assertTrue(msgAlreadyExistingFile, files.contains("file1"));    append("file1");    files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAfterAppend, 2, files.size());    assertTrue(msgAfterAppend, files.contains("file0"));    assertTrue(msgAfterAppend, files.contains("file1"));    append("file2");    append("file3");    files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAfterNewFileCreated, 4, files.size());    assertTrue(msgAfterNewFileCreated, files.contains("file0"));    assertTrue(msgAfterNewFileCreated, files.contains("file1"));    assertTrue(msgAfterNewFileCreated, files.contains("file2"));    assertTrue(msgAfterNewFileCreated, files.contains("file3"));    this.files.get("file0").delete();    files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAfterDelete, 3, files.size());    assertFalse(msgAfterDelete, files.contains("file0"));    assertTrue(msgNoChange, files.contains("file1"));    assertTrue(msgNoChange, files.contains("file2"));    assertTrue(msgNoChange, files.contains("file3"));}
public void flume_f6355_0() throws Exception
{    TaildirMatcher tm = new TaildirMatcher("empty", tmpDir.getAbsolutePath() + File.separator + ".*", isCachingNeeded);    List<File> files = tm.getMatchingFiles();    assertNotNull(msgEmptyDir, files);    assertTrue(msgEmptyDir, files.isEmpty());}
public void flume_f6356_0() throws Exception
{    TaildirMatcher tm = new TaildirMatcher("nomatch", tmpDir.getAbsolutePath() + File.separator + "abracadabra_nonexisting", isCachingNeeded);    List<File> files = tm.getMatchingFiles();    assertNotNull(msgNoMatch, files);    assertTrue(msgNoMatch, files.isEmpty());}
public void flume_f6357_0()
{    TaildirMatcher tm = new TaildirMatcher("exception", "/abracadabra/doesntexist/.*", isCachingNeeded);}
public void flume_f6358_0() throws Exception
{    new File(tmpDir, "outerFile").createNewFile();    new File(tmpDir, "recursiveDir").mkdir();    new File(tmpDir + File.separator + "recursiveDir", "innerFile").createNewFile();    TaildirMatcher tm = new TaildirMatcher("f1", tmpDir.getAbsolutePath() + File.separator + ".*", isCachingNeeded);    List<String> files = filesToNames(tm.getMatchingFiles());    assertEquals(msgSubDirs, 1, files.size());    assertTrue(msgSubDirs, files.contains("outerFile"));}
public void flume_f6359_0() throws IOException
{    append("a.log");    append("a.log.1");    append("b.log");    append("c.log.yyyy.MM-01");    append("c.log.yyyy.MM-02");        TaildirMatcher tm1 = new TaildirMatcher("ab", tmpDir.getAbsolutePath() + File.separator + "[ab].log", isCachingNeeded);        TaildirMatcher tm2 = new TaildirMatcher("c", tmpDir.getAbsolutePath() + File.separator + "c.log.*", isCachingNeeded);    List<String> files1 = filesToNames(tm1.getMatchingFiles());    List<String> files2 = filesToNames(tm2.getMatchingFiles());    assertEquals(2, files1.size());    assertEquals(2, files2.size());        assertTrue("Regex pattern for ab should have matched a.log file", files1.contains("a.log"));    assertFalse("Regex pattern for ab should NOT have matched a.log.1 file", files1.contains("a.log.1"));    assertTrue("Regex pattern for ab should have matched b.log file", files1.contains("b.log"));    assertTrue("Regex pattern for c should have matched c.log.yyyy-MM-01 file", files2.contains("c.log.yyyy.MM-01"));    assertTrue("Regex pattern for c should have matched c.log.yyyy-MM-02 file", files2.contains("c.log.yyyy.MM-02"));}
public void flume_f6360_0()
{    source = new TaildirSource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    tmpDir = Files.createTempDir();    posFilePath = tmpDir.getAbsolutePath() + "/taildir_position_test.json";}
public void flume_f6361_0()
{    for (File f : tmpDir.listFiles()) {        f.delete();    }    tmpDir.delete();}
public void flume_f6362_0() throws IOException
{    File f1 = new File(tmpDir, "a.log");    File f2 = new File(tmpDir, "a.log.1");    File f3 = new File(tmpDir, "b.log");    File f4 = new File(tmpDir, "c.log.yyyy-MM-01");    File f5 = new File(tmpDir, "c.log.yyyy-MM-02");    Files.write("a.log\n", f1, Charsets.UTF_8);    Files.write("a.log.1\n", f2, Charsets.UTF_8);    Files.write("b.log\n", f3, Charsets.UTF_8);    Files.write("c.log.yyyy-MM-01\n", f4, Charsets.UTF_8);    Files.write("c.log.yyyy-MM-02\n", f5, Charsets.UTF_8);    Context context = new Context();    context.put(POSITION_FILE, posFilePath);    context.put(FILE_GROUPS, "ab c");        context.put(FILE_GROUPS_PREFIX + "ab", tmpDir.getAbsolutePath() + "/[ab].log");        context.put(FILE_GROUPS_PREFIX + "c", tmpDir.getAbsolutePath() + "/c.log.*");    Configurables.configure(source, context);    source.start();    source.process();    Transaction txn = channel.getTransaction();    txn.begin();    List<String> out = Lists.newArrayList();    for (int i = 0; i < 5; i++) {        Event e = channel.take();        if (e != null) {            out.add(TestTaildirEventReader.bodyAsString(e));        }    }    txn.commit();    txn.close();    assertEquals(4, out.size());        assertTrue(out.contains("a.log"));    assertFalse(out.contains("a.log.1"));    assertTrue(out.contains("b.log"));    assertTrue(out.contains("c.log.yyyy-MM-01"));    assertTrue(out.contains("c.log.yyyy-MM-02"));}
public void flume_f6363_0() throws IOException
{    File f1 = new File(tmpDir, "file1");    File f2 = new File(tmpDir, "file2");    File f3 = new File(tmpDir, "file3");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);    Files.write("file3line1\nfile3line2\n", f3, Charsets.UTF_8);    Context context = new Context();    context.put(POSITION_FILE, posFilePath);    context.put(FILE_GROUPS, "f1 f2 f3");    context.put(FILE_GROUPS_PREFIX + "f1", tmpDir.getAbsolutePath() + "/file1$");    context.put(FILE_GROUPS_PREFIX + "f2", tmpDir.getAbsolutePath() + "/file2$");    context.put(FILE_GROUPS_PREFIX + "f3", tmpDir.getAbsolutePath() + "/file3$");    context.put(HEADERS_PREFIX + "f1.headerKeyTest", "value1");    context.put(HEADERS_PREFIX + "f2.headerKeyTest", "value2");    context.put(HEADERS_PREFIX + "f2.headerKeyTest2", "value2-2");    Configurables.configure(source, context);    source.start();    source.process();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 6; i++) {        Event e = channel.take();        String body = new String(e.getBody(), Charsets.UTF_8);        String headerValue = e.getHeaders().get("headerKeyTest");        String headerValue2 = e.getHeaders().get("headerKeyTest2");        if (body.startsWith("file1")) {            assertEquals("value1", headerValue);            assertNull(headerValue2);        } else if (body.startsWith("file2")) {            assertEquals("value2", headerValue);            assertEquals("value2-2", headerValue2);        } else if (body.startsWith("file3")) {                        assertNull(headerValue);            assertNull(headerValue2);        }    }    txn.commit();    txn.close();}
public void flume_f6364_0() throws IOException, InterruptedException
{    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Context context = new Context();    context.put(POSITION_FILE, posFilePath);    context.put(FILE_GROUPS, "f1");    context.put(FILE_GROUPS_PREFIX + "f1", tmpDir.getAbsolutePath() + "/file1$");    Configurables.configure(source, context);    for (int i = 0; i < 3; i++) {        source.start();        source.process();        assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));        assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());        source.stop();        assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));        assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());    }}
private ArrayList<String> flume_f6365_0() throws IOException
{    System.out.println(tmpDir.toString());        File f1 = new File(tmpDir, "file1");    String line1 = "file1line1\n";    String line2 = "file1line2\n";    String line3 = "file1line3\n";    Files.write(line1 + line2 + line3, f1, Charsets.UTF_8);    try {                Thread.sleep(1000);    } catch (InterruptedException e) {    }        String line1b = "file2line1\n";    String line2b = "file2line2\n";    String line3b = "file2line3\n";    File f2 = new File(tmpDir, "file2");    Files.write(line1b + line2b + line3b, f2, Charsets.UTF_8);    try {                Thread.sleep(1000);    } catch (InterruptedException e) {    }        String line1c = "file3line1\n";    String line2c = "file3line2\n";    String line3c = "file3line3\n";    File f3 = new File(tmpDir, "file3");    Files.write(line1c + line2c + line3c, f3, Charsets.UTF_8);    try {                Thread.sleep(1000);    } catch (InterruptedException e) {    }        String line1d = "file4line1\n";    String line2d = "file4line2\n";    String line3d = "file4line3\n";    File f4 = new File(tmpDir, "file4");    Files.write(line1d + line2d + line3d, f4, Charsets.UTF_8);    try {                Thread.sleep(1000);    } catch (InterruptedException e) {    }        f3.setLastModified(System.currentTimeMillis());        Context context = new Context();    context.put(POSITION_FILE, posFilePath);    context.put(FILE_GROUPS, "g1");    context.put(FILE_GROUPS_PREFIX + "g1", tmpDir.getAbsolutePath() + "/.*");    Configurables.configure(source, context);        ArrayList<String> expected =     Lists.newArrayList(    line1,     line2,     line3,     line1b,     line2b,     line3b,     line1d,     line2d,     line3d,     line1c,     line2c,     line3c);    for (int i = 0; i != expected.size(); ++i) {        expected.set(i, expected.get(i).trim());    }    return expected;}
public void flume_f6366_0() throws IOException
{    ArrayList<String> consumedOrder = Lists.newArrayList();    ArrayList<String> expected = prepareFileConsumeOrder();    source.start();    source.process();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 12; i++) {        Event e = channel.take();        String body = new String(e.getBody(), Charsets.UTF_8);        consumedOrder.add(body);    }    txn.commit();    txn.close();    System.out.println(consumedOrder);    assertArrayEquals("Files not consumed in expected order", expected.toArray(), consumedOrder.toArray());}
private File flume_f6367_0() throws IOException
{    File f1 = new File(tmpDir, "file1");    Files.write("f1\n", f1, Charsets.UTF_8);    Context context = new Context();    context.put(POSITION_FILE, posFilePath);    context.put(FILE_GROUPS, "fg");    context.put(FILE_GROUPS_PREFIX + "fg", tmpDir.getAbsolutePath() + "/file.*");    context.put(FILENAME_HEADER, "true");    context.put(FILENAME_HEADER_KEY, "path");    Configurables.configure(source, context);    return f1;}
public void flume_f6368_0() throws IOException
{    File f1 = configureSource();    source.start();    source.process();    Transaction txn = channel.getTransaction();    txn.begin();    Event e = channel.take();    txn.commit();    txn.close();    assertNotNull(e.getHeaders().get("path"));    assertEquals(f1.getAbsolutePath(), e.getHeaders().get("path"));}
public void flume_f6369_0() throws Exception
{    configureSource();    source.start();    ReliableTaildirEventReader reader = Mockito.mock(ReliableTaildirEventReader.class);    Whitebox.setInternalState(source, "reader", reader);    when(reader.updateTailFiles()).thenReturn(Collections.singletonList(123L));    when(reader.getTailFiles()).thenThrow(new RuntimeException("hello"));    source.process();    assertEquals(1, source.getSourceCounter().getEventReadFail());    source.stop();}
public void flume_f6370_0() throws Exception
{    configureSource();    Whitebox.setInternalState(source, "idleTimeout", 0);    Whitebox.setInternalState(source, "checkIdleInterval", 60);    source.start();    ReliableTaildirEventReader reader = Mockito.mock(ReliableTaildirEventReader.class);    when(reader.getTailFiles()).thenThrow(new RuntimeException("hello"));    Whitebox.setInternalState(source, "reader", reader);    TimeUnit.MILLISECONDS.sleep(200);    assertTrue(0 < source.getSourceCounter().getGenericProcessingFail());    source.stop();}
public void flume_f6371_0() throws Exception
{    prepareFileConsumeOrder();    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    source.setChannelProcessor(cp);    doThrow(new ChannelException("dummy")).doNothing().when(cp).processEventBatch(anyListOf(Event.class));    source.start();    source.process();    assertEquals(1, source.getSourceCounter().getChannelWriteFail());    source.stop();}
public void flume_f6372_0() throws IOException
{    File f1 = new File(tmpDir, "file1");    File f2 = new File(tmpDir, "file2");    Files.write("file1line1\nfile1line2\n" + "file1line3\nfile1line4\n", f1, Charsets.UTF_8);    Files.write("file2line1\nfile2line2\n" + "file2line3\nfile2line4\n", f2, Charsets.UTF_8);    Context context = new Context();    context.put(POSITION_FILE, posFilePath);    context.put(FILE_GROUPS, "fg");    context.put(FILE_GROUPS_PREFIX + "fg", tmpDir.getAbsolutePath() + "/file.*");    context.put(BATCH_SIZE, String.valueOf(1));    context.put(MAX_BATCH_COUNT, String.valueOf(2));    Configurables.configure(source, context);    source.start();        source.process();    source.process();    List<Event> eventList = new ArrayList<Event>();    for (int i = 0; i < 8; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        Event e = channel.take();        txn.commit();        txn.close();        if (e == null) {            break;        }        eventList.add(e);    }    assertEquals("1", context.getString(BATCH_SIZE));    assertEquals("2", context.getString(MAX_BATCH_COUNT));    assertEquals(8, eventList.size());        String firstFile = new String(eventList.get(0).getBody()).substring(0, 5);    String secondFile = firstFile.equals("file1") ? "file2" : "file1";    assertEquals(firstFile + "line1", new String(eventList.get(0).getBody()));    assertEquals(firstFile + "line2", new String(eventList.get(1).getBody()));    assertEquals(secondFile + "line1", new String(eventList.get(2).getBody()));    assertEquals(secondFile + "line2", new String(eventList.get(3).getBody()));    assertEquals(firstFile + "line3", new String(eventList.get(4).getBody()));    assertEquals(firstFile + "line4", new String(eventList.get(5).getBody()));    assertEquals(secondFile + "line3", new String(eventList.get(6).getBody()));    assertEquals(secondFile + "line4", new String(eventList.get(7).getBody()));}
public void flume_f6373_0() throws IOException
{    File f1 = new File(tmpDir, "file1");    File f2 = new File(tmpDir, "file2");    Files.write("file1line1\nfile1line2\n" + "file1line3\nfile1line4\nfile1line5\n", f1, Charsets.UTF_8);    Files.write("file2line1\nfile2line2\n" + "file2line3\n", f2, Charsets.UTF_8);    Context context = new Context();    context.put(POSITION_FILE, posFilePath);    context.put(FILE_GROUPS, "fg");    context.put(FILE_GROUPS_PREFIX + "fg", tmpDir.getAbsolutePath() + "/file.*");    context.put(BATCH_SIZE, String.valueOf(1));    context.put(MAX_BATCH_COUNT, String.valueOf(2));    Configurables.configure(source, context);    source.start();    Status status;    status = source.process();    assertEquals(Status.READY, status);    status = source.process();    assertEquals(Status.READY, status);    status = source.process();    assertEquals(Status.BACKOFF, status);    status = source.process();    assertEquals(Status.BACKOFF, status);}
public void flume_f6374_0(Context context)
{    String consumerKey = context.getString("consumerKey");    String consumerSecret = context.getString("consumerSecret");    String accessToken = context.getString("accessToken");    String accessTokenSecret = context.getString("accessTokenSecret");    twitterStream = new TwitterStreamFactory().getInstance();    twitterStream.setOAuthConsumer(consumerKey, consumerSecret);    twitterStream.setOAuthAccessToken(new AccessToken(accessToken, accessTokenSecret));    twitterStream.addListener(this);    avroSchema = createAvroSchema();    dataFileWriter = new DataFileWriter<GenericRecord>(new GenericDatumWriter<GenericRecord>(avroSchema));    maxBatchSize = context.getInteger("maxBatchSize", maxBatchSize);    maxBatchDurationMillis = context.getInteger("maxBatchDurationMillis", maxBatchDurationMillis);}
public synchronized void flume_f6375_1()
{        docCount = 0;    startTime = System.currentTimeMillis();    exceptionCount = 0;    totalTextIndexed = 0;    skippedDocs = 0;    batchEndTime = System.currentTimeMillis() + maxBatchDurationMillis;    twitterStream.sample();                            super.start();}
public synchronized void flume_f6376_1()
{        twitterStream.shutdown();    super.stop();    }
public void flume_f6377_1(Status status)
{    Record doc = extractRecord("", avroSchema, status);    if (doc == null) {                return;    }    docs.add(doc);    if (docs.size() >= maxBatchSize || System.currentTimeMillis() >= batchEndTime) {        batchEndTime = System.currentTimeMillis() + maxBatchDurationMillis;        byte[] bytes;        try {            bytes = serializeToAvro(avroSchema, docs);        } catch (IOException e) {                                    return;        }        Event event = EventBuilder.withBody(bytes);                getChannelProcessor().processEvent(event);        docs.clear();    }    docCount++;    if ((docCount % REPORT_INTERVAL) == 0) {            }    if ((docCount % STATS_INTERVAL) == 0) {        logStats();    }}
private Schema flume_f6378_0()
{    Schema avroSchema = Schema.createRecord("Doc", "adoc", null, false);    List<Field> fields = new ArrayList<Field>();    fields.add(new Field("id", Schema.create(Type.STRING), null, null));    fields.add(new Field("user_friends_count", createOptional(Schema.create(Type.INT)), null, null));    fields.add(new Field("user_location", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("user_description", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("user_statuses_count", createOptional(Schema.create(Type.INT)), null, null));    fields.add(new Field("user_followers_count", createOptional(Schema.create(Type.INT)), null, null));    fields.add(new Field("user_name", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("user_screen_name", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("created_at", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("text", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("retweet_count", createOptional(Schema.create(Type.LONG)), null, null));    fields.add(new Field("retweeted", createOptional(Schema.create(Type.BOOLEAN)), null, null));    fields.add(new Field("in_reply_to_user_id", createOptional(Schema.create(Type.LONG)), null, null));    fields.add(new Field("source", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("in_reply_to_status_id", createOptional(Schema.create(Type.LONG)), null, null));    fields.add(new Field("media_url_https", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("expanded_url", createOptional(Schema.create(Type.STRING)), null, null));    avroSchema.setFields(fields);    return avroSchema;}
private Record flume_f6379_0(String idPrefix, Schema avroSchema, Status status)
{    User user = status.getUser();    Record doc = new Record(avroSchema);    doc.put("id", idPrefix + status.getId());    doc.put("created_at", formatterTo.format(status.getCreatedAt()));    doc.put("retweet_count", status.getRetweetCount());    doc.put("retweeted", status.isRetweet());    doc.put("in_reply_to_user_id", status.getInReplyToUserId());    doc.put("in_reply_to_status_id", status.getInReplyToStatusId());    addString(doc, "source", status.getSource());    addString(doc, "text", status.getText());    MediaEntity[] mediaEntities = status.getMediaEntities();    if (mediaEntities.length > 0) {        addString(doc, "media_url_https", mediaEntities[0].getMediaURLHttps());        addString(doc, "expanded_url", mediaEntities[0].getExpandedURL());    }    doc.put("user_friends_count", user.getFriendsCount());    doc.put("user_statuses_count", user.getStatusesCount());    doc.put("user_followers_count", user.getFollowersCount());    addString(doc, "user_location", user.getLocation());    addString(doc, "user_description", user.getDescription());    addString(doc, "user_screen_name", user.getScreenName());    addString(doc, "user_name", user.getName());    return doc;}
private byte[] flume_f6380_0(Schema avroSchema, List<Record> docList) throws IOException
{    serializationBuffer.reset();    dataFileWriter.create(avroSchema, serializationBuffer);    for (Record doc2 : docList) {        dataFileWriter.append(doc2);    }    dataFileWriter.close();    return serializationBuffer.toByteArray();}
private Schema flume_f6381_0(Schema schema)
{    return Schema.createUnion(Arrays.asList(new Schema[] { schema, Schema.create(Type.NULL) }));}
private void flume_f6382_0(Record doc, String avroField, String val)
{    if (val == null) {        return;    }    doc.put(avroField, val);    totalTextIndexed += val.length();}
private void flume_f6383_1()
{    double mbIndexed = totalTextIndexed / (1024 * 1024.0);    long seconds = (System.currentTimeMillis() - startTime) / 1000;    seconds = Math.max(seconds, 1);                        }
public void flume_f6384_0(StatusDeletionNotice statusDeletionNotice)
{}
public void flume_f6385_0(long userId, long upToStatusId)
{}
public void flume_f6386_0(StallWarning warning)
{}
public void flume_f6387_0(int numberOfLimitedStatuses)
{}
public void flume_f6388_1(Exception e)
{    }
public long flume_f6389_0()
{    return maxBatchSize;}
public static void flume_f6390_0()
{    try {        Assume.assumeNotNull(InetAddress.getByName("stream.twitter.com"));    } catch (UnknownHostException e) {                Assume.assumeTrue(false);    }}
public void flume_f6391_0() throws Exception
{    String consumerKey = System.getProperty("twitter.consumerKey");    Assume.assumeNotNull(consumerKey);    String consumerSecret = System.getProperty("twitter.consumerSecret");    Assume.assumeNotNull(consumerSecret);    String accessToken = System.getProperty("twitter.accessToken");    Assume.assumeNotNull(accessToken);    String accessTokenSecret = System.getProperty("twitter.accessTokenSecret");    Assume.assumeNotNull(accessTokenSecret);    Context context = new Context();    context.put("consumerKey", consumerKey);    context.put("consumerSecret", consumerSecret);    context.put("accessToken", accessToken);    context.put("accessTokenSecret", accessTokenSecret);    context.put("maxBatchDurationMillis", "1000");    TwitterSource source = new TwitterSource();    source.configure(context);    Map<String, String> channelContext = new HashMap();    channelContext.put("capacity", "1000000");        channelContext.put("keep-alive", "0");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context(channelContext));    Sink sink = new LoggerSink();    sink.setChannel(channel);    sink.start();    DefaultSinkProcessor proc = new DefaultSinkProcessor();    proc.setSinks(Collections.singletonList(sink));    SinkRunner sinkRunner = new SinkRunner(proc);    sinkRunner.start();    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(Collections.singletonList(channel));    ChannelProcessor chp = new ChannelProcessor(rcs);    source.setChannelProcessor(chp);    source.start();    Thread.sleep(5000);    source.stop();    sinkRunner.stop();    sink.stop();}
public void flume_f6392_0() throws Exception
{    SimpleDateFormat formatterFrom = new SimpleDateFormat("EEE MMM dd HH:mm:ss Z yyyy");    formatterFrom.parse("Fri Oct 26 22:53:55 +0000 2012");}
public void flume_f6393_1() throws Exception
{    File agentDir = StagedInstall.getInstance().getStageDir();        File testDir = new File(agentDir, TestConfig.class.getName());    if (testDir.exists()) {        FileUtils.deleteDirectory(testDir);    }    assertTrue(testDir.mkdirs());    agentProps = new Properties();    agentEnv = new HashMap<>();    agentOptions = new HashMap<>();    agentOptions.put("-C", getAdditionalClassPath());        agentProps.put("agent.sources.seq-01.type", "seq");    agentProps.put("agent.sources.seq-01.totalEvents", "100");    agentProps.put("agent.sources.seq-01.channels", "mem-01 mem-02 mem-03");    agentProps.put("agent.channels.mem-01.type", "MEMORY");    agentProps.put("agent.channels.mem-01.capacity", String.valueOf(100000));    agentProps.put("agent.channels.mem-02.type", "MEMORY");    agentProps.put("agent.channels.mem-02.capacity", String.valueOf(100000));    agentProps.put("agent.channels.mem-03.type", "MEMORY");    agentProps.put("agent.channels.mem-04.capacity", String.valueOf(100000));    sinkOutputDir1 = new File(testDir, "out1");    assertTrue("Unable to create sink output dir: " + sinkOutputDir1.getPath(), sinkOutputDir1.mkdir());    sinkOutputDir2 = new File(testDir, "out2");    assertTrue("Unable to create sink output dir: " + sinkOutputDir2.getPath(), sinkOutputDir2.mkdir());    sinkOutputDir3 = new File(testDir, "out3");    assertTrue("Unable to create sink output dir: " + sinkOutputDir3.getPath(), sinkOutputDir3.mkdir());    environmentVariables.set("HADOOP_CREDSTORE_PASSWORD", "envSecret");    agentEnv.put("dirname_env", sinkOutputDir1.getAbsolutePath());    agentEnv.put("HADOOP_CREDSTORE_PASSWORD", "envSecret");    hadoopCredStore = new File(testDir, "credstore.jceks");    String providerPath = "jceks://file/" + hadoopCredStore.getAbsolutePath();    ToolRunner.run(new Configuration(), new CredentialShell(), ("create dirname_hadoop -value " + sinkOutputDir3.getAbsolutePath() + " -provider " + providerPath).split(" "));    agentProps.put("agent.sinks.roll-01.channel", "mem-01");    agentProps.put("agent.sinks.roll-01.type", "FILE_ROLL");    agentProps.put("agent.sinks.roll-01.sink.directory", "${filter-01[\"dirname_env\"]}");    agentProps.put("agent.sinks.roll-01.sink.rollInterval", "0");    agentProps.put("agent.sinks.roll-02.channel", "mem-02");    agentProps.put("agent.sinks.roll-02.type", "FILE_ROLL");    agentProps.put("agent.sinks.roll-02.sink.directory", sinkOutputDir2.getParentFile().getAbsolutePath() + "/${filter-02['out2']}");    agentProps.put("agent.sinks.roll-02.sink.rollInterval", "0");    agentProps.put("agent.sinks.roll-03.channel", "mem-03");    agentProps.put("agent.sinks.roll-03.type", "FILE_ROLL");    agentProps.put("agent.sinks.roll-03.sink.directory", "${filter-03[dirname_hadoop]}");    agentProps.put("agent.sinks.roll-03.sink.rollInterval", "0");    agentProps.put("agent.configfilters.filter-01.type", "env");    agentProps.put("agent.configfilters.filter-02.type", "external");    agentProps.put("agent.configfilters.filter-02.command", "echo");    agentProps.put("agent.configfilters.filter-03.type", "hadoop");    agentProps.put("agent.configfilters.filter-03.credential.provider.path", providerPath);    agentProps.put("agent.sources", "seq-01");    agentProps.put("agent.channels", "mem-01 mem-02 mem-03");    agentProps.put("agent.sinks", "roll-01 roll-02 roll-03");    agentProps.put("agent.configfilters", "filter-01 filter-02 filter-03");}
private String flume_f6394_0() throws Exception
{    URL resource = this.getClass().getClassLoader().getResource("classpath.txt");    Path path = Paths.get(Objects.requireNonNull(resource).getPath());    return Files.readAllLines(path).stream().findFirst().orElse("");}
public void flume_f6395_0() throws Exception
{    StagedInstall.getInstance().stopAgent();}
private void flume_f6396_0(File outDir, int outFiles, int events) throws IOException
{    File[] sinkOutputDirChildren = outDir.listFiles();    assertEquals("Unexpected number of files in output dir", outFiles, sinkOutputDirChildren.length);    Set<String> seenEvents = new HashSet<>();    for (File outFile : sinkOutputDirChildren) {        Scanner scanner = new Scanner(outFile);        while (scanner.hasNext()) {            seenEvents.add(scanner.nextLine());        }    }    for (int event = 0; event < events; event++) {        assertTrue("Missing event: {" + event + "}", seenEvents.contains(String.valueOf(event)));    }}
public void flume_f6397_1() throws Exception
{        StagedInstall.getInstance().startAgent("agent", agentProps, agentEnv, agentOptions);        TimeUnit.SECONDS.sleep(10);        validateSeenEvents(sinkOutputDir1, 1, 100);    validateSeenEvents(sinkOutputDir2, 1, 100);    validateSeenEvents(sinkOutputDir3, 1, 100);        }
public void flume_f6398_1() throws Exception
{        agentProps.put("agent.channels.mem-01.transactionCapacity", "10");    agentProps.put("agent.sinks.roll-01.sink.batchSize", "20");    StagedInstall.getInstance().startAgent("agent", agentProps, agentEnv, agentOptions);        TimeUnit.SECONDS.sleep(10);        validateSeenEvents(sinkOutputDir1, 0, 0);        validateSeenEvents(sinkOutputDir2, 1, 100);    validateSeenEvents(sinkOutputDir3, 1, 100);            agentProps.put("agent.channels.mem-01.transactionCapacity", "20");    StagedInstall.getInstance().reconfigure(agentProps);        TimeUnit.SECONDS.sleep(40);        validateSeenEvents(sinkOutputDir1, 1, 100);    }
public void flume_f6399_1() throws Exception
{    /* Create 3 temp dirs, each used as value within agentProps */    final File sinkOutputDir = Files.createTempDir();    tempResources.add(sinkOutputDir);    final String sinkOutputDirPath = sinkOutputDir.getCanonicalPath();        final File channelCheckpointDir = Files.createTempDir();    tempResources.add(channelCheckpointDir);    final String channelCheckpointDirPath = channelCheckpointDir.getCanonicalPath();        final File channelDataDir = Files.createTempDir();    tempResources.add(channelDataDir);    final String channelDataDirPath = channelDataDir.getCanonicalPath();        /* Build props to pass to flume agent */    Properties agentProps = new Properties();        agentProps.put("a1.channels", "c1");    agentProps.put("a1.sources", "r1");    agentProps.put("a1.sinks", "k1");        agentProps.put("a1.channels.c1.type", "FILE");    agentProps.put("a1.channels.c1.checkpointDir", channelCheckpointDirPath);    agentProps.put("a1.channels.c1.dataDirs", channelDataDirPath);        agentProps.put("a1.sources.r1.channels", "c1");    agentProps.put("a1.sources.r1.type", "EXEC");    agentProps.put("a1.sources.r1.command", "seq 1 100");        agentProps.put("a1.sinks.k1.channel", "c1");    agentProps.put("a1.sinks.k1.type", "FILE_ROLL");    agentProps.put("a1.sinks.k1.sink.directory", sinkOutputDirPath);    agentProps.put("a1.sinks.k1.sink.rollInterval", "0");    this.agentProps = agentProps;    this.sinkOutputDir = sinkOutputDir;}
public void flume_f6400_0() throws Exception
{    StagedInstall.getInstance().stopAgent();    for (File tempResource : tempResources) {        tempResource.delete();    }    agentProps = null;}
public void flume_f6401_1() throws Exception
{        StagedInstall.getInstance().startAgent("a1", agentProps);        TimeUnit.SECONDS.sleep(10);        /* Create expected output */    StringBuffer sb = new StringBuffer();    for (int i = 1; i <= 100; i++) {        sb.append(i).append("\n");    }    String expectedOutput = sb.toString();        /* Create actual output file */    File[] sinkOutputDirChildren = sinkOutputDir.listFiles();        Assert.assertEquals("Expected FILE_ROLL sink's dir to have only 1 child," + " but found " + sinkOutputDirChildren.length + " children.", 1, sinkOutputDirChildren.length);    File actualOutput = sinkOutputDirChildren[0];    if (!Files.toString(actualOutput, Charsets.UTF_8).equals(expectedOutput)) {                throw new AssertionError("FILE_ROLL sink's actual output doesn't " + "match expected output.");    }    }
public void flume_f6402_0() throws Exception
{    port = StagedInstall.getInstance().startAgent("rpccagent", CONFIG_FILE_PRCCLIENT_TEST);}
public void flume_f6403_0() throws Exception
{    StagedInstall.getInstance().stopAgent();}
public void flume_f6404_0() throws Exception
{    StagedInstall.waitUntilPortOpens("localhost", port, 20000);    RpcClient client = RpcClientFactory.getDefaultInstance("localhost", port);    String[] text = { "foo", "bar", "xyz", "abc" };    for (String str : text) {        client.append(EventBuilder.withBody(str.getBytes()));    }}
public void flume_f6405_0() throws Exception
{    try {        int port = StagedInstall.getInstance().startAgent("rpccagent", CONFIG_FILE_PRCCLIENT_TEST);        StagedInstall.waitUntilPortOpens("localhost", port, 20000);        RpcClient client = RpcClientFactory.getDefaultInstance("localhost", port);        String[] text = { "foo", "bar", "xyz", "abc" };        for (String str : text) {            client.append(EventBuilder.withBody(str.getBytes()));        }                StagedInstall.getInstance().stopAgent();                try {            client.append(EventBuilder.withBody("test".getBytes()));            Assert.fail("EventDeliveryException expected but not raised");        } catch (EventDeliveryException ex) {            System.out.println("Attempting to close client");            client.close();        }    } finally {        if (StagedInstall.getInstance().isRunning()) {            StagedInstall.getInstance().stopAgent();        }    }}
public void flume_f6406_1() throws Exception
{    File agentDir = StagedInstall.getInstance().getStageDir();        File testDir = new File(agentDir, TestSpooldirSource.class.getName());    assertTrue(testDir.mkdirs());    File spoolParentDir = new File(testDir, "spools");    assertTrue("Unable to create sink output dir: " + spoolParentDir.getPath(), spoolParentDir.mkdir());    final int NUM_SOURCES = 100;    agentProps = new Properties();    List<String> spooldirSrcNames = Lists.newArrayList();    String channelName = "mem-01";        for (int i = 0; i < NUM_SOURCES; i++) {        String srcName = String.format("spooldir-%03d", i);        File spoolDir = new File(spoolParentDir, srcName);        assertTrue(spoolDir.mkdir());        spooldirSrcNames.add(srcName);        spoolDirs.add(spoolDir);        agentProps.put(String.format("agent.sources.%s.type", srcName), "SPOOLDIR");        agentProps.put(String.format("agent.sources.%s.spoolDir", srcName), spoolDir.getPath());        agentProps.put(String.format("agent.sources.%s.channels", srcName), channelName);    }        agentProps.put("agent.channels.mem-01.type", "MEMORY");    agentProps.put("agent.channels.mem-01.capacity", String.valueOf(100000));    sinkOutputDir = new File(testDir, "out");    assertTrue("Unable to create sink output dir: " + sinkOutputDir.getPath(), sinkOutputDir.mkdir());    agentProps.put("agent.sinks.roll-01.channel", channelName);    agentProps.put("agent.sinks.roll-01.type", "FILE_ROLL");    agentProps.put("agent.sinks.roll-01.sink.directory", sinkOutputDir.getPath());    agentProps.put("agent.sinks.roll-01.sink.rollInterval", "0");    agentProps.put("agent.sources", Joiner.on(" ").join(spooldirSrcNames));    agentProps.put("agent.channels", channelName);    agentProps.put("agent.sinks", "roll-01");}
public void flume_f6407_0() throws Exception
{    StagedInstall.getInstance().stopAgent();}
private String flume_f6408_0(int dirNum, int fileNum)
{    return String.format("Test dir %03d, test file %03d.\n", dirNum, fileNum);}
private void flume_f6409_0(List<File> spoolDirs, int numFiles, int startNum) throws IOException
{    int numSpoolDirs = spoolDirs.size();    for (int dirNum = 0; dirNum < numSpoolDirs; dirNum++) {        File spoolDir = spoolDirs.get(dirNum);        for (int fileNum = startNum; fileNum < numFiles; fileNum++) {                        File tmp = new File(spoolDir.getParent(), UUID.randomUUID().toString());            Files.append(getTestString(dirNum, fileNum), tmp, Charsets.UTF_8);            File dst = new File(spoolDir, String.format("test-file-%03d", fileNum));                        assertTrue(String.format("Failed to rename %s to %s", tmp, dst), tmp.renameTo(dst));        }    }}
private void flume_f6410_0(File outDir, int outFiles, int dirs, int events) throws IOException
{    File[] sinkOutputDirChildren = outDir.listFiles();    assertEquals("Unexpected number of files in output dir", outFiles, sinkOutputDirChildren.length);    Set<String> seenEvents = Sets.newHashSet();    for (File outFile : sinkOutputDirChildren) {        List<String> lines = Files.readLines(outFile, Charsets.UTF_8);        for (String line : lines) {            seenEvents.add(line);        }    }    for (int dirNum = 0; dirNum < dirs; dirNum++) {        for (int fileNum = 0; fileNum < events; fileNum++) {            String event = getTestString(dirNum, fileNum).trim();            assertTrue("Missing event: {" + event + "}", seenEvents.contains(event));        }    }}
public void flume_f6411_1() throws Exception
{        StagedInstall.getInstance().startAgent("agent", agentProps);    final int NUM_FILES_PER_DIR = 10;    createInputTestFiles(spoolDirs, NUM_FILES_PER_DIR, 0);        TimeUnit.SECONDS.sleep(10);        validateSeenEvents(sinkOutputDir, 1, spoolDirs.size(), NUM_FILES_PER_DIR);        }
public static Collection flume_f6412_0()
{    List<Object[]> sourceTypes = new ArrayList<Object[]>();    for (SyslogAgent.SyslogSourceType sourceType : SyslogAgent.SyslogSourceType.values()) {        sourceTypes.add(new Object[] { sourceType });    }    return sourceTypes;}
public void flume_f6413_0() throws Exception
{    agent = new SyslogAgent();    agent.configure(sourceType);}
public void flume_f6414_0() throws Exception
{    if (agent != null) {        agent.stop();        agent = null;    }}
public void flume_f6415_1() throws Exception
{        agent.start("all");    agent.runKeepFieldsTest();    }
public void flume_f6416_1() throws Exception
{        agent.start("none");    agent.runKeepFieldsTest();    }
public void flume_f6417_1() throws Exception
{        agent.start("timestamp hostname");    agent.runKeepFieldsTest();    }
public static synchronized StagedInstall flume_f6418_0() throws Exception
{    if (INSTANCE == null) {        INSTANCE = new StagedInstall();    }    return INSTANCE;}
public static int flume_f6419_0() throws IOException
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
public synchronized boolean flume_f6420_0()
{    return process != null;}
public synchronized void flume_f6421_1() throws Exception
{    if (process == null) {        throw new Exception("Process not found");    }        process.destroy();    process.waitFor();    process = null;    consumer.interrupt();    consumer = null;    configFilePath = null;    Runtime.getRuntime().removeShutdownHook(shutdownHook);    shutdownHook = null;        Thread.sleep(3000);}
public synchronized int flume_f6422_0(String name, String configResource) throws Exception
{    if (process != null) {        throw new Exception("A process is already running");    }    int port = findFreePort();    Properties props = new Properties();    props.load(ClassLoader.getSystemResourceAsStream(configResource));    props.put("rpccagent.sources.src1.port", String.valueOf(port));    startAgent(name, props);    return port;}
public synchronized void flume_f6423_0(String name, Properties properties) throws Exception
{    startAgent(name, properties, new HashMap<>(), new HashMap<>());}
public synchronized void flume_f6424_1(String name, Properties properties, Map<String, String> environmentVariables, Map<String, String> commandOptions) throws Exception
{    Preconditions.checkArgument(!name.isEmpty(), "agent name must not be empty");    Preconditions.checkNotNull(properties, "properties object must not be null");    agentName = name;    if (process != null) {        throw new Exception("A process is already running");    }        File configFile = createConfigurationFile(agentName, properties);    configFilePath = configFile.getCanonicalPath();    String configFileName = configFile.getName();    String logFileName = "flume-" + agentName + "-" + configFileName.substring(0, configFileName.indexOf('.')) + ".log";        ImmutableList.Builder<String> builder = new ImmutableList.Builder<String>();    builder.add(launchScriptPath);    builder.add("agent");    builder.add("--conf", confDirPath);    builder.add("--conf-file", configFilePath);    builder.add("--name", agentName);    builder.add("-D" + ENV_FLUME_LOG_DIR + "=" + logDirPath);    builder.add("-D" + ENV_FLUME_ROOT_LOGGER + "=" + ENV_FLUME_ROOT_LOGGER_VALUE);    builder.add("-D" + ENV_FLUME_LOG_FILE + "=" + logFileName);    commandOptions.forEach((key, value) -> builder.add(key, value));    List<String> cmdArgs = builder.build();        ProcessBuilder pb = new ProcessBuilder(cmdArgs);    Map<String, String> env = pb.environment();    env.putAll(environmentVariables);        pb.directory(baseDir);    pb.redirectErrorStream(true);    process = pb.start();    consumer = new ProcessInputStreamConsumer(process.getInputStream());    consumer.start();    shutdownHook = new ProcessShutdownHook();    Runtime.getRuntime().addShutdownHook(shutdownHook);        Thread.sleep(3000);}
public synchronized void flume_f6425_1(Properties properties) throws Exception
{    File configFile = createConfigurationFile(agentName, properties);    Files.copy(configFile, new File(configFilePath));    configFile.delete();    }
public synchronized File flume_f6426_0()
{    return stageDir;}
private File flume_f6427_1(String agentName, Properties properties) throws Exception
{    Preconditions.checkNotNull(properties, "properties object must not be null");    File file = File.createTempFile("agent", "config.properties", stageDir);    OutputStream os = null;    try {        os = new FileOutputStream(file);        properties.store(os, "Config file for agent: " + agentName);    } catch (Exception ex) {                throw ex;    } finally {        if (os != null) {            try {                os.close();            } catch (Exception ex) {                            }        }    }    return file;}
private void flume_f6428_1(File file) throws Exception
{    String[] args = { "chmod", "+x", file.getCanonicalPath() };    Runtime.getRuntime().exec(args);    }
private void flume_f6429_1(File tarFile, File destDir) throws Exception
{    TarArchiveInputStream tarInputStream = null;    try {        tarInputStream = new TarArchiveInputStream(new FileInputStream(tarFile));        TarArchiveEntry entry = null;        while ((entry = tarInputStream.getNextTarEntry()) != null) {            String name = entry.getName();                        File destFile = new File(destDir, entry.getName());            if (entry.isDirectory()) {                destFile.mkdirs();                continue;            }            File destParent = destFile.getParentFile();            destParent.mkdirs();            OutputStream entryOutputStream = null;            try {                entryOutputStream = new FileOutputStream(destFile);                byte[] buffer = new byte[2048];                int length = 0;                while ((length = tarInputStream.read(buffer, 0, 2048)) != -1) {                    entryOutputStream.write(buffer, 0, length);                }            } catch (Exception ex) {                                throw ex;            } finally {                if (entryOutputStream != null) {                    try {                        entryOutputStream.close();                    } catch (Exception ex) {                                            }                }            }        }    } catch (Exception ex) {                throw ex;    } finally {        if (tarInputStream != null) {            try {                tarInputStream.close();            } catch (Exception ex) {                            }        }    }}
private File flume_f6430_1(File tarballFile, File destDir) throws Exception
{    File tarFile = null;    InputStream tarballInputStream = null;    OutputStream tarFileOutputStream = null;    try {        tarballInputStream = new GZIPInputStream(new FileInputStream(tarballFile));        File temp2File = File.createTempFile("flume", "-bin", destDir);        String temp2FilePath = temp2File.getCanonicalPath();        temp2File.delete();        tarFile = new File(temp2FilePath + ".tar");                tarFileOutputStream = new FileOutputStream(tarFile);        int length = 0;        byte[] buffer = new byte[10240];        while ((length = tarballInputStream.read(buffer, 0, 10240)) != -1) {            tarFileOutputStream.write(buffer, 0, length);        }    } catch (Exception ex) {                throw ex;    } finally {        if (tarballInputStream != null) {            try {                tarballInputStream.close();            } catch (Exception ex) {                            }        }        if (tarFileOutputStream != null) {            try {                tarFileOutputStream.close();            } catch (Exception ex) {                            }        }    }    return tarFile;}
private File flume_f6431_1() throws Exception
{    File targetDir = new File("target");    if (!targetDir.exists() || !targetDir.isDirectory()) {                targetDir = new File(System.getProperty("java.io.tmpdir"));    }    File testDir = new File(targetDir, "test");    testDir.mkdirs();    File tempFile = File.createTempFile("flume", "_stage", testDir);    String absFileName = tempFile.getCanonicalPath();    tempFile.delete();    File stageDir = new File(absFileName + "_dir");    if (stageDir.exists()) {        throw new Exception("Stage directory exists: " + stageDir.getCanonicalPath());    }    stageDir.mkdirs();        return stageDir;}
private String flume_f6432_1() throws Exception
{    String tarballPath = null;    File dir = new File("..");    while (dir != null && dir.isDirectory()) {        File testFile = new File(dir, "flume-ng-dist/target");        if (testFile.exists() && testFile.isDirectory()) {                        File[] candidateFiles = testFile.listFiles(new FileFilter() {                @Override                public boolean accept(File pathname) {                    String name = pathname.getName();                    if (name != null && name.startsWith("apache-flume-") && name.endsWith("-bin.tar.gz")) {                        return true;                    }                    return false;                }            });                        if (candidateFiles != null && candidateFiles.length > 0) {                if (candidateFiles.length == 1) {                                        File file = candidateFiles[0];                    if (file.isFile() && file.canRead()) {                        tarballPath = file.getCanonicalPath();                                                break;                    } else {                                            }                } else {                    StringBuilder sb = new StringBuilder("Multiple candate tarballs");                    sb.append(" found in directory ");                    sb.append(testFile.getCanonicalPath()).append(": ");                    boolean first = true;                    for (File file : candidateFiles) {                        if (first) {                            first = false;                            sb.append(" ");                        } else {                            sb.append(", ");                        }                        sb.append(file.getCanonicalPath());                    }                    sb.append(". All these files will be ignored.");                                    }            }        }        dir = dir.getParentFile();    }    return tarballPath;}
public boolean flume_f6433_0(File pathname)
{    String name = pathname.getName();    if (name != null && name.startsWith("apache-flume-") && name.endsWith("-bin.tar.gz")) {        return true;    }    return false;}
public static void flume_f6434_0(String host, int port, long timeout) throws IOException, InterruptedException
{    long startTime = System.currentTimeMillis();    Socket socket;    boolean connected = false;        while (System.currentTimeMillis() - startTime < timeout) {        try {            socket = new Socket(host, port);            socket.close();            connected = true;            break;        } catch (IOException e) {            Thread.sleep(2000);        }    }    if (!connected) {        throw new IOException("Port not opened within specified timeout.");    }}
public void flume_f6435_0()
{    synchronized (StagedInstall.this) {        if (StagedInstall.this.process != null) {            process.destroy();        }    }}
public void flume_f6436_1()
{    try {        byte[] buffer = new byte[1024];        int length = 0;        while ((length = is.read(buffer, 0, 1024)) != -1) {                    }    } catch (Exception ex) {            }}
public String flume_f6437_0()
{    return syslogSourceType;}
public void flume_f6438_0() throws IOException
{    ServerSocket s = new ServerSocket(0);    port = s.getLocalPort();    s.close();}
public void flume_f6439_1(SyslogSourceType sourceType) throws IOException
{    /* Create 3 temp dirs, each used as value within agentProps */    sinkOutputDir = Files.createTempDir();    tempResources.add(sinkOutputDir);    final String sinkOutputDirPath = sinkOutputDir.getCanonicalPath();        /* Build props to pass to flume agent */    agentProps = new Properties();        agentProps.put("a1.channels", "c1");    agentProps.put("a1.sources", "r1");    agentProps.put("a1.sinks", "k1");        agentProps.put("a1.channels.c1.type", "memory");    agentProps.put("a1.channels.c1.capacity", "1000");    agentProps.put("a1.channels.c1.transactionCapacity", "100");        agentProps.put("a1.sources.r1.channels", "c1");    agentProps.put("a1.sources.r1.type", sourceType.toString());    agentProps.put("a1.sources.r1.host", hostname);    if (sourceType.equals(SyslogSourceType.MULTIPORTTCP)) {        agentProps.put("a1.sources.r1.ports", Integer.toString(port));    } else {        agentProps.put("a1.sources.r1.port", Integer.toString(port));    }        agentProps.put("a1.sinks.k1.channel", "c1");    agentProps.put("a1.sinks.k1.sink.directory", sinkOutputDirPath);    agentProps.put("a1.sinks.k1.type", "FILE_ROLL");    agentProps.put("a1.sinks.k1.sink.rollInterval", "0");}
public void flume_f6440_1(String keepFields) throws Exception
{    this.keepFields = keepFields;        agentProps.put("a1.sources.r1.keepFields", keepFields);        sinkOutputDir.mkdir();    /* Start flume agent */    StagedInstall.getInstance().startAgent("a1", agentProps);            int numberOfAttempts = 0;    while (client == null) {        try {            client = new BufferedOutputStream(new Socket(hostname, port).getOutputStream());        } catch (IOException e) {            if (++numberOfAttempts >= DEFAULT_ATTEMPTS) {                throw new AssertionError("Could not connect to source after " + DEFAULT_ATTEMPTS + " attempts with " + DEFAULT_TIMEOUT + " ms timeout.");            }            TimeUnit.MILLISECONDS.sleep(DEFAULT_TIMEOUT);        }    }}
public boolean flume_f6441_0() throws Exception
{    return StagedInstall.getInstance().isRunning();}
public void flume_f6442_0() throws Exception
{    if (client != null) {        client.close();    }    client = null;    StagedInstall.getInstance().stopAgent();    for (File tempResource : tempResources) {                FileUtils.deleteDirectory(tempResource);    }}
public void flume_f6443_1() throws Exception
{    /* Create expected output and log message */    String logMessage = "<34>1 Oct 11 22:14:15 mymachine su: Test\n";    String expectedOutput = "su: Test\n";    if (keepFields.equals("true") || keepFields.equals("all")) {        expectedOutput = logMessage;    } else if (!keepFields.equals("false") && !keepFields.equals("none")) {        if (keepFields.indexOf("hostname") != -1) {            expectedOutput = "mymachine " + expectedOutput;        }        if (keepFields.indexOf("timestamp") != -1) {            expectedOutput = "Oct 11 22:14:15 " + expectedOutput;        }        if (keepFields.indexOf("version") != -1) {            expectedOutput = "1 " + expectedOutput;        }        if (keepFields.indexOf("priority") != -1) {            expectedOutput = "<34>" + expectedOutput;        }    }        /* Send test message to agent */    sendMessage(logMessage);    /* Wait for output file */    int numberOfListDirAttempts = 0;    while (sinkOutputDir.listFiles().length == 0) {        if (++numberOfListDirAttempts >= DEFAULT_ATTEMPTS) {            throw new AssertionError("FILE_ROLL sink hasn't written any files after " + DEFAULT_ATTEMPTS + " attempts with " + DEFAULT_TIMEOUT + " ms timeout.");        }        TimeUnit.MILLISECONDS.sleep(DEFAULT_TIMEOUT);    }        File[] sinkOutputDirChildren = sinkOutputDir.listFiles();    Assert.assertEquals("Expected FILE_ROLL sink's dir to have only 1 child," + " but found " + sinkOutputDirChildren.length + " children.", 1, sinkOutputDirChildren.length);    /* Wait for output file stats to be as expected. */    File outputDirChild = sinkOutputDirChildren[0];    int numberOfStatsAttempts = 0;    while (outputDirChild.length() != expectedOutput.length()) {        if (++numberOfStatsAttempts >= DEFAULT_ATTEMPTS) {            throw new AssertionError("Expected output and FILE_ROLL sink's" + " lengths did not match after " + DEFAULT_ATTEMPTS + " attempts with " + DEFAULT_TIMEOUT + " ms timeout.");        }        TimeUnit.MILLISECONDS.sleep(DEFAULT_TIMEOUT);    }    File actualOutput = sinkOutputDirChildren[0];    if (!Files.toString(actualOutput, Charsets.UTF_8).equals(expectedOutput)) {                        throw new AssertionError("FILE_ROLL sink's actual output doesn't " + "match expected output.");    }}
private void flume_f6444_0(String message) throws IOException
{    client.write(message.getBytes());    client.flush();}
public static void flume_f6445_0(Properties kafkaProps)
{    if (isSSLEnabled(kafkaProps)) {        addGlobalSSLParameter(kafkaProps, SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, SSLUtil.getGlobalKeystorePath());        addGlobalSSLParameter(kafkaProps, SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, SSLUtil.getGlobalKeystorePassword());        addGlobalSSLParameter(kafkaProps, SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, SSLUtil.getGlobalKeystoreType(null));        addGlobalSSLParameter(kafkaProps, SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, SSLUtil.getGlobalTruststorePath());        addGlobalSSLParameter(kafkaProps, SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, SSLUtil.getGlobalTruststorePassword());        addGlobalSSLParameter(kafkaProps, SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, SSLUtil.getGlobalTruststoreType(null));    }}
private static void flume_f6446_0(Properties kafkaProps, String propName, String globalValue)
{    if (!kafkaProps.containsKey(propName) && globalValue != null) {        kafkaProps.put(propName, globalValue);    }}
private static boolean flume_f6447_0(Properties kafkaProps)
{    String securityProtocol = kafkaProps.getProperty(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG);    return securityProtocol != null && (securityProtocol.equals(SecurityProtocol.SSL.name) || securityProtocol.equals(SecurityProtocol.SASL_SSL.name));}
public void flume_f6448_0()
{    System.setProperty("javax.net.ssl.keyStore", "global-keystore-path");    System.setProperty("javax.net.ssl.keyStorePassword", "global-keystore-password");    System.setProperty("javax.net.ssl.keyStoreType", "global-keystore-type");    System.setProperty("javax.net.ssl.trustStore", "global-truststore-path");    System.setProperty("javax.net.ssl.trustStorePassword", "global-truststore-password");    System.setProperty("javax.net.ssl.trustStoreType", "global-truststore-type");}
public void flume_f6449_0()
{    System.clearProperty("javax.net.ssl.keyStore");    System.clearProperty("javax.net.ssl.keyStorePassword");    System.clearProperty("javax.net.ssl.keyStoreType");    System.clearProperty("javax.net.ssl.trustStore");    System.clearProperty("javax.net.ssl.trustStorePassword");    System.clearProperty("javax.net.ssl.trustStoreType");}
public void flume_f6450_0()
{    Properties kafkaProps = new Properties();    kafkaProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, SecurityProtocol.PLAINTEXT.name);    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);    assertNoSSLParameters(kafkaProps);}
public void flume_f6451_0()
{    Properties kafkaProps = new Properties();    kafkaProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, SecurityProtocol.SASL_PLAINTEXT.name);    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);    assertNoSSLParameters(kafkaProps);}
public void flume_f6452_0()
{    Properties kafkaProps = new Properties();    kafkaProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, SecurityProtocol.SSL.name);    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);    assertGlobalSSLParameters(kafkaProps);}
public void flume_f6453_0()
{    Properties kafkaProps = new Properties();    kafkaProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, SecurityProtocol.SASL_SSL.name);    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);    assertGlobalSSLParameters(kafkaProps);}
public void flume_f6454_0()
{    Properties kafkaProps = new Properties();    kafkaProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, SecurityProtocol.SSL.name);    kafkaProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, "component-keystore-path");    kafkaProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "component-keystore-password");    kafkaProps.put(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, "component-keystore-type");    kafkaProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, "component-truststore-path");    kafkaProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "component-truststore-password");    kafkaProps.put(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, "component-truststore-type");    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);    assertComponentSSLParameters(kafkaProps);}
public void flume_f6455_0()
{    Properties kafkaProps = new Properties();    kafkaProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, SecurityProtocol.SSL.name);    clearSystemProperties();    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);    assertNoSSLParameters(kafkaProps);}
private void flume_f6456_0(Properties kafkaProps)
{    assertFalse(kafkaProps.containsKey(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG));    assertFalse(kafkaProps.containsKey(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG));    assertFalse(kafkaProps.containsKey(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG));    assertFalse(kafkaProps.containsKey(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));    assertFalse(kafkaProps.containsKey(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG));    assertFalse(kafkaProps.containsKey(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG));}
private void flume_f6457_0(Properties kafkaProps)
{    assertEquals("global-keystore-path", kafkaProps.getProperty(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG));    assertEquals("global-keystore-password", kafkaProps.getProperty(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG));    assertEquals("global-keystore-type", kafkaProps.getProperty(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG));    assertEquals("global-truststore-path", kafkaProps.getProperty(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));    assertEquals("global-truststore-password", kafkaProps.getProperty(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG));    assertEquals("global-truststore-type", kafkaProps.getProperty(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG));}
private void flume_f6458_0(Properties kafkaProps)
{    assertEquals("component-keystore-path", kafkaProps.getProperty(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG));    assertEquals("component-keystore-password", kafkaProps.getProperty(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG));    assertEquals("component-keystore-type", kafkaProps.getProperty(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG));    assertEquals("component-truststore-path", kafkaProps.getProperty(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));    assertEquals("component-truststore-password", kafkaProps.getProperty(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG));    assertEquals("component-truststore-type", kafkaProps.getProperty(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG));}
public static void flume_f6459_0(PartitionTestScenario scenario, Map<Integer, List<Event>> partitionMap, Map<Integer, List<byte[]>> resultsMap, int staticPtn, int numMsgs)
{    int numPtns = partitionMap.size();    if (scenario == PartitionTestScenario.NO_PARTITION_HEADERS && numMsgs % numPtns != 0) {        throw new IllegalArgumentException("This method is not designed to work with scenarios" + " where there is expected to be a non-even distribution of messages");    }    for (int ptn = 0; ptn < numPtns; ptn++) {        List<Event> expectedResults = partitionMap.get(ptn);        List<byte[]> actualResults = resultsMap.get(ptn);        if (scenario == PartitionTestScenario.PARTITION_ID_HEADER_ONLY || scenario == PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID) {                        Assert.assertEquals(expectedResults.size(), actualResults.size());                        for (int idx = 0; idx < expectedResults.size(); idx++) {                Assert.assertArrayEquals(expectedResults.get(idx).getBody(), actualResults.get(idx));            }        } else if (scenario == PartitionTestScenario.STATIC_HEADER_ONLY) {                        if (ptn == staticPtn) {                Assert.assertEquals(numMsgs, actualResults.size());            } else {                Assert.assertEquals(0, actualResults.size());            }        } else if (scenario == PartitionTestScenario.NO_PARTITION_HEADERS) {                        Assert.assertEquals(numMsgs / numPtns, actualResults.size());        }    }}
public static List<Event> flume_f6460_0(PartitionTestScenario scenario, int numMsgs, Map<Integer, List<Event>> partitionMap, int numPtns, int staticPtn)
{    List<Event> msgs = new ArrayList<Event>(numMsgs);        if (numMsgs < 0) {        throw new IllegalArgumentException("Number of messages must be greater than zero");    }    if (staticPtn >= numPtns) {        throw new IllegalArgumentException("The static partition must be less than the " + "number of partitions");    }    if (numPtns < 5) {        throw new IllegalArgumentException("This method is designed to work with at least 5 " + "partitions");    }    if (partitionMap.size() != numPtns) {        throw new IllegalArgumentException("partitionMap has not been correctly initialised");    }    for (int i = 0; i < numMsgs; i++) {        Map<String, String> headers = new HashMap<String, String>();        Integer partition = null;        if (scenario == PartitionTestScenario.NO_PARTITION_HEADERS) {                } else if (scenario == PartitionTestScenario.STATIC_HEADER_ONLY) {            partition = staticPtn;        } else {                        if (i % 5 == 0) {                partition = 4;                headers.put(PARTITION_HEADER, String.valueOf(partition));            } else if (i % 3 == 0) {                partition = 3;                headers.put(PARTITION_HEADER, String.valueOf(partition));            } else if (scenario == PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID) {                                                partition = staticPtn;            } else if (scenario == PartitionTestScenario.PARTITION_ID_HEADER_ONLY) {                partition = 2;                headers.put(PARTITION_HEADER, String.valueOf(partition));            }                }                Event event = EventBuilder.withBody(String.valueOf(i).getBytes(), headers);        if (scenario != PartitionTestScenario.NO_PARTITION_HEADERS) {                        partitionMap.get(partition).add(event);        }                msgs.add(event);    }    return msgs;}
public static Map<Integer, List<byte[]>> flume_f6461_0(String topic, int numPtns, Properties consumerProperties)
{    Map<Integer, List<byte[]>> resultsMap = new HashMap<Integer, List<byte[]>>();    for (int i = 0; i < numPtns; i++) {        List<byte[]> partitionResults = new ArrayList<byte[]>();        resultsMap.put(i, partitionResults);        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<String, byte[]>(consumerProperties);        TopicPartition partition = new TopicPartition(topic, i);        consumer.assign(Arrays.asList(partition));        ConsumerRecords<String, byte[]> records = consumer.poll(1000);        for (ConsumerRecord<String, byte[]> record : records) {            partitionResults.add(record.value());        }        consumer.close();    }    return resultsMap;}
public boolean flume_f6462_0(Event event)
{    return true;}
public void flume_f6463_1(String[] args) throws IOException, ParseException
{    boolean shouldContinue = parseCommandLineOpts(args);    if (!shouldContinue) {                System.exit(1);    }    for (File dataDir : dataDirs) {        File[] dataFiles = dataDir.listFiles(new FilenameFilter() {            @Override            public boolean accept(File dir, String name) {                if (!name.endsWith(Serialization.METADATA_FILENAME) && !name.endsWith(Serialization.METADATA_TMP_FILENAME) && !name.endsWith(Serialization.OLD_METADATA_FILENAME) && !name.equals(Log.FILE_LOCK)) {                    return true;                }                return false;            }        });        if (dataFiles != null && dataFiles.length > 0) {            for (File dataFile : dataFiles) {                                LogFile.SequentialReader reader = new LogFileV3.SequentialReader(dataFile, null, true);                LogFile.OperationRecordUpdater updater = new LogFile.OperationRecordUpdater(dataFile);                boolean fileDone = false;                boolean fileBackedup = false;                while (!fileDone) {                    long eventPosition = 0;                    try {                                                                        eventPosition = reader.getPosition();                                                                                                LogRecord record = reader.next();                        totalChannelEvents++;                        if (record != null) {                            TransactionEventRecord recordEvent = record.getEvent();                            Event event = EventUtils.getEventFromTransactionEvent(recordEvent);                            if (event != null) {                                totalPutEvents++;                                try {                                    if (!eventValidator.validateEvent(event)) {                                        if (!fileBackedup) {                                            Serialization.copyFile(dataFile, new File(dataFile.getParent(), dataFile.getName() + ".bak"));                                            fileBackedup = true;                                        }                                        invalidEvents++;                                        updater.markRecordAsNoop(eventPosition);                                    } else {                                        validEvents++;                                    }                                } catch (Exception e) {                                                                                                                                                System.err.println("Encountered Exception while validating event, " + "marking as invalid");                                    updater.markRecordAsNoop(eventPosition);                                    eventsWithException++;                                }                            }                        } else {                            fileDone = true;                        }                    } catch (CorruptEventException e) {                        corruptEvents++;                        totalChannelEvents++;                                                if (!fileBackedup) {                            Serialization.copyFile(dataFile, new File(dataFile.getParent(), dataFile.getName() + ".bak"));                            fileBackedup = true;                        }                        updater.markRecordAsNoop(eventPosition);                    }                }                updater.close();                reader.close();            }        }    }    printSummary();}
public boolean flume_f6464_0(File dir, String name)
{    if (!name.endsWith(Serialization.METADATA_FILENAME) && !name.endsWith(Serialization.METADATA_TMP_FILENAME) && !name.endsWith(Serialization.OLD_METADATA_FILENAME) && !name.equals(Log.FILE_LOCK)) {        return true;    }    return false;}
private boolean flume_f6465_0(String[] args) throws ParseException
{    Options options = new Options();    options.addOption("l", "dataDirs", true, "Comma-separated list of data " + "directories which the tool must verify. This option is mandatory").addOption("h", "help", false, "Display help").addOption("e", "eventValidator", true, "Fully Qualified Name of Event Validator Implementation");    Option property = OptionBuilder.withArgName("property=value").hasArgs(2).withValueSeparator().withDescription("custom properties").create("D");    options.addOption(property);    CommandLineParser parser = new GnuParser();    CommandLine commandLine = parser.parse(options, args);    if (commandLine.hasOption("help")) {        new HelpFormatter().printHelp("bin/flume-ng tool fcintegritytool ", options, true);        return false;    }    if (!commandLine.hasOption("dataDirs")) {        new HelpFormatter().printHelp("bin/flume-ng tool fcintegritytool ", "", options, "dataDirs is required.", true);        return false;    } else {        String[] dataDirStr = commandLine.getOptionValue("dataDirs").split(",");        for (String dataDir : dataDirStr) {            File f = new File(dataDir);            if (!f.exists()) {                throw new FlumeException("Data directory, " + dataDir + " does not exist.");            }            dataDirs.add(f);        }    }    if (commandLine.hasOption("eventValidator")) {        try {            Class<? extends EventValidator.Builder> eventValidatorClassName = (Class<? extends EventValidator.Builder>) Class.forName(commandLine.getOptionValue("eventValidator"));            EventValidator.Builder eventValidatorBuilder = eventValidatorClassName.newInstance();                        Properties systemProperties = commandLine.getOptionProperties("D");            Context context = new Context();            Set<String> keys = systemProperties.stringPropertyNames();            for (String key : keys) {                context.put(key, systemProperties.getProperty(key));            }            eventValidatorBuilder.configure(context);            eventValidator = eventValidatorBuilder.build();        } catch (Exception e) {            System.err.println(String.format("Could find class %s in lib folder", commandLine.getOptionValue("eventValidator")));            e.printStackTrace();            return false;        }    }    return true;}
private void flume_f6466_0()
{    System.out.println("---------- Summary --------------------");    System.out.println("Number of Events in the Channel = " + totalChannelEvents++);    System.out.println("Number of Put Events Processed = " + totalPutEvents);    System.out.println("Number of Valid Put Events = " + validEvents);    System.out.println("Number of Invalid Put Events = " + invalidEvents);    System.out.println("Number of Put Events that threw Exception during validation = " + eventsWithException);    System.out.println("Number of Corrupt Events = " + corruptEvents);    System.out.println("---------------------------------------");}
public static void flume_f6467_0(String[] args) throws Exception
{    new FlumeToolsMain().run(args);}
public void flume_f6468_0(String[] args) throws Exception
{    String error = "Expected name of tool and arguments for" + " tool to be passed in on the command line. Please pass one of the " + "following as arguments to this command: \n";    StringBuilder builder = new StringBuilder(error);    for (FlumeToolType type : FlumeToolType.values()) {        builder.append(type.name()).append("\n");    }    if (args == null || args.length == 0) {        System.out.println(builder.toString());        System.exit(1);    }    String toolName = args[0];    FlumeTool tool = null;    for (FlumeToolType type : FlumeToolType.values()) {        if (toolName.equalsIgnoreCase(type.name())) {            tool = type.getClassInstance().newInstance();            break;        }    }    Preconditions.checkNotNull(tool, "Cannot find tool matching " + toolName + ". Please select one of: \n " + FlumeToolType.getNames());    if (args.length == 1) {        tool.run(new String[0]);    } else {        tool.run(Arrays.asList(args).subList(1, args.length).toArray(new String[0]));    }}
public Class<? extends FlumeTool> flume_f6469_0()
{    return this.klass;}
public static String flume_f6470_0()
{    StringBuilder builder = new StringBuilder();    for (FlumeToolType type : values()) {        builder.append(type.name().toLowerCase(Locale.ENGLISH) + "\n");    }    return builder.toString();}
public static void flume_f6471_0() throws Exception
{    createDataFiles();}
public void flume_f6472_0() throws Exception
{    checkpointDir = new File(baseDir, "checkpoint");    dataDir = new File(baseDir, "dataDir");    Assert.assertTrue(checkpointDir.mkdirs() || checkpointDir.isDirectory());    Assert.assertTrue(dataDir.mkdirs() || dataDir.isDirectory());    File[] dataFiles = origDataDir.listFiles(new FilenameFilter() {        @Override        public boolean accept(File dir, String name) {            if (name.contains("lock")) {                return false;            }            return true;        }    });    for (File dataFile : dataFiles) {        Serialization.copyFile(dataFile, new File(dataDir, dataFile.getName()));    }}
public boolean flume_f6473_0(File dir, String name)
{    if (name.contains("lock")) {        return false;    }    return true;}
public void flume_f6474_0() throws Exception
{    FileUtils.deleteDirectory(checkpointDir);    FileUtils.deleteDirectory(dataDir);}
public static void flume_f6475_0() throws Exception
{    FileUtils.deleteDirectory(origCheckpointDir);    FileUtils.deleteDirectory(origDataDir);}
public void flume_f6476_0() throws Exception
{    doTestFixCorruptEvents(true);}
public void flume_f6477_0() throws Exception
{    doTestFixCorruptEvents(false);}
public void flume_f6478_0() throws Exception
{    doTestFixInvalidEvents(false, DummyEventVerifier.Builder.class.getName());}
public void flume_f6479_0() throws Exception
{    doTestFixInvalidEvents(true, DummyEventVerifier.Builder.class.getName());}
public void flume_f6480_0(boolean withCheckpoint, String eventHandler) throws Exception
{    FileChannelIntegrityTool tool = new FileChannelIntegrityTool();    tool.run(new String[] { "-l", dataDir.toString(), "-e", eventHandler, "-DvalidatorValue=0" });    FileChannel channel = new FileChannel();    channel.setName("channel");    if (withCheckpoint) {        File[] cpFiles = origCheckpointDir.listFiles(new FilenameFilter() {            @Override            public boolean accept(File dir, String name) {                if (name.contains("lock") || name.contains("queueset")) {                    return false;                }                return true;            }        });        for (File cpFile : cpFiles) {            Serialization.copyFile(cpFile, new File(checkpointDir, cpFile.getName()));        }    } else {        FileUtils.deleteDirectory(checkpointDir);        Assert.assertTrue(checkpointDir.mkdirs());    }    ctx.put(FileChannelConfiguration.CHECKPOINT_DIR, checkpointDir.toString());    ctx.put(FileChannelConfiguration.DATA_DIRS, dataDir.toString());    channel.configure(ctx);    channel.start();    Transaction tx = channel.getTransaction();    tx.begin();    int i = 0;    while (channel.take() != null) {        i++;    }    tx.commit();    tx.close();    channel.stop();    Assert.assertTrue(invalidEvent != 0);    Assert.assertEquals(25 - invalidEvent, i);}
public boolean flume_f6481_0(File dir, String name)
{    if (name.contains("lock") || name.contains("queueset")) {        return false;    }    return true;}
public void flume_f6482_0(boolean withCheckpoint) throws Exception
{    Set<String> corruptFiles = new HashSet<String>();    File[] files = dataDir.listFiles(new FilenameFilter() {        @Override        public boolean accept(File dir, String name) {            if (name.contains("lock") || name.contains("meta")) {                return false;            }            return true;        }    });    Random random = new Random();    int corrupted = 0;    for (File dataFile : files) {        LogFile.SequentialReader reader = new LogFileV3.SequentialReader(dataFile, null, true);        RandomAccessFile handle = new RandomAccessFile(dataFile, "rw");        long eventPosition1 = reader.getPosition();        LogRecord rec = reader.next();                if (rec == null || rec.getEvent().getClass().getName().equals("org.apache.flume.channel.file.Commit")) {            handle.close();            reader.close();            continue;        }        long eventPosition2 = reader.getPosition();        rec = reader.next();        handle.seek(eventPosition1 + 100);        handle.writeInt(random.nextInt());        corrupted++;        corruptFiles.add(dataFile.getName());        if (rec == null || rec.getEvent().getClass().getName().equals("org.apache.flume.channel.file.Commit")) {            handle.close();            reader.close();            continue;        }        handle.seek(eventPosition2 + 100);        handle.writeInt(random.nextInt());        corrupted++;        handle.close();        reader.close();    }    FileChannelIntegrityTool tool = new FileChannelIntegrityTool();    tool.run(new String[] { "-l", dataDir.toString() });    FileChannel channel = new FileChannel();    channel.setName("channel");    if (withCheckpoint) {        File[] cpFiles = origCheckpointDir.listFiles(new FilenameFilter() {            @Override            public boolean accept(File dir, String name) {                if (name.contains("lock") || name.contains("queueset")) {                    return false;                }                return true;            }        });        for (File cpFile : cpFiles) {            Serialization.copyFile(cpFile, new File(checkpointDir, cpFile.getName()));        }    } else {        FileUtils.deleteDirectory(checkpointDir);        Assert.assertTrue(checkpointDir.mkdirs());    }    ctx.put(FileChannelConfiguration.CHECKPOINT_DIR, checkpointDir.toString());    ctx.put(FileChannelConfiguration.DATA_DIRS, dataDir.toString());    channel.configure(ctx);    channel.start();    Transaction tx = channel.getTransaction();    tx.begin();    int i = 0;    while (channel.take() != null) {        i++;    }    tx.commit();    tx.close();    channel.stop();    Assert.assertEquals(25 - corrupted, i);    files = dataDir.listFiles(new FilenameFilter() {        @Override        public boolean accept(File dir, String name) {            if (name.contains(".bak")) {                return true;            }            return false;        }    });    Assert.assertEquals(corruptFiles.size(), files.length);    for (File file : files) {        String name = file.getName();        name = name.replaceAll(".bak", "");        Assert.assertTrue(corruptFiles.remove(name));    }    Assert.assertTrue(corruptFiles.isEmpty());}
public boolean flume_f6483_0(File dir, String name)
{    if (name.contains("lock") || name.contains("meta")) {        return false;    }    return true;}
public boolean flume_f6484_0(File dir, String name)
{    if (name.contains("lock") || name.contains("queueset")) {        return false;    }    return true;}
public boolean flume_f6485_0(File dir, String name)
{    if (name.contains(".bak")) {        return true;    }    return false;}
private static void flume_f6486_0() throws Exception
{    final byte[] eventData = new byte[2000];    for (int i = 0; i < 2000; i++) {        eventData[i] = 1;    }    WriteOrderOracle.setSeed(System.currentTimeMillis());    event = EventBuilder.withBody(eventData);    baseDir = Files.createTempDir();    if (baseDir.exists()) {        FileUtils.deleteDirectory(baseDir);    }    baseDir = Files.createTempDir();    origCheckpointDir = new File(baseDir, "chkpt");    Assert.assertTrue(origCheckpointDir.mkdirs() || origCheckpointDir.isDirectory());    origDataDir = new File(baseDir, "data");    Assert.assertTrue(origDataDir.mkdirs() || origDataDir.isDirectory());    FileChannel channel = new FileChannel();    channel.setName("channel");    ctx = new Context();    ctx.put(FileChannelConfiguration.CAPACITY, "1000");    ctx.put(FileChannelConfiguration.CHECKPOINT_DIR, origCheckpointDir.toString());    ctx.put(FileChannelConfiguration.DATA_DIRS, origDataDir.toString());    ctx.put(FileChannelConfiguration.MAX_FILE_SIZE, "10000");    ctx.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "100");    channel.configure(ctx);    channel.start();    for (int j = 0; j < 5; j++) {        Transaction tx = channel.getTransaction();        tx.begin();        for (int i = 0; i < 5; i++) {            if (i % 3 == 0) {                event.getBody()[0] = 0;                invalidEvent++;            } else {                event.getBody()[0] = 1;            }            channel.put(event);        }        tx.commit();        tx.close();    }    Log log = field("log").ofType(Log.class).in(channel).get();    Assert.assertTrue("writeCheckpoint returned false", method("writeCheckpoint").withReturnType(Boolean.class).withParameterTypes(Boolean.class).in(log).invoke(true));    channel.stop();}
public boolean flume_f6487_0(Event event)
{    return event.getBody()[0] != value;}
public EventValidator flume_f6488_0()
{    return new DummyEventVerifier(binaryValidator);}
public void flume_f6489_0(Context context)
{    binaryValidator = context.getInteger("validatorValue");}
