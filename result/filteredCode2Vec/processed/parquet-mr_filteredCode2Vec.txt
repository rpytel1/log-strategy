public GroupType parquet-mr_f0_0()
{    return list;}
public GroupType parquet-mr_f1_0()
{    return repeated;}
public Type parquet-mr_f2_0()
{    return element;}
public SchemaMapping parquet-mr_f3_0(Schema arrowSchema)
{    List<Field> fields = arrowSchema.getFields();    List<TypeMapping> parquetFields = fromArrow(fields);    MessageType parquetType = addToBuilder(parquetFields, Types.buildMessage()).named("root");    return new SchemaMapping(arrowSchema, parquetType, parquetFields);}
private GroupBuilder<T> parquet-mr_f4_0(List<TypeMapping> parquetFields, GroupBuilder<T> builder)
{    for (TypeMapping type : parquetFields) {        builder = builder.addField(type.getParquetType());    }    return builder;}
private List<TypeMapping> parquet-mr_f5_0(List<Field> fields)
{    List<TypeMapping> result = new ArrayList<>(fields.size());    for (Field field : fields) {        result.add(fromArrow(field));    }    return result;}
private TypeMapping parquet-mr_f6_0(final Field field)
{    return fromArrow(field, field.getName());}
private TypeMapping parquet-mr_f7_0(final Field field, final String fieldName)
{    final List<Field> children = field.getChildren();    return field.getType().accept(new ArrowTypeVisitor<TypeMapping>() {        @Override        public TypeMapping visit(Null type) {                        return primitive(BINARY);        }        @Override        public TypeMapping visit(Struct type) {            List<TypeMapping> parquetTypes = fromArrow(children);            return new StructTypeMapping(field, addToBuilder(parquetTypes, Types.buildGroup(OPTIONAL)).named(fieldName), parquetTypes);        }        @Override        public TypeMapping visit(org.apache.arrow.vector.types.pojo.ArrowType.List type) {            return createListTypeMapping();        }        @Override        public TypeMapping visit(org.apache.arrow.vector.types.pojo.ArrowType.FixedSizeList type) {            return createListTypeMapping();        }        private ListTypeMapping createListTypeMapping() {            if (children.size() != 1) {                throw new IllegalArgumentException("list fields must have exactly one child: " + field);            }            TypeMapping parquetChild = fromArrow(children.get(0), "element");            GroupType list = Types.optionalList().element(parquetChild.getParquetType()).named(fieldName);            return new ListTypeMapping(field, new List3Levels(list), parquetChild);        }        @Override        public TypeMapping visit(Union type) {                        List<TypeMapping> parquetTypes = fromArrow(children);            return new UnionTypeMapping(field, addToBuilder(parquetTypes, Types.buildGroup(OPTIONAL)).named(fieldName), parquetTypes);        }        @Override        public TypeMapping visit(Int type) {            boolean signed = type.getIsSigned();            switch(type.getBitWidth()) {                case 8:                case 16:                case 32:                    return primitive(INT32, intType(type.getBitWidth(), signed));                case 64:                    return primitive(INT64, intType(64, signed));                default:                    throw new IllegalArgumentException("Illegal int type: " + field);            }        }        @Override        public TypeMapping visit(FloatingPoint type) {            switch(type.getPrecision()) {                case HALF:                                        return primitive(FLOAT);                case SINGLE:                    return primitive(FLOAT);                case DOUBLE:                    return primitive(DOUBLE);                default:                    throw new IllegalArgumentException("Illegal float type: " + field);            }        }        @Override        public TypeMapping visit(Utf8 type) {            return primitive(BINARY, stringType());        }        @Override        public TypeMapping visit(Binary type) {            return primitive(BINARY);        }        @Override        public TypeMapping visit(Bool type) {            return primitive(BOOLEAN);        }        /**         * See https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#decimal         * @param type an arrow decimal type         * @return a mapping from the arrow decimal to the Parquet type         */        @Override        public TypeMapping visit(Decimal type) {            int precision = type.getPrecision();            int scale = type.getScale();            if (1 <= precision && precision <= 9) {                return decimal(INT32, precision, scale);            } else if (1 <= precision && precision <= 18) {                return decimal(INT64, precision, scale);            } else {                                return decimal(BINARY, precision, scale);            }        }        @Override        public TypeMapping visit(Date type) {            return primitive(INT32, dateType());        }        @Override        public TypeMapping visit(Time type) {            int bitWidth = type.getBitWidth();            TimeUnit timeUnit = type.getUnit();            if (bitWidth == 32 && timeUnit == TimeUnit.MILLISECOND) {                return primitive(INT32, timeType(false, MILLIS));            } else if (bitWidth == 64 && timeUnit == TimeUnit.MICROSECOND) {                return primitive(INT64, timeType(false, MICROS));            } else if (bitWidth == 64 && timeUnit == TimeUnit.NANOSECOND) {                return primitive(INT64, timeType(false, NANOS));            }            throw new UnsupportedOperationException("Unsupported type " + type);        }        @Override        public TypeMapping visit(Timestamp type) {            TimeUnit timeUnit = type.getUnit();            if (timeUnit == TimeUnit.MILLISECOND) {                return primitive(INT64, timestampType(isUtcNormalized(type), MILLIS));            } else if (timeUnit == TimeUnit.MICROSECOND) {                return primitive(INT64, timestampType(isUtcNormalized(type), MICROS));            } else if (timeUnit == TimeUnit.NANOSECOND) {                return primitive(INT64, timestampType(isUtcNormalized(type), NANOS));            }            throw new UnsupportedOperationException("Unsupported type " + type);        }        private boolean isUtcNormalized(Timestamp timestamp) {            String timeZone = timestamp.getTimezone();            return timeZone != null && !timeZone.isEmpty();        }        /**         * See https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#interval         */        @Override        public TypeMapping visit(Interval type) {                        return primitiveFLBA(12, LogicalTypeAnnotation.IntervalLogicalTypeAnnotation.getInstance());        }        @Override        public TypeMapping visit(ArrowType.FixedSizeBinary fixedSizeBinary) {            return primitive(BINARY);        }        private TypeMapping mapping(PrimitiveType parquetType) {            return new PrimitiveTypeMapping(field, parquetType);        }        private TypeMapping decimal(PrimitiveTypeName type, int precision, int scale) {            return mapping(Types.optional(type).as(decimalType(scale, precision)).named(fieldName));        }        private TypeMapping primitive(PrimitiveTypeName type) {            return mapping(Types.optional(type).named(fieldName));        }        private TypeMapping primitive(PrimitiveTypeName type, LogicalTypeAnnotation otype) {            return mapping(Types.optional(type).as(otype).named(fieldName));        }        private TypeMapping primitiveFLBA(int length, LogicalTypeAnnotation otype) {            return mapping(Types.optional(FIXED_LEN_BYTE_ARRAY).length(length).as(otype).named(fieldName));        }    });}
public TypeMapping parquet-mr_f8_0(Null type)
{        return primitive(BINARY);}
public TypeMapping parquet-mr_f9_0(Struct type)
{    List<TypeMapping> parquetTypes = fromArrow(children);    return new StructTypeMapping(field, addToBuilder(parquetTypes, Types.buildGroup(OPTIONAL)).named(fieldName), parquetTypes);}
public TypeMapping parquet-mr_f10_0(org.apache.arrow.vector.types.pojo.ArrowType.List type)
{    return createListTypeMapping();}
public TypeMapping parquet-mr_f11_0(org.apache.arrow.vector.types.pojo.ArrowType.FixedSizeList type)
{    return createListTypeMapping();}
private ListTypeMapping parquet-mr_f12_0()
{    if (children.size() != 1) {        throw new IllegalArgumentException("list fields must have exactly one child: " + field);    }    TypeMapping parquetChild = fromArrow(children.get(0), "element");    GroupType list = Types.optionalList().element(parquetChild.getParquetType()).named(fieldName);    return new ListTypeMapping(field, new List3Levels(list), parquetChild);}
public TypeMapping parquet-mr_f13_0(Union type)
{        List<TypeMapping> parquetTypes = fromArrow(children);    return new UnionTypeMapping(field, addToBuilder(parquetTypes, Types.buildGroup(OPTIONAL)).named(fieldName), parquetTypes);}
public TypeMapping parquet-mr_f14_0(Int type)
{    boolean signed = type.getIsSigned();    switch(type.getBitWidth()) {        case 8:        case 16:        case 32:            return primitive(INT32, intType(type.getBitWidth(), signed));        case 64:            return primitive(INT64, intType(64, signed));        default:            throw new IllegalArgumentException("Illegal int type: " + field);    }}
public TypeMapping parquet-mr_f15_0(FloatingPoint type)
{    switch(type.getPrecision()) {        case HALF:                        return primitive(FLOAT);        case SINGLE:            return primitive(FLOAT);        case DOUBLE:            return primitive(DOUBLE);        default:            throw new IllegalArgumentException("Illegal float type: " + field);    }}
public TypeMapping parquet-mr_f16_0(Utf8 type)
{    return primitive(BINARY, stringType());}
public TypeMapping parquet-mr_f17_0(Binary type)
{    return primitive(BINARY);}
public TypeMapping parquet-mr_f18_0(Bool type)
{    return primitive(BOOLEAN);}
public TypeMapping parquet-mr_f19_0(Decimal type)
{    int precision = type.getPrecision();    int scale = type.getScale();    if (1 <= precision && precision <= 9) {        return decimal(INT32, precision, scale);    } else if (1 <= precision && precision <= 18) {        return decimal(INT64, precision, scale);    } else {                return decimal(BINARY, precision, scale);    }}
public TypeMapping parquet-mr_f20_0(Date type)
{    return primitive(INT32, dateType());}
public TypeMapping parquet-mr_f21_0(Time type)
{    int bitWidth = type.getBitWidth();    TimeUnit timeUnit = type.getUnit();    if (bitWidth == 32 && timeUnit == TimeUnit.MILLISECOND) {        return primitive(INT32, timeType(false, MILLIS));    } else if (bitWidth == 64 && timeUnit == TimeUnit.MICROSECOND) {        return primitive(INT64, timeType(false, MICROS));    } else if (bitWidth == 64 && timeUnit == TimeUnit.NANOSECOND) {        return primitive(INT64, timeType(false, NANOS));    }    throw new UnsupportedOperationException("Unsupported type " + type);}
public TypeMapping parquet-mr_f22_0(Timestamp type)
{    TimeUnit timeUnit = type.getUnit();    if (timeUnit == TimeUnit.MILLISECOND) {        return primitive(INT64, timestampType(isUtcNormalized(type), MILLIS));    } else if (timeUnit == TimeUnit.MICROSECOND) {        return primitive(INT64, timestampType(isUtcNormalized(type), MICROS));    } else if (timeUnit == TimeUnit.NANOSECOND) {        return primitive(INT64, timestampType(isUtcNormalized(type), NANOS));    }    throw new UnsupportedOperationException("Unsupported type " + type);}
private boolean parquet-mr_f23_0(Timestamp timestamp)
{    String timeZone = timestamp.getTimezone();    return timeZone != null && !timeZone.isEmpty();}
public TypeMapping parquet-mr_f24_0(Interval type)
{        return primitiveFLBA(12, LogicalTypeAnnotation.IntervalLogicalTypeAnnotation.getInstance());}
public TypeMapping parquet-mr_f25_0(ArrowType.FixedSizeBinary fixedSizeBinary)
{    return primitive(BINARY);}
private TypeMapping parquet-mr_f26_0(PrimitiveType parquetType)
{    return new PrimitiveTypeMapping(field, parquetType);}
private TypeMapping parquet-mr_f27_0(PrimitiveTypeName type, int precision, int scale)
{    return mapping(Types.optional(type).as(decimalType(scale, precision)).named(fieldName));}
private TypeMapping parquet-mr_f28_0(PrimitiveTypeName type)
{    return mapping(Types.optional(type).named(fieldName));}
private TypeMapping parquet-mr_f29_0(PrimitiveTypeName type, LogicalTypeAnnotation otype)
{    return mapping(Types.optional(type).as(otype).named(fieldName));}
private TypeMapping parquet-mr_f30_0(int length, LogicalTypeAnnotation otype)
{    return mapping(Types.optional(FIXED_LEN_BYTE_ARRAY).length(length).as(otype).named(fieldName));}
public SchemaMapping parquet-mr_f31_0(MessageType parquetSchema)
{    List<Type> fields = parquetSchema.getFields();    List<TypeMapping> mappings = fromParquet(fields);    List<Field> arrowFields = fields(mappings);    return new SchemaMapping(new Schema(arrowFields), parquetSchema, mappings);}
private List<Field> parquet-mr_f32_0(List<TypeMapping> mappings)
{    List<Field> result = new ArrayList<>(mappings.size());    for (TypeMapping typeMapping : mappings) {        result.add(typeMapping.getArrowField());    }    return result;}
private List<TypeMapping> parquet-mr_f33_0(List<Type> fields)
{    List<TypeMapping> result = new ArrayList<>(fields.size());    for (Type type : fields) {        result.add(fromParquet(type));    }    return result;}
private TypeMapping parquet-mr_f34_0(Type type)
{    return fromParquet(type, type.getName(), type.getRepetition());}
private TypeMapping parquet-mr_f35_0(Type type, String name, Repetition repetition)
{    if (repetition == REPEATED) {                TypeMapping child = fromParquet(type, null, REQUIRED);        Field arrowField = new Field(name, false, new ArrowType.List(), asList(child.getArrowField()));        return new RepeatedTypeMapping(arrowField, type, child);    }    if (type.isPrimitive()) {        return fromParquetPrimitive(type.asPrimitiveType(), name);    } else {        return fromParquetGroup(type.asGroupType(), name);    }}
private TypeMapping parquet-mr_f36_0(GroupType type, String name)
{    LogicalTypeAnnotation logicalType = type.getLogicalTypeAnnotation();    if (logicalType == null) {        List<TypeMapping> typeMappings = fromParquet(type.getFields());        Field arrowField = new Field(name, type.isRepetition(OPTIONAL), new Struct(), fields(typeMappings));        return new StructTypeMapping(arrowField, type, typeMappings);    } else {        return logicalType.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {            @Override            public Optional<TypeMapping> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType) {                List3Levels list3Levels = new List3Levels(type);                TypeMapping child = fromParquet(list3Levels.getElement(), null, list3Levels.getElement().getRepetition());                Field arrowField = new Field(name, type.isRepetition(OPTIONAL), new ArrowType.List(), asList(child.getArrowField()));                return of(new ListTypeMapping(arrowField, list3Levels, child));            }        }).orElseThrow(() -> new UnsupportedOperationException("Unsupported type " + type));    }}
public Optional<TypeMapping> parquet-mr_f37_0(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    List3Levels list3Levels = new List3Levels(type);    TypeMapping child = fromParquet(list3Levels.getElement(), null, list3Levels.getElement().getRepetition());    Field arrowField = new Field(name, type.isRepetition(OPTIONAL), new ArrowType.List(), asList(child.getArrowField()));    return of(new ListTypeMapping(arrowField, list3Levels, child));}
private TypeMapping parquet-mr_f38_0(final PrimitiveType type, final String name)
{    return type.getPrimitiveTypeName().convert(new PrimitiveType.PrimitiveTypeNameConverter<TypeMapping, RuntimeException>() {        private TypeMapping field(ArrowType arrowType) {            Field field = new Field(name, type.isRepetition(OPTIONAL), arrowType, null);            return new PrimitiveTypeMapping(field, type);        }        @Override        public TypeMapping convertFLOAT(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return field(new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE));        }        @Override        public TypeMapping convertDOUBLE(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return field(new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE));        }        @Override        public TypeMapping convertINT32(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();            if (logicalTypeAnnotation == null) {                return integer(32, true);            }            return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {                    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {                    return of(field(new ArrowType.Date(DateUnit.DAY)));                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {                    return timeLogicalType.getUnit() == MILLIS ? of(field(new ArrowType.Time(TimeUnit.MILLISECOND, 32))) : empty();                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {                    if (intLogicalType.getBitWidth() == 64) {                        return empty();                    }                    return of(integer(intLogicalType.getBitWidth(), intLogicalType.isSigned()));                }            }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));        }        @Override        public TypeMapping convertINT64(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();            if (logicalTypeAnnotation == null) {                return integer(64, true);            }            return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {                    return of(field(new ArrowType.Date(DateUnit.DAY)));                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {                    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {                    return of(integer(intLogicalType.getBitWidth(), intLogicalType.isSigned()));                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {                    if (timeLogicalType.getUnit() == MICROS) {                        return of(field(new ArrowType.Time(TimeUnit.MICROSECOND, 64)));                    } else if (timeLogicalType.getUnit() == NANOS) {                        return of(field(new ArrowType.Time(TimeUnit.NANOSECOND, 64)));                    }                    return empty();                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {                    switch(timestampLogicalType.getUnit()) {                        case MICROS:                            return of(field(new ArrowType.Timestamp(TimeUnit.MICROSECOND, getTimeZone(timestampLogicalType))));                        case MILLIS:                            return of(field(new ArrowType.Timestamp(TimeUnit.MILLISECOND, getTimeZone(timestampLogicalType))));                        case NANOS:                            return of(field(new ArrowType.Timestamp(TimeUnit.NANOSECOND, getTimeZone(timestampLogicalType))));                    }                    return empty();                }                private String getTimeZone(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {                    return timestampLogicalType.isAdjustedToUTC() ? "UTC" : null;                }            }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));        }        @Override        public TypeMapping convertINT96(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            if (convertInt96ToArrowTimestamp) {                return field(new ArrowType.Timestamp(TimeUnit.NANOSECOND, null));            } else {                return field(new ArrowType.Binary());            }        }        @Override        public TypeMapping convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();            if (logicalTypeAnnotation == null) {                return field(new ArrowType.Binary());            }            return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {                    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));                }            }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));        }        @Override        public TypeMapping convertBOOLEAN(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return field(new ArrowType.Bool());        }        @Override        public TypeMapping convertBINARY(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();            if (logicalTypeAnnotation == null) {                return field(new ArrowType.Binary());            }            return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType) {                    return of(field(new ArrowType.Utf8()));                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {                    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));                }            }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));        }        private TypeMapping decimal(int precision, int scale) {            return field(new ArrowType.Decimal(precision, scale));        }        private TypeMapping integer(int width, boolean signed) {            return field(new ArrowType.Int(width, signed));        }    });}
private TypeMapping parquet-mr_f39_0(ArrowType arrowType)
{    Field field = new Field(name, type.isRepetition(OPTIONAL), arrowType, null);    return new PrimitiveTypeMapping(field, type);}
public TypeMapping parquet-mr_f40_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return field(new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE));}
public TypeMapping parquet-mr_f41_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return field(new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE));}
public TypeMapping parquet-mr_f42_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();    if (logicalTypeAnnotation == null) {        return integer(32, true);    }    return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {            return of(field(new ArrowType.Date(DateUnit.DAY)));        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {            return timeLogicalType.getUnit() == MILLIS ? of(field(new ArrowType.Time(TimeUnit.MILLISECOND, 32))) : empty();        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {            if (intLogicalType.getBitWidth() == 64) {                return empty();            }            return of(integer(intLogicalType.getBitWidth(), intLogicalType.isSigned()));        }    }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));}
public Optional<TypeMapping> parquet-mr_f43_0(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));}
public Optional<TypeMapping> parquet-mr_f44_0(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    return of(field(new ArrowType.Date(DateUnit.DAY)));}
public Optional<TypeMapping> parquet-mr_f45_0(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    return timeLogicalType.getUnit() == MILLIS ? of(field(new ArrowType.Time(TimeUnit.MILLISECOND, 32))) : empty();}
public Optional<TypeMapping> parquet-mr_f46_0(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    if (intLogicalType.getBitWidth() == 64) {        return empty();    }    return of(integer(intLogicalType.getBitWidth(), intLogicalType.isSigned()));}
public TypeMapping parquet-mr_f47_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();    if (logicalTypeAnnotation == null) {        return integer(64, true);    }    return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {            return of(field(new ArrowType.Date(DateUnit.DAY)));        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {            return of(integer(intLogicalType.getBitWidth(), intLogicalType.isSigned()));        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {            if (timeLogicalType.getUnit() == MICROS) {                return of(field(new ArrowType.Time(TimeUnit.MICROSECOND, 64)));            } else if (timeLogicalType.getUnit() == NANOS) {                return of(field(new ArrowType.Time(TimeUnit.NANOSECOND, 64)));            }            return empty();        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {            switch(timestampLogicalType.getUnit()) {                case MICROS:                    return of(field(new ArrowType.Timestamp(TimeUnit.MICROSECOND, getTimeZone(timestampLogicalType))));                case MILLIS:                    return of(field(new ArrowType.Timestamp(TimeUnit.MILLISECOND, getTimeZone(timestampLogicalType))));                case NANOS:                    return of(field(new ArrowType.Timestamp(TimeUnit.NANOSECOND, getTimeZone(timestampLogicalType))));            }            return empty();        }        private String getTimeZone(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {            return timestampLogicalType.isAdjustedToUTC() ? "UTC" : null;        }    }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));}
public Optional<TypeMapping> parquet-mr_f48_0(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    return of(field(new ArrowType.Date(DateUnit.DAY)));}
public Optional<TypeMapping> parquet-mr_f49_0(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));}
public Optional<TypeMapping> parquet-mr_f50_0(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    return of(integer(intLogicalType.getBitWidth(), intLogicalType.isSigned()));}
public Optional<TypeMapping> parquet-mr_f51_0(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    if (timeLogicalType.getUnit() == MICROS) {        return of(field(new ArrowType.Time(TimeUnit.MICROSECOND, 64)));    } else if (timeLogicalType.getUnit() == NANOS) {        return of(field(new ArrowType.Time(TimeUnit.NANOSECOND, 64)));    }    return empty();}
public Optional<TypeMapping> parquet-mr_f52_0(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    switch(timestampLogicalType.getUnit()) {        case MICROS:            return of(field(new ArrowType.Timestamp(TimeUnit.MICROSECOND, getTimeZone(timestampLogicalType))));        case MILLIS:            return of(field(new ArrowType.Timestamp(TimeUnit.MILLISECOND, getTimeZone(timestampLogicalType))));        case NANOS:            return of(field(new ArrowType.Timestamp(TimeUnit.NANOSECOND, getTimeZone(timestampLogicalType))));    }    return empty();}
private String parquet-mr_f53_0(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    return timestampLogicalType.isAdjustedToUTC() ? "UTC" : null;}
public TypeMapping parquet-mr_f54_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    if (convertInt96ToArrowTimestamp) {        return field(new ArrowType.Timestamp(TimeUnit.NANOSECOND, null));    } else {        return field(new ArrowType.Binary());    }}
public TypeMapping parquet-mr_f55_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();    if (logicalTypeAnnotation == null) {        return field(new ArrowType.Binary());    }    return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));        }    }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));}
public Optional<TypeMapping> parquet-mr_f56_0(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));}
public TypeMapping parquet-mr_f57_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return field(new ArrowType.Bool());}
public TypeMapping parquet-mr_f58_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();    if (logicalTypeAnnotation == null) {        return field(new ArrowType.Binary());    }    return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType) {            return of(field(new ArrowType.Utf8()));        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));        }    }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));}
public Optional<TypeMapping> parquet-mr_f59_0(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    return of(field(new ArrowType.Utf8()));}
public Optional<TypeMapping> parquet-mr_f60_0(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));}
private TypeMapping parquet-mr_f61_0(int precision, int scale)
{    return field(new ArrowType.Decimal(precision, scale));}
private TypeMapping parquet-mr_f62_0(int width, boolean signed)
{    return field(new ArrowType.Int(width, signed));}
public SchemaMapping parquet-mr_f63_0(Schema arrowSchema, MessageType parquetSchema)
{    List<TypeMapping> children = map(arrowSchema.getFields(), parquetSchema.getFields());    return new SchemaMapping(arrowSchema, parquetSchema, children);}
private List<TypeMapping> parquet-mr_f64_0(List<Field> arrowFields, List<Type> parquetFields)
{    if (arrowFields.size() != parquetFields.size()) {        throw new IllegalArgumentException("Can not map schemas as sizes differ: " + arrowFields + " != " + parquetFields);    }    List<TypeMapping> result = new ArrayList<>(arrowFields.size());    for (int i = 0; i < arrowFields.size(); i++) {        Field arrowField = arrowFields.get(i);        Type parquetField = parquetFields.get(i);        result.add(map(arrowField, parquetField));    }    return result;}
private TypeMapping parquet-mr_f65_0(final Field arrowField, final Type parquetField)
{    return arrowField.getType().accept(new ArrowTypeVisitor<TypeMapping>() {        @Override        public TypeMapping visit(Null type) {            if (!parquetField.isRepetition(OPTIONAL)) {                throw new IllegalArgumentException("Parquet type can't be null: " + parquetField);            }            return primitive();        }        @Override        public TypeMapping visit(Struct type) {            if (parquetField.isPrimitive()) {                throw new IllegalArgumentException("Parquet type not a group: " + parquetField);            }            GroupType groupType = parquetField.asGroupType();            return new StructTypeMapping(arrowField, groupType, map(arrowField.getChildren(), groupType.getFields()));        }        @Override        public TypeMapping visit(org.apache.arrow.vector.types.pojo.ArrowType.List type) {            return createListTypeMapping(type);        }        @Override        public TypeMapping visit(org.apache.arrow.vector.types.pojo.ArrowType.FixedSizeList type) {            return createListTypeMapping(type);        }        private TypeMapping createListTypeMapping(ArrowType.ComplexType type) {            if (arrowField.getChildren().size() != 1) {                throw new IllegalArgumentException("Invalid list type: " + type);            }            Field arrowChild = arrowField.getChildren().get(0);            if (parquetField.isRepetition(REPEATED)) {                return new RepeatedTypeMapping(arrowField, parquetField, map(arrowChild, parquetField));            }            if (parquetField.isPrimitive()) {                throw new IllegalArgumentException("Parquet type not a group: " + parquetField);            }            List3Levels list3Levels = new List3Levels(parquetField.asGroupType());            if (arrowField.getChildren().size() != 1) {                throw new IllegalArgumentException("invalid arrow list: " + arrowField);            }            return new ListTypeMapping(arrowField, list3Levels, map(arrowChild, list3Levels.getElement()));        }        @Override        public TypeMapping visit(Union type) {            if (parquetField.isPrimitive()) {                throw new IllegalArgumentException("Parquet type not a group: " + parquetField);            }            GroupType groupType = parquetField.asGroupType();            return new UnionTypeMapping(arrowField, groupType, map(arrowField.getChildren(), groupType.getFields()));        }        @Override        public TypeMapping visit(Int type) {            return primitive();        }        @Override        public TypeMapping visit(FloatingPoint type) {            return primitive();        }        @Override        public TypeMapping visit(Utf8 type) {            return primitive();        }        @Override        public TypeMapping visit(Binary type) {            return primitive();        }        @Override        public TypeMapping visit(Bool type) {            return primitive();        }        @Override        public TypeMapping visit(Decimal type) {            return primitive();        }        @Override        public TypeMapping visit(Date type) {            return primitive();        }        @Override        public TypeMapping visit(Time type) {            return primitive();        }        @Override        public TypeMapping visit(Timestamp type) {            return primitive();        }        @Override        public TypeMapping visit(Interval type) {            return primitive();        }        @Override        public TypeMapping visit(ArrowType.FixedSizeBinary fixedSizeBinary) {            return primitive();        }        private TypeMapping primitive() {            if (!parquetField.isPrimitive()) {                throw new IllegalArgumentException("Can not map schemas as one is primitive and the other is not: " + arrowField + " != " + parquetField);            }            return new PrimitiveTypeMapping(arrowField, parquetField.asPrimitiveType());        }    });}
public TypeMapping parquet-mr_f66_0(Null type)
{    if (!parquetField.isRepetition(OPTIONAL)) {        throw new IllegalArgumentException("Parquet type can't be null: " + parquetField);    }    return primitive();}
public TypeMapping parquet-mr_f67_0(Struct type)
{    if (parquetField.isPrimitive()) {        throw new IllegalArgumentException("Parquet type not a group: " + parquetField);    }    GroupType groupType = parquetField.asGroupType();    return new StructTypeMapping(arrowField, groupType, map(arrowField.getChildren(), groupType.getFields()));}
public TypeMapping parquet-mr_f68_0(org.apache.arrow.vector.types.pojo.ArrowType.List type)
{    return createListTypeMapping(type);}
public TypeMapping parquet-mr_f69_0(org.apache.arrow.vector.types.pojo.ArrowType.FixedSizeList type)
{    return createListTypeMapping(type);}
private TypeMapping parquet-mr_f70_0(ArrowType.ComplexType type)
{    if (arrowField.getChildren().size() != 1) {        throw new IllegalArgumentException("Invalid list type: " + type);    }    Field arrowChild = arrowField.getChildren().get(0);    if (parquetField.isRepetition(REPEATED)) {        return new RepeatedTypeMapping(arrowField, parquetField, map(arrowChild, parquetField));    }    if (parquetField.isPrimitive()) {        throw new IllegalArgumentException("Parquet type not a group: " + parquetField);    }    List3Levels list3Levels = new List3Levels(parquetField.asGroupType());    if (arrowField.getChildren().size() != 1) {        throw new IllegalArgumentException("invalid arrow list: " + arrowField);    }    return new ListTypeMapping(arrowField, list3Levels, map(arrowChild, list3Levels.getElement()));}
public TypeMapping parquet-mr_f71_0(Union type)
{    if (parquetField.isPrimitive()) {        throw new IllegalArgumentException("Parquet type not a group: " + parquetField);    }    GroupType groupType = parquetField.asGroupType();    return new UnionTypeMapping(arrowField, groupType, map(arrowField.getChildren(), groupType.getFields()));}
public TypeMapping parquet-mr_f72_0(Int type)
{    return primitive();}
public TypeMapping parquet-mr_f73_0(FloatingPoint type)
{    return primitive();}
public TypeMapping parquet-mr_f74_0(Utf8 type)
{    return primitive();}
public TypeMapping parquet-mr_f75_0(Binary type)
{    return primitive();}
public TypeMapping parquet-mr_f76_0(Bool type)
{    return primitive();}
public TypeMapping parquet-mr_f77_0(Decimal type)
{    return primitive();}
public TypeMapping parquet-mr_f78_0(Date type)
{    return primitive();}
public TypeMapping parquet-mr_f79_0(Time type)
{    return primitive();}
public TypeMapping parquet-mr_f80_0(Timestamp type)
{    return primitive();}
public TypeMapping parquet-mr_f81_0(Interval type)
{    return primitive();}
public TypeMapping parquet-mr_f82_0(ArrowType.FixedSizeBinary fixedSizeBinary)
{    return primitive();}
private TypeMapping parquet-mr_f83_0()
{    if (!parquetField.isPrimitive()) {        throw new IllegalArgumentException("Can not map schemas as one is primitive and the other is not: " + arrowField + " != " + parquetField);    }    return new PrimitiveTypeMapping(arrowField, parquetField.asPrimitiveType());}
public Schema parquet-mr_f84_0()
{    return arrowSchema;}
public MessageType parquet-mr_f85_0()
{    return parquetSchema;}
public List<TypeMapping> parquet-mr_f86_0()
{    return children;}
public Field parquet-mr_f87_0()
{    return arrowField;}
public Type parquet-mr_f88_0()
{    return parquetType;}
public List<TypeMapping> parquet-mr_f89_0()
{    return children;}
public T parquet-mr_f90_0(TypeMappingVisitor<T> visitor)
{    return visitor.visit(this);}
public T parquet-mr_f91_0(TypeMappingVisitor<T> visitor)
{    return visitor.visit(this);}
public T parquet-mr_f92_0(TypeMappingVisitor<T> visitor)
{    return visitor.visit(this);}
public List3Levels parquet-mr_f93_0()
{    return list3Levels;}
public TypeMapping parquet-mr_f94_0()
{    return child;}
public T parquet-mr_f95_0(TypeMappingVisitor<T> visitor)
{    return visitor.visit(this);}
public TypeMapping parquet-mr_f96_0()
{    return child;}
public T parquet-mr_f97_0(TypeMappingVisitor<T> visitor)
{    return visitor.visit(this);}
private static Field parquet-mr_f98_0(String name, boolean nullable, ArrowType type, Field... children)
{    return new Field(name, nullable, type, asList(children));}
private static Field parquet-mr_f99_0(String name, ArrowType type, Field... children)
{    return field(name, true, type, children);}
public void parquet-mr_f100_0() throws IOException
{    MessageType parquet = converter.fromArrow(complexArrowSchema).getParquetSchema();        Assert.assertEquals(complexParquetSchema.toString(), parquet.toString());    Assert.assertEquals(complexParquetSchema, parquet);}
public void parquet-mr_f101_0() throws IOException
{    MessageType parquet = converter.fromArrow(allTypesArrowSchema).getParquetSchema();        Assert.assertEquals(allTypesParquetSchema.toString(), parquet.toString());    Assert.assertEquals(allTypesParquetSchema, parquet);}
public void parquet-mr_f102_0() throws IOException
{    Schema arrow = converter.fromParquet(supportedTypesParquetSchema).getArrowSchema();    assertEquals(supportedTypesArrowSchema, arrow);}
public void parquet-mr_f103_0() throws IOException
{    Schema arrow = converter.fromParquet(Paper.schema).getArrowSchema();    assertEquals(paperArrowSchema, arrow);}
public void parquet-mr_f104_0(Schema left, Schema right)
{    compareFields(left.getFields(), right.getFields());    Assert.assertEquals(left, right);}
private void parquet-mr_f105_0(List<Field> left, List<Field> right)
{    Assert.assertEquals(left + "\n" + right, left.size(), right.size());    int size = left.size();    for (int i = 0; i < size; i++) {        Field expectedField = left.get(i);        Field field = right.get(i);        compareFields(expectedField.getChildren(), field.getChildren());        Assert.assertEquals(expectedField, field);    }}
public void parquet-mr_f106_0() throws IOException
{    SchemaMapping map = converter.map(allTypesArrowSchema, allTypesParquetSchema);    Assert.assertEquals("p, s<p>, l<p>, l<p>, u<p>, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p", toSummaryString(map));}
private String parquet-mr_f107_0(SchemaMapping map)
{    List<TypeMapping> fields = map.getChildren();    return toSummaryString(fields);}
private String parquet-mr_f108_0(List<TypeMapping> fields)
{    final StringBuilder sb = new StringBuilder();    for (TypeMapping typeMapping : fields) {        if (sb.length() != 0) {            sb.append(", ");        }        sb.append(typeMapping.accept(new TypeMappingVisitor<String>() {            @Override            public String visit(PrimitiveTypeMapping primitiveTypeMapping) {                return "p";            }            @Override            public String visit(StructTypeMapping structTypeMapping) {                return "s";            }            @Override            public String visit(UnionTypeMapping unionTypeMapping) {                return "u";            }            @Override            public String visit(ListTypeMapping listTypeMapping) {                return "l";            }            @Override            public String visit(RepeatedTypeMapping repeatedTypeMapping) {                return "r";            }        }));        if (typeMapping.getChildren() != null && !typeMapping.getChildren().isEmpty()) {            sb.append("<").append(toSummaryString(typeMapping.getChildren())).append(">");        }    }    return sb.toString();}
public String parquet-mr_f109_0(PrimitiveTypeMapping primitiveTypeMapping)
{    return "p";}
public String parquet-mr_f110_0(StructTypeMapping structTypeMapping)
{    return "s";}
public String parquet-mr_f111_0(UnionTypeMapping unionTypeMapping)
{    return "u";}
public String parquet-mr_f112_0(ListTypeMapping listTypeMapping)
{    return "l";}
public String parquet-mr_f113_0(RepeatedTypeMapping repeatedTypeMapping)
{    return "r";}
public void parquet-mr_f114_0() throws IOException
{    SchemaMapping map = converter.map(paperArrowSchema, Paper.schema);    Assert.assertEquals("p, s<r<p>, r<p>>, r<s<r<s<p, p>>, p>>", toSummaryString(map));}
public void parquet-mr_f115_0()
{    converter.fromArrow(new Schema(asList(field("a", new ArrowType.Time(TimeUnit.SECOND, 32))))).getParquetSchema();}
public void parquet-mr_f116_0()
{    MessageType expected = converter.fromArrow(new Schema(asList(field("a", new ArrowType.Time(TimeUnit.MILLISECOND, 32))))).getParquetSchema();    Assert.assertEquals(expected, Types.buildMessage().addField(Types.optional(INT32).as(timeType(false, MILLIS)).named("a")).named("root"));}
public void parquet-mr_f117_0()
{    MessageType expected = converter.fromArrow(new Schema(asList(field("a", new ArrowType.Time(TimeUnit.MICROSECOND, 64))))).getParquetSchema();    Assert.assertEquals(expected, Types.buildMessage().addField(Types.optional(INT64).as(timeType(false, MICROS)).named("a")).named("root"));}
public void parquet-mr_f118_0()
{    MessageType parquet = Types.buildMessage().addField(Types.optional(INT32).as(TIME_MILLIS).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Time(TimeUnit.MILLISECOND, 32))));    Assert.assertEquals(expected, converter.fromParquet(parquet).getArrowSchema());}
public void parquet-mr_f119_0()
{    MessageType parquet = Types.buildMessage().addField(Types.optional(INT64).as(TIME_MICROS).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Time(TimeUnit.MICROSECOND, 64))));    Assert.assertEquals(expected, converter.fromParquet(parquet).getArrowSchema());}
public void parquet-mr_f120_0()
{    MessageType parquet = Types.buildMessage().addField(Types.optional(FIXED_LEN_BYTE_ARRAY).length(12).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Binary())));    Assert.assertEquals(expected, converter.fromParquet(parquet).getArrowSchema());}
public void parquet-mr_f121_0()
{    MessageType parquet = Types.buildMessage().addField(Types.optional(FIXED_LEN_BYTE_ARRAY).length(5).as(DECIMAL).precision(8).scale(2).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Decimal(8, 2))));    Assert.assertEquals(expected, converter.fromParquet(parquet).getArrowSchema());}
public void parquet-mr_f122_0()
{    MessageType parquet = Types.buildMessage().addField(Types.optional(INT96).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Binary())));    Assert.assertEquals(expected, converter.fromParquet(parquet).getArrowSchema());}
public void parquet-mr_f123_0()
{    final SchemaConverter converterInt96ToTimestamp = new SchemaConverter(true);    MessageType parquet = Types.buildMessage().addField(Types.optional(INT96).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Timestamp(TimeUnit.NANOSECOND, null))));    Assert.assertEquals(expected, converterInt96ToTimestamp.fromParquet(parquet).getArrowSchema());}
public void parquet-mr_f124_0()
{    converter.fromParquet(Types.buildMessage().addField(Types.optional(INT64).as(TIME_MILLIS).named("a")).named("root"));}
public void parquet-mr_f125_0()
{    converter.fromParquet(Types.buildMessage().addField(Types.optional(INT32).as(TIME_MICROS).named("a")).named("root"));}
public void parquet-mr_f126_0()
{    converter.fromArrow(new Schema(asList(field("a", new ArrowType.Timestamp(TimeUnit.SECOND, "UTC"))))).getParquetSchema();}
public void parquet-mr_f127_0()
{    MessageType expected = converter.fromArrow(new Schema(asList(field("a", new ArrowType.Timestamp(TimeUnit.MILLISECOND, "UTC"))))).getParquetSchema();    Assert.assertEquals(expected, Types.buildMessage().addField(Types.optional(INT64).as(TIMESTAMP_MILLIS).named("a")).named("root"));}
public void parquet-mr_f128_0()
{    MessageType expected = converter.fromArrow(new Schema(asList(field("a", new ArrowType.Timestamp(TimeUnit.MICROSECOND, "UTC"))))).getParquetSchema();    Assert.assertEquals(expected, Types.buildMessage().addField(Types.optional(INT64).as(TIMESTAMP_MICROS).named("a")).named("root"));}
public void parquet-mr_f129_0()
{    MessageType parquet = Types.buildMessage().addField(Types.optional(INT64).as(TIMESTAMP_MILLIS).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Timestamp(TimeUnit.MILLISECOND, "UTC"))));    Assert.assertEquals(expected, converter.fromParquet(parquet).getArrowSchema());}
public void parquet-mr_f130_0()
{    MessageType parquet = Types.buildMessage().addField(Types.optional(INT64).as(TIMESTAMP_MICROS).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Timestamp(TimeUnit.MICROSECOND, "UTC"))));    Assert.assertEquals(expected, converter.fromParquet(parquet).getArrowSchema());}
public void parquet-mr_f131_0()
{    converter.fromParquet(Types.buildMessage().addField(Types.optional(INT32).as(TIMESTAMP_MILLIS).named("a")).named("root"));}
public void parquet-mr_f132_0()
{    converter.fromParquet(Types.buildMessage().addField(Types.optional(INT32).as(TIMESTAMP_MICROS).named("a")).named("root"));}
public T parquet-mr_f133_0()
{    return root.getCurrentRecord();}
public GroupConverter parquet-mr_f134_0()
{    return root;}
public void parquet-mr_f135_0(Binary value)
{    parent.add(convert(value));}
public boolean parquet-mr_f136_0()
{    return true;}
public void parquet-mr_f137_0(Dictionary dictionary)
{    dict = (T[]) new Object[dictionary.getMaxId() + 1];    for (int i = 0; i <= dictionary.getMaxId(); i++) {        dict[i] = convert(dictionary.decodeToBinary(i));    }}
public T parquet-mr_f138_0(T value)
{    return value;}
public void parquet-mr_f139_0(int dictionaryId)
{    parent.add(prepareDictionaryValue(dict[dictionaryId]));}
public void parquet-mr_f140_0(int value)
{    parent.addByte((byte) value);}
public void parquet-mr_f141_0(int value)
{    parent.addShort((short) value);}
public void parquet-mr_f142_0(int value)
{    parent.addChar((char) value);}
public final void parquet-mr_f143_0(boolean value)
{    parent.addBoolean(value);}
public final void parquet-mr_f144_0(int value)
{    parent.addInt(value);}
public final void parquet-mr_f145_0(int value)
{    parent.addLong((long) value);}
public final void parquet-mr_f146_0(long value)
{    parent.addLong(value);}
public final void parquet-mr_f147_0(int value)
{    parent.addFloat((float) value);}
public final void parquet-mr_f148_0(long value)
{    parent.addFloat((float) value);}
public final void parquet-mr_f149_0(float value)
{    parent.addFloat(value);}
public final void parquet-mr_f150_0(int value)
{    parent.addDouble((double) value);}
public final void parquet-mr_f151_0(long value)
{    parent.addDouble((double) value);}
public final void parquet-mr_f152_0(float value)
{    parent.addDouble((double) value);}
public final void parquet-mr_f153_0(double value)
{    parent.addDouble(value);}
public byte[] parquet-mr_f154_0(Binary binary)
{    return binary.getBytes();}
public ByteBuffer parquet-mr_f155_0(Binary binary)
{    return ByteBuffer.wrap(binary.getBytes());}
public ByteBuffer parquet-mr_f156_0(ByteBuffer value)
{    return value.duplicate();}
public String parquet-mr_f157_0(Binary binary)
{    return binary.toStringUsingUTF8();}
public Utf8 parquet-mr_f158_0(Binary binary)
{    return new Utf8(binary.getBytes());}
public Object parquet-mr_f159_0(Binary binary)
{    try {        return ctor.newInstance(binary.toStringUsingUTF8());    } catch (InstantiationException e) {        throw new ParquetDecodingException("Cannot convert binary to " + stringableName, e);    } catch (IllegalAccessException e) {        throw new ParquetDecodingException("Cannot convert binary to " + stringableName, e);    } catch (InvocationTargetException e) {        throw new ParquetDecodingException("Cannot convert binary to " + stringableName, e);    }}
public Object parquet-mr_f160_0(Binary binary)
{    return model.createEnum(binary.toStringUsingUTF8(), schema);}
public Object parquet-mr_f161_0(Binary binary)
{    return model.createFixed(null, /* reuse */    binary.getBytes(), schema);}
public void parquet-mr_f162_0(Object value)
{    AvroIndexedRecordConverter.this.set(finalAvroIndex, value);}
private static Class<T> parquet-mr_f163_0(GenericData model, Schema schema)
{    if (model.getConversionFor(schema.getLogicalType()) != null) {                return null;    }    if (model instanceof SpecificData) {        return (Class<T>) ((SpecificData) model).getClass(schema);    }    return null;}
private Schema.Field parquet-mr_f164_0(String parquetFieldName)
{    Schema.Field avroField = avroSchema.getField(parquetFieldName);    for (Schema.Field f : avroSchema.getFields()) {        if (f.aliases().contains(parquetFieldName)) {            return f;        }    }    if (avroField == null) {        throw new InvalidRecordException(String.format("Parquet/Avro schema mismatch. Avro field '%s' not found.", parquetFieldName));    }    return avroField;}
private static Converter parquet-mr_f165_0(Schema schema, Type type, GenericData model, ParentValueContainer setter)
{    LogicalType logicalType = schema.getLogicalType();                Conversion<?> conversion = model.getConversionFor(logicalType);    ParentValueContainer parent = ParentValueContainer.getConversionContainer(setter, conversion, schema);    if (schema.getType().equals(Schema.Type.BOOLEAN)) {        return new AvroConverters.FieldBooleanConverter(parent);    } else if (schema.getType().equals(Schema.Type.INT)) {        return new AvroConverters.FieldIntegerConverter(parent);    } else if (schema.getType().equals(Schema.Type.LONG)) {        return new AvroConverters.FieldLongConverter(parent);    } else if (schema.getType().equals(Schema.Type.FLOAT)) {        return new AvroConverters.FieldFloatConverter(parent);    } else if (schema.getType().equals(Schema.Type.DOUBLE)) {        return new AvroConverters.FieldDoubleConverter(parent);    } else if (schema.getType().equals(Schema.Type.BYTES)) {        return new AvroConverters.FieldByteBufferConverter(parent);    } else if (schema.getType().equals(Schema.Type.STRING)) {        return new AvroConverters.FieldStringConverter(parent);    } else if (schema.getType().equals(Schema.Type.RECORD)) {        return new AvroIndexedRecordConverter(parent, type.asGroupType(), schema, model);    } else if (schema.getType().equals(Schema.Type.ENUM)) {        return new FieldEnumConverter(parent, schema, model);    } else if (schema.getType().equals(Schema.Type.ARRAY)) {        return new AvroArrayConverter(parent, type.asGroupType(), schema, model);    } else if (schema.getType().equals(Schema.Type.MAP)) {        return new MapConverter(parent, type.asGroupType(), schema, model);    } else if (schema.getType().equals(Schema.Type.UNION)) {        return new AvroUnionConverter(parent, type, schema, model);    } else if (schema.getType().equals(Schema.Type.FIXED)) {        return new FieldFixedConverter(parent, schema, model);    }    throw new UnsupportedOperationException(String.format("Cannot convert Avro type: %s" + " (Parquet type: %s) ", schema, type));}
private void parquet-mr_f166_0(int index, Object value)
{    this.currentRecord.put(index, value);}
public Converter parquet-mr_f167_0(int fieldIndex)
{    return converters[fieldIndex];}
public void parquet-mr_f168_0()
{        this.currentRecord = (T) ((this.specificClass == null) ? new GenericData.Record(avroSchema) : SpecificData.newInstance(specificClass, avroSchema));}
public void parquet-mr_f169_0()
{    fillInDefaults();    if (parent != null) {        parent.add(currentRecord);    }}
private void parquet-mr_f170_0()
{    for (Map.Entry<Schema.Field, Object> entry : recordDefaults.entrySet()) {        Schema.Field f = entry.getKey();                Object defaultValue = deepCopy(f.schema(), entry.getValue());        this.currentRecord.put(f.pos(), defaultValue);    }}
private Object parquet-mr_f171_0(Schema schema, Object value)
{    switch(schema.getType()) {        case BOOLEAN:        case INT:        case LONG:        case FLOAT:        case DOUBLE:            return value;        default:            return model.deepCopy(schema, value);    }}
 T parquet-mr_f172_0()
{    return currentRecord;}
public final void parquet-mr_f173_0(Binary value)
{    Object enumValue = value.toStringUsingUTF8();    if (enumClass != null) {        enumValue = (Enum.valueOf(enumClass, (String) enumValue));    }    parent.add(enumValue);}
public final void parquet-mr_f174_0(Binary value)
{    if (fixedClass == null) {        parent.add(new GenericData.Fixed(avroSchema, value.getBytes()));    } else {        if (fixedClassCtor == null) {            throw new IllegalArgumentException("fixedClass specified but fixedClassCtor is null.");        }        try {            Object fixed = fixedClassCtor.newInstance(value.getBytes());            parent.add(fixed);        } catch (Exception e) {            throw new RuntimeException(e);        }    }}
public void parquet-mr_f175_0(Object value)
{    array.add(value);}
public Converter parquet-mr_f176_0(int fieldIndex)
{    return converter;}
public void parquet-mr_f177_0()
{    array = new GenericData.Array<Object>(0, avroSchema);}
public void parquet-mr_f178_0()
{    parent.add(array);}
public void parquet-mr_f179_0(Object value)
{    ElementConverter.this.element = value;}
public Converter parquet-mr_f180_0(int fieldIndex)
{    Preconditions.checkArgument(fieldIndex == 0, "Illegal field index: " + fieldIndex);    return elementConverter;}
public void parquet-mr_f181_0()
{    element = null;}
public void parquet-mr_f182_0()
{    array.add(element);}
public void parquet-mr_f183_0(Object value)
{    Preconditions.checkArgument(memberValue == null, "Union is resolving to more than one type");    memberValue = value;}
public Converter parquet-mr_f184_0(int fieldIndex)
{    return memberConverters[fieldIndex];}
public void parquet-mr_f185_0()
{    memberValue = null;}
public void parquet-mr_f186_0()
{    parent.add(memberValue);}
public Converter parquet-mr_f187_0(int fieldIndex)
{    return keyValueConverter;}
public void parquet-mr_f188_0()
{    this.map = new HashMap<String, V>();}
public void parquet-mr_f189_0()
{    parent.add(map);}
public final void parquet-mr_f190_0(Binary value)
{    key = value.toStringUsingUTF8();}
public void parquet-mr_f191_0(Object value)
{    MapKeyValueConverter.this.value = (V) value;}
public Converter parquet-mr_f192_0(int fieldIndex)
{    if (fieldIndex == 0) {        return keyConverter;    } else if (fieldIndex == 1) {        return valueConverter;    }    throw new IllegalArgumentException("only the key (0) and value (1) fields expected: " + fieldIndex);}
public void parquet-mr_f193_0()
{    key = null;    value = null;}
public void parquet-mr_f194_0()
{    map.put(key, value);}
public static void parquet-mr_f195_0(Job job, Schema requestedProjection)
{    AvroReadSupport.setRequestedProjection(ContextUtil.getConfiguration(job), requestedProjection);}
public static void parquet-mr_f196_0(Job job, Schema avroReadSchema)
{    AvroReadSupport.setAvroReadSchema(ContextUtil.getConfiguration(job), avroReadSchema);}
public static void parquet-mr_f197_0(Job job, Class<? extends AvroDataSupplier> supplierClass)
{    AvroReadSupport.setAvroDataSupplier(ContextUtil.getConfiguration(job), supplierClass);}
public static void parquet-mr_f198_0(Job job, Schema schema)
{    AvroWriteSupport.setSchema(ContextUtil.getConfiguration(job), schema);}
public static void parquet-mr_f199_0(Job job, Class<? extends AvroDataSupplier> supplierClass)
{    AvroWriteSupport.setAvroDataSupplier(ContextUtil.getConfiguration(job), supplierClass);}
public static Builder<T> parquet-mr_f200_0(Path file)
{    return new Builder<T>(file);}
public static Builder<T> parquet-mr_f201_0(InputFile file)
{    return new Builder<T>(file);}
public Builder<T> parquet-mr_f202_0(GenericData model)
{    this.model = model;        if (model.getClass() != GenericData.class && model.getClass() != SpecificData.class) {        isReflect = true;    }    return this;}
public Builder<T> parquet-mr_f203_0()
{    this.enableCompatibility = false;    return this;}
public Builder<T> parquet-mr_f204_0(boolean enableCompatibility)
{    this.enableCompatibility = enableCompatibility;    return this;}
protected ReadSupport<T> parquet-mr_f205_0()
{    if (isReflect) {        conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    } else {        conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, enableCompatibility);    }    return new AvroReadSupport<T>(model);}
public static Builder<T> parquet-mr_f206_0(Path file)
{    return new Builder<T>(file);}
public static Builder<T> parquet-mr_f207_0(OutputFile file)
{    return new Builder<T>(file);}
private static WriteSupport<T> parquet-mr_f208_0(Schema avroSchema, GenericData model)
{    return new AvroWriteSupport<T>(new AvroSchemaConverter().convert(avroSchema), avroSchema, model);}
private static WriteSupport<T> parquet-mr_f209_0(Configuration conf, Schema avroSchema, GenericData model)
{    return new AvroWriteSupport<T>(new AvroSchemaConverter(conf).convert(avroSchema), avroSchema, model);}
public Builder<T> parquet-mr_f210_0(Schema schema)
{    this.schema = schema;    return this;}
public Builder<T> parquet-mr_f211_0(GenericData model)
{    this.model = model;    return this;}
protected Builder<T> parquet-mr_f212_0()
{    return this;}
protected WriteSupport<T> parquet-mr_f213_0(Configuration conf)
{    return AvroParquetWriter.writeSupport(conf, schema, model);}
public static void parquet-mr_f214_0(Configuration configuration, Schema requestedProjection)
{    configuration.set(AVRO_REQUESTED_PROJECTION, requestedProjection.toString());}
public static void parquet-mr_f215_0(Configuration configuration, Schema avroReadSchema)
{    configuration.set(AVRO_READ_SCHEMA, avroReadSchema.toString());}
public static void parquet-mr_f216_0(Configuration configuration, Class<? extends AvroDataSupplier> clazz)
{    configuration.set(AVRO_DATA_SUPPLIER, clazz.getName());}
public ReadContext parquet-mr_f217_0(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema)
{    MessageType projection = fileSchema;    Map<String, String> metadata = new LinkedHashMap<String, String>();    String requestedProjectionString = configuration.get(AVRO_REQUESTED_PROJECTION);    if (requestedProjectionString != null) {        Schema avroRequestedProjection = new Schema.Parser().parse(requestedProjectionString);        projection = new AvroSchemaConverter(configuration).convert(avroRequestedProjection);    }    String avroReadSchema = configuration.get(AVRO_READ_SCHEMA);    if (avroReadSchema != null) {        metadata.put(AVRO_READ_SCHEMA_METADATA_KEY, avroReadSchema);    }    if (configuration.getBoolean(AVRO_COMPATIBILITY, AVRO_DEFAULT_COMPATIBILITY)) {        metadata.put(AVRO_COMPATIBILITY, "true");    }    return new ReadContext(projection, metadata);}
public RecordMaterializer<T> parquet-mr_f218_0(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema, ReadContext readContext)
{    Map<String, String> metadata = readContext.getReadSupportMetadata();    MessageType parquetSchema = readContext.getRequestedSchema();    Schema avroSchema;    if (metadata.get(AVRO_READ_SCHEMA_METADATA_KEY) != null) {                avroSchema = new Schema.Parser().parse(metadata.get(AVRO_READ_SCHEMA_METADATA_KEY));    } else if (keyValueMetaData.get(AVRO_SCHEMA_METADATA_KEY) != null) {                avroSchema = new Schema.Parser().parse(keyValueMetaData.get(AVRO_SCHEMA_METADATA_KEY));    } else if (keyValueMetaData.get(OLD_AVRO_SCHEMA_METADATA_KEY) != null) {                avroSchema = new Schema.Parser().parse(keyValueMetaData.get(OLD_AVRO_SCHEMA_METADATA_KEY));    } else {                avroSchema = new AvroSchemaConverter(configuration).convert(parquetSchema);    }    GenericData model = getDataModel(configuration);    String compatEnabled = metadata.get(AvroReadSupport.AVRO_COMPATIBILITY);    if (compatEnabled != null && Boolean.valueOf(compatEnabled)) {        return newCompatMaterializer(parquetSchema, avroSchema, model);    }    return new AvroRecordMaterializer<T>(parquetSchema, avroSchema, model);}
private static RecordMaterializer<T> parquet-mr_f219_0(MessageType parquetSchema, Schema avroSchema, GenericData model)
{    return (RecordMaterializer<T>) new AvroCompatRecordMaterializer(parquetSchema, avroSchema, model);}
private GenericData parquet-mr_f220_0(Configuration conf)
{    if (model != null) {        return model;    }    Class<? extends AvroDataSupplier> suppClass = conf.getClass(AVRO_DATA_SUPPLIER, SpecificDataSupplier.class, AvroDataSupplier.class);    return ReflectionUtils.newInstance(suppClass, conf).get();}
public void parquet-mr_f221_0(Object value)
{    AvroRecordConverter.this.currentRecord = (T) value;}
public void parquet-mr_f222_0(Object value)
{    AvroRecordConverter.this.set(avroField.name(), finalAvroIndex, value);}
private static Map<String, Class<?>> parquet-mr_f223_0(Class<?> recordClass, boolean excludeJava)
{    Map<String, Class<?>> fields = new LinkedHashMap<String, Class<?>>();    if (recordClass != null) {        Class<?> current = recordClass;        do {            if (excludeJava && current.getPackage() != null && current.getPackage().getName().startsWith("java.")) {                                break;            }            for (Field field : current.getDeclaredFields()) {                if (field.isAnnotationPresent(AvroIgnore.class) || isTransientOrStatic(field)) {                    continue;                }                AvroName altName = field.getAnnotation(AvroName.class);                Class<?> existing = fields.put(altName != null ? altName.value() : field.getName(), field.getType());                if (existing != null) {                    throw new AvroTypeException(current + " contains two fields named: " + field.getName());                }            }            current = current.getSuperclass();        } while (current != null);    }    return fields;}
private static boolean parquet-mr_f224_0(Field field)
{    return (field.getModifiers() & (Modifier.TRANSIENT | Modifier.STATIC)) != 0;}
private Schema.Field parquet-mr_f225_0(String parquetFieldName)
{    Schema.Field avroField = avroSchema.getField(parquetFieldName);    if (avroField != null) {        return avroField;    }    for (Schema.Field f : avroSchema.getFields()) {        if (f.aliases().contains(parquetFieldName)) {            return f;        }    }    throw new InvalidRecordException(String.format("Parquet/Avro schema mismatch: Avro field '%s' not found", parquetFieldName));}
private static Converter parquet-mr_f226_0(Schema schema, Type type, GenericData model, ParentValueContainer setter)
{    return newConverter(schema, type, model, null, setter);}
private static Converter parquet-mr_f227_0(Schema schema, Type type, GenericData model, Class<?> knownClass, ParentValueContainer setter)
{    LogicalType logicalType = schema.getLogicalType();    Conversion<?> conversion;    if (knownClass != null) {        conversion = model.getConversionByClass(knownClass, logicalType);    } else {        conversion = model.getConversionFor(logicalType);    }    ParentValueContainer parent = ParentValueContainer.getConversionContainer(setter, conversion, schema);    if (schema.getType().equals(Schema.Type.BOOLEAN)) {        return new AvroConverters.FieldBooleanConverter(parent);    } else if (schema.getType().equals(Schema.Type.INT)) {        Class<?> datumClass = getDatumClass(conversion, knownClass, schema, model);        if (datumClass == null) {            return new AvroConverters.FieldIntegerConverter(parent);        } else if (datumClass == byte.class || datumClass == Byte.class) {            return new AvroConverters.FieldByteConverter(parent);        } else if (datumClass == short.class || datumClass == Short.class) {            return new AvroConverters.FieldShortConverter(parent);        } else if (datumClass == char.class || datumClass == Character.class) {            return new AvroConverters.FieldCharConverter(parent);        }        return new AvroConverters.FieldIntegerConverter(parent);    } else if (schema.getType().equals(Schema.Type.LONG)) {        return new AvroConverters.FieldLongConverter(parent);    } else if (schema.getType().equals(Schema.Type.FLOAT)) {        return new AvroConverters.FieldFloatConverter(parent);    } else if (schema.getType().equals(Schema.Type.DOUBLE)) {        return new AvroConverters.FieldDoubleConverter(parent);    } else if (schema.getType().equals(Schema.Type.BYTES)) {        Class<?> datumClass = getDatumClass(conversion, knownClass, schema, model);        if (datumClass == null) {            return new AvroConverters.FieldByteBufferConverter(parent);        } else if (datumClass.isArray() && datumClass.getComponentType() == byte.class) {            return new AvroConverters.FieldByteArrayConverter(parent);        }        return new AvroConverters.FieldByteBufferConverter(parent);    } else if (schema.getType().equals(Schema.Type.STRING)) {        return newStringConverter(schema, model, parent);    } else if (schema.getType().equals(Schema.Type.RECORD)) {        return new AvroRecordConverter(parent, type.asGroupType(), schema, model);    } else if (schema.getType().equals(Schema.Type.ENUM)) {        return new AvroConverters.FieldEnumConverter(parent, schema, model);    } else if (schema.getType().equals(Schema.Type.ARRAY)) {        Class<?> datumClass = getDatumClass(conversion, knownClass, schema, model);        if (datumClass != null && datumClass.isArray()) {            return new AvroArrayConverter(parent, type.asGroupType(), schema, model, datumClass);        } else {            return new AvroCollectionConverter(parent, type.asGroupType(), schema, model, datumClass);        }    } else if (schema.getType().equals(Schema.Type.MAP)) {        return new MapConverter(parent, type.asGroupType(), schema, model);    } else if (schema.getType().equals(Schema.Type.UNION)) {        return new AvroUnionConverter(parent, type, schema, model);    } else if (schema.getType().equals(Schema.Type.FIXED)) {        return new AvroConverters.FieldFixedConverter(parent, schema, model);    }    throw new UnsupportedOperationException(String.format("Cannot convert Avro type: %s to Parquet type: %s", schema, type));}
private static Converter parquet-mr_f228_0(Schema schema, GenericData model, ParentValueContainer parent)
{    Class<?> stringableClass = getStringableClass(schema, model);    if (stringableClass == String.class) {        return new FieldStringConverter(parent);    } else if (stringableClass == CharSequence.class) {        return new AvroConverters.FieldUTF8Converter(parent);    }    return new FieldStringableConverter(parent, stringableClass);}
private static Class<?> parquet-mr_f229_0(Schema schema, GenericData model)
{    if (model instanceof SpecificData) {                boolean isMap = (schema.getType() == Schema.Type.MAP);        String stringableClass = schema.getProp(isMap ? JAVA_KEY_CLASS_PROP : JAVA_CLASS_PROP);        if (stringableClass != null) {            try {                return ClassUtils.forName(model.getClassLoader(), stringableClass);            } catch (ClassNotFoundException e) {                        }        }    }    if (ReflectData.class.isAssignableFrom(model.getClass())) {                return String.class;    }        String name = schema.getProp(STRINGABLE_PROP);    if (name == null) {        return CharSequence.class;    }    switch(GenericData.StringType.valueOf(name)) {        case String:            return String.class;        default:                        return CharSequence.class;    }}
private static Class<T> parquet-mr_f230_0(Schema schema, GenericData model)
{    return getDatumClass(null, null, schema, model);}
private static Class<T> parquet-mr_f231_0(Conversion<?> conversion, Class<T> knownClass, Schema schema, GenericData model)
{    if (conversion != null) {                return null;    }        if (knownClass != null) {        return knownClass;    }    if (model instanceof SpecificData) {                return ((SpecificData) model).getClass(schema);    } else if (model.getClass() == GenericData.class) {        return null;    } else {                Class<? extends GenericData> modelClass = model.getClass();        Method getClassMethod;        try {            getClassMethod = modelClass.getMethod("getClass", Schema.class);        } catch (NoSuchMethodException e) {                        return null;        }        try {            return (Class<T>) getClassMethod.invoke(schema);        } catch (IllegalAccessException e) {            return null;        } catch (InvocationTargetException e) {            return null;        }    }}
protected void parquet-mr_f232_0(String name, int avroIndex, Object value)
{    model.setField(currentRecord, name, avroIndex, value);}
public Converter parquet-mr_f233_0(int fieldIndex)
{    return converters[fieldIndex];}
public void parquet-mr_f234_0()
{    this.currentRecord = (T) model.newRecord(null, avroSchema);}
public void parquet-mr_f235_0()
{    fillInDefaults();    if (parent != null) {        parent.add(currentRecord);    } else {                rootContainer.add(currentRecord);    }}
private void parquet-mr_f236_0()
{    for (Map.Entry<Schema.Field, Object> entry : recordDefaults.entrySet()) {        Schema.Field f = entry.getKey();                Object defaultValue = deepCopy(f.schema(), entry.getValue());        set(f.name(), f.pos(), defaultValue);    }}
private Object parquet-mr_f237_0(Schema schema, Object value)
{    switch(schema.getType()) {        case BOOLEAN:        case INT:        case LONG:        case FLOAT:        case DOUBLE:            return value;        default:            return model.deepCopy(schema, value);    }}
 T parquet-mr_f238_0()
{    return currentRecord;}
public void parquet-mr_f239_0(Object value)
{    container.add(value);}
public Converter parquet-mr_f240_0(int fieldIndex)
{    return converter;}
public void parquet-mr_f241_0()
{    container = newContainer();}
public void parquet-mr_f242_0()
{    parent.add(container);}
private Collection<Object> parquet-mr_f243_0()
{    if (containerClass == null) {        return new GenericData.Array<Object>(0, avroSchema);    } else if (containerClass.isAssignableFrom(ArrayList.class)) {        return new ArrayList<Object>();    } else {                return (Collection<Object>) ReflectData.newInstance(containerClass, avroSchema);    }}
public void parquet-mr_f244_0(Object value)
{    ElementConverter.this.element = value;}
public Converter parquet-mr_f245_0(int fieldIndex)
{    Preconditions.checkArgument(fieldIndex == 0, "Illegal field index: " + fieldIndex);    return elementConverter;}
public void parquet-mr_f246_0()
{    element = null;}
public void parquet-mr_f247_0()
{    container.add(element);}
public Converter parquet-mr_f248_0(int fieldIndex)
{    return converter;}
public void parquet-mr_f249_0()
{        container.clear();}
public void parquet-mr_f250_0()
{    if (elementClass == boolean.class) {        parent.add(((BooleanArrayList) container).toBooleanArray());    } else if (elementClass == byte.class) {        parent.add(((ByteArrayList) container).toByteArray());    } else if (elementClass == char.class) {        parent.add(((CharArrayList) container).toCharArray());    } else if (elementClass == short.class) {        parent.add(((ShortArrayList) container).toShortArray());    } else if (elementClass == int.class) {        parent.add(((IntArrayList) container).toIntArray());    } else if (elementClass == long.class) {        parent.add(((LongArrayList) container).toLongArray());    } else if (elementClass == float.class) {        parent.add(((FloatArrayList) container).toFloatArray());    } else if (elementClass == double.class) {        parent.add(((DoubleArrayList) container).toDoubleArray());    } else {        parent.add(((ArrayList) container).toArray());    }}
private ParentValueContainer parquet-mr_f251_0()
{    if (elementClass == boolean.class) {        final BooleanArrayList list = new BooleanArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addBoolean(boolean value) {                list.add(value);            }        };    } else if (elementClass == byte.class) {        final ByteArrayList list = new ByteArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addByte(byte value) {                list.add(value);            }        };    } else if (elementClass == char.class) {        final CharArrayList list = new CharArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addChar(char value) {                list.add(value);            }        };    } else if (elementClass == short.class) {        final ShortArrayList list = new ShortArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addShort(short value) {                list.add(value);            }        };    } else if (elementClass == int.class) {        final IntArrayList list = new IntArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addInt(int value) {                list.add(value);            }        };    } else if (elementClass == long.class) {        final LongArrayList list = new LongArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addLong(long value) {                list.add(value);            }        };    } else if (elementClass == float.class) {        final FloatArrayList list = new FloatArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addFloat(float value) {                list.add(value);            }        };    } else if (elementClass == double.class) {        final DoubleArrayList list = new DoubleArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addDouble(double value) {                list.add(value);            }        };    } else {                final List<Object> list = new ArrayList<Object>();        this.container = list;        return new ParentValueContainer() {            @Override            public void add(Object value) {                list.add(value);            }        };    }}
public void parquet-mr_f252_0(boolean value)
{    list.add(value);}
public void parquet-mr_f253_0(byte value)
{    list.add(value);}
public void parquet-mr_f254_0(char value)
{    list.add(value);}
public void parquet-mr_f255_0(short value)
{    list.add(value);}
public void parquet-mr_f256_0(int value)
{    list.add(value);}
public void parquet-mr_f257_0(long value)
{    list.add(value);}
public void parquet-mr_f258_0(float value)
{    list.add(value);}
public void parquet-mr_f259_0(double value)
{    list.add(value);}
public void parquet-mr_f260_0(Object value)
{    list.add(value);}
public void parquet-mr_f261_0(Object value)
{    isSet = true;    setter.add(value);}
public void parquet-mr_f262_0(byte value)
{    isSet = true;    setter.addByte(value);}
public void parquet-mr_f263_0(boolean value)
{    isSet = true;    setter.addBoolean(value);}
public void parquet-mr_f264_0(char value)
{    isSet = true;    setter.addChar(value);}
public void parquet-mr_f265_0(short value)
{    isSet = true;    setter.addShort(value);}
public void parquet-mr_f266_0(int value)
{    isSet = true;    setter.addInt(value);}
public void parquet-mr_f267_0(long value)
{    isSet = true;    setter.addLong(value);}
public void parquet-mr_f268_0(float value)
{    isSet = true;    setter.addFloat(value);}
public void parquet-mr_f269_0(double value)
{    isSet = true;    setter.addDouble(value);}
public Converter parquet-mr_f270_0(int fieldIndex)
{    Preconditions.checkArgument(fieldIndex == 0, "Illegal field index: " + fieldIndex);    return elementConverter;}
public void parquet-mr_f271_0()
{    isSet = false;}
public void parquet-mr_f272_0()
{    if (!isSet) {        container.add(null);    }}
 static boolean parquet-mr_f273_0(Type repeatedType, Schema elementSchema)
{    if (repeatedType.isPrimitive() || repeatedType.asGroupType().getFieldCount() > 1 || repeatedType.asGroupType().getType(0).isRepetition(REPEATED)) {                return true;    } else if (elementSchema != null && elementSchema.getType() == Schema.Type.RECORD) {        Schema schemaFromRepeated = CONVERTER.convert(repeatedType.asGroupType());        if (checkReaderWriterCompatibility(elementSchema, schemaFromRepeated).getType() == COMPATIBLE) {            return true;        }    }    return false;}
public void parquet-mr_f274_0(Object value)
{    Preconditions.checkArgument(AvroUnionConverter.this.memberValue == null, "Union is resolving to more than one type");    memberValue = value;}
public Converter parquet-mr_f275_0(int fieldIndex)
{    return memberConverters[fieldIndex];}
public void parquet-mr_f276_0()
{    memberValue = null;}
public void parquet-mr_f277_0()
{    parent.add(memberValue);}
public Converter parquet-mr_f278_0(int fieldIndex)
{    return keyValueConverter;}
public void parquet-mr_f279_0()
{    this.map = newMap();}
public void parquet-mr_f280_0()
{    parent.add(map);}
private Map<K, V> parquet-mr_f281_0()
{    if (mapClass == null || mapClass.isAssignableFrom(HashMap.class)) {        return new HashMap<K, V>();    } else {        return (Map<K, V>) ReflectData.newInstance(mapClass, schema);    }}
public void parquet-mr_f282_0(Object value)
{    MapKeyValueConverter.this.key = (K) value;}
public void parquet-mr_f283_0(Object value)
{    MapKeyValueConverter.this.value = (V) value;}
public Converter parquet-mr_f284_0(int fieldIndex)
{    if (fieldIndex == 0) {        return keyConverter;    } else if (fieldIndex == 1) {        return valueConverter;    }    throw new IllegalArgumentException("only the key (0) and value (1) fields expected: " + fieldIndex);}
public void parquet-mr_f285_0()
{    key = null;    value = null;}
public void parquet-mr_f286_0()
{    map.put(key, value);}
public T parquet-mr_f287_0()
{    return root.getCurrentRecord();}
public GroupConverter parquet-mr_f288_0()
{    return root;}
public static Schema parquet-mr_f289_0(Schema schema)
{    if (schema.getType().equals(Schema.Type.UNION)) {        List<Schema> schemas = schema.getTypes();        if (schemas.size() == 2) {            if (schemas.get(0).getType().equals(Schema.Type.NULL)) {                return schemas.get(1);            } else if (schemas.get(1).getType().equals(Schema.Type.NULL)) {                return schemas.get(0);            } else {                return schema;            }        } else {            return schema;        }    } else {        return schema;    }}
public MessageType parquet-mr_f290_0(Schema avroSchema)
{    if (!avroSchema.getType().equals(Schema.Type.RECORD)) {        throw new IllegalArgumentException("Avro schema must be a record.");    }    return new MessageType(avroSchema.getFullName(), convertFields(avroSchema.getFields()));}
private List<Type> parquet-mr_f291_0(List<Schema.Field> fields)
{    List<Type> types = new ArrayList<Type>();    for (Schema.Field field : fields) {        if (field.schema().getType().equals(Schema.Type.NULL)) {                        continue;        }        types.add(convertField(field));    }    return types;}
private Type parquet-mr_f292_0(String fieldName, Schema schema)
{    return convertField(fieldName, schema, Type.Repetition.REQUIRED);}
private Type parquet-mr_f293_0(String fieldName, Schema schema, Type.Repetition repetition)
{    Types.PrimitiveBuilder<PrimitiveType> builder;    Schema.Type type = schema.getType();    if (type.equals(Schema.Type.BOOLEAN)) {        builder = Types.primitive(BOOLEAN, repetition);    } else if (type.equals(Schema.Type.INT)) {        builder = Types.primitive(INT32, repetition);    } else if (type.equals(Schema.Type.LONG)) {        builder = Types.primitive(INT64, repetition);    } else if (type.equals(Schema.Type.FLOAT)) {        builder = Types.primitive(FLOAT, repetition);    } else if (type.equals(Schema.Type.DOUBLE)) {        builder = Types.primitive(DOUBLE, repetition);    } else if (type.equals(Schema.Type.BYTES)) {        builder = Types.primitive(BINARY, repetition);    } else if (type.equals(Schema.Type.STRING)) {        builder = Types.primitive(BINARY, repetition).as(stringType());    } else if (type.equals(Schema.Type.RECORD)) {        return new GroupType(repetition, fieldName, convertFields(schema.getFields()));    } else if (type.equals(Schema.Type.ENUM)) {        builder = Types.primitive(BINARY, repetition).as(enumType());    } else if (type.equals(Schema.Type.ARRAY)) {        if (writeOldListStructure) {            return ConversionPatterns.listType(repetition, fieldName, convertField("array", schema.getElementType(), REPEATED));        } else {            return ConversionPatterns.listOfElements(repetition, fieldName, convertField(AvroWriteSupport.LIST_ELEMENT_NAME, schema.getElementType()));        }    } else if (type.equals(Schema.Type.MAP)) {        Type valType = convertField("value", schema.getValueType());                return ConversionPatterns.stringKeyMapType(repetition, fieldName, valType);    } else if (type.equals(Schema.Type.FIXED)) {        builder = Types.primitive(FIXED_LEN_BYTE_ARRAY, repetition).length(schema.getFixedSize());    } else if (type.equals(Schema.Type.UNION)) {        return convertUnion(fieldName, schema, repetition);    } else {        throw new UnsupportedOperationException("Cannot convert Avro type " + type);    }            LogicalType logicalType = schema.getLogicalType();    if (logicalType != null) {        if (logicalType instanceof LogicalTypes.Decimal) {            LogicalTypes.Decimal decimal = (LogicalTypes.Decimal) logicalType;            builder = builder.as(decimalType(decimal.getScale(), decimal.getPrecision()));        } else {            LogicalTypeAnnotation annotation = convertLogicalType(logicalType);            if (annotation != null) {                builder.as(annotation);            }        }    }    return builder.named(fieldName);}
private Type parquet-mr_f294_0(String fieldName, Schema schema, Type.Repetition repetition)
{    List<Schema> nonNullSchemas = new ArrayList<Schema>(schema.getTypes().size());        boolean foundNullSchema = false;    for (Schema childSchema : schema.getTypes()) {        if (childSchema.getType().equals(Schema.Type.NULL)) {            foundNullSchema = true;            if (Type.Repetition.REQUIRED == repetition) {                repetition = Type.Repetition.OPTIONAL;            }        } else {            nonNullSchemas.add(childSchema);        }    }        switch(nonNullSchemas.size()) {        case 0:            throw new UnsupportedOperationException("Cannot convert Avro union of only nulls");        case 1:            return foundNullSchema ? convertField(fieldName, nonNullSchemas.get(0), repetition) : convertUnionToGroupType(fieldName, repetition, nonNullSchemas);        default:                        return convertUnionToGroupType(fieldName, repetition, nonNullSchemas);    }}
private Type parquet-mr_f295_0(String fieldName, Type.Repetition repetition, List<Schema> nonNullSchemas)
{    List<Type> unionTypes = new ArrayList<Type>(nonNullSchemas.size());    int index = 0;    for (Schema childSchema : nonNullSchemas) {        unionTypes.add(convertField("member" + index++, childSchema, Type.Repetition.OPTIONAL));    }    return new GroupType(repetition, fieldName, unionTypes);}
private Type parquet-mr_f296_0(Schema.Field field)
{    return convertField(field.name(), field.schema());}
public Schema parquet-mr_f297_0(MessageType parquetSchema)
{    return convertFields(parquetSchema.getName(), parquetSchema.getFields(), new HashMap<>());}
 Schema parquet-mr_f298_0(GroupType parquetSchema)
{    return convertFields(parquetSchema.getName(), parquetSchema.getFields(), new HashMap<>());}
private Schema parquet-mr_f299_0(String name, List<Type> parquetFields, Map<String, Integer> names)
{    List<Schema.Field> fields = new ArrayList<Schema.Field>();    Integer nameCount = names.merge(name, 1, (oldValue, value) -> oldValue + 1);    for (Type parquetType : parquetFields) {        Schema fieldSchema = convertField(parquetType, names);        if (parquetType.isRepetition(REPEATED)) {            throw new UnsupportedOperationException("REPEATED not supported outside LIST or MAP. Type: " + parquetType);        } else if (parquetType.isRepetition(Type.Repetition.OPTIONAL)) {            fields.add(new Schema.Field(parquetType.getName(), optional(fieldSchema), null, NULL_VALUE));        } else {                        fields.add(new Schema.Field(parquetType.getName(), fieldSchema, null, (Object) null));        }    }    Schema schema = Schema.createRecord(name, null, nameCount > 1 ? name + nameCount : null, false);    schema.setFields(fields);    return schema;}
private Schema parquet-mr_f300_0(final Type parquetType, Map<String, Integer> names)
{    if (parquetType.isPrimitive()) {        final PrimitiveType asPrimitive = parquetType.asPrimitiveType();        final PrimitiveTypeName parquetPrimitiveTypeName = asPrimitive.getPrimitiveTypeName();        final LogicalTypeAnnotation annotation = parquetType.getLogicalTypeAnnotation();        Schema schema = parquetPrimitiveTypeName.convert(new PrimitiveType.PrimitiveTypeNameConverter<Schema, RuntimeException>() {            @Override            public Schema convertBOOLEAN(PrimitiveTypeName primitiveTypeName) {                return Schema.create(Schema.Type.BOOLEAN);            }            @Override            public Schema convertINT32(PrimitiveTypeName primitiveTypeName) {                return Schema.create(Schema.Type.INT);            }            @Override            public Schema convertINT64(PrimitiveTypeName primitiveTypeName) {                return Schema.create(Schema.Type.LONG);            }            @Override            public Schema convertINT96(PrimitiveTypeName primitiveTypeName) {                throw new IllegalArgumentException("INT96 not implemented and is deprecated");            }            @Override            public Schema convertFLOAT(PrimitiveTypeName primitiveTypeName) {                return Schema.create(Schema.Type.FLOAT);            }            @Override            public Schema convertDOUBLE(PrimitiveTypeName primitiveTypeName) {                return Schema.create(Schema.Type.DOUBLE);            }            @Override            public Schema convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) {                int size = parquetType.asPrimitiveType().getTypeLength();                return Schema.createFixed(parquetType.getName(), null, null, size);            }            @Override            public Schema convertBINARY(PrimitiveTypeName primitiveTypeName) {                if (annotation instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation || annotation instanceof LogicalTypeAnnotation.EnumLogicalTypeAnnotation) {                    return Schema.create(Schema.Type.STRING);                } else {                    return Schema.create(Schema.Type.BYTES);                }            }        });        LogicalType logicalType = convertLogicalType(annotation);        if (logicalType != null && (!(annotation instanceof LogicalTypeAnnotation.DecimalLogicalTypeAnnotation) || parquetPrimitiveTypeName == BINARY || parquetPrimitiveTypeName == FIXED_LEN_BYTE_ARRAY)) {            schema = logicalType.addToSchema(schema);        }        return schema;    } else {        GroupType parquetGroupType = parquetType.asGroupType();        LogicalTypeAnnotation logicalTypeAnnotation = parquetGroupType.getLogicalTypeAnnotation();        if (logicalTypeAnnotation != null) {            return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<Schema>() {                @Override                public Optional<Schema> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType) {                    if (parquetGroupType.getFieldCount() != 1) {                        throw new UnsupportedOperationException("Invalid list type " + parquetGroupType);                    }                    Type repeatedType = parquetGroupType.getType(0);                    if (!repeatedType.isRepetition(REPEATED)) {                        throw new UnsupportedOperationException("Invalid list type " + parquetGroupType);                    }                    if (isElementType(repeatedType, parquetGroupType.getName())) {                                                return of(Schema.createArray(convertField(repeatedType, names)));                    } else {                        Type elementType = repeatedType.asGroupType().getType(0);                        if (elementType.isRepetition(Type.Repetition.OPTIONAL)) {                            return of(Schema.createArray(optional(convertField(elementType, names))));                        } else {                            return of(Schema.createArray(convertField(elementType, names)));                        }                    }                }                @Override                public                 Optional<Schema> visit(LogicalTypeAnnotation.MapKeyValueTypeAnnotation mapKeyValueLogicalType) {                    return visitMapOrMapKeyValue();                }                @Override                public Optional<Schema> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType) {                    return visitMapOrMapKeyValue();                }                private Optional<Schema> visitMapOrMapKeyValue() {                    if (parquetGroupType.getFieldCount() != 1 || parquetGroupType.getType(0).isPrimitive()) {                        throw new UnsupportedOperationException("Invalid map type " + parquetGroupType);                    }                    GroupType mapKeyValType = parquetGroupType.getType(0).asGroupType();                    if (!mapKeyValType.isRepetition(REPEATED) || mapKeyValType.getFieldCount() != 2) {                        throw new UnsupportedOperationException("Invalid map type " + parquetGroupType);                    }                    Type keyType = mapKeyValType.getType(0);                    if (!keyType.isPrimitive() || !keyType.asPrimitiveType().getPrimitiveTypeName().equals(PrimitiveTypeName.BINARY) || !keyType.getLogicalTypeAnnotation().equals(stringType())) {                        throw new IllegalArgumentException("Map key type must be binary (UTF8): " + keyType);                    }                    Type valueType = mapKeyValType.getType(1);                    if (valueType.isRepetition(Type.Repetition.OPTIONAL)) {                        return of(Schema.createMap(optional(convertField(valueType, names))));                    } else {                        return of(Schema.createMap(convertField(valueType, names)));                    }                }                @Override                public Optional<Schema> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType) {                    return of(Schema.create(Schema.Type.STRING));                }            }).orElseThrow(() -> new UnsupportedOperationException("Cannot convert Parquet type " + parquetType));        } else {                        return convertFields(parquetGroupType.getName(), parquetGroupType.getFields(), names);        }    }}
public Schema parquet-mr_f301_0(PrimitiveTypeName primitiveTypeName)
{    return Schema.create(Schema.Type.BOOLEAN);}
public Schema parquet-mr_f302_0(PrimitiveTypeName primitiveTypeName)
{    return Schema.create(Schema.Type.INT);}
public Schema parquet-mr_f303_0(PrimitiveTypeName primitiveTypeName)
{    return Schema.create(Schema.Type.LONG);}
public Schema parquet-mr_f304_0(PrimitiveTypeName primitiveTypeName)
{    throw new IllegalArgumentException("INT96 not implemented and is deprecated");}
public Schema parquet-mr_f305_0(PrimitiveTypeName primitiveTypeName)
{    return Schema.create(Schema.Type.FLOAT);}
public Schema parquet-mr_f306_0(PrimitiveTypeName primitiveTypeName)
{    return Schema.create(Schema.Type.DOUBLE);}
public Schema parquet-mr_f307_0(PrimitiveTypeName primitiveTypeName)
{    int size = parquetType.asPrimitiveType().getTypeLength();    return Schema.createFixed(parquetType.getName(), null, null, size);}
public Schema parquet-mr_f308_0(PrimitiveTypeName primitiveTypeName)
{    if (annotation instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation || annotation instanceof LogicalTypeAnnotation.EnumLogicalTypeAnnotation) {        return Schema.create(Schema.Type.STRING);    } else {        return Schema.create(Schema.Type.BYTES);    }}
public Optional<Schema> parquet-mr_f309_0(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    if (parquetGroupType.getFieldCount() != 1) {        throw new UnsupportedOperationException("Invalid list type " + parquetGroupType);    }    Type repeatedType = parquetGroupType.getType(0);    if (!repeatedType.isRepetition(REPEATED)) {        throw new UnsupportedOperationException("Invalid list type " + parquetGroupType);    }    if (isElementType(repeatedType, parquetGroupType.getName())) {                return of(Schema.createArray(convertField(repeatedType, names)));    } else {        Type elementType = repeatedType.asGroupType().getType(0);        if (elementType.isRepetition(Type.Repetition.OPTIONAL)) {            return of(Schema.createArray(optional(convertField(elementType, names))));        } else {            return of(Schema.createArray(convertField(elementType, names)));        }    }}
public Optional<Schema> parquet-mr_f310_0(LogicalTypeAnnotation.MapKeyValueTypeAnnotation mapKeyValueLogicalType)
{    return visitMapOrMapKeyValue();}
public Optional<Schema> parquet-mr_f311_0(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    return visitMapOrMapKeyValue();}
private Optional<Schema> parquet-mr_f312_0()
{    if (parquetGroupType.getFieldCount() != 1 || parquetGroupType.getType(0).isPrimitive()) {        throw new UnsupportedOperationException("Invalid map type " + parquetGroupType);    }    GroupType mapKeyValType = parquetGroupType.getType(0).asGroupType();    if (!mapKeyValType.isRepetition(REPEATED) || mapKeyValType.getFieldCount() != 2) {        throw new UnsupportedOperationException("Invalid map type " + parquetGroupType);    }    Type keyType = mapKeyValType.getType(0);    if (!keyType.isPrimitive() || !keyType.asPrimitiveType().getPrimitiveTypeName().equals(PrimitiveTypeName.BINARY) || !keyType.getLogicalTypeAnnotation().equals(stringType())) {        throw new IllegalArgumentException("Map key type must be binary (UTF8): " + keyType);    }    Type valueType = mapKeyValType.getType(1);    if (valueType.isRepetition(Type.Repetition.OPTIONAL)) {        return of(Schema.createMap(optional(convertField(valueType, names))));    } else {        return of(Schema.createMap(convertField(valueType, names)));    }}
public Optional<Schema> parquet-mr_f313_0(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType)
{    return of(Schema.create(Schema.Type.STRING));}
private LogicalTypeAnnotation parquet-mr_f314_0(LogicalType logicalType)
{    if (logicalType == null) {        return null;    } else if (logicalType instanceof LogicalTypes.Decimal) {        LogicalTypes.Decimal decimal = (LogicalTypes.Decimal) logicalType;        return decimalType(decimal.getScale(), decimal.getPrecision());    } else if (logicalType instanceof LogicalTypes.Date) {        return dateType();    } else if (logicalType instanceof LogicalTypes.TimeMillis) {        return timeType(true, MILLIS);    } else if (logicalType instanceof LogicalTypes.TimeMicros) {        return timeType(true, MICROS);    } else if (logicalType instanceof LogicalTypes.TimestampMillis) {        return timestampType(true, MILLIS);    } else if (logicalType instanceof LogicalTypes.TimestampMicros) {        return timestampType(true, MICROS);    }    return null;}
private LogicalType parquet-mr_f315_0(LogicalTypeAnnotation annotation)
{    if (annotation == null) {        return null;    }    return annotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<LogicalType>() {        @Override        public Optional<LogicalType> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(LogicalTypes.decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));        }        @Override        public Optional<LogicalType> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {            return of(LogicalTypes.date());        }        @Override        public Optional<LogicalType> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {            LogicalTypeAnnotation.TimeUnit unit = timeLogicalType.getUnit();            switch(unit) {                case MILLIS:                    return of(LogicalTypes.timeMillis());                case MICROS:                    return of(LogicalTypes.timeMicros());            }            return empty();        }        @Override        public Optional<LogicalType> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {            LogicalTypeAnnotation.TimeUnit unit = timestampLogicalType.getUnit();            switch(unit) {                case MILLIS:                    return of(LogicalTypes.timestampMillis());                case MICROS:                    return of(LogicalTypes.timestampMicros());            }            return empty();        }    }).orElse(null);}
public Optional<LogicalType> parquet-mr_f316_0(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(LogicalTypes.decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));}
public Optional<LogicalType> parquet-mr_f317_0(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    return of(LogicalTypes.date());}
public Optional<LogicalType> parquet-mr_f318_0(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    LogicalTypeAnnotation.TimeUnit unit = timeLogicalType.getUnit();    switch(unit) {        case MILLIS:            return of(LogicalTypes.timeMillis());        case MICROS:            return of(LogicalTypes.timeMicros());    }    return empty();}
public Optional<LogicalType> parquet-mr_f319_0(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    LogicalTypeAnnotation.TimeUnit unit = timestampLogicalType.getUnit();    switch(unit) {        case MILLIS:            return of(LogicalTypes.timestampMillis());        case MICROS:            return of(LogicalTypes.timestampMicros());    }    return empty();}
private boolean parquet-mr_f320_0(Type repeatedType, String parentName)
{    return (    repeatedType.isPrimitive() || repeatedType.asGroupType().getFieldCount() > 1 || repeatedType.asGroupType().getType(0).isRepetition(REPEATED) ||     repeatedType.getName().equals("array") || repeatedType.getName().equals(parentName + "_tuple") ||     assumeRepeatedIsListElement);}
private static Schema parquet-mr_f321_0(Schema original)
{        return Schema.createUnion(Arrays.asList(Schema.create(Schema.Type.NULL), original));}
public static void parquet-mr_f322_0(Configuration configuration, Class<? extends AvroDataSupplier> suppClass)
{    configuration.set(AVRO_DATA_SUPPLIER, suppClass.getName());}
public String parquet-mr_f323_0()
{    return "avro";}
public static void parquet-mr_f324_0(Configuration configuration, Schema schema)
{    configuration.set(AVRO_SCHEMA, schema.toString());}
public WriteContext parquet-mr_f325_0(Configuration configuration)
{    if (rootAvroSchema == null) {        this.rootAvroSchema = new Schema.Parser().parse(configuration.get(AVRO_SCHEMA));        this.rootSchema = new AvroSchemaConverter().convert(rootAvroSchema);    }    if (model == null) {        this.model = getDataModel(configuration);    }    boolean writeOldListStructure = configuration.getBoolean(WRITE_OLD_LIST_STRUCTURE, WRITE_OLD_LIST_STRUCTURE_DEFAULT);    if (writeOldListStructure) {        this.listWriter = new TwoLevelListWriter();    } else {        this.listWriter = new ThreeLevelListWriter();    }    Map<String, String> extraMetaData = new HashMap<String, String>();    extraMetaData.put(AvroReadSupport.AVRO_SCHEMA_METADATA_KEY, rootAvroSchema.toString());    return new WriteContext(rootSchema, extraMetaData);}
public void parquet-mr_f326_0(RecordConsumer recordConsumer)
{    this.recordConsumer = recordConsumer;}
public void parquet-mr_f327_0(IndexedRecord record)
{    write((T) record);}
public void parquet-mr_f328_0(T record)
{    if (rootLogicalType != null) {        Conversion<?> conversion = model.getConversionByClass(record.getClass(), rootLogicalType);        recordConsumer.startMessage();        writeRecordFields(rootSchema, rootAvroSchema, convert(rootAvroSchema, rootLogicalType, conversion, record));        recordConsumer.endMessage();    } else {        recordConsumer.startMessage();        writeRecordFields(rootSchema, rootAvroSchema, record);        recordConsumer.endMessage();    }}
private void parquet-mr_f329_0(GroupType schema, Schema avroSchema, Object record)
{    recordConsumer.startGroup();    writeRecordFields(schema, avroSchema, record);    recordConsumer.endGroup();}
private void parquet-mr_f330_0(GroupType schema, Schema avroSchema, Object record)
{    List<Type> fields = schema.getFields();    List<Schema.Field> avroFields = avroSchema.getFields();        int index = 0;    for (int avroIndex = 0; avroIndex < avroFields.size(); avroIndex++) {        Schema.Field avroField = avroFields.get(avroIndex);        if (avroField.schema().getType().equals(Schema.Type.NULL)) {            continue;        }        Type fieldType = fields.get(index);        Object value = model.getField(record, avroField.name(), avroIndex);        if (value != null) {            recordConsumer.startField(fieldType.getName(), index);            writeValue(fieldType, avroField.schema(), value);            recordConsumer.endField(fieldType.getName(), index);        } else if (fieldType.isRepetition(Type.Repetition.REQUIRED)) {            throw new RuntimeException("Null-value for required field: " + avroField.name());        }        index++;    }}
private void parquet-mr_f331_0(GroupType schema, Schema avroSchema, Map<CharSequence, V> map)
{    GroupType innerGroup = schema.getType(0).asGroupType();    Type keyType = innerGroup.getType(0);    Type valueType = innerGroup.getType(1);        recordConsumer.startGroup();    if (map.size() > 0) {        recordConsumer.startField(MAP_REPEATED_NAME, 0);        for (Map.Entry<CharSequence, V> entry : map.entrySet()) {                        recordConsumer.startGroup();            recordConsumer.startField(MAP_KEY_NAME, 0);            writeValue(keyType, MAP_KEY_SCHEMA, entry.getKey());            recordConsumer.endField(MAP_KEY_NAME, 0);            V value = entry.getValue();            if (value != null) {                recordConsumer.startField(MAP_VALUE_NAME, 1);                writeValue(valueType, avroSchema.getValueType(), value);                recordConsumer.endField(MAP_VALUE_NAME, 1);            } else if (!valueType.isRepetition(Type.Repetition.OPTIONAL)) {                throw new RuntimeException("Null map value for " + avroSchema.getName());            }            recordConsumer.endGroup();        }        recordConsumer.endField(MAP_REPEATED_NAME, 0);    }    recordConsumer.endGroup();}
private void parquet-mr_f332_0(GroupType parquetSchema, Schema avroSchema, Object value)
{    recordConsumer.startGroup();            int avroIndex = model.resolveUnion(avroSchema, value);        GroupType parquetGroup = parquetSchema.asGroupType();    int parquetIndex = avroIndex;    for (int i = 0; i < avroIndex; i++) {        if (avroSchema.getTypes().get(i).getType().equals(Schema.Type.NULL)) {            parquetIndex--;        }    }                String memberName = "member" + parquetIndex;    recordConsumer.startField(memberName, parquetIndex);    writeValue(parquetGroup.getType(parquetIndex), avroSchema.getTypes().get(avroIndex), value);    recordConsumer.endField(memberName, parquetIndex);    recordConsumer.endGroup();}
private void parquet-mr_f333_0(Type type, Schema avroSchema, Object value)
{    Schema nonNullAvroSchema = AvroSchemaConverter.getNonNull(avroSchema);    LogicalType logicalType = nonNullAvroSchema.getLogicalType();    if (logicalType != null) {        Conversion<?> conversion = model.getConversionByClass(value.getClass(), logicalType);        writeValueWithoutConversion(type, nonNullAvroSchema, convert(nonNullAvroSchema, logicalType, conversion, value));    } else {        writeValueWithoutConversion(type, nonNullAvroSchema, value);    }}
private Object parquet-mr_f334_0(Schema schema, LogicalType logicalType, Conversion<D> conversion, Object datum)
{    if (conversion == null) {        return datum;    }    Class<D> fromClass = conversion.getConvertedType();    switch(schema.getType()) {        case RECORD:            return conversion.toRecord(fromClass.cast(datum), schema, logicalType);        case ENUM:            return conversion.toEnumSymbol(fromClass.cast(datum), schema, logicalType);        case ARRAY:            return conversion.toArray(fromClass.cast(datum), schema, logicalType);        case MAP:            return conversion.toMap(fromClass.cast(datum), schema, logicalType);        case FIXED:            return conversion.toFixed(fromClass.cast(datum), schema, logicalType);        case STRING:            return conversion.toCharSequence(fromClass.cast(datum), schema, logicalType);        case BYTES:            return conversion.toBytes(fromClass.cast(datum), schema, logicalType);        case INT:            return conversion.toInt(fromClass.cast(datum), schema, logicalType);        case LONG:            return conversion.toLong(fromClass.cast(datum), schema, logicalType);        case FLOAT:            return conversion.toFloat(fromClass.cast(datum), schema, logicalType);        case DOUBLE:            return conversion.toDouble(fromClass.cast(datum), schema, logicalType);        case BOOLEAN:            return conversion.toBoolean(fromClass.cast(datum), schema, logicalType);    }    return datum;}
private void parquet-mr_f335_0(Type type, Schema avroSchema, Object value)
{    switch(avroSchema.getType()) {        case BOOLEAN:            recordConsumer.addBoolean((Boolean) value);            break;        case INT:            if (value instanceof Character) {                recordConsumer.addInteger((Character) value);            } else {                recordConsumer.addInteger(((Number) value).intValue());            }            break;        case LONG:            recordConsumer.addLong(((Number) value).longValue());            break;        case FLOAT:            recordConsumer.addFloat(((Number) value).floatValue());            break;        case DOUBLE:            recordConsumer.addDouble(((Number) value).doubleValue());            break;        case FIXED:            recordConsumer.addBinary(Binary.fromReusedByteArray(((GenericFixed) value).bytes()));            break;        case BYTES:            if (value instanceof byte[]) {                recordConsumer.addBinary(Binary.fromReusedByteArray((byte[]) value));            } else {                recordConsumer.addBinary(Binary.fromReusedByteBuffer((ByteBuffer) value));            }            break;        case STRING:            recordConsumer.addBinary(fromAvroString(value));            break;        case RECORD:            writeRecord(type.asGroupType(), avroSchema, value);            break;        case ENUM:            recordConsumer.addBinary(Binary.fromString(value.toString()));            break;        case ARRAY:            listWriter.writeList(type.asGroupType(), avroSchema, value);            break;        case MAP:            writeMap(type.asGroupType(), avroSchema, (Map<CharSequence, ?>) value);            break;        case UNION:            writeUnion(type.asGroupType(), avroSchema, value);            break;    }}
private Binary parquet-mr_f336_0(Object value)
{    if (value instanceof Utf8) {        Utf8 utf8 = (Utf8) value;        return Binary.fromReusedByteArray(utf8.getBytes(), 0, utf8.getByteLength());    } else if (value instanceof CharSequence) {        return Binary.fromCharSequence((CharSequence) value);    }    return Binary.fromCharSequence(value.toString());}
private static GenericData parquet-mr_f337_0(Configuration conf)
{    Class<? extends AvroDataSupplier> suppClass = conf.getClass(AVRO_DATA_SUPPLIER, SpecificDataSupplier.class, AvroDataSupplier.class);    return ReflectionUtils.newInstance(suppClass, conf).get();}
public void parquet-mr_f338_0(GroupType schema, Schema avroSchema, Object value)
{        recordConsumer.startGroup();    if (value instanceof Collection) {        writeCollection(schema, avroSchema, (Collection) value);    } else {        Class<?> arrayClass = value.getClass();        Preconditions.checkArgument(arrayClass.isArray(), "Cannot write unless collection or array: " + arrayClass.getName());        writeJavaArray(schema, avroSchema, arrayClass, value);    }    recordConsumer.endGroup();}
public void parquet-mr_f339_0(GroupType schema, Schema avroSchema, Class<?> arrayClass, Object value)
{    Class<?> elementClass = arrayClass.getComponentType();    if (!elementClass.isPrimitive()) {        writeObjectArray(schema, avroSchema, (Object[]) value);        return;    }    switch(avroSchema.getElementType().getType()) {        case BOOLEAN:            Preconditions.checkArgument(elementClass == boolean.class, "Cannot write as boolean array: " + arrayClass.getName());            writeBooleanArray((boolean[]) value);            break;        case INT:            if (elementClass == byte.class) {                writeByteArray((byte[]) value);            } else if (elementClass == char.class) {                writeCharArray((char[]) value);            } else if (elementClass == short.class) {                writeShortArray((short[]) value);            } else if (elementClass == int.class) {                writeIntArray((int[]) value);            } else {                throw new IllegalArgumentException("Cannot write as an int array: " + arrayClass.getName());            }            break;        case LONG:            Preconditions.checkArgument(elementClass == long.class, "Cannot write as long array: " + arrayClass.getName());            writeLongArray((long[]) value);            break;        case FLOAT:            Preconditions.checkArgument(elementClass == float.class, "Cannot write as float array: " + arrayClass.getName());            writeFloatArray((float[]) value);            break;        case DOUBLE:            Preconditions.checkArgument(elementClass == double.class, "Cannot write as double array: " + arrayClass.getName());            writeDoubleArray((double[]) value);            break;        default:            throw new IllegalArgumentException("Cannot write " + avroSchema.getElementType() + " array: " + arrayClass.getName());    }}
protected void parquet-mr_f340_0(boolean[] array)
{    if (array.length > 0) {        startArray();        for (boolean element : array) {            recordConsumer.addBoolean(element);        }        endArray();    }}
protected void parquet-mr_f341_0(byte[] array)
{    if (array.length > 0) {        startArray();        for (byte element : array) {            recordConsumer.addInteger(element);        }        endArray();    }}
protected void parquet-mr_f342_0(short[] array)
{    if (array.length > 0) {        startArray();        for (short element : array) {            recordConsumer.addInteger(element);        }        endArray();    }}
protected void parquet-mr_f343_0(char[] array)
{    if (array.length > 0) {        startArray();        for (char element : array) {            recordConsumer.addInteger(element);        }        endArray();    }}
protected void parquet-mr_f344_0(int[] array)
{    if (array.length > 0) {        startArray();        for (int element : array) {            recordConsumer.addInteger(element);        }        endArray();    }}
protected void parquet-mr_f345_0(long[] array)
{    if (array.length > 0) {        startArray();        for (long element : array) {            recordConsumer.addLong(element);        }        endArray();    }}
protected void parquet-mr_f346_0(float[] array)
{    if (array.length > 0) {        startArray();        for (float element : array) {            recordConsumer.addFloat(element);        }        endArray();    }}
protected void parquet-mr_f347_0(double[] array)
{    if (array.length > 0) {        startArray();        for (double element : array) {            recordConsumer.addDouble(element);        }        endArray();    }}
public void parquet-mr_f348_0(GroupType schema, Schema avroSchema, Collection<?> array)
{    if (array.size() > 0) {        recordConsumer.startField(OLD_LIST_REPEATED_NAME, 0);        try {            for (Object elt : array) {                writeValue(schema.getType(0), avroSchema.getElementType(), elt);            }        } catch (NullPointerException e) {                        int i = 0;            for (Object elt : array) {                if (elt == null) {                    throw new NullPointerException("Array contains a null element at " + i + "\n" + "Set parquet.avro.write-old-list-structure=false to turn " + "on support for arrays with null elements.");                }                i += 1;            }                        throw e;        }        recordConsumer.endField(OLD_LIST_REPEATED_NAME, 0);    }}
protected void parquet-mr_f349_0(GroupType type, Schema schema, Object[] array)
{    if (array.length > 0) {        recordConsumer.startField(OLD_LIST_REPEATED_NAME, 0);        try {            for (Object element : array) {                writeValue(type.getType(0), schema.getElementType(), element);            }        } catch (NullPointerException e) {                        for (int i = 0; i < array.length; i += 1) {                if (array[i] == null) {                    throw new NullPointerException("Array contains a null element at " + i + "\n" + "Set parquet.avro.write-old-list-structure=false to turn " + "on support for arrays with null elements.");                }            }                        throw e;        }        recordConsumer.endField(OLD_LIST_REPEATED_NAME, 0);    }}
protected void parquet-mr_f350_0()
{    recordConsumer.startField(OLD_LIST_REPEATED_NAME, 0);}
protected void parquet-mr_f351_0()
{    recordConsumer.endField(OLD_LIST_REPEATED_NAME, 0);}
protected void parquet-mr_f352_0(GroupType type, Schema schema, Collection<?> collection)
{    if (collection.size() > 0) {        recordConsumer.startField(LIST_REPEATED_NAME, 0);        GroupType repeatedType = type.getType(0).asGroupType();        Type elementType = repeatedType.getType(0);        for (Object element : collection) {                        recordConsumer.startGroup();            if (element != null) {                recordConsumer.startField(LIST_ELEMENT_NAME, 0);                writeValue(elementType, schema.getElementType(), element);                recordConsumer.endField(LIST_ELEMENT_NAME, 0);            } else if (!elementType.isRepetition(Type.Repetition.OPTIONAL)) {                throw new RuntimeException("Null list element for " + schema.getName());            }            recordConsumer.endGroup();        }        recordConsumer.endField(LIST_REPEATED_NAME, 0);    }}
protected void parquet-mr_f353_0(GroupType type, Schema schema, Object[] array)
{    if (array.length > 0) {        recordConsumer.startField(LIST_REPEATED_NAME, 0);        GroupType repeatedType = type.getType(0).asGroupType();        Type elementType = repeatedType.getType(0);        for (Object element : array) {                        recordConsumer.startGroup();            if (element != null) {                recordConsumer.startField(LIST_ELEMENT_NAME, 0);                writeValue(elementType, schema.getElementType(), element);                recordConsumer.endField(LIST_ELEMENT_NAME, 0);            } else if (!elementType.isRepetition(Type.Repetition.OPTIONAL)) {                throw new RuntimeException("Null list element for " + schema.getName());            }            recordConsumer.endGroup();        }        recordConsumer.endField(LIST_REPEATED_NAME, 0);    }}
protected void parquet-mr_f354_0()
{    recordConsumer.startField(LIST_REPEATED_NAME, 0);        recordConsumer.startGroup();    recordConsumer.startField(LIST_ELEMENT_NAME, 0);}
protected void parquet-mr_f355_0()
{    recordConsumer.endField(LIST_ELEMENT_NAME, 0);    recordConsumer.endGroup();    recordConsumer.endField(LIST_REPEATED_NAME, 0);}
public GenericData parquet-mr_f356_0()
{    return GenericData.get();}
public void parquet-mr_f357_0(Object value)
{    throw new RuntimeException("[BUG] ParentValueContainer#add was not overridden");}
public void parquet-mr_f358_0(boolean value)
{    add(value);}
public void parquet-mr_f359_0(byte value)
{    add(value);}
public void parquet-mr_f360_0(char value)
{    add(value);}
public void parquet-mr_f361_0(short value)
{    add(value);}
public void parquet-mr_f362_0(int value)
{    add(value);}
public void parquet-mr_f363_0(long value)
{    add(value);}
public void parquet-mr_f364_0(float value)
{    add(value);}
public void parquet-mr_f365_0(double value)
{    add(value);}
public void parquet-mr_f366_0(double value)
{    wrapped.add(conversion.fromDouble(value, schema, logicalType));}
public void parquet-mr_f367_0(float value)
{    wrapped.add(conversion.fromFloat(value, schema, logicalType));}
public void parquet-mr_f368_0(long value)
{    wrapped.add(conversion.fromLong(value, schema, logicalType));}
public void parquet-mr_f369_0(int value)
{    wrapped.add(conversion.fromInt(value, schema, logicalType));}
public void parquet-mr_f370_0(short value)
{    wrapped.add(conversion.fromInt((int) value, schema, logicalType));}
public void parquet-mr_f371_0(char value)
{    wrapped.add(conversion.fromInt((int) value, schema, logicalType));}
public void parquet-mr_f372_0(byte value)
{    wrapped.add(conversion.fromInt((int) value, schema, logicalType));}
public void parquet-mr_f373_0(boolean value)
{    wrapped.add(conversion.fromBoolean(value, schema, logicalType));}
 static ParentValueContainer parquet-mr_f374_0(final ParentValueContainer parent, final Conversion<?> conversion, final Schema schema)
{    if (conversion == null) {        return parent;    }    final LogicalType logicalType = schema.getLogicalType();    switch(schema.getType()) {        case STRING:            return new ParentValueContainer() {                @Override                public void add(Object value) {                    parent.add(conversion.fromCharSequence((CharSequence) value, schema, logicalType));                }            };        case BOOLEAN:            return new LogicalTypePrimitiveContainer(parent, schema, conversion) {                @Override                public void add(Object value) {                    parent.add(conversion.fromBoolean((Boolean) value, schema, logicalType));                }            };        case INT:            return new LogicalTypePrimitiveContainer(parent, schema, conversion) {                @Override                public void add(Object value) {                    parent.add(conversion.fromInt((Integer) value, schema, logicalType));                }            };        case LONG:            return new LogicalTypePrimitiveContainer(parent, schema, conversion) {                @Override                public void add(Object value) {                    parent.add(conversion.fromLong((Long) value, schema, logicalType));                }            };        case FLOAT:            return new LogicalTypePrimitiveContainer(parent, schema, conversion) {                @Override                public void add(Object value) {                    parent.add(conversion.fromFloat((Float) value, schema, logicalType));                }            };        case DOUBLE:            return new LogicalTypePrimitiveContainer(parent, schema, conversion) {                @Override                public void add(Object value) {                    parent.add(conversion.fromDouble((Double) value, schema, logicalType));                }            };        case BYTES:            return new ParentValueContainer() {                @Override                public void add(Object value) {                    parent.add(conversion.fromBytes((ByteBuffer) value, schema, logicalType));                }            };        case FIXED:            return new ParentValueContainer() {                @Override                public void add(Object value) {                    parent.add(conversion.fromFixed((GenericData.Fixed) value, schema, logicalType));                }            };        case RECORD:            return new ParentValueContainer() {                @Override                public void add(Object value) {                    parent.add(conversion.fromRecord((IndexedRecord) value, schema, logicalType));                }            };        case ARRAY:            return new ParentValueContainer() {                @Override                public void add(Object value) {                    parent.add(conversion.fromArray((Collection<?>) value, schema, logicalType));                }            };        case MAP:            return new ParentValueContainer() {                @Override                public void add(Object value) {                    parent.add(conversion.fromMap((Map<?, ?>) value, schema, logicalType));                }            };        case ENUM:            return new ParentValueContainer() {                @Override                public void add(Object value) {                    parent.add(conversion.fromEnumSymbol((GenericEnumSymbol) value, schema, logicalType));                }            };        default:            return new LogicalTypePrimitiveContainer(parent, schema, conversion);    }}
public void parquet-mr_f375_0(Object value)
{    parent.add(conversion.fromCharSequence((CharSequence) value, schema, logicalType));}
public void parquet-mr_f376_0(Object value)
{    parent.add(conversion.fromBoolean((Boolean) value, schema, logicalType));}
public void parquet-mr_f377_0(Object value)
{    parent.add(conversion.fromInt((Integer) value, schema, logicalType));}
public void parquet-mr_f378_0(Object value)
{    parent.add(conversion.fromLong((Long) value, schema, logicalType));}
public void parquet-mr_f379_0(Object value)
{    parent.add(conversion.fromFloat((Float) value, schema, logicalType));}
public void parquet-mr_f380_0(Object value)
{    parent.add(conversion.fromDouble((Double) value, schema, logicalType));}
public void parquet-mr_f381_0(Object value)
{    parent.add(conversion.fromBytes((ByteBuffer) value, schema, logicalType));}
public void parquet-mr_f382_0(Object value)
{    parent.add(conversion.fromFixed((GenericData.Fixed) value, schema, logicalType));}
public void parquet-mr_f383_0(Object value)
{    parent.add(conversion.fromRecord((IndexedRecord) value, schema, logicalType));}
public void parquet-mr_f384_0(Object value)
{    parent.add(conversion.fromArray((Collection<?>) value, schema, logicalType));}
public void parquet-mr_f385_0(Object value)
{    parent.add(conversion.fromMap((Map<?, ?>) value, schema, logicalType));}
public void parquet-mr_f386_0(Object value)
{    parent.add(conversion.fromEnumSymbol((GenericEnumSymbol) value, schema, logicalType));}
public GenericData parquet-mr_f387_0()
{    return ReflectData.get();}
public GenericData parquet-mr_f388_0()
{    return SpecificData.get();}
public static Schema parquet-mr_f389_0(String name, String namespace, Schema.Field... fields)
{    Schema record = Schema.createRecord(name, null, namespace, false);    record.setFields(Arrays.asList(fields));    return record;}
public static Schema parquet-mr_f390_0(String name, Schema.Field... fields)
{    return record(name, null, fields);}
public static Schema.Field parquet-mr_f391_0(String name, Schema schema)
{    return new Schema.Field(name, schema, null, null);}
public static Schema.Field parquet-mr_f392_0(String name, Schema schema)
{    return new Schema.Field(name, optional(schema), null, JsonProperties.NULL_VALUE);}
public static Schema parquet-mr_f393_0(Schema element)
{    return Schema.createArray(element);}
public static Schema parquet-mr_f394_0(Schema.Type type)
{    return Schema.create(type);}
public static Schema parquet-mr_f395_0(Schema original)
{    return Schema.createUnion(Lists.newArrayList(Schema.create(Schema.Type.NULL), original));}
public static GenericRecord parquet-mr_f396_0(Schema schema, Object... pairs)
{    if ((pairs.length % 2) != 0) {        throw new RuntimeException("Not enough values");    }    GenericRecord record = new GenericData.Record(schema);    for (int i = 0; i < pairs.length; i += 2) {        record.put(pairs[i].toString(), pairs[i + 1]);    }    return record;}
public static List<D> parquet-mr_f397_0(GenericData model, Schema schema, File file) throws IOException
{    List<D> data = new ArrayList<D>();    Configuration conf = new Configuration(false);    AvroReadSupport.setRequestedProjection(conf, schema);    AvroReadSupport.setAvroReadSchema(conf, schema);    try (ParquetReader<D> fileReader = AvroParquetReader.<D>builder(new Path(file.toString())).withDataModel(    model).withConf(conf).build()) {        D datum;        while ((datum = fileReader.read()) != null) {            data.add(datum);        }    }    return data;}
public static File parquet-mr_f398_0(TemporaryFolder temp, GenericData model, Schema schema, D... data) throws IOException
{    File file = temp.newFile();    Assert.assertTrue(file.delete());    try (ParquetWriter<D> writer = AvroParquetWriter.<D>builder(new Path(file.toString())).withDataModel(model).withSchema(schema).build()) {        for (D datum : data) {            writer.write(datum);        }    }    return file;}
public static void parquet-mr_f399_0()
{    OLD_BEHAVIOR_CONF.setBoolean(AvroSchemaConverter.ADD_LIST_ELEMENT_RECORDS, true);    NEW_BEHAVIOR_CONF.setBoolean(AvroSchemaConverter.ADD_LIST_ELEMENT_RECORDS, false);}
public void parquet-mr_f400_0() throws Exception
{    Path test = writeDirect("message UnannotatedListOfPrimitives {" + "  repeated int32 list_of_ints;" + "}", rc -> {        rc.startMessage();        rc.startField("list_of_ints", 0);        rc.addInteger(34);        rc.addInteger(35);        rc.addInteger(36);        rc.endField("list_of_ints", 0);        rc.endMessage();    });    Schema expectedSchema = record("OldPrimitiveInList", field("list_of_ints", array(primitive(Schema.Type.INT))));    GenericRecord expectedRecord = instance(expectedSchema, "list_of_ints", Arrays.asList(34, 35, 36));        assertReaderContains(oldBehaviorReader(test), expectedSchema, expectedRecord);    assertReaderContains(newBehaviorReader(test), expectedSchema, expectedRecord);}
public void parquet-mr_f401_0() throws Exception
{    Path test = writeDirect("message UnannotatedListOfGroups {" + "  repeated group list_of_points {" + "    required float x;" + "    required float y;" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("list_of_points", 0);        rc.startGroup();        rc.startField("x", 0);        rc.addFloat(1.0f);        rc.endField("x", 0);        rc.startField("y", 1);        rc.addFloat(1.0f);        rc.endField("y", 1);        rc.endGroup();        rc.startGroup();        rc.startField("x", 0);        rc.addFloat(2.0f);        rc.endField("x", 0);        rc.startField("y", 1);        rc.addFloat(2.0f);        rc.endField("y", 1);        rc.endGroup();        rc.endField("list_of_points", 0);        rc.endMessage();    });    Schema point = record("?", field("x", primitive(Schema.Type.FLOAT)), field("y", primitive(Schema.Type.FLOAT)));    Schema expectedSchema = record("OldPrimitiveInList", field("list_of_points", array(point)));    GenericRecord expectedRecord = instance(expectedSchema, "list_of_points", Arrays.asList(instance(point, "x", 1.0f, "y", 1.0f), instance(point, "x", 2.0f, "y", 2.0f)));        assertReaderContains(oldBehaviorReader(test), expectedSchema, expectedRecord);    assertReaderContains(newBehaviorReader(test), expectedSchema, expectedRecord);}
public void parquet-mr_f402_0() throws Exception
{    Path test = writeDirect("message RepeatedPrimitiveInList {" + "  required group list_of_ints (LIST) {" + "    repeated int32 array;" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("list_of_ints", 0);        rc.startGroup();        rc.startField("array", 0);        rc.addInteger(34);        rc.addInteger(35);        rc.addInteger(36);        rc.endField("array", 0);        rc.endGroup();        rc.endField("list_of_ints", 0);        rc.endMessage();    });    Schema expectedSchema = record("RepeatedPrimitiveInList", field("list_of_ints", array(Schema.create(Schema.Type.INT))));    GenericRecord expectedRecord = instance(expectedSchema, "list_of_ints", Arrays.asList(34, 35, 36));        assertReaderContains(oldBehaviorReader(test), expectedSchema, expectedRecord);    assertReaderContains(newBehaviorReader(test), expectedSchema, expectedRecord);}
public void parquet-mr_f403_0() throws Exception
{        Path test = writeDirect("message MultiFieldGroupInList {" + "  optional group locations (LIST) {" + "    repeated group element {" + "      required double latitude;" + "      required double longitude;" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(0.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(180.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema location = record("element", field("latitude", primitive(Schema.Type.DOUBLE)), field("longitude", primitive(Schema.Type.DOUBLE)));    Schema expectedSchema = record("MultiFieldGroupInList", optionalField("locations", array(location)));    GenericRecord expectedRecord = instance(expectedSchema, "locations", Arrays.asList(instance(location, "latitude", 0.0, "longitude", 0.0), instance(location, "latitude", 0.0, "longitude", 180.0)));        assertReaderContains(oldBehaviorReader(test), expectedSchema, expectedRecord);    assertReaderContains(newBehaviorReader(test), expectedSchema, expectedRecord);}
public void parquet-mr_f404_0() throws Exception
{        Path test = writeDirect("message SingleFieldGroupInList {" + "  optional group single_element_groups (LIST) {" + "    repeated group single_element_group {" + "      required int64 count;" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("single_element_groups", 0);        rc.startGroup();                rc.startField("single_element_group", 0);        rc.startGroup();        rc.startField("count", 0);        rc.addLong(1234L);        rc.endField("count", 0);        rc.endGroup();        rc.startGroup();        rc.startField("count", 0);        rc.addLong(2345L);        rc.endField("count", 0);        rc.endGroup();                rc.endField("single_element_group", 0);        rc.endGroup();        rc.endField("single_element_groups", 0);        rc.endMessage();    });                Schema singleElementGroupSchema = record("single_element_group", field("count", primitive(Schema.Type.LONG)));    Schema oldSchema = record("SingleFieldGroupInList", optionalField("single_element_groups", array(singleElementGroupSchema)));    GenericRecord oldRecord = instance(oldSchema, "single_element_groups", Arrays.asList(instance(singleElementGroupSchema, "count", 1234L), instance(singleElementGroupSchema, "count", 2345L)));    assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);        Schema newSchema = record("SingleFieldGroupInList", optionalField("single_element_groups", array(primitive(Schema.Type.LONG))));    GenericRecord newRecord = instance(newSchema, "single_element_groups", Arrays.asList(1234L, 2345L));    assertReaderContains(newBehaviorReader(test), newSchema, newRecord);}
public void parquet-mr_f405_0() throws Exception
{            Schema singleElementRecord = record("single_element_group", field("count", primitive(Schema.Type.LONG)));    Schema expectedSchema = record("SingleFieldGroupInList", optionalField("single_element_groups", array(singleElementRecord)));    Map<String, String> metadata = new HashMap<String, String>();    metadata.put(AvroWriteSupport.AVRO_SCHEMA, expectedSchema.toString());    Path test = writeDirect("message SingleFieldGroupInList {" + "  optional group single_element_groups (LIST) {" + "    repeated group single_element_group {" + "      required int64 count;" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("single_element_groups", 0);        rc.startGroup();                rc.startField("single_element_group", 0);        rc.startGroup();        rc.startField("count", 0);        rc.addLong(1234L);        rc.endField("count", 0);        rc.endGroup();        rc.startGroup();        rc.startField("count", 0);        rc.addLong(2345L);        rc.endField("count", 0);        rc.endGroup();                rc.endField("single_element_group", 0);        rc.endGroup();        rc.endField("single_element_groups", 0);        rc.endMessage();    }, metadata);    GenericRecord expectedRecord = instance(expectedSchema, "single_element_groups", Arrays.asList(instance(singleElementRecord, "count", 1234L), instance(singleElementRecord, "count", 2345L)));        assertReaderContains(oldBehaviorReader(test), expectedSchema, expectedRecord);    assertReaderContains(newBehaviorReader(test), expectedSchema, expectedRecord);}
public void parquet-mr_f406_0() throws Exception
{    Path test = writeDirect("message NewOptionalGroupInList {" + "  optional group locations (LIST) {" + "    repeated group list {" + "      optional group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("list", 0);                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(0.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                        rc.startGroup();                rc.endGroup();                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(180.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                rc.endField("list", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema location = record("element", field("latitude", primitive(Schema.Type.DOUBLE)), field("longitude", primitive(Schema.Type.DOUBLE)));        Schema elementRecord = record("list", optionalField("element", location));    Schema oldSchema = record("NewOptionalGroupInList", optionalField("locations", array(elementRecord)));    GenericRecord oldRecord = instance(oldSchema, "locations", Arrays.asList(instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 0.0)), instance(elementRecord), instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 180.0))));    assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);        Schema newSchema = record("NewOptionalGroupInList", optionalField("locations", array(optional(location))));    GenericRecord newRecord = instance(newSchema, "locations", Arrays.asList(instance(location, "latitude", 0.0, "longitude", 0.0), null, instance(location, "latitude", 0.0, "longitude", 180.0)));    assertReaderContains(newBehaviorReader(test), newSchema, newRecord);}
public void parquet-mr_f407_0() throws Exception
{    Path test = writeDirect("message NewRequiredGroupInList {" + "  optional group locations (LIST) {" + "    repeated group list {" + "      required group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("list", 0);                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(180.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(0.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                rc.endField("list", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema location = record("element", field("latitude", primitive(Schema.Type.DOUBLE)), field("longitude", primitive(Schema.Type.DOUBLE)));        Schema elementRecord = record("list", field("element", location));    Schema oldSchema = record("NewRequiredGroupInList", optionalField("locations", array(elementRecord)));    GenericRecord oldRecord = instance(oldSchema, "locations", Arrays.asList(instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 180.0)), instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 0.0))));    assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);        Schema newSchema = record("NewRequiredGroupInList", optionalField("locations", array(location)));    GenericRecord newRecord = instance(newSchema, "locations", Arrays.asList(instance(location, "latitude", 0.0, "longitude", 180.0), instance(location, "latitude", 0.0, "longitude", 0.0)));    assertReaderContains(newBehaviorReader(test), newSchema, newRecord);}
public void parquet-mr_f408_0() throws Exception
{    Path test = writeDirect("message AvroCompatOptionalGroupInList {" + "  optional group locations (LIST) {" + "    repeated group array {" + "      optional group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("array", 0);                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(180.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(0.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                rc.endField("array", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema location = record("element", field("latitude", primitive(Schema.Type.DOUBLE)), field("longitude", primitive(Schema.Type.DOUBLE)));        Schema elementRecord = record("array", optionalField("element", location));    Schema oldSchema = record("AvroCompatOptionalGroupInList", optionalField("locations", array(elementRecord)));    GenericRecord oldRecord = instance(oldSchema, "locations", Arrays.asList(instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 180.0)), instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 0.0))));        assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);    assertReaderContains(newBehaviorReader(test), oldSchema, oldRecord);}
public void parquet-mr_f409_0() throws Exception
{    Path test = writeDirect("message AvroCompatOptionalGroupInListWithSchema {" + "  optional group locations (LIST) {" + "    repeated group array {" + "      optional group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("array", 0);                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(180.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(0.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                rc.endField("array", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema location = record("element", field("latitude", primitive(Schema.Type.DOUBLE)), field("longitude", primitive(Schema.Type.DOUBLE)));    Schema newSchema = record("AvroCompatOptionalGroupInListWithSchema", optionalField("locations", array(optional(location))));    GenericRecord newRecord = instance(newSchema, "locations", Arrays.asList(instance(location, "latitude", 0.0, "longitude", 180.0), instance(location, "latitude", 0.0, "longitude", 0.0)));    Configuration oldConfWithSchema = new Configuration();    AvroReadSupport.setAvroReadSchema(oldConfWithSchema, newSchema);        assertReaderContains(new AvroParquetReader<GenericRecord>(oldConfWithSchema, test), newSchema, newRecord);    Configuration newConfWithSchema = new Configuration(NEW_BEHAVIOR_CONF);    AvroReadSupport.setAvroReadSchema(newConfWithSchema, newSchema);    assertReaderContains(new AvroParquetReader<GenericRecord>(newConfWithSchema, test), newSchema, newRecord);}
public void parquet-mr_f410_0() throws Exception
{    Path test = writeDirect("message AvroCompatListInList {" + "  optional group listOfLists (LIST) {" + "    repeated group array (LIST) {" + "      repeated int32 array;" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("array", 0);        rc.startGroup();                rc.startField("array", 0);                rc.addInteger(34);        rc.addInteger(35);        rc.addInteger(36);                rc.endField("array", 0);        rc.endGroup();                rc.startGroup();        rc.endGroup();        rc.startGroup();                rc.startField("array", 0);                rc.addInteger(32);        rc.addInteger(33);        rc.addInteger(34);                rc.endField("array", 0);        rc.endGroup();                rc.endField("array", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema listOfLists = array(array(primitive(Schema.Type.INT)));    Schema oldSchema = record("AvroCompatListInList", optionalField("listOfLists", listOfLists));    GenericRecord oldRecord = instance(oldSchema, "listOfLists", Arrays.asList(Arrays.asList(34, 35, 36), Arrays.asList(), Arrays.asList(32, 33, 34)));        assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);    assertReaderContains(newBehaviorReader(test), oldSchema, oldRecord);}
public void parquet-mr_f411_0() throws Exception
{    Path test = writeDirect("message ThriftCompatListInList {" + "  optional group listOfLists (LIST) {" + "    repeated group listOfLists_tuple (LIST) {" + "      repeated int32 listOfLists_tuple_tuple;" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("listOfLists_tuple", 0);        rc.startGroup();                rc.startField("listOfLists_tuple_tuple", 0);                rc.addInteger(34);        rc.addInteger(35);        rc.addInteger(36);                rc.endField("listOfLists_tuple_tuple", 0);        rc.endGroup();                rc.startGroup();        rc.endGroup();        rc.startGroup();                rc.startField("listOfLists_tuple_tuple", 0);                rc.addInteger(32);        rc.addInteger(33);        rc.addInteger(34);                rc.endField("listOfLists_tuple_tuple", 0);        rc.endGroup();                rc.endField("listOfLists_tuple", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema listOfLists = array(array(primitive(Schema.Type.INT)));    Schema oldSchema = record("ThriftCompatListInList", optionalField("listOfLists", listOfLists));    GenericRecord oldRecord = instance(oldSchema, "listOfLists", Arrays.asList(Arrays.asList(34, 35, 36), Arrays.asList(), Arrays.asList(32, 33, 34)));        assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);    assertReaderContains(newBehaviorReader(test), oldSchema, oldRecord);}
public void parquet-mr_f412_0() throws Exception
{    Path test = writeDirect("message ThriftCompatRequiredGroupInList {" + "  optional group locations (LIST) {" + "    repeated group locations_tuple {" + "      optional group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("locations_tuple", 0);                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(180.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(0.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                rc.endField("locations_tuple", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema location = record("element", field("latitude", primitive(Schema.Type.DOUBLE)), field("longitude", primitive(Schema.Type.DOUBLE)));        Schema elementRecord = record("locations_tuple", optionalField("element", location));    Schema oldSchema = record("ThriftCompatRequiredGroupInList", optionalField("locations", array(elementRecord)));    GenericRecord oldRecord = instance(oldSchema, "locations", Arrays.asList(instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 180.0)), instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 0.0))));        assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);    assertReaderContains(newBehaviorReader(test), oldSchema, oldRecord);}
public void parquet-mr_f413_0() throws Exception
{    Path test = writeDirect("message HiveCompatOptionalGroupInList {" + "  optional group locations (LIST) {" + "    repeated group bag {" + "      optional group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("bag", 0);                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(180.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(0.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                rc.endField("bag", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema location = record("element", field("latitude", primitive(Schema.Type.DOUBLE)), field("longitude", primitive(Schema.Type.DOUBLE)));        Schema elementRecord = record("bag", optionalField("element", location));    Schema oldSchema = record("HiveCompatOptionalGroupInList", optionalField("locations", array(elementRecord)));    GenericRecord oldRecord = instance(oldSchema, "locations", Arrays.asList(instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 180.0)), instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 0.0))));        assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);    Schema newSchema = record("HiveCompatOptionalGroupInList", optionalField("locations", array(optional(location))));    GenericRecord newRecord = instance(newSchema, "locations", Arrays.asList(instance(location, "latitude", 0.0, "longitude", 180.0), instance(location, "latitude", 0.0, "longitude", 0.0)));    assertReaderContains(newBehaviorReader(test), newSchema, newRecord);}
public void parquet-mr_f414_0() throws Exception
{    Path test = writeDirect("message ListOfSingleElementStructsWithElementField {" + "  optional group list_of_structs (LIST) {" + "    repeated group list {" + "      required group element {" + "        required float element;" + "      }" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("list_of_structs", 0);        rc.startGroup();                rc.startField("list", 0);                        rc.startGroup();        rc.startField("element", 0);                rc.startGroup();        rc.startField("element", 0);        rc.addFloat(33.0F);        rc.endField("element", 0);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                        rc.startGroup();        rc.startField("element", 0);                rc.startGroup();        rc.startField("element", 0);        rc.addFloat(34.0F);        rc.endField("element", 0);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                rc.endField("list", 0);        rc.endGroup();        rc.endField("list_of_structs", 0);        rc.endMessage();    });    Schema structWithElementField = record("element", field("element", primitive(Schema.Type.FLOAT)));        Schema elementRecord = record("list", field("element", structWithElementField));    Schema oldSchema = record("ListOfSingleElementStructsWithElementField", optionalField("list_of_structs", array(elementRecord)));    GenericRecord oldRecord = instance(oldSchema, "list_of_structs", Arrays.asList(instance(elementRecord, "element", instance(structWithElementField, "element", 33.0F)), instance(elementRecord, "element", instance(structWithElementField, "element", 34.0F))));        final MessageType fileSchema;    try (ParquetFileReader reader = ParquetFileReader.open(new Configuration(), test)) {        fileSchema = reader.getFileMetaData().getSchema();        Assert.assertEquals("Converted schema should assume 2-layer structure", oldSchema, new AvroSchemaConverter(OLD_BEHAVIOR_CONF).convert(fileSchema));    }        assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);    Schema newSchema = record("ListOfSingleElementStructsWithElementField", optionalField("list_of_structs", array(structWithElementField)));    GenericRecord newRecord = instance(newSchema, "list_of_structs", Arrays.asList(instance(structWithElementField, "element", 33.0F), instance(structWithElementField, "element", 34.0F)));        Assert.assertEquals("Converted schema should assume 3-layer structure", newSchema, new AvroSchemaConverter(NEW_BEHAVIOR_CONF).convert(fileSchema));    assertReaderContains(newBehaviorReader(test), newSchema, newRecord);        Schema structWithDoubleElementField = record("element", field("element", primitive(Schema.Type.DOUBLE)));    Schema doubleElementRecord = record("list", field("element", structWithDoubleElementField));    Schema oldDoubleSchema = record("ListOfSingleElementStructsWithElementField", optionalField("list_of_structs", array(doubleElementRecord)));    GenericRecord oldDoubleRecord = instance(oldDoubleSchema, "list_of_structs", Arrays.asList(instance(doubleElementRecord, "element", instance(structWithDoubleElementField, "element", 33.0)), instance(doubleElementRecord, "element", instance(structWithDoubleElementField, "element", 34.0))));    assertReaderContains(oldBehaviorReader(test, oldDoubleSchema), oldDoubleSchema, oldDoubleRecord);    Schema newDoubleSchema = record("ListOfSingleElementStructsWithElementField", optionalField("list_of_structs", array(structWithDoubleElementField)));    GenericRecord newDoubleRecord = instance(newDoubleSchema, "list_of_structs", Arrays.asList(instance(structWithDoubleElementField, "element", 33.0), instance(structWithDoubleElementField, "element", 34.0)));    assertReaderContains(newBehaviorReader(test, newDoubleSchema), newDoubleSchema, newDoubleRecord);}
public AvroParquetReader<T> parquet-mr_f415_0(Path path) throws IOException
{    return new AvroParquetReader<T>(OLD_BEHAVIOR_CONF, path);}
public AvroParquetReader<T> parquet-mr_f416_0(Path path, Schema expectedSchema) throws IOException
{    Configuration conf = new Configuration(OLD_BEHAVIOR_CONF);    AvroReadSupport.setAvroReadSchema(conf, expectedSchema);    return new AvroParquetReader<T>(conf, path);}
public AvroParquetReader<T> parquet-mr_f417_0(Path path) throws IOException
{    return new AvroParquetReader<T>(NEW_BEHAVIOR_CONF, path);}
public AvroParquetReader<T> parquet-mr_f418_0(Path path, Schema expectedSchema) throws IOException
{    Configuration conf = new Configuration(NEW_BEHAVIOR_CONF);    AvroReadSupport.setAvroReadSchema(conf, expectedSchema);    return new AvroParquetReader<T>(conf, path);}
public void parquet-mr_f419_0(AvroParquetReader<T> reader, Schema expectedSchema, T... expectedRecords) throws IOException
{    for (T expectedRecord : expectedRecords) {        T actualRecord = reader.read();        Assert.assertEquals("Should match expected schema", expectedSchema, actualRecord.getSchema());        Assert.assertEquals("Should match the expected record", expectedRecord, actualRecord);    }    Assert.assertNull("Should only contain " + expectedRecords.length + " record" + (expectedRecords.length == 1 ? "" : "s"), reader.read());}
public GenericData parquet-mr_f420_0()
{    return GenericData.get();}
public void parquet-mr_f421_0()
{    Configuration conf = new Configuration(false);    AvroReadSupport.setAvroDataSupplier(conf, GenericDataSupplier.class);    Assert.assertEquals("Should contain the class name", "org.apache.parquet.avro.TestAvroDataSupplier$GenericDataSupplier", conf.get(AvroReadSupport.AVRO_DATA_SUPPLIER));}
public static void parquet-mr_f422_0()
{    NEW_BEHAVIOR.setBoolean("parquet.avro.add-list-element-records", false);    NEW_BEHAVIOR.setBoolean("parquet.avro.write-old-list-structure", false);}
private void parquet-mr_f423_0(Schema avroSchema, String schemaString) throws Exception
{    testAvroToParquetConversion(new Configuration(false), avroSchema, schemaString);}
private void parquet-mr_f424_0(Configuration conf, Schema avroSchema, String schemaString) throws Exception
{    AvroSchemaConverter avroSchemaConverter = new AvroSchemaConverter(conf);    MessageType schema = avroSchemaConverter.convert(avroSchema);    MessageType expectedMT = MessageTypeParser.parseMessageType(schemaString);    assertEquals("converting " + schema + " to " + schemaString, expectedMT.toString(), schema.toString());}
private void parquet-mr_f425_0(Schema avroSchema, String schemaString) throws Exception
{    testParquetToAvroConversion(new Configuration(false), avroSchema, schemaString);}
private void parquet-mr_f426_0(Configuration conf, Schema avroSchema, String schemaString) throws Exception
{    AvroSchemaConverter avroSchemaConverter = new AvroSchemaConverter(conf);    Schema schema = avroSchemaConverter.convert(MessageTypeParser.parseMessageType(schemaString));    assertEquals("converting " + schemaString + " to " + avroSchema, avroSchema.toString(), schema.toString());}
private void parquet-mr_f427_0(Schema avroSchema, String schemaString) throws Exception
{    testRoundTripConversion(new Configuration(), avroSchema, schemaString);}
private void parquet-mr_f428_0(Configuration conf, Schema avroSchema, String schemaString) throws Exception
{    AvroSchemaConverter avroSchemaConverter = new AvroSchemaConverter(conf);    MessageType schema = avroSchemaConverter.convert(avroSchema);    MessageType expectedMT = MessageTypeParser.parseMessageType(schemaString);    assertEquals("converting " + schema + " to " + schemaString, expectedMT.toString(), schema.toString());    Schema convertedAvroSchema = avroSchemaConverter.convert(expectedMT);    assertEquals("converting " + expectedMT + " to " + avroSchema.toString(true), avroSchema.toString(), convertedAvroSchema.toString());}
public void parquet-mr_f429_0()
{    new AvroSchemaConverter().convert(Schema.create(INT));}
public void parquet-mr_f430_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("all.avsc").openStream());    testAvroToParquetConversion(NEW_BEHAVIOR, schema, "message org.apache.parquet.avro.myrecord {\n" +     "  required boolean myboolean;\n" + "  required int32 myint;\n" + "  required int64 mylong;\n" + "  required float myfloat;\n" + "  required double mydouble;\n" + "  required binary mybytes;\n" + "  required binary mystring (UTF8);\n" + "  required group mynestedrecord {\n" + "    required int32 mynestedint;\n" + "  }\n" + "  required binary myenum (ENUM);\n" + "  required group myarray (LIST) {\n" + "    repeated group list {\n" + "      required int32 element;\n" + "    }\n" + "  }\n" + "  required group myemptyarray (LIST) {\n" + "    repeated group list {\n" + "      required int32 element;\n" + "    }\n" + "  }\n" + "  optional group myoptionalarray (LIST) {\n" + "    repeated group list {\n" + "      required int32 element;\n" + "    }\n" + "  }\n" + "  required group myarrayofoptional (LIST) {\n" + "    repeated group list {\n" + "      optional int32 element;\n" + "    }\n" + "  }\n" + "  required group mymap (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      required int32 value;\n" + "    }\n" + "  }\n" + "  required group myemptymap (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      required int32 value;\n" + "    }\n" + "  }\n" + "  required fixed_len_byte_array(1) myfixed;\n" + "}\n");}
public void parquet-mr_f431_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("all.avsc").openStream());    testAvroToParquetConversion(schema, "message org.apache.parquet.avro.myrecord {\n" +     "  required boolean myboolean;\n" + "  required int32 myint;\n" + "  required int64 mylong;\n" + "  required float myfloat;\n" + "  required double mydouble;\n" + "  required binary mybytes;\n" + "  required binary mystring (UTF8);\n" + "  required group mynestedrecord {\n" + "    required int32 mynestedint;\n" + "  }\n" + "  required binary myenum (ENUM);\n" + "  required group myarray (LIST) {\n" + "    repeated int32 array;\n" + "  }\n" + "  required group myemptyarray (LIST) {\n" + "    repeated int32 array;\n" + "  }\n" + "  optional group myoptionalarray (LIST) {\n" + "    repeated int32 array;\n" + "  }\n" + "  required group myarrayofoptional (LIST) {\n" + "    repeated int32 array;\n" + "  }\n" + "  required group mymap (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      required int32 value;\n" + "    }\n" + "  }\n" + "  required group myemptymap (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      required int32 value;\n" + "    }\n" + "  }\n" + "  required fixed_len_byte_array(1) myfixed;\n" + "}\n");}
public void parquet-mr_f432_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("allFromParquetNewBehavior.avsc").openStream());        testParquetToAvroConversion(NEW_BEHAVIOR, schema, ALL_PARQUET_SCHEMA);}
public void parquet-mr_f433_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("allFromParquetOldBehavior.avsc").openStream());        testParquetToAvroConversion(schema, ALL_PARQUET_SCHEMA);}
public void parquet-mr_f434_0() throws Exception
{    MessageType parquetSchema = MessageTypeParser.parseMessageType("message myrecord {\n" + "  required group mymap (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required int32 key;\n" + "      required int32 value;\n" + "    }\n" + "  }\n" + "}\n");    new AvroSchemaConverter().convert(parquetSchema);}
public void parquet-mr_f435_0() throws Exception
{    Schema schema = Schema.createRecord("record1", null, null, false);    Schema optionalInt = optional(Schema.create(INT));    schema.setFields(Collections.singletonList(new Schema.Field("myint", optionalInt, null, JsonProperties.NULL_VALUE)));    testRoundTripConversion(schema, "message record1 {\n" + "  optional int32 myint;\n" + "}\n");}
public void parquet-mr_f436_0() throws Exception
{    Schema schema = Schema.createRecord("record1", null, null, false);    Schema optionalIntMap = Schema.createMap(optional(Schema.create(INT)));    schema.setFields(Arrays.asList(new Schema.Field("myintmap", optionalIntMap, null, null)));    testRoundTripConversion(schema, "message record1 {\n" + "  required group myintmap (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional int32 value;\n" + "    }\n" + "  }\n" + "}\n");}
public void parquet-mr_f437_0() throws Exception
{    Schema schema = Schema.createRecord("record1", null, null, false);    Schema optionalIntArray = Schema.createArray(optional(Schema.create(INT)));    schema.setFields(Arrays.asList(new Schema.Field("myintarray", optionalIntArray, null, null)));    testRoundTripConversion(NEW_BEHAVIOR, schema, "message record1 {\n" + "  required group myintarray (LIST) {\n" + "    repeated group list {\n" + "      optional int32 element;\n" + "    }\n" + "  }\n" + "}\n");}
public void parquet-mr_f438_0() throws Exception
{    Schema schema = Schema.createRecord("record2", null, null, false);    Schema multipleTypes = Schema.createUnion(Arrays.asList(Schema.create(Schema.Type.NULL), Schema.create(INT), Schema.create(Schema.Type.FLOAT)));    schema.setFields(Arrays.asList(new Schema.Field("myunion", multipleTypes, null, JsonProperties.NULL_VALUE)));            testAvroToParquetConversion(schema, "message record2 {\n" + "  optional group myunion {\n" + "    optional int32 member0;\n" + "    optional float member1;\n" + "  }\n" + "}\n");}
public void parquet-mr_f439_0() throws Exception
{    Schema innerRecord = Schema.createRecord("element", null, null, false);    Schema optionalString = optional(Schema.create(Schema.Type.STRING));    innerRecord.setFields(Lists.newArrayList(new Schema.Field("s1", optionalString, null, JsonProperties.NULL_VALUE), new Schema.Field("s2", optionalString, null, JsonProperties.NULL_VALUE)));    Schema schema = Schema.createRecord("HasArray", null, null, false);    schema.setFields(Lists.newArrayList(new Schema.Field("myarray", Schema.createArray(optional(innerRecord)), null, null)));    System.err.println("Avro schema: " + schema.toString(true));    testRoundTripConversion(NEW_BEHAVIOR, schema, "message HasArray {\n" + "  required group myarray (LIST) {\n" + "    repeated group list {\n" + "      optional group element {\n" + "        optional binary s1 (UTF8);\n" + "        optional binary s2 (UTF8);\n" + "      }\n" + "    }\n" + "  }\n" + "}\n");}
public void parquet-mr_f440_0() throws Exception
{    Schema innerRecord = Schema.createRecord("InnerRecord", null, null, false);    Schema optionalString = optional(Schema.create(Schema.Type.STRING));    innerRecord.setFields(Lists.newArrayList(new Schema.Field("s1", optionalString, null, JsonProperties.NULL_VALUE), new Schema.Field("s2", optionalString, null, JsonProperties.NULL_VALUE)));    Schema schema = Schema.createRecord("HasArray", null, null, false);    schema.setFields(Lists.newArrayList(new Schema.Field("myarray", Schema.createArray(optional(innerRecord)), null, null)));    System.err.println("Avro schema: " + schema.toString(true));        testAvroToParquetConversion(schema, "message HasArray {\n" + "  required group myarray (LIST) {\n" + "    repeated group array {\n" + "      optional binary s1 (UTF8);\n" + "      optional binary s2 (UTF8);\n" + "    }\n" + "  }\n" + "}\n");}
public void parquet-mr_f441_0() throws Exception
{    Schema listOfLists = optional(Schema.createArray(Schema.createArray(Schema.create(INT))));    Schema schema = Schema.createRecord("AvroCompatListInList", null, null, false);    schema.setFields(Lists.newArrayList(new Schema.Field("listOfLists", listOfLists, null, JsonProperties.NULL_VALUE)));    System.err.println("Avro schema: " + schema.toString(true));    testRoundTripConversion(schema, "message AvroCompatListInList {\n" + "  optional group listOfLists (LIST) {\n" + "    repeated group array (LIST) {\n" + "      repeated int32 array;\n" + "    }\n" + "  }\n" + "}");        testParquetToAvroConversion(NEW_BEHAVIOR, schema, "message AvroCompatListInList {\n" + "  optional group listOfLists (LIST) {\n" + "    repeated group array (LIST) {\n" + "      repeated int32 array;\n" + "    }\n" + "  }\n" + "}");}
public void parquet-mr_f442_0() throws Exception
{    Schema listOfLists = optional(Schema.createArray(Schema.createArray(Schema.create(INT))));    Schema schema = Schema.createRecord("ThriftCompatListInList", null, null, false);    schema.setFields(Lists.newArrayList(new Schema.Field("listOfLists", listOfLists, null, JsonProperties.NULL_VALUE)));    System.err.println("Avro schema: " + schema.toString(true));        testParquetToAvroConversion(schema, "message ThriftCompatListInList {\n" + "  optional group listOfLists (LIST) {\n" + "    repeated group listOfLists_tuple (LIST) {\n" + "      repeated int32 listOfLists_tuple_tuple;\n" + "    }\n" + "  }\n" + "}");        testParquetToAvroConversion(NEW_BEHAVIOR, schema, "message ThriftCompatListInList {\n" + "  optional group listOfLists (LIST) {\n" + "    repeated group listOfLists_tuple (LIST) {\n" + "      repeated int32 listOfLists_tuple_tuple;\n" + "    }\n" + "  }\n" + "}");}
public void parquet-mr_f443_0() throws Exception
{                Schema listOfLists = optional(Schema.createArray(Schema.createArray(Schema.create(INT))));    Schema schema = Schema.createRecord("UnknownTwoLevelListInList", null, null, false);    schema.setFields(Lists.newArrayList(new Schema.Field("listOfLists", listOfLists, null, JsonProperties.NULL_VALUE)));    System.err.println("Avro schema: " + schema.toString(true));        testParquetToAvroConversion(schema, "message UnknownTwoLevelListInList {\n" + "  optional group listOfLists (LIST) {\n" + "    repeated group mylist (LIST) {\n" + "      repeated int32 innerlist;\n" + "    }\n" + "  }\n" + "}");        testParquetToAvroConversion(NEW_BEHAVIOR, schema, "message UnknownTwoLevelListInList {\n" + "  optional group listOfLists (LIST) {\n" + "    repeated group mylist (LIST) {\n" + "      repeated int32 innerlist;\n" + "    }\n" + "  }\n" + "}");}
public void parquet-mr_f444_0() throws Exception
{    Schema schema = Schema.createRecord("myrecord", null, null, false);    Schema map = Schema.createMap(Schema.create(INT));    schema.setFields(Collections.singletonList(new Schema.Field("mymap", map, null, null)));    String parquetSchema = "message myrecord {\n" + "  required group mymap (MAP) {\n" + "    repeated group map {\n" + "      required binary key (UTF8);\n" + "      required int32 value;\n" + "    }\n" + "  }\n" + "}\n";    testParquetToAvroConversion(schema, parquetSchema);    testParquetToAvroConversion(NEW_BEHAVIOR, schema, parquetSchema);}
public void parquet-mr_f445_0() throws Exception
{    Schema schema = Schema.createRecord("myrecord", null, null, false);    Schema decimal = LogicalTypes.decimal(9, 2).addToSchema(Schema.create(Schema.Type.BYTES));    schema.setFields(Collections.singletonList(new Schema.Field("dec", decimal, null, null)));    testRoundTripConversion(schema, "message myrecord {\n" + "  required binary dec (DECIMAL(9,2));\n" + "}\n");}
public void parquet-mr_f446_0() throws Exception
{    Schema schema = Schema.createRecord("myrecord", null, null, false);    Schema decimal = LogicalTypes.decimal(9, 2).addToSchema(Schema.createFixed("dec", null, null, 8));    schema.setFields(Collections.singletonList(new Schema.Field("dec", decimal, null, null)));    testRoundTripConversion(schema, "message myrecord {\n" + "  required fixed_len_byte_array(8) dec (DECIMAL(9,2));\n" + "}\n");}
public void parquet-mr_f447_0() throws Exception
{    Schema expected = Schema.createRecord("myrecord", null, null, false, Arrays.asList(new Schema.Field("dec", Schema.create(INT), null, null)));        testParquetToAvroConversion(expected, "message myrecord {\n" + "  required int32 dec (DECIMAL(9,2));\n" + "}\n");}
public void parquet-mr_f448_0() throws Exception
{    Schema expected = Schema.createRecord("myrecord", null, null, false, Arrays.asList(new Schema.Field("dec", Schema.create(LONG), null, null)));        testParquetToAvroConversion(expected, "message myrecord {\n" + "  required int64 dec (DECIMAL(9,2));\n" + "}\n");}
public void parquet-mr_f449_0() throws Exception
{    Schema date = LogicalTypes.date().addToSchema(Schema.create(INT));    Schema expected = Schema.createRecord("myrecord", null, null, false, Arrays.asList(new Schema.Field("date", date, null, null)));    testRoundTripConversion(expected, "message myrecord {\n" + "  required int32 date (DATE);\n" + "}\n");    for (PrimitiveTypeName primitive : new PrimitiveTypeName[] { INT64, INT96, FLOAT, DOUBLE, BOOLEAN, BINARY, FIXED_LEN_BYTE_ARRAY }) {        final PrimitiveType type;        if (primitive == FIXED_LEN_BYTE_ARRAY) {            type = new PrimitiveType(REQUIRED, primitive, 12, "test", DATE);        } else {            type = new PrimitiveType(REQUIRED, primitive, "test", DATE);        }        assertThrows("Should not allow TIME_MICROS with " + primitive, IllegalArgumentException.class, () -> new AvroSchemaConverter().convert(message(type)));    }}
public void parquet-mr_f450_0() throws Exception
{    Schema date = LogicalTypes.timeMillis().addToSchema(Schema.create(INT));    Schema expected = Schema.createRecord("myrecord", null, null, false, Arrays.asList(new Schema.Field("time", date, null, null)));    testRoundTripConversion(expected, "message myrecord {\n" + "  required int32 time (TIME(MILLIS,true));\n" + "}\n");    for (PrimitiveTypeName primitive : new PrimitiveTypeName[] { INT64, INT96, FLOAT, DOUBLE, BOOLEAN, BINARY, FIXED_LEN_BYTE_ARRAY }) {        final PrimitiveType type;        if (primitive == FIXED_LEN_BYTE_ARRAY) {            type = new PrimitiveType(REQUIRED, primitive, 12, "test", TIME_MILLIS);        } else {            type = new PrimitiveType(REQUIRED, primitive, "test", TIME_MILLIS);        }        assertThrows("Should not allow TIME_MICROS with " + primitive, IllegalArgumentException.class, () -> new AvroSchemaConverter().convert(message(type)));    }}
public void parquet-mr_f451_0() throws Exception
{    Schema date = LogicalTypes.timeMicros().addToSchema(Schema.create(LONG));    Schema expected = Schema.createRecord("myrecord", null, null, false, Arrays.asList(new Schema.Field("time", date, null, null)));    testRoundTripConversion(expected, "message myrecord {\n" + "  required int64 time (TIME(MICROS,true));\n" + "}\n");    for (PrimitiveTypeName primitive : new PrimitiveTypeName[] { INT32, INT96, FLOAT, DOUBLE, BOOLEAN, BINARY, FIXED_LEN_BYTE_ARRAY }) {        final PrimitiveType type;        if (primitive == FIXED_LEN_BYTE_ARRAY) {            type = new PrimitiveType(REQUIRED, primitive, 12, "test", TIME_MICROS);        } else {            type = new PrimitiveType(REQUIRED, primitive, "test", TIME_MICROS);        }        assertThrows("Should not allow TIME_MICROS with " + primitive, IllegalArgumentException.class, () -> new AvroSchemaConverter().convert(message(type)));    }}
public void parquet-mr_f452_0() throws Exception
{    Schema date = LogicalTypes.timestampMillis().addToSchema(Schema.create(LONG));    Schema expected = Schema.createRecord("myrecord", null, null, false, Arrays.asList(new Schema.Field("timestamp", date, null, null)));    testRoundTripConversion(expected, "message myrecord {\n" + "  required int64 timestamp (TIMESTAMP(MILLIS,true));\n" + "}\n");    for (PrimitiveTypeName primitive : new PrimitiveTypeName[] { INT32, INT96, FLOAT, DOUBLE, BOOLEAN, BINARY, FIXED_LEN_BYTE_ARRAY }) {        final PrimitiveType type;        if (primitive == FIXED_LEN_BYTE_ARRAY) {            type = new PrimitiveType(REQUIRED, primitive, 12, "test", TIMESTAMP_MILLIS);        } else {            type = new PrimitiveType(REQUIRED, primitive, "test", TIMESTAMP_MILLIS);        }        assertThrows("Should not allow TIMESTAMP_MILLIS with " + primitive, IllegalArgumentException.class, () -> new AvroSchemaConverter().convert(message(type)));    }}
public void parquet-mr_f453_0() throws Exception
{    Schema date = LogicalTypes.timestampMicros().addToSchema(Schema.create(LONG));    Schema expected = Schema.createRecord("myrecord", null, null, false, Arrays.asList(new Schema.Field("timestamp", date, null, null)));    testRoundTripConversion(expected, "message myrecord {\n" + "  required int64 timestamp (TIMESTAMP(MICROS,true));\n" + "}\n");    for (PrimitiveTypeName primitive : new PrimitiveTypeName[] { INT32, INT96, FLOAT, DOUBLE, BOOLEAN, BINARY, FIXED_LEN_BYTE_ARRAY }) {        final PrimitiveType type;        if (primitive == FIXED_LEN_BYTE_ARRAY) {            type = new PrimitiveType(REQUIRED, primitive, 12, "test", TIMESTAMP_MICROS);        } else {            type = new PrimitiveType(REQUIRED, primitive, "test", TIMESTAMP_MICROS);        }        assertThrows("Should not allow TIMESTAMP_MICROS with " + primitive, IllegalArgumentException.class, () -> new AvroSchemaConverter().convert(message(type)));    }}
public void parquet-mr_f454_0() throws Exception
{    Schema innerA1 = record("a1", "a12", field("a4", primitive(Schema.Type.FLOAT)));    Schema outerA1 = record("a1", field("a2", primitive(Schema.Type.FLOAT)), optionalField("a1", innerA1));    Schema schema = record("Message", optionalField("a1", outerA1));    String parquetSchema = "message Message {\n" + "      optional group a1 {\n" + "        required float a2;\n" + "        optional group a1 {\n" + "          required float a4;\n" + "         }\n" + "      }\n" + "}\n";    testParquetToAvroConversion(schema, parquetSchema);    testParquetToAvroConversion(NEW_BEHAVIOR, schema, parquetSchema);}
public void parquet-mr_f455_0() throws Exception
{    Schema a2 = record("a2", field("a4", primitive(Schema.Type.FLOAT)));    Schema a22 = record("a2", "a22", field("a4", primitive(Schema.Type.FLOAT)), field("a5", primitive(Schema.Type.FLOAT)));    Schema a1 = record("a1", optionalField("a2", a2));    Schema a3 = record("a3", optionalField("a2", a22));    Schema schema = record("Message", optionalField("a1", a1), optionalField("a3", a3));    String parquetSchema = "message Message {\n" + "      optional group a1 {\n" + "        optional group a2 {\n" + "          required float a4;\n" + "         }\n" + "      }\n" + "      optional group a3 {\n" + "        optional group a2 {\n" + "          required float a4;\n" + "          required float a5;\n" + "         }\n" + "      }\n" + "}\n";    testParquetToAvroConversion(schema, parquetSchema);    testParquetToAvroConversion(NEW_BEHAVIOR, schema, parquetSchema);}
public static Schema parquet-mr_f456_0(Schema original)
{    return Schema.createUnion(Lists.newArrayList(Schema.create(Schema.Type.NULL), original));}
public static MessageType parquet-mr_f457_0(PrimitiveType primitive)
{    return Types.buildMessage().addField(primitive).named("myrecord");}
public static void parquet-mr_f458_0(String message, Class<? extends Exception> expected, Runnable runnable)
{    try {        runnable.run();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        try {            Assert.assertEquals(message, expected, actual.getClass());        } catch (AssertionError e) {            e.addSuppressed(actual);            throw e;        }    }}
public void parquet-mr_f459_0() throws IOException
{                    Path testFile = new Path(Resources.getResource("strings-2.parquet").getFile());    Configuration conf = new Configuration();    ParquetReader<GenericRecord> reader = AvroParquetReader.builder(new AvroReadSupport<GenericRecord>(), testFile).withConf(conf).build();    GenericRecord r;    while ((r = reader.read()) != null) {        Assert.assertTrue("Should read value into a String", r.get("text") instanceof String);    }}
public void parquet-mr_f460_0() throws IOException
{    Path testFile = new Path(Resources.getResource("strings-2.parquet").getFile());    Configuration conf = new Configuration();    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    ParquetReader<GenericRecord> reader = AvroParquetReader.builder(new AvroReadSupport<GenericRecord>(), testFile).withConf(conf).build();    GenericRecord r;    while ((r = reader.read()) != null) {        Assert.assertTrue("Should read value into a String", r.get("text") instanceof Utf8);    }}
public Schema parquet-mr_f461_0(Schema schema)
{    super.addToSchema(schema);    schema.addProp(REF_FIELD_NAME, refFieldName);    return schema;}
public String parquet-mr_f462_0()
{    return REFERENCE;}
public String parquet-mr_f463_0()
{    return refFieldName;}
public void parquet-mr_f464_0(Schema schema)
{    super.validate(schema);    if (schema.getField(refFieldName) == null) {        throw new IllegalArgumentException("Invalid field name for reference field: " + refFieldName);    }}
public Schema parquet-mr_f465_0(Schema schema)
{    super.addToSchema(schema);    schema.addProp(ID_FIELD_NAME, idFieldName);    return schema;}
public String parquet-mr_f466_0()
{    return REFERENCEABLE;}
public String parquet-mr_f467_0()
{    return idFieldName;}
public void parquet-mr_f468_0(Schema schema)
{    super.validate(schema);    Schema.Field idField = schema.getField(idFieldName);    if (idField == null || idField.schema().getType() != Schema.Type.LONG) {        throw new IllegalArgumentException("Invalid ID field: " + idFieldName + ": " + idField);    }}
public static void parquet-mr_f469_0()
{    LogicalTypes.register(Referenceable.REFERENCEABLE, new LogicalTypes.LogicalTypeFactory() {        @Override        public LogicalType fromSchema(Schema schema) {            return new Referenceable(schema);        }    });    LogicalTypes.register(Reference.REFERENCE, new LogicalTypes.LogicalTypeFactory() {        @Override        public LogicalType fromSchema(Schema schema) {            return new Reference(schema);        }    });}
public LogicalType parquet-mr_f470_0(Schema schema)
{    return new Referenceable(schema);}
public LogicalType parquet-mr_f471_0(Schema schema)
{    return new Reference(schema);}
public ReferenceableTracker parquet-mr_f472_0()
{    return tracker;}
public ReferenceHandler parquet-mr_f473_0()
{    return handler;}
public Class<IndexedRecord> parquet-mr_f474_0()
{    return (Class) Record.class;}
public String parquet-mr_f475_0()
{    return Referenceable.REFERENCEABLE;}
public IndexedRecord parquet-mr_f476_0(IndexedRecord value, Schema schema, LogicalType type)
{        long id = getId(value, schema);        references.put(id, value);        List<Callback> callbacks = callbacksById.get(id);    for (Callback callback : callbacks) {        callback.set(value);    }    return value;}
public IndexedRecord parquet-mr_f477_0(IndexedRecord value, Schema schema, LogicalType type)
{        long id = getId(value, schema);            ids.put(value, id);    return value;}
private long parquet-mr_f478_0(IndexedRecord referenceable, Schema schema)
{    Referenceable info = (Referenceable) schema.getLogicalType();    int idField = schema.getField(info.getIdFieldName()).pos();    return (Long) referenceable.get(idField);}
public Class<IndexedRecord> parquet-mr_f479_0()
{    return (Class) Record.class;}
public String parquet-mr_f480_0()
{    return Reference.REFERENCE;}
public IndexedRecord parquet-mr_f481_0(final IndexedRecord record, Schema schema, LogicalType type)
{        final Schema.Field refField = schema.getField(((Reference) type).getRefFieldName());    Long id = (Long) record.get(refField.pos());    if (id != null) {        if (references.containsKey(id)) {            record.put(refField.pos(), references.get(id));        } else {            List<Callback> callbacks = callbacksById.get(id);            if (callbacks == null) {                callbacks = new ArrayList<Callback>();                callbacksById.put(id, callbacks);            }                        callbacks.add(new Callback() {                @Override                public void set(Object referenceable) {                    record.put(refField.pos(), referenceable);                }            });        }    }    return record;}
public void parquet-mr_f482_0(Object referenceable)
{    record.put(refField.pos(), referenceable);}
public IndexedRecord parquet-mr_f483_0(IndexedRecord record, Schema schema, LogicalType type)
{        Schema.Field refField = schema.getField(((Reference) type).getRefFieldName());    IndexedRecord referenced = (IndexedRecord) record.get(refField.pos());    if (referenced == null) {        return record;    }        return new HijackingIndexedRecord(record, refField.pos(), ids.get(referenced));}
public void parquet-mr_f484_0(int i, Object v)
{    throw new RuntimeException("[BUG] This is a read-only class.");}
public Object parquet-mr_f485_0(int i)
{    if (i == index) {        return data;    }    return wrapped.get(i);}
public Schema parquet-mr_f486_0()
{    return wrapped.getSchema();}
public void parquet-mr_f487_0() throws IOException
{    ReferenceManager manager = new ReferenceManager();    GenericData model = new GenericData();    model.addLogicalTypeConversion(manager.getTracker());    model.addLogicalTypeConversion(manager.getHandler());    Schema parentSchema = Schema.createRecord("Parent", null, null, false);    Schema placeholderSchema = Schema.createRecord("Placeholder", null, null, false);    List<Schema.Field> placeholderFields = new ArrayList<Schema.Field>();        placeholderFields.add(new Schema.Field("id", Schema.create(Schema.Type.LONG), null, null));    placeholderSchema.setFields(placeholderFields);    Referenceable idRef = new Referenceable("id");    Schema parentRefSchema = Schema.createUnion(Schema.create(Schema.Type.NULL), Schema.create(Schema.Type.LONG), idRef.addToSchema(placeholderSchema));    Reference parentRef = new Reference("parent");    List<Schema.Field> childFields = new ArrayList<Schema.Field>();    childFields.add(new Schema.Field("c", Schema.create(Schema.Type.STRING), null, null));    childFields.add(new Schema.Field("parent", parentRefSchema, null, null));    Schema childSchema = parentRef.addToSchema(Schema.createRecord("Child", null, null, false, childFields));    List<Schema.Field> parentFields = new ArrayList<Schema.Field>();    parentFields.add(new Schema.Field("id", Schema.create(Schema.Type.LONG), null, null));    parentFields.add(new Schema.Field("p", Schema.create(Schema.Type.STRING), null, null));    parentFields.add(new Schema.Field("child", childSchema, null, null));    parentSchema.setFields(parentFields);    Schema schema = idRef.addToSchema(parentSchema);    System.out.println("Schema: " + schema.toString(true));    Record parent = new Record(schema);    parent.put("id", 1L);    parent.put("p", "parent data!");    Record child = new Record(childSchema);    child.put("c", "child data!");    child.put("parent", parent);    parent.put("child", child);        File data = AvroTestUtil.write(temp, model, schema, parent);    List<Record> records = AvroTestUtil.read(model, schema, data);    Record actual = records.get(0);        Assert.assertEquals("Should correctly read back the parent id", 1L, actual.get("id"));    Assert.assertEquals("Should correctly read back the parent data", new Utf8("parent data!"), actual.get("p"));    Record actualChild = (Record) actual.get("child");    Assert.assertEquals("Should correctly read back the child data", new Utf8("child data!"), actualChild.get("c"));    Object childParent = actualChild.get("parent");    Assert.assertTrue("Should have a parent Record object", childParent instanceof Record);    Record childParentRecord = (Record) actualChild.get("parent");    Assert.assertEquals("Should have the right parent id", 1L, childParentRecord.get("id"));    Assert.assertEquals("Should have the right parent data", new Utf8("parent data!"), childParentRecord.get("p"));}
public static void parquet-mr_f488_0()
{    GENERIC.addLogicalTypeConversion(new Conversions.DecimalConversion());    GENERIC.addLogicalTypeConversion(new Conversions.UUIDConversion());}
private List<T> parquet-mr_f489_0(Collection<GenericRecord> records, String field, Class<T> expectedClass)
{    List<T> values = new ArrayList<T>();    for (GenericRecord record : records) {        values.add(expectedClass.cast(record.get(field)));    }    return values;}
public void parquet-mr_f490_0() throws IOException
{    Schema uuidSchema = record("R", field("uuid", LogicalTypes.uuid().addToSchema(Schema.create(STRING))));    GenericRecord u1 = instance(uuidSchema, "uuid", UUID.randomUUID());    GenericRecord u2 = instance(uuidSchema, "uuid", UUID.randomUUID());    Schema stringSchema = record("R", field("uuid", Schema.create(STRING)));    GenericRecord s1 = instance(stringSchema, "uuid", u1.get("uuid").toString());    GenericRecord s2 = instance(stringSchema, "uuid", u2.get("uuid").toString());    File test = write(stringSchema, s1, s2);    Assert.assertEquals("Should convert Strings to UUIDs", Arrays.asList(u1, u2), read(GENERIC, uuidSchema, test));}
public void parquet-mr_f491_0() throws IOException
{    Schema uuidSchema = record("R", field("uuid", LogicalTypes.uuid().addToSchema(Schema.create(STRING))));    GenericRecord u1 = instance(uuidSchema, "uuid", UUID.randomUUID());    GenericRecord u2 = instance(uuidSchema, "uuid", UUID.randomUUID());    Schema stringUuidSchema = Schema.create(STRING);    stringUuidSchema.addProp(GenericData.STRING_PROP, "String");    Schema stringSchema = record("R", field("uuid", stringUuidSchema));    GenericRecord s1 = instance(stringSchema, "uuid", u1.get("uuid").toString());    GenericRecord s2 = instance(stringSchema, "uuid", u2.get("uuid").toString());    File test = write(GENERIC, uuidSchema, u1, u2);    Assert.assertEquals("Should read UUIDs as Strings", Arrays.asList(s1, s2), read(GENERIC, stringSchema, test));}
public void parquet-mr_f492_0() throws IOException
{    Schema uuidSchema = record("R", field("uuid", LogicalTypes.uuid().addToSchema(Schema.create(STRING))));    GenericRecord u1 = instance(uuidSchema, "uuid", UUID.randomUUID());    GenericRecord u2 = instance(uuidSchema, "uuid", UUID.randomUUID());    GenericRecord s1 = instance(uuidSchema, "uuid", new Utf8(u1.get("uuid").toString()));    GenericRecord s2 = instance(uuidSchema, "uuid", new Utf8(u2.get("uuid").toString()));    File test = write(GENERIC, uuidSchema, u1, u2);    Assert.assertEquals("Should read UUIDs as Strings", Arrays.asList(s1, s2), read(GenericData.get(), uuidSchema, test));}
public void parquet-mr_f493_0() throws IOException
{    Schema nullableUuidSchema = record("R", optionalField("uuid", LogicalTypes.uuid().addToSchema(Schema.create(STRING))));    GenericRecord u1 = instance(nullableUuidSchema, "uuid", UUID.randomUUID());    GenericRecord u2 = instance(nullableUuidSchema, "uuid", UUID.randomUUID());    Schema stringUuidSchema = Schema.create(STRING);    stringUuidSchema.addProp(GenericData.STRING_PROP, "String");    Schema nullableStringSchema = record("R", optionalField("uuid", stringUuidSchema));    GenericRecord s1 = instance(nullableStringSchema, "uuid", u1.get("uuid").toString());    GenericRecord s2 = instance(nullableStringSchema, "uuid", u2.get("uuid").toString());    File test = write(GENERIC, nullableUuidSchema, u1, u2);    Assert.assertEquals("Should read UUIDs as Strings", Arrays.asList(s1, s2), read(GENERIC, nullableStringSchema, test));}
public void parquet-mr_f494_0() throws IOException
{    Schema fixedSchema = Schema.createFixed("aFixed", null, null, 4);    Schema fixedRecord = record("R", field("dec", fixedSchema));    Schema decimalSchema = DECIMAL_9_2.addToSchema(Schema.createFixed("aFixed", null, null, 4));    Schema decimalRecord = record("R", field("dec", decimalSchema));    GenericRecord r1 = instance(decimalRecord, "dec", D1);    GenericRecord r2 = instance(decimalRecord, "dec", D2);    List<GenericRecord> expected = Arrays.asList(r1, r2);    Conversion<BigDecimal> conversion = new Conversions.DecimalConversion();        GenericRecord r1fixed = instance(fixedRecord, "dec", conversion.toFixed(D1, fixedSchema, DECIMAL_9_2));    GenericRecord r2fixed = instance(fixedRecord, "dec", conversion.toFixed(D2, fixedSchema, DECIMAL_9_2));    File test = write(fixedRecord, r1fixed, r2fixed);    Assert.assertEquals("Should convert fixed to BigDecimals", expected, read(GENERIC, decimalRecord, test));}
public void parquet-mr_f495_0() throws IOException
{    Schema fixedSchema = Schema.createFixed("aFixed", null, null, 4);    Schema fixedRecord = record("R", field("dec", fixedSchema));    Schema decimalSchema = DECIMAL_9_2.addToSchema(Schema.createFixed("aFixed", null, null, 4));    Schema decimalRecord = record("R", field("dec", decimalSchema));    GenericRecord r1 = instance(decimalRecord, "dec", D1);    GenericRecord r2 = instance(decimalRecord, "dec", D2);    Conversion<BigDecimal> conversion = new Conversions.DecimalConversion();        GenericRecord r1fixed = instance(fixedRecord, "dec", conversion.toFixed(D1, fixedSchema, DECIMAL_9_2));    GenericRecord r2fixed = instance(fixedRecord, "dec", conversion.toFixed(D2, fixedSchema, DECIMAL_9_2));    List<GenericRecord> expected = Arrays.asList(r1fixed, r2fixed);    File test = write(GENERIC, decimalRecord, r1, r2);    Assert.assertEquals("Should read BigDecimals as fixed", expected, read(GENERIC, fixedRecord, test));}
public void parquet-mr_f496_0() throws IOException
{    Schema bytesSchema = Schema.create(Schema.Type.BYTES);    Schema bytesRecord = record("R", field("dec", bytesSchema));    Schema decimalSchema = DECIMAL_9_2.addToSchema(Schema.create(Schema.Type.BYTES));    Schema decimalRecord = record("R", field("dec", decimalSchema));    GenericRecord r1 = instance(decimalRecord, "dec", D1);    GenericRecord r2 = instance(decimalRecord, "dec", D2);    List<GenericRecord> expected = Arrays.asList(r1, r2);    Conversion<BigDecimal> conversion = new Conversions.DecimalConversion();        GenericRecord r1bytes = instance(bytesRecord, "dec", conversion.toBytes(D1, bytesSchema, DECIMAL_9_2));    GenericRecord r2bytes = instance(bytesRecord, "dec", conversion.toBytes(D2, bytesSchema, DECIMAL_9_2));    File test = write(bytesRecord, r1bytes, r2bytes);    Assert.assertEquals("Should convert bytes to BigDecimals", expected, read(GENERIC, decimalRecord, test));}
public void parquet-mr_f497_0() throws IOException
{    Schema bytesSchema = Schema.create(Schema.Type.BYTES);    Schema bytesRecord = record("R", field("dec", bytesSchema));    Schema decimalSchema = DECIMAL_9_2.addToSchema(Schema.create(Schema.Type.BYTES));    Schema decimalRecord = record("R", field("dec", decimalSchema));    GenericRecord r1 = instance(decimalRecord, "dec", D1);    GenericRecord r2 = instance(decimalRecord, "dec", D2);    Conversion<BigDecimal> conversion = new Conversions.DecimalConversion();        GenericRecord r1bytes = instance(bytesRecord, "dec", conversion.toBytes(D1, bytesSchema, DECIMAL_9_2));    GenericRecord r2bytes = instance(bytesRecord, "dec", conversion.toBytes(D2, bytesSchema, DECIMAL_9_2));    List<GenericRecord> expected = Arrays.asList(r1bytes, r2bytes);    File test = write(GENERIC, decimalRecord, r1, r2);    Assert.assertEquals("Should read BigDecimals as bytes", expected, read(GENERIC, bytesRecord, test));}
private File parquet-mr_f498_0(Schema schema, D... data) throws IOException
{    return write(GenericData.get(), schema, data);}
private File parquet-mr_f499_0(GenericData model, Schema schema, D... data) throws IOException
{    return AvroTestUtil.write(temp, model, schema, data);}
public static GenericRecord parquet-mr_f500_0(Integer i)
{    return new GenericRecordBuilder(avroSchema).set("a", i).build();}
public void parquet-mr_f501_0(Context context) throws IOException, InterruptedException
{    for (int i = 0; i < 10; i++) {        GenericRecord a;        a = TestInputOutputFormat.nextRecord(i == 4 ? null : i);        context.write(null, a);    }}
protected void parquet-mr_f502_0(Void key, GenericRecord value, Context context) throws IOException, InterruptedException
{    context.write(null, new Text(value.toString()));}
public void parquet-mr_f503_0() throws Exception
{    final Configuration conf = new Configuration();    final Path inputPath = new Path("src/test/java/org/apache/parquet/avro/TestInputOutputFormat.java");    final Path parquetPath = new Path("target/test/hadoop/TestInputOutputFormat/parquet");    final Path outputPath = new Path("target/test/hadoop/TestInputOutputFormat/out");    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        final Job job = new Job(conf, "write");                TextInputFormat.addInputPath(job, inputPath);        job.setInputFormatClass(TextInputFormat.class);        job.setMapperClass(TestInputOutputFormat.MyMapper.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(AvroParquetOutputFormat.class);        AvroParquetOutputFormat.setOutputPath(job, parquetPath);        AvroParquetOutputFormat.setSchema(job, avroSchema);        waitForJob(job);    }    {        final Job job = new Job(conf, "read");        job.setInputFormatClass(AvroParquetInputFormat.class);        AvroParquetInputFormat.setInputPaths(job, parquetPath);        job.setMapperClass(TestInputOutputFormat.MyMapper2.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(TextOutputFormat.class);        TextOutputFormat.setOutputPath(job, outputPath);        waitForJob(job);    }    try (final BufferedReader out = new BufferedReader(new FileReader(new File(outputPath.toString(), "part-m-00000")))) {        String lineOut;        int lineNumber = 0;        while ((lineOut = out.readLine()) != null) {            lineOut = lineOut.substring(lineOut.indexOf("\t") + 1);            GenericRecord a = nextRecord(lineNumber == 4 ? null : lineNumber);            assertEquals("line " + lineNumber, a.toString(), lineOut);            ++lineNumber;        }        assertNull("line " + lineNumber, out.readLine());    }}
private void parquet-mr_f504_1(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {                sleep(100);    }        if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
public static Collection<Object[]> parquet-mr_f505_0()
{    Object[][] data = new Object[][] {     { false },     { true } };    return Arrays.asList(data);}
public void parquet-mr_f506_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("array.avsc").openStream());        List<Integer> emptyArray = new ArrayList<>();    Path file = new Path(createTempFile().getPath());    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(file).withSchema(schema).withConf(testConf).build()) {        GenericData.Record record = new GenericRecordBuilder(schema).set("myarray", emptyArray).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(emptyArray, nextRecord.get("myarray"));    }}
public void parquet-mr_f507_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("map.avsc").openStream());    Path file = new Path(createTempFile().getPath());    ImmutableMap<String, Integer> emptyMap = new ImmutableMap.Builder<String, Integer>().build();    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(file).withSchema(schema).withConf(testConf).build()) {                GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", emptyMap).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<GenericRecord>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(emptyMap, nextRecord.get("mymap"));    }}
public void parquet-mr_f508_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("map_with_nulls.avsc").openStream());    Path file = new Path(createTempFile().getPath());        Map<CharSequence, Integer> map = new HashMap<>();    map.put(str("thirty-four"), 34);    map.put(str("eleventy-one"), null);    map.put(str("one-hundred"), 100);    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(file).withSchema(schema).withConf(testConf).build()) {        GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", map).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(map, nextRecord.get("mymap"));    }}
public void parquet-mr_f509_0() throws Exception
{    Schema schema = Schema.createRecord("record1", null, null, false);    schema.setFields(Lists.newArrayList(new Schema.Field("mymap", Schema.createMap(Schema.create(Schema.Type.INT)), null, null)));    Path file = new Path(createTempFile().getPath());    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(file).withSchema(schema).withConf(testConf).build()) {                Map<String, Integer> map = new HashMap<String, Integer>();        map.put("thirty-four", 34);        map.put("eleventy-one", null);        map.put("one-hundred", 100);        GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", map).build();        writer.write(record);    }}
public void parquet-mr_f510_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("map.avsc").openStream());    Path file = new Path(createTempFile().getPath());    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(file).withSchema(schema).withConf(testConf).build()) {                GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", ImmutableMap.of(new Utf8("a"), 1, new Utf8("b"), 2)).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(ImmutableMap.of(str("a"), 1, str("b"), 2), nextRecord.get("mymap"));    }}
public void parquet-mr_f511_0() throws Exception
{    Schema decimalSchema = Schema.createRecord("myrecord", null, null, false);    Schema decimal = LogicalTypes.decimal(9, 2).addToSchema(Schema.create(Schema.Type.BYTES));    decimalSchema.setFields(Collections.singletonList(new Schema.Field("dec", decimal, null, null)));        GenericData decimalSupport = new GenericData();    decimalSupport.addLogicalTypeConversion(new Conversions.DecimalConversion());    File file = temp.newFile("decimal.parquet");    file.delete();    Path path = new Path(file.toString());    List<GenericRecord> expected = Lists.newArrayList();    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(path).withDataModel(decimalSupport).withSchema(decimalSchema).build()) {        Random random = new Random(34L);        GenericRecordBuilder builder = new GenericRecordBuilder(decimalSchema);        for (int i = 0; i < 1000; i += 1) {            BigDecimal dec = new BigDecimal(new BigInteger(31, random), 2);            builder.set("dec", dec);            GenericRecord rec = builder.build();            expected.add(rec);            writer.write(builder.build());        }    }    List<GenericRecord> records = Lists.newArrayList();    try (ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(path).withDataModel(decimalSupport).disableCompatibility().build()) {        GenericRecord rec;        while ((rec = reader.read()) != null) {            records.add(rec);        }    }    Assert.assertTrue("dec field should be a BigDecimal instance", records.get(0).get("dec") instanceof BigDecimal);    Assert.assertEquals("Content should match", expected, records);}
public void parquet-mr_f512_0() throws Exception
{    Schema decimalSchema = Schema.createRecord("myrecord", null, null, false);    Schema decimal = LogicalTypes.decimal(9, 2).addToSchema(Schema.createFixed("dec", null, null, 4));    decimalSchema.setFields(Collections.singletonList(new Schema.Field("dec", decimal, null, null)));        GenericData decimalSupport = new GenericData();    decimalSupport.addLogicalTypeConversion(new Conversions.DecimalConversion());    File file = temp.newFile("decimal.parquet");    file.delete();    Path path = new Path(file.toString());    List<GenericRecord> expected = Lists.newArrayList();    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(path).withDataModel(decimalSupport).withSchema(decimalSchema).build()) {        Random random = new Random(34L);        GenericRecordBuilder builder = new GenericRecordBuilder(decimalSchema);        for (int i = 0; i < 1000; i += 1) {            BigDecimal dec = new BigDecimal(new BigInteger(31, random), 2);            builder.set("dec", dec);            GenericRecord rec = builder.build();            expected.add(rec);            writer.write(builder.build());        }    }    List<GenericRecord> records = Lists.newArrayList();    try (ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(path).withDataModel(decimalSupport).disableCompatibility().build()) {        GenericRecord rec;        while ((rec = reader.read()) != null) {            records.add(rec);        }    }    Assert.assertTrue("dec field should be a BigDecimal instance", records.get(0).get("dec") instanceof BigDecimal);    Assert.assertEquals("Content should match", expected, records);}
public void parquet-mr_f513_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("all.avsc").openStream());    Path file = new Path(createTempFile().getPath());    List<Integer> integerArray = Arrays.asList(1, 2, 3);    GenericData.Record nestedRecord = new GenericRecordBuilder(schema.getField("mynestedrecord").schema()).set("mynestedint", 1).build();    List<Integer> emptyArray = new ArrayList<Integer>();    Schema arrayOfOptionalIntegers = Schema.createArray(optional(Schema.create(Schema.Type.INT)));    GenericData.Array<Integer> genericIntegerArrayWithNulls = new GenericData.Array<Integer>(arrayOfOptionalIntegers, Arrays.asList(1, null, 2, null, 3));    GenericFixed genericFixed = new GenericData.Fixed(Schema.createFixed("fixed", null, null, 1), new byte[] { (byte) 65 });    ImmutableMap<String, Integer> emptyMap = new ImmutableMap.Builder<String, Integer>().build();    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(file).withSchema(schema).withConf(testConf).build()) {        GenericData.Array<Integer> genericIntegerArray = new GenericData.Array<Integer>(Schema.createArray(Schema.create(Schema.Type.INT)), integerArray);        GenericData.Record record = new GenericRecordBuilder(schema).set("mynull", null).set("myboolean", true).set("myint", 1).set("mylong", 2L).set("myfloat", 3.1f).set("mydouble", 4.1).set("mybytes", ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8))).set("mystring", "hello").set("mynestedrecord", nestedRecord).set("myenum", "a").set("myarray", genericIntegerArray).set("myemptyarray", emptyArray).set("myoptionalarray", genericIntegerArray).set("myarrayofoptional", genericIntegerArrayWithNulls).set("mymap", ImmutableMap.of("a", 1, "b", 2)).set("myemptymap", emptyMap).set("myfixed", genericFixed).build();        writer.write(record);    }    final GenericRecord nextRecord;    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<GenericRecord>(testConf, file)) {        nextRecord = reader.read();    }    Object expectedEnumSymbol = compat ? "a" : new GenericData.EnumSymbol(schema.getField("myenum").schema(), "a");    assertNotNull(nextRecord);    assertEquals(null, nextRecord.get("mynull"));    assertEquals(true, nextRecord.get("myboolean"));    assertEquals(1, nextRecord.get("myint"));    assertEquals(2L, nextRecord.get("mylong"));    assertEquals(3.1f, nextRecord.get("myfloat"));    assertEquals(4.1, nextRecord.get("mydouble"));    assertEquals(ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)), nextRecord.get("mybytes"));    assertEquals(str("hello"), nextRecord.get("mystring"));    assertEquals(expectedEnumSymbol, nextRecord.get("myenum"));    assertEquals(nestedRecord, nextRecord.get("mynestedrecord"));    assertEquals(integerArray, nextRecord.get("myarray"));    assertEquals(emptyArray, nextRecord.get("myemptyarray"));    assertEquals(integerArray, nextRecord.get("myoptionalarray"));    assertEquals(genericIntegerArrayWithNulls, nextRecord.get("myarrayofoptional"));    assertEquals(ImmutableMap.of(str("a"), 1, str("b"), 2), nextRecord.get("mymap"));    assertEquals(emptyMap, nextRecord.get("myemptymap"));    assertEquals(genericFixed, nextRecord.get("myfixed"));}
public void parquet-mr_f514_0() throws Exception
{    Path file = new Path(createTempFile().getPath());        try (ParquetWriter<Map<String, Object>> parquetWriter = new ParquetWriter<>(file, new WriteSupport<Map<String, Object>>() {        private RecordConsumer recordConsumer;        @Override        public WriteContext init(Configuration configuration) {            return new WriteContext(MessageTypeParser.parseMessageType(TestAvroSchemaConverter.ALL_PARQUET_SCHEMA), new HashMap<String, String>());        }        @Override        public void prepareForWrite(RecordConsumer recordConsumer) {            this.recordConsumer = recordConsumer;        }        @Override        public void write(Map<String, Object> record) {            recordConsumer.startMessage();            int index = 0;            recordConsumer.startField("myboolean", index);            recordConsumer.addBoolean((Boolean) record.get("myboolean"));            recordConsumer.endField("myboolean", index++);            recordConsumer.startField("myint", index);            recordConsumer.addInteger((Integer) record.get("myint"));            recordConsumer.endField("myint", index++);            recordConsumer.startField("mylong", index);            recordConsumer.addLong((Long) record.get("mylong"));            recordConsumer.endField("mylong", index++);            recordConsumer.startField("myfloat", index);            recordConsumer.addFloat((Float) record.get("myfloat"));            recordConsumer.endField("myfloat", index++);            recordConsumer.startField("mydouble", index);            recordConsumer.addDouble((Double) record.get("mydouble"));            recordConsumer.endField("mydouble", index++);            recordConsumer.startField("mybytes", index);            recordConsumer.addBinary(Binary.fromReusedByteBuffer((ByteBuffer) record.get("mybytes")));            recordConsumer.endField("mybytes", index++);            recordConsumer.startField("mystring", index);            recordConsumer.addBinary(Binary.fromString((String) record.get("mystring")));            recordConsumer.endField("mystring", index++);            recordConsumer.startField("mynestedrecord", index);            recordConsumer.startGroup();            recordConsumer.startField("mynestedint", 0);            recordConsumer.addInteger((Integer) record.get("mynestedint"));            recordConsumer.endField("mynestedint", 0);            recordConsumer.endGroup();            recordConsumer.endField("mynestedrecord", index++);            recordConsumer.startField("myenum", index);            recordConsumer.addBinary(Binary.fromString((String) record.get("myenum")));            recordConsumer.endField("myenum", index++);            recordConsumer.startField("myarray", index);            recordConsumer.startGroup();            recordConsumer.startField("array", 0);            for (int val : (int[]) record.get("myarray")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("array", 0);            recordConsumer.endGroup();            recordConsumer.endField("myarray", index++);            recordConsumer.startField("myoptionalarray", index);            recordConsumer.startGroup();            recordConsumer.startField("array", 0);            for (int val : (int[]) record.get("myoptionalarray")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("array", 0);            recordConsumer.endGroup();            recordConsumer.endField("myoptionalarray", index++);            recordConsumer.startField("myarrayofoptional", index);            recordConsumer.startGroup();            recordConsumer.startField("list", 0);            for (Integer val : (Integer[]) record.get("myarrayofoptional")) {                recordConsumer.startGroup();                if (val != null) {                    recordConsumer.startField("element", 0);                    recordConsumer.addInteger(val);                    recordConsumer.endField("element", 0);                }                recordConsumer.endGroup();            }            recordConsumer.endField("list", 0);            recordConsumer.endGroup();            recordConsumer.endField("myarrayofoptional", index++);            recordConsumer.startField("myrecordarray", index);            recordConsumer.startGroup();            recordConsumer.startField("array", 0);            recordConsumer.startGroup();            recordConsumer.startField("a", 0);            for (int val : (int[]) record.get("myrecordarraya")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("a", 0);            recordConsumer.startField("b", 1);            for (int val : (int[]) record.get("myrecordarrayb")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("b", 1);            recordConsumer.endGroup();            recordConsumer.endField("array", 0);            recordConsumer.endGroup();            recordConsumer.endField("myrecordarray", index++);            recordConsumer.startField("mymap", index);            recordConsumer.startGroup();            recordConsumer.startField("map", 0);            recordConsumer.startGroup();            Map<String, Integer> mymap = (Map<String, Integer>) record.get("mymap");            recordConsumer.startField("key", 0);            for (String key : mymap.keySet()) {                recordConsumer.addBinary(Binary.fromString(key));            }            recordConsumer.endField("key", 0);            recordConsumer.startField("value", 1);            for (int val : mymap.values()) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("value", 1);            recordConsumer.endGroup();            recordConsumer.endField("map", 0);            recordConsumer.endGroup();            recordConsumer.endField("mymap", index++);            recordConsumer.startField("myfixed", index);            recordConsumer.addBinary(Binary.fromReusedByteArray((byte[]) record.get("myfixed")));            recordConsumer.endField("myfixed", index++);            recordConsumer.endMessage();        }    })) {        Map<String, Object> record = new HashMap<String, Object>();        record.put("myboolean", true);        record.put("myint", 1);        record.put("mylong", 2L);        record.put("myfloat", 3.1f);        record.put("mydouble", 4.1);        record.put("mybytes", ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)));        record.put("mystring", "hello");        record.put("myenum", "a");        record.put("mynestedint", 1);        record.put("myarray", new int[] { 1, 2, 3 });        record.put("myoptionalarray", new int[] { 1, 2, 3 });        record.put("myarrayofoptional", new Integer[] { 1, null, 2, null, 3 });        record.put("myrecordarraya", new int[] { 1, 2, 3 });        record.put("myrecordarrayb", new int[] { 4, 5, 6 });        record.put("mymap", ImmutableMap.of("a", 1, "b", 2));        record.put("myfixed", new byte[] { (byte) 65 });        parquetWriter.write(record);    }    Schema nestedRecordSchema = Schema.createRecord("mynestedrecord", null, null, false);    nestedRecordSchema.setFields(Arrays.asList(new Schema.Field("mynestedint", Schema.create(Schema.Type.INT), null, null)));    GenericData.Record nestedRecord = new GenericRecordBuilder(nestedRecordSchema).set("mynestedint", 1).build();    List<Integer> integerArray = Arrays.asList(1, 2, 3);    List<Integer> ingeterArrayWithNulls = Arrays.asList(1, null, 2, null, 3);    Schema recordArraySchema = Schema.createRecord("array", null, null, false);    recordArraySchema.setFields(Arrays.asList(new Schema.Field("a", Schema.create(Schema.Type.INT), null, null), new Schema.Field("b", Schema.create(Schema.Type.INT), null, null)));    GenericRecordBuilder builder = new GenericRecordBuilder(recordArraySchema);    List<GenericData.Record> recordArray = new ArrayList<GenericData.Record>();    recordArray.add(builder.set("a", 1).set("b", 4).build());    recordArray.add(builder.set("a", 2).set("b", 5).build());    recordArray.add(builder.set("a", 3).set("b", 6).build());    GenericData.Array<GenericData.Record> genericRecordArray = new GenericData.Array<GenericData.Record>(Schema.createArray(recordArraySchema), recordArray);    GenericFixed genericFixed = new GenericData.Fixed(Schema.createFixed("fixed", null, null, 1), new byte[] { (byte) 65 });    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(true, nextRecord.get("myboolean"));        assertEquals(1, nextRecord.get("myint"));        assertEquals(2L, nextRecord.get("mylong"));        assertEquals(3.1f, nextRecord.get("myfloat"));        assertEquals(4.1, nextRecord.get("mydouble"));        assertEquals(ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)), nextRecord.get("mybytes"));        assertEquals(str("hello"), nextRecord.get("mystring"));                assertEquals(str("a"), nextRecord.get("myenum"));        assertEquals(nestedRecord, nextRecord.get("mynestedrecord"));        assertEquals(integerArray, nextRecord.get("myarray"));        assertEquals(integerArray, nextRecord.get("myoptionalarray"));        assertEquals(ingeterArrayWithNulls, nextRecord.get("myarrayofoptional"));        assertEquals(genericRecordArray, nextRecord.get("myrecordarray"));        assertEquals(ImmutableMap.of(str("a"), 1, str("b"), 2), nextRecord.get("mymap"));        assertEquals(genericFixed, nextRecord.get("myfixed"));    }}
public WriteContext parquet-mr_f515_0(Configuration configuration)
{    return new WriteContext(MessageTypeParser.parseMessageType(TestAvroSchemaConverter.ALL_PARQUET_SCHEMA), new HashMap<String, String>());}
public void parquet-mr_f516_0(RecordConsumer recordConsumer)
{    this.recordConsumer = recordConsumer;}
public void parquet-mr_f517_0(Map<String, Object> record)
{    recordConsumer.startMessage();    int index = 0;    recordConsumer.startField("myboolean", index);    recordConsumer.addBoolean((Boolean) record.get("myboolean"));    recordConsumer.endField("myboolean", index++);    recordConsumer.startField("myint", index);    recordConsumer.addInteger((Integer) record.get("myint"));    recordConsumer.endField("myint", index++);    recordConsumer.startField("mylong", index);    recordConsumer.addLong((Long) record.get("mylong"));    recordConsumer.endField("mylong", index++);    recordConsumer.startField("myfloat", index);    recordConsumer.addFloat((Float) record.get("myfloat"));    recordConsumer.endField("myfloat", index++);    recordConsumer.startField("mydouble", index);    recordConsumer.addDouble((Double) record.get("mydouble"));    recordConsumer.endField("mydouble", index++);    recordConsumer.startField("mybytes", index);    recordConsumer.addBinary(Binary.fromReusedByteBuffer((ByteBuffer) record.get("mybytes")));    recordConsumer.endField("mybytes", index++);    recordConsumer.startField("mystring", index);    recordConsumer.addBinary(Binary.fromString((String) record.get("mystring")));    recordConsumer.endField("mystring", index++);    recordConsumer.startField("mynestedrecord", index);    recordConsumer.startGroup();    recordConsumer.startField("mynestedint", 0);    recordConsumer.addInteger((Integer) record.get("mynestedint"));    recordConsumer.endField("mynestedint", 0);    recordConsumer.endGroup();    recordConsumer.endField("mynestedrecord", index++);    recordConsumer.startField("myenum", index);    recordConsumer.addBinary(Binary.fromString((String) record.get("myenum")));    recordConsumer.endField("myenum", index++);    recordConsumer.startField("myarray", index);    recordConsumer.startGroup();    recordConsumer.startField("array", 0);    for (int val : (int[]) record.get("myarray")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("array", 0);    recordConsumer.endGroup();    recordConsumer.endField("myarray", index++);    recordConsumer.startField("myoptionalarray", index);    recordConsumer.startGroup();    recordConsumer.startField("array", 0);    for (int val : (int[]) record.get("myoptionalarray")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("array", 0);    recordConsumer.endGroup();    recordConsumer.endField("myoptionalarray", index++);    recordConsumer.startField("myarrayofoptional", index);    recordConsumer.startGroup();    recordConsumer.startField("list", 0);    for (Integer val : (Integer[]) record.get("myarrayofoptional")) {        recordConsumer.startGroup();        if (val != null) {            recordConsumer.startField("element", 0);            recordConsumer.addInteger(val);            recordConsumer.endField("element", 0);        }        recordConsumer.endGroup();    }    recordConsumer.endField("list", 0);    recordConsumer.endGroup();    recordConsumer.endField("myarrayofoptional", index++);    recordConsumer.startField("myrecordarray", index);    recordConsumer.startGroup();    recordConsumer.startField("array", 0);    recordConsumer.startGroup();    recordConsumer.startField("a", 0);    for (int val : (int[]) record.get("myrecordarraya")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("a", 0);    recordConsumer.startField("b", 1);    for (int val : (int[]) record.get("myrecordarrayb")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("b", 1);    recordConsumer.endGroup();    recordConsumer.endField("array", 0);    recordConsumer.endGroup();    recordConsumer.endField("myrecordarray", index++);    recordConsumer.startField("mymap", index);    recordConsumer.startGroup();    recordConsumer.startField("map", 0);    recordConsumer.startGroup();    Map<String, Integer> mymap = (Map<String, Integer>) record.get("mymap");    recordConsumer.startField("key", 0);    for (String key : mymap.keySet()) {        recordConsumer.addBinary(Binary.fromString(key));    }    recordConsumer.endField("key", 0);    recordConsumer.startField("value", 1);    for (int val : mymap.values()) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("value", 1);    recordConsumer.endGroup();    recordConsumer.endField("map", 0);    recordConsumer.endGroup();    recordConsumer.endField("mymap", index++);    recordConsumer.startField("myfixed", index);    recordConsumer.addBinary(Binary.fromReusedByteArray((byte[]) record.get("myfixed")));    recordConsumer.endField("myfixed", index++);    recordConsumer.endMessage();}
public void parquet-mr_f518_0() throws Exception
{    Schema avroSchema = Schema.createRecord("SingleStringUnionRecord", null, null, false);    avroSchema.setFields(Collections.singletonList(new Schema.Field("value", Schema.createUnion(Schema.create(Schema.Type.STRING)), null, null)));    Path file = new Path(createTempFile().getPath());        try (ParquetWriter parquetWriter = AvroParquetWriter.builder(file).withSchema(avroSchema).withConf(new Configuration()).build()) {        GenericRecord record = new GenericRecordBuilder(avroSchema).set("value", "theValue").build();        parquetWriter.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(str("theValue"), nextRecord.get("value"));    }}
public void parquet-mr_f519_0() throws Exception
{    Schema schema = SchemaBuilder.record("spark_schema").fields().optionalBytes("value").endRecord();    Path file = new Path(createTempFile().getPath());    String[] records = { "one", "two", "three", "three", "two", "one", "zero" };    try (ParquetWriter<GenericData.Record> writer = AvroParquetWriter.<GenericData.Record>builder(file).withSchema(schema).withConf(testConf).build()) {        for (String record : records) {            writer.write(new GenericRecordBuilder(schema).set("value", record.getBytes()).build());        }    }    try (ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(file).withConf(testConf).build()) {        GenericRecord rec;        int i = 0;        while ((rec = reader.read()) != null) {            ByteBuffer buf = (ByteBuffer) rec.get("value");            byte[] bytes = new byte[buf.remaining()];            buf.get(bytes);            assertEquals(records[i++], new String(bytes));        }    }}
public void parquet-mr_f520_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("nested_array.avsc").openStream());    Path file = new Path(createTempFile().getPath());        ParquetWriter parquetWriter = AvroParquetWriter.builder(file).withSchema(schema).withConf(testConf).build();    Schema innerRecordSchema = schema.getField("l1").schema().getTypes().get(1).getElementType().getTypes().get(1);    GenericRecord record = new GenericRecordBuilder(schema).set("l1", Collections.singletonList(new GenericRecordBuilder(innerRecordSchema).set("l2", Collections.singletonList("hello")).build())).build();    parquetWriter.write(record);    parquetWriter.close();    AvroParquetReader<GenericRecord> reader = new AvroParquetReader(testConf, file);    GenericRecord nextRecord = reader.read();    assertNotNull(nextRecord);    assertNotNull(nextRecord.get("l1"));    List l1List = (List) nextRecord.get("l1");    assertNotNull(l1List.get(0));    List l2List = (List) ((GenericRecord) l1List.get(0)).get("l2");    assertEquals(str("hello"), l2List.get(0));}
private File parquet-mr_f521_0() throws IOException
{    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    return tmp;}
public CharSequence parquet-mr_f522_0(String value)
{    return compat ? value : new Utf8(value);}
public static Collection<Object[]> parquet-mr_f523_0()
{    Object[][] data = new Object[][] {     { false },     { true } };    return Arrays.asList(data);}
public void parquet-mr_f524_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("array.avsc").openStream());    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());    List<Integer> emptyArray = new ArrayList<Integer>();    try (AvroParquetWriter<GenericRecord> writer = new AvroParquetWriter<GenericRecord>(file, schema)) {                GenericData.Record record = new GenericRecordBuilder(schema).set("myarray", emptyArray).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(emptyArray, nextRecord.get("myarray"));    }}
public void parquet-mr_f525_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("map.avsc").openStream());    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());    ImmutableMap emptyMap = new ImmutableMap.Builder<String, Integer>().build();    try (AvroParquetWriter<GenericRecord> writer = new AvroParquetWriter<GenericRecord>(file, schema)) {                GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", emptyMap).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(emptyMap, nextRecord.get("mymap"));    }}
public void parquet-mr_f526_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("map_with_nulls.avsc").openStream());    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());    Map<CharSequence, Integer> map = new HashMap<>();    try (AvroParquetWriter<GenericRecord> writer = new AvroParquetWriter<GenericRecord>(file, schema)) {                map.put(str("thirty-four"), 34);        map.put(str("eleventy-one"), null);        map.put(str("one-hundred"), 100);        GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", map).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(map, nextRecord.get("mymap"));    }}
public void parquet-mr_f527_0() throws Exception
{    Schema schema = Schema.createRecord("record1", null, null, false);    schema.setFields(Lists.newArrayList(new Schema.Field("mymap", Schema.createMap(Schema.create(Schema.Type.INT)), null, null)));    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());    try (AvroParquetWriter<GenericRecord> writer = new AvroParquetWriter<GenericRecord>(file, schema)) {                Map<String, Integer> map = new HashMap<String, Integer>();        map.put("thirty-four", 34);        map.put("eleventy-one", null);        map.put("one-hundred", 100);        GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", map).build();        writer.write(record);    }}
public void parquet-mr_f528_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("map.avsc").openStream());    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());    try (AvroParquetWriter<GenericRecord> writer = new AvroParquetWriter<GenericRecord>(file, schema)) {                GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", ImmutableMap.of(new Utf8("a"), 1, new Utf8("b"), 2)).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(ImmutableMap.of(str("a"), 1, str("b"), 2), nextRecord.get("mymap"));    }}
public void parquet-mr_f529_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("all.avsc").openStream());    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());    GenericData.Record nestedRecord = new GenericRecordBuilder(schema.getField("mynestedrecord").schema()).set("mynestedint", 1).build();    List<Integer> integerArray = Arrays.asList(1, 2, 3);    GenericData.Array<Integer> genericIntegerArray = new GenericData.Array<Integer>(Schema.createArray(Schema.create(Schema.Type.INT)), integerArray);    GenericFixed genericFixed = new GenericData.Fixed(Schema.createFixed("fixed", null, null, 1), new byte[] { (byte) 65 });    List<Integer> emptyArray = new ArrayList<Integer>();    ImmutableMap emptyMap = new ImmutableMap.Builder<String, Integer>().build();    try (AvroParquetWriter<GenericRecord> writer = new AvroParquetWriter<>(file, schema)) {        GenericData.Record record = new GenericRecordBuilder(schema).set("mynull", null).set("myboolean", true).set("myint", 1).set("mylong", 2L).set("myfloat", 3.1f).set("mydouble", 4.1).set("mybytes", ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8))).set("mystring", "hello").set("mynestedrecord", nestedRecord).set("myenum", "a").set("myarray", genericIntegerArray).set("myemptyarray", emptyArray).set("myoptionalarray", genericIntegerArray).set("myarrayofoptional", genericIntegerArray).set("mymap", ImmutableMap.of("a", 1, "b", 2)).set("myemptymap", emptyMap).set("myfixed", genericFixed).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        Object expectedEnumSymbol = compat ? "a" : new GenericData.EnumSymbol(schema.getField("myenum").schema(), "a");        assertNotNull(nextRecord);        assertEquals(null, nextRecord.get("mynull"));        assertEquals(true, nextRecord.get("myboolean"));        assertEquals(1, nextRecord.get("myint"));        assertEquals(2L, nextRecord.get("mylong"));        assertEquals(3.1f, nextRecord.get("myfloat"));        assertEquals(4.1, nextRecord.get("mydouble"));        assertEquals(ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)), nextRecord.get("mybytes"));        assertEquals(str("hello"), nextRecord.get("mystring"));        assertEquals(expectedEnumSymbol, nextRecord.get("myenum"));        assertEquals(nestedRecord, nextRecord.get("mynestedrecord"));        assertEquals(integerArray, nextRecord.get("myarray"));        assertEquals(emptyArray, nextRecord.get("myemptyarray"));        assertEquals(integerArray, nextRecord.get("myoptionalarray"));        assertEquals(integerArray, nextRecord.get("myarrayofoptional"));        assertEquals(ImmutableMap.of(str("a"), 1, str("b"), 2), nextRecord.get("mymap"));        assertEquals(emptyMap, nextRecord.get("myemptymap"));        assertEquals(genericFixed, nextRecord.get("myfixed"));    }}
public void parquet-mr_f530_0() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("all.avsc").openStream());    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());    GenericData.Record nestedRecord = new GenericRecordBuilder(schema.getField("mynestedrecord").schema()).set("mynestedint", 1).build();    List<Integer> integerArray = Arrays.asList(1, 2, 3);    GenericData.Array<Integer> genericIntegerArray = new GenericData.Array<Integer>(Schema.createArray(Schema.create(Schema.Type.INT)), integerArray);    GenericFixed genericFixed = new GenericData.Fixed(Schema.createFixed("fixed", null, null, 1), new byte[] { (byte) 65 });    List<Integer> emptyArray = new ArrayList<Integer>();    ImmutableMap emptyMap = new ImmutableMap.Builder<String, Integer>().build();    Schema arrayOfOptionalIntegers = Schema.createArray(optional(Schema.create(Schema.Type.INT)));    GenericData.Array<Integer> genericIntegerArrayWithNulls = new GenericData.Array<>(arrayOfOptionalIntegers, Arrays.asList(1, null, 2, null, 3));    GenericData.Record record = new GenericRecordBuilder(schema).set("mynull", null).set("myboolean", true).set("myint", 1).set("mylong", 2L).set("myfloat", 3.1f).set("mydouble", 4.1).set("mybytes", ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8))).set("mystring", "hello").set("mynestedrecord", nestedRecord).set("myenum", "a").set("myarray", genericIntegerArray).set("myemptyarray", emptyArray).set("myoptionalarray", genericIntegerArray).set("myarrayofoptional", genericIntegerArrayWithNulls).set("mymap", ImmutableMap.of("a", 1, "b", 2)).set("myemptymap", emptyMap).set("myfixed", genericFixed).build();    try (AvroParquetWriter<GenericRecord> writer = new AvroParquetWriter<>(file, schema)) {        writer.write(record);        fail("Should not succeed writing an array with null values");    } catch (Exception e) {        Assert.assertTrue("Error message should provide context and help", e.getMessage().contains("parquet.avro.write-old-list-structure"));    }}
public void parquet-mr_f531_0() throws Exception
{    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());        try (ParquetWriter<Map<String, Object>> parquetWriter = new ParquetWriter<Map<String, Object>>(file, new WriteSupport<Map<String, Object>>() {        private RecordConsumer recordConsumer;        @Override        public WriteContext init(Configuration configuration) {            return new WriteContext(MessageTypeParser.parseMessageType(TestAvroSchemaConverter.ALL_PARQUET_SCHEMA), new HashMap<String, String>());        }        @Override        public void prepareForWrite(RecordConsumer recordConsumer) {            this.recordConsumer = recordConsumer;        }        @Override        public void write(Map<String, Object> record) {            recordConsumer.startMessage();            int index = 0;            recordConsumer.startField("myboolean", index);            recordConsumer.addBoolean((Boolean) record.get("myboolean"));            recordConsumer.endField("myboolean", index++);            recordConsumer.startField("myint", index);            recordConsumer.addInteger((Integer) record.get("myint"));            recordConsumer.endField("myint", index++);            recordConsumer.startField("mylong", index);            recordConsumer.addLong((Long) record.get("mylong"));            recordConsumer.endField("mylong", index++);            recordConsumer.startField("myfloat", index);            recordConsumer.addFloat((Float) record.get("myfloat"));            recordConsumer.endField("myfloat", index++);            recordConsumer.startField("mydouble", index);            recordConsumer.addDouble((Double) record.get("mydouble"));            recordConsumer.endField("mydouble", index++);            recordConsumer.startField("mybytes", index);            recordConsumer.addBinary(Binary.fromReusedByteBuffer((ByteBuffer) record.get("mybytes")));            recordConsumer.endField("mybytes", index++);            recordConsumer.startField("mystring", index);            recordConsumer.addBinary(Binary.fromString((String) record.get("mystring")));            recordConsumer.endField("mystring", index++);            recordConsumer.startField("mynestedrecord", index);            recordConsumer.startGroup();            recordConsumer.startField("mynestedint", 0);            recordConsumer.addInteger((Integer) record.get("mynestedint"));            recordConsumer.endField("mynestedint", 0);            recordConsumer.endGroup();            recordConsumer.endField("mynestedrecord", index++);            recordConsumer.startField("myenum", index);            recordConsumer.addBinary(Binary.fromString((String) record.get("myenum")));            recordConsumer.endField("myenum", index++);            recordConsumer.startField("myarray", index);            recordConsumer.startGroup();            recordConsumer.startField("array", 0);            for (int val : (int[]) record.get("myarray")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("array", 0);            recordConsumer.endGroup();            recordConsumer.endField("myarray", index++);            recordConsumer.startField("myoptionalarray", index);            recordConsumer.startGroup();            recordConsumer.startField("array", 0);            for (int val : (int[]) record.get("myoptionalarray")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("array", 0);            recordConsumer.endGroup();            recordConsumer.endField("myoptionalarray", index++);            recordConsumer.startField("myarrayofoptional", index);            recordConsumer.startGroup();            recordConsumer.startField("list", 0);            for (Integer val : (Integer[]) record.get("myarrayofoptional")) {                recordConsumer.startGroup();                if (val != null) {                    recordConsumer.startField("element", 0);                    recordConsumer.addInteger(val);                    recordConsumer.endField("element", 0);                }                recordConsumer.endGroup();            }            recordConsumer.endField("list", 0);            recordConsumer.endGroup();            recordConsumer.endField("myarrayofoptional", index++);            recordConsumer.startField("myrecordarray", index);            recordConsumer.startGroup();            recordConsumer.startField("array", 0);            recordConsumer.startGroup();            recordConsumer.startField("a", 0);            for (int val : (int[]) record.get("myrecordarraya")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("a", 0);            recordConsumer.startField("b", 1);            for (int val : (int[]) record.get("myrecordarrayb")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("b", 1);            recordConsumer.endGroup();            recordConsumer.endField("array", 0);            recordConsumer.endGroup();            recordConsumer.endField("myrecordarray", index++);            recordConsumer.startField("mymap", index);            recordConsumer.startGroup();            recordConsumer.startField("map", 0);            recordConsumer.startGroup();            Map<String, Integer> mymap = (Map<String, Integer>) record.get("mymap");            recordConsumer.startField("key", 0);            for (String key : mymap.keySet()) {                recordConsumer.addBinary(Binary.fromString(key));            }            recordConsumer.endField("key", 0);            recordConsumer.startField("value", 1);            for (int val : mymap.values()) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("value", 1);            recordConsumer.endGroup();            recordConsumer.endField("map", 0);            recordConsumer.endGroup();            recordConsumer.endField("mymap", index++);            recordConsumer.startField("myfixed", index);            recordConsumer.addBinary(Binary.fromReusedByteArray((byte[]) record.get("myfixed")));            recordConsumer.endField("myfixed", index++);            recordConsumer.endMessage();        }    })) {        Map<String, Object> record = new HashMap<String, Object>();        record.put("myboolean", true);        record.put("myint", 1);        record.put("mylong", 2L);        record.put("myfloat", 3.1f);        record.put("mydouble", 4.1);        record.put("mybytes", ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)));        record.put("mystring", "hello");        record.put("myenum", "a");        record.put("mynestedint", 1);        record.put("myarray", new int[] { 1, 2, 3 });        record.put("myoptionalarray", new int[] { 1, 2, 3 });        record.put("myarrayofoptional", new Integer[] { 1, null, 2, null, 3 });        record.put("myrecordarraya", new int[] { 1, 2, 3 });        record.put("myrecordarrayb", new int[] { 4, 5, 6 });        record.put("mymap", ImmutableMap.of("a", 1, "b", 2));        record.put("myfixed", new byte[] { (byte) 65 });        parquetWriter.write(record);    }    Schema nestedRecordSchema = Schema.createRecord("mynestedrecord", null, null, false);    nestedRecordSchema.setFields(Arrays.asList(new Schema.Field("mynestedint", Schema.create(Schema.Type.INT), null, null)));    GenericData.Record nestedRecord = new GenericRecordBuilder(nestedRecordSchema).set("mynestedint", 1).build();    List<Integer> integerArray = Arrays.asList(1, 2, 3);    Schema recordArraySchema = Schema.createRecord("array", null, null, false);    recordArraySchema.setFields(Arrays.asList(new Schema.Field("a", Schema.create(Schema.Type.INT), null, null), new Schema.Field("b", Schema.create(Schema.Type.INT), null, null)));    GenericRecordBuilder builder = new GenericRecordBuilder(recordArraySchema);    List<GenericData.Record> recordArray = new ArrayList<GenericData.Record>();    recordArray.add(builder.set("a", 1).set("b", 4).build());    recordArray.add(builder.set("a", 2).set("b", 5).build());    recordArray.add(builder.set("a", 3).set("b", 6).build());    GenericData.Array<GenericData.Record> genericRecordArray = new GenericData.Array<GenericData.Record>(Schema.createArray(recordArraySchema), recordArray);    GenericFixed genericFixed = new GenericData.Fixed(Schema.createFixed("fixed", null, null, 1), new byte[] { (byte) 65 });        Schema elementSchema = record("list", optionalField("element", primitive(Schema.Type.INT)));    GenericRecordBuilder elementBuilder = new GenericRecordBuilder(elementSchema);    GenericData.Array<GenericData.Record> genericRecordArrayWithNullIntegers = new GenericData.Array<GenericData.Record>(array(elementSchema), Arrays.asList(elementBuilder.set("element", 1).build(), elementBuilder.set("element", null).build(), elementBuilder.set("element", 2).build(), elementBuilder.set("element", null).build(), elementBuilder.set("element", 3).build()));    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(true, nextRecord.get("myboolean"));        assertEquals(1, nextRecord.get("myint"));        assertEquals(2L, nextRecord.get("mylong"));        assertEquals(3.1f, nextRecord.get("myfloat"));        assertEquals(4.1, nextRecord.get("mydouble"));        assertEquals(ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)), nextRecord.get("mybytes"));        assertEquals(str("hello"), nextRecord.get("mystring"));        assertEquals(str("a"), nextRecord.get("myenum"));        assertEquals(nestedRecord, nextRecord.get("mynestedrecord"));        assertEquals(integerArray, nextRecord.get("myarray"));        assertEquals(integerArray, nextRecord.get("myoptionalarray"));        assertEquals(genericRecordArrayWithNullIntegers, nextRecord.get("myarrayofoptional"));        assertEquals(genericRecordArray, nextRecord.get("myrecordarray"));        assertEquals(ImmutableMap.of(str("a"), 1, str("b"), 2), nextRecord.get("mymap"));        assertEquals(genericFixed, nextRecord.get("myfixed"));    }}
public WriteContext parquet-mr_f532_0(Configuration configuration)
{    return new WriteContext(MessageTypeParser.parseMessageType(TestAvroSchemaConverter.ALL_PARQUET_SCHEMA), new HashMap<String, String>());}
public void parquet-mr_f533_0(RecordConsumer recordConsumer)
{    this.recordConsumer = recordConsumer;}
public void parquet-mr_f534_0(Map<String, Object> record)
{    recordConsumer.startMessage();    int index = 0;    recordConsumer.startField("myboolean", index);    recordConsumer.addBoolean((Boolean) record.get("myboolean"));    recordConsumer.endField("myboolean", index++);    recordConsumer.startField("myint", index);    recordConsumer.addInteger((Integer) record.get("myint"));    recordConsumer.endField("myint", index++);    recordConsumer.startField("mylong", index);    recordConsumer.addLong((Long) record.get("mylong"));    recordConsumer.endField("mylong", index++);    recordConsumer.startField("myfloat", index);    recordConsumer.addFloat((Float) record.get("myfloat"));    recordConsumer.endField("myfloat", index++);    recordConsumer.startField("mydouble", index);    recordConsumer.addDouble((Double) record.get("mydouble"));    recordConsumer.endField("mydouble", index++);    recordConsumer.startField("mybytes", index);    recordConsumer.addBinary(Binary.fromReusedByteBuffer((ByteBuffer) record.get("mybytes")));    recordConsumer.endField("mybytes", index++);    recordConsumer.startField("mystring", index);    recordConsumer.addBinary(Binary.fromString((String) record.get("mystring")));    recordConsumer.endField("mystring", index++);    recordConsumer.startField("mynestedrecord", index);    recordConsumer.startGroup();    recordConsumer.startField("mynestedint", 0);    recordConsumer.addInteger((Integer) record.get("mynestedint"));    recordConsumer.endField("mynestedint", 0);    recordConsumer.endGroup();    recordConsumer.endField("mynestedrecord", index++);    recordConsumer.startField("myenum", index);    recordConsumer.addBinary(Binary.fromString((String) record.get("myenum")));    recordConsumer.endField("myenum", index++);    recordConsumer.startField("myarray", index);    recordConsumer.startGroup();    recordConsumer.startField("array", 0);    for (int val : (int[]) record.get("myarray")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("array", 0);    recordConsumer.endGroup();    recordConsumer.endField("myarray", index++);    recordConsumer.startField("myoptionalarray", index);    recordConsumer.startGroup();    recordConsumer.startField("array", 0);    for (int val : (int[]) record.get("myoptionalarray")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("array", 0);    recordConsumer.endGroup();    recordConsumer.endField("myoptionalarray", index++);    recordConsumer.startField("myarrayofoptional", index);    recordConsumer.startGroup();    recordConsumer.startField("list", 0);    for (Integer val : (Integer[]) record.get("myarrayofoptional")) {        recordConsumer.startGroup();        if (val != null) {            recordConsumer.startField("element", 0);            recordConsumer.addInteger(val);            recordConsumer.endField("element", 0);        }        recordConsumer.endGroup();    }    recordConsumer.endField("list", 0);    recordConsumer.endGroup();    recordConsumer.endField("myarrayofoptional", index++);    recordConsumer.startField("myrecordarray", index);    recordConsumer.startGroup();    recordConsumer.startField("array", 0);    recordConsumer.startGroup();    recordConsumer.startField("a", 0);    for (int val : (int[]) record.get("myrecordarraya")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("a", 0);    recordConsumer.startField("b", 1);    for (int val : (int[]) record.get("myrecordarrayb")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("b", 1);    recordConsumer.endGroup();    recordConsumer.endField("array", 0);    recordConsumer.endGroup();    recordConsumer.endField("myrecordarray", index++);    recordConsumer.startField("mymap", index);    recordConsumer.startGroup();    recordConsumer.startField("map", 0);    recordConsumer.startGroup();    Map<String, Integer> mymap = (Map<String, Integer>) record.get("mymap");    recordConsumer.startField("key", 0);    for (String key : mymap.keySet()) {        recordConsumer.addBinary(Binary.fromString(key));    }    recordConsumer.endField("key", 0);    recordConsumer.startField("value", 1);    for (int val : mymap.values()) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("value", 1);    recordConsumer.endGroup();    recordConsumer.endField("map", 0);    recordConsumer.endGroup();    recordConsumer.endField("mymap", index++);    recordConsumer.startField("myfixed", index);    recordConsumer.addBinary(Binary.fromReusedByteArray((byte[]) record.get("myfixed")));    recordConsumer.endField("myfixed", index++);    recordConsumer.endMessage();}
public CharSequence parquet-mr_f535_0(String value)
{    return compat ? value : new Utf8(value);}
public boolean parquet-mr_f536_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Service service = (Service) o;    if (date != service.date)        return false;    if (!mechanic.equals(service.mechanic))        return false;    return true;}
public int parquet-mr_f537_0()
{    int result = (int) (date ^ (date >>> 32));    result = 31 * result + mechanic.hashCode();    return result;}
public boolean parquet-mr_f538_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Engine engine = (Engine) o;    if (Float.compare(engine.capacity, capacity) != 0)        return false;    if (hasTurboCharger != engine.hasTurboCharger)        return false;    if (type != engine.type)        return false;    return true;}
public int parquet-mr_f539_0()
{    int result = type.hashCode();    result = 31 * result + (capacity != +0.0f ? Float.floatToIntBits(capacity) : 0);    result = 31 * result + (hasTurboCharger ? 1 : 0);    return result;}
public boolean parquet-mr_f540_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Stereo stereo = (Stereo) o;    if (speakers != stereo.speakers)        return false;    if (!make.equals(stereo.make))        return false;    return true;}
public int parquet-mr_f541_0()
{    int result = make.hashCode();    result = 31 * result + speakers;    return result;}
public boolean parquet-mr_f542_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    LeatherTrim that = (LeatherTrim) o;    if (!colour.equals(that.colour))        return false;    return true;}
public int parquet-mr_f543_0()
{    return colour.hashCode();}
public boolean parquet-mr_f544_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Car car = (Car) o;    if (doors != car.doors)        return false;    if (year != car.year)        return false;    if (!engine.equals(car.engine))        return false;    if (!make.equals(car.make))        return false;    if (!model.equals(car.model))        return false;    if (optionalExtra != null ? !optionalExtra.equals(car.optionalExtra) : car.optionalExtra != null)        return false;    if (!registration.equals(car.registration))        return false;    if (serviceHistory != null ? !serviceHistory.equals(car.serviceHistory) : car.serviceHistory != null)        return false;    if (!Arrays.equals(vin, car.vin))        return false;    return true;}
public int parquet-mr_f545_0()
{    int result = (int) (year ^ (year >>> 32));    result = 31 * result + registration.hashCode();    result = 31 * result + make.hashCode();    result = 31 * result + model.hashCode();    result = 31 * result + Arrays.hashCode(vin);    result = 31 * result + doors;    result = 31 * result + engine.hashCode();    result = 31 * result + (optionalExtra != null ? optionalExtra.hashCode() : 0);    result = 31 * result + (serviceHistory != null ? serviceHistory.hashCode() : 0);    return result;}
public boolean parquet-mr_f546_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ShortCar shortCar = (ShortCar) o;    if (year != shortCar.year)        return false;    if (!engine.equals(shortCar.engine))        return false;    if (make != null ? !make.equals(shortCar.make) : shortCar.make != null)        return false;    if (!Arrays.equals(vin, shortCar.vin))        return false;    return true;}
public int parquet-mr_f547_0()
{    int result = make != null ? make.hashCode() : 0;    result = 31 * result + engine.hashCode();    result = 31 * result + (int) (year ^ (year >>> 32));    result = 31 * result + Arrays.hashCode(vin);    return result;}
public static Car parquet-mr_f548_0(int i)
{    Car car = new Car();    car.doors = 2;    car.make = "Tesla";    car.model = String.format("Model X v%d", i % 2);    car.vin = String.format("1VXBR12EXCP%06d", i).getBytes();    car.year = 2014 + i;    car.registration = "California";    LeatherTrim trim = new LeatherTrim();    trim.colour = "black";    car.optionalExtra = trim;    Engine engine = new Engine();    engine.capacity = 85.0f;    engine.type = (i % 2) == 0 ? EngineType.ELECTRIC : EngineType.PETROL;    engine.hasTurboCharger = false;    car.engine = engine;    if (i % 4 == 0) {        Service service = new Service();        service.date = 1374084640;        service.mechanic = "Elon Musk";        car.serviceHistory = Lists.newArrayList();        car.serviceHistory.add(service);    }    return car;}
public void parquet-mr_f549_0(Context context) throws IOException, InterruptedException
{    for (int i = 0; i < 10; i++) {        context.write(null, nextRecord(i));    }}
protected void parquet-mr_f550_0(Void key, Car car, Context context) throws IOException, InterruptedException
{        if (car != null) {        context.write(null, car);    }}
protected void parquet-mr_f551_0(Void key, ShortCar car, Context context) throws IOException, InterruptedException
{        if (car != null) {        context.write(null, car);    }}
public RecordFilter parquet-mr_f552_0(Iterable<ColumnReader> readers)
{    return filter.bind(readers);}
public void parquet-mr_f553_0() throws Exception
{        conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    AvroReadSupport.setAvroDataSupplier(conf, ReflectDataSupplier.class);    AvroWriteSupport.setAvroDataSupplier(conf, ReflectDataSupplier.class);    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        final Job job = new Job(conf, "write");                TextInputFormat.addInputPath(job, inputPath);        job.setInputFormatClass(TextInputFormat.class);        job.setMapperClass(TestReflectInputOutputFormat.MyMapper.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(AvroParquetOutputFormat.class);        AvroParquetOutputFormat.setOutputPath(job, parquetPath);        AvroParquetOutputFormat.setSchema(job, CAR_SCHEMA);        AvroParquetOutputFormat.setAvroDataSupplier(job, ReflectDataSupplier.class);        waitForJob(job);    }}
public void parquet-mr_f554_0() throws Exception
{    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    final Job job = new Job(conf, "read");    job.setInputFormatClass(AvroParquetInputFormat.class);    AvroParquetInputFormat.setInputPaths(job, parquetPath);        AvroParquetInputFormat.setUnboundRecordFilter(job, ElectricCarFilter.class);        Schema projection = Schema.createRecord(CAR_SCHEMA.getName(), CAR_SCHEMA.getDoc(), CAR_SCHEMA.getNamespace(), false);    List<Schema.Field> fields = Lists.newArrayList();    for (Schema.Field field : ReflectData.get().getSchema(Car.class).getFields()) {        if (!"optionalExtra".equals(field.name())) {            fields.add(new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultVal(), field.order()));        }    }    projection.setFields(fields);    AvroParquetInputFormat.setRequestedProjection(job, projection);    job.setMapperClass(TestReflectInputOutputFormat.MyMapper2.class);    job.setNumReduceTasks(0);    job.setOutputFormatClass(AvroParquetOutputFormat.class);    AvroParquetOutputFormat.setOutputPath(job, outputPath);    AvroParquetOutputFormat.setSchema(job, CAR_SCHEMA);    waitForJob(job);    final Path mapperOutput = new Path(outputPath.toString(), "part-m-00000.parquet");    try (final AvroParquetReader<Car> out = new AvroParquetReader<Car>(conf, mapperOutput)) {        Car car;        Car previousCar = null;        int lineNumber = 0;        while ((car = out.read()) != null) {            if (previousCar != null) {                                assertTrue(car.model == previousCar.model);            }                        if (car.engine.type == EngineType.PETROL) {                fail("UnboundRecordFilter failed to remove cars with PETROL engines");            }                        Car expectedCar = nextRecord(lineNumber * 2);                                    expectedCar.optionalExtra = null;            assertEquals("line " + lineNumber, expectedCar, car);            ++lineNumber;            previousCar = car;        }    }}
public void parquet-mr_f555_0() throws Exception
{    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    final Job job = new Job(conf, "read changed/short");    job.setInputFormatClass(AvroParquetInputFormat.class);    AvroParquetInputFormat.setInputPaths(job, parquetPath);        AvroParquetInputFormat.setUnboundRecordFilter(job, ElectricCarFilter.class);            Schema projection = Schema.createRecord(CAR_SCHEMA.getName(), CAR_SCHEMA.getDoc(), CAR_SCHEMA.getNamespace(), false);    List<Schema.Field> fields = Lists.newArrayList();    for (Schema.Field field : CAR_SCHEMA.getFields()) {                if ("engine".equals(field.name()) || "year".equals(field.name()) || "vin".equals(field.name())) {            fields.add(new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultVal(), field.order()));        }    }    projection.setFields(fields);    AvroParquetInputFormat.setRequestedProjection(job, projection);    AvroParquetInputFormat.setAvroReadSchema(job, SHORT_CAR_SCHEMA);    job.setMapperClass(TestReflectInputOutputFormat.MyMapperShort.class);    job.setNumReduceTasks(0);    job.setOutputFormatClass(AvroParquetOutputFormat.class);    AvroParquetOutputFormat.setOutputPath(job, outputPath);    AvroParquetOutputFormat.setSchema(job, SHORT_CAR_SCHEMA);    waitForJob(job);    final Path mapperOutput = new Path(outputPath.toString(), "part-m-00000.parquet");    try (final AvroParquetReader<ShortCar> out = new AvroParquetReader<ShortCar>(conf, mapperOutput)) {        ShortCar car;        int lineNumber = 0;        while ((car = out.read()) != null) {                                    Car expectedCar = nextRecord(lineNumber * 2);                        assertNull(car.make);            assertEquals(car.engine, expectedCar.engine);            assertEquals(car.year, expectedCar.year);            assertArrayEquals(car.vin, expectedCar.vin);            ++lineNumber;        }    }}
private void parquet-mr_f556_1(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {                sleep(100);    }        if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
public void parquet-mr_f557_0() throws IOException
{    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);}
public static void parquet-mr_f558_0()
{    REFLECT.addLogicalTypeConversion(new Conversions.UUIDConversion());    REFLECT.addLogicalTypeConversion(new Conversions.DecimalConversion());}
public void parquet-mr_f559_0()
{    Schema expected = SchemaBuilder.record(RecordWithUUIDList.class.getName()).fields().name("uuids").type().array().items().stringType().noDefault().endRecord();    expected.getField("uuids").schema().addProp(SpecificData.CLASS_PROP, List.class.getName());    LogicalTypes.uuid().addToSchema(expected.getField("uuids").schema().getElementType());    Schema actual = REFLECT.getSchema(RecordWithUUIDList.class);    Assert.assertEquals("Should use the UUID logical type", expected, actual);}
public boolean parquet-mr_f560_0(Object other)
{    if (this == other) {        return true;    }    if (other == null || getClass() != other.getClass()) {        return false;    }    DecimalRecordBytes that = (DecimalRecordBytes) other;    if (decimal == null) {        return (that.decimal == null);    }    return decimal.equals(that.decimal);}
public int parquet-mr_f561_0()
{    return decimal != null ? decimal.hashCode() : 0;}
public void parquet-mr_f562_0() throws IOException
{    Schema schema = REFLECT.getSchema(DecimalRecordBytes.class);    Assert.assertEquals("Should have the correct record name", "org.apache.parquet.avro.TestReflectLogicalTypes", schema.getNamespace());    Assert.assertEquals("Should have the correct record name", "DecimalRecordBytes", schema.getName());    Assert.assertEquals("Should have the correct logical type", LogicalTypes.decimal(9, 2), LogicalTypes.fromSchema(schema.getField("decimal").schema()));    DecimalRecordBytes record = new DecimalRecordBytes();    record.decimal = new BigDecimal("3.14");    File test = write(REFLECT, schema, record);    Assert.assertEquals("Should match the decimal after round trip", Arrays.asList(record), read(REFLECT, schema, test));}
public boolean parquet-mr_f563_0(Object other)
{    if (this == other) {        return true;    }    if (other == null || getClass() != other.getClass()) {        return false;    }    DecimalRecordFixed that = (DecimalRecordFixed) other;    if (decimal == null) {        return (that.decimal == null);    }    return decimal.equals(that.decimal);}
public int parquet-mr_f564_0()
{    return decimal != null ? decimal.hashCode() : 0;}
public void parquet-mr_f565_0() throws IOException
{    Schema schema = REFLECT.getSchema(DecimalRecordFixed.class);    Assert.assertEquals("Should have the correct record name", "org.apache.parquet.avro.TestReflectLogicalTypes", schema.getNamespace());    Assert.assertEquals("Should have the correct record name", "DecimalRecordFixed", schema.getName());    Assert.assertEquals("Should have the correct logical type", LogicalTypes.decimal(9, 2), LogicalTypes.fromSchema(schema.getField("decimal").schema()));    DecimalRecordFixed record = new DecimalRecordFixed();    record.decimal = new BigDecimal("3.14");    File test = write(REFLECT, schema, record);    Assert.assertEquals("Should match the decimal after round trip", Arrays.asList(record), read(REFLECT, schema, test));}
public boolean parquet-mr_f566_0(Object other)
{    if (this == other) {        return true;    }    if (other == null || getClass() != other.getClass()) {        return false;    }    Pair<?, ?> that = (Pair<?, ?>) other;    if (first == null) {        if (that.first != null) {            return false;        }    } else if (first.equals(that.first)) {        return false;    }    if (second == null) {        if (that.second != null) {            return false;        }    } else if (second.equals(that.second)) {        return false;    }    return true;}
public int parquet-mr_f567_0()
{    return Arrays.hashCode(new Object[] { first, second });}
public static Pair<X, Y> parquet-mr_f568_0(X first, Y second)
{    return new Pair<X, Y>(first, second);}
public void parquet-mr_f569_0() throws IOException
{    ReflectData model = new ReflectData();    model.addLogicalTypeConversion(new Conversion<Pair>() {        @Override        public Class<Pair> getConvertedType() {            return Pair.class;        }        @Override        public String getLogicalTypeName() {            return "pair";        }        @Override        public Pair fromRecord(IndexedRecord value, Schema schema, LogicalType type) {            return Pair.of(value.get(0), value.get(1));        }        @Override        public IndexedRecord toRecord(Pair value, Schema schema, LogicalType type) {            GenericData.Record record = new GenericData.Record(schema);            record.put(0, value.first);            record.put(1, value.second);            return record;        }    });    LogicalTypes.register("pair", new LogicalTypes.LogicalTypeFactory() {        private final LogicalType PAIR = new LogicalType("pair");        @Override        public LogicalType fromSchema(Schema schema) {            return PAIR;        }    });    Schema schema = model.getSchema(PairRecord.class);    Assert.assertEquals("Should have the correct record name", "org.apache.parquet.avro.TestReflectLogicalTypes", schema.getNamespace());    Assert.assertEquals("Should have the correct record name", "PairRecord", schema.getName());    Assert.assertEquals("Should have the correct logical type", "pair", LogicalTypes.fromSchema(schema.getField("pair").schema()).getName());    PairRecord record = new PairRecord();    record.pair = Pair.of(34L, 35L);    List<PairRecord> expected = new ArrayList<PairRecord>();    expected.add(record);    File test = write(model, schema, record);    Pair<Long, Long> actual = AvroTestUtil.<PairRecord>read(model, schema, test).get(0).pair;    Assert.assertEquals("Data should match after serialization round-trip", 34L, (long) actual.first);    Assert.assertEquals("Data should match after serialization round-trip", 35L, (long) actual.second);}
public Class<Pair> parquet-mr_f570_0()
{    return Pair.class;}
public String parquet-mr_f571_0()
{    return "pair";}
public Pair parquet-mr_f572_0(IndexedRecord value, Schema schema, LogicalType type)
{    return Pair.of(value.get(0), value.get(1));}
public IndexedRecord parquet-mr_f573_0(Pair value, Schema schema, LogicalType type)
{    GenericData.Record record = new GenericData.Record(schema);    record.put(0, value.first);    record.put(1, value.second);    return record;}
public LogicalType parquet-mr_f574_0(Schema schema)
{    return PAIR;}
public void parquet-mr_f575_0() throws IOException
{    Schema uuidSchema = SchemaBuilder.record(RecordWithUUID.class.getName()).fields().requiredString("uuid").endRecord();    LogicalTypes.uuid().addToSchema(uuidSchema.getField("uuid").schema());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    RecordWithStringUUID r1 = new RecordWithStringUUID();    r1.uuid = u1.toString();    RecordWithStringUUID r2 = new RecordWithStringUUID();    r2.uuid = u2.toString();    List<RecordWithUUID> expected = Arrays.asList(new RecordWithUUID(), new RecordWithUUID());    expected.get(0).uuid = u1;    expected.get(1).uuid = u2;    File test = write(ReflectData.get().getSchema(RecordWithStringUUID.class), r1, r2);    Assert.assertEquals("Should convert Strings to UUIDs", expected, read(REFLECT, uuidSchema, test));        Schema uuidStringSchema = SchemaBuilder.record(RecordWithStringUUID.class.getName()).fields().requiredString("uuid").endRecord();    LogicalTypes.uuid().addToSchema(uuidStringSchema.getField("uuid").schema());    Assert.assertEquals("Should not convert to UUID if accessor is String", Arrays.asList(r1, r2), read(REFLECT, uuidStringSchema, test));}
public void parquet-mr_f576_0() throws IOException
{    Schema uuidSchema = SchemaBuilder.record(RecordWithUUID.class.getName()).fields().requiredString("uuid").endRecord();    LogicalTypes.uuid().addToSchema(uuidSchema.getField("uuid").schema());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    RecordWithUUID r1 = new RecordWithUUID();    r1.uuid = u1;    RecordWithUUID r2 = new RecordWithUUID();    r2.uuid = u2;    List<RecordWithStringUUID> expected = Arrays.asList(new RecordWithStringUUID(), new RecordWithStringUUID());    expected.get(0).uuid = u1.toString();    expected.get(1).uuid = u2.toString();    File test = write(REFLECT, uuidSchema, r1, r2);        Schema uuidStringSchema = SchemaBuilder.record(RecordWithStringUUID.class.getName()).fields().requiredString("uuid").endRecord();    Assert.assertEquals("Should read uuid as String without UUID conversion", expected, read(REFLECT, uuidStringSchema, test));    LogicalTypes.uuid().addToSchema(uuidStringSchema.getField("uuid").schema());    Assert.assertEquals("Should read uuid as String without UUID logical type", expected, read(ReflectData.get(), uuidStringSchema, test));}
public void parquet-mr_f577_0() throws IOException
{    Schema nullableUuidSchema = SchemaBuilder.record(RecordWithUUID.class.getName()).fields().optionalString("uuid").endRecord();    LogicalTypes.uuid().addToSchema(nullableUuidSchema.getField("uuid").schema().getTypes().get(1));    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    RecordWithUUID r1 = new RecordWithUUID();    r1.uuid = u1;    RecordWithUUID r2 = new RecordWithUUID();    r2.uuid = u2;    List<RecordWithStringUUID> expected = Arrays.asList(new RecordWithStringUUID(), new RecordWithStringUUID());    expected.get(0).uuid = u1.toString();    expected.get(1).uuid = u2.toString();    File test = write(REFLECT, nullableUuidSchema, r1, r2);        Schema nullableUuidStringSchema = SchemaBuilder.record(RecordWithStringUUID.class.getName()).fields().optionalString("uuid").endRecord();    Assert.assertEquals("Should read uuid as String without UUID conversion", expected, read(REFLECT, nullableUuidStringSchema, test));}
public void parquet-mr_f578_0() throws IOException
{    Schema uuidSchema = SchemaBuilder.record(RecordWithUUID.class.getName()).fields().requiredString("uuid").endRecord();    LogicalTypes.uuid().addToSchema(uuidSchema.getField("uuid").schema());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    RecordWithUUID r1 = new RecordWithUUID();    r1.uuid = u1;    RecordWithUUID r2 = new RecordWithUUID();    r2.uuid = u2;    List<RecordWithStringUUID> expected = Arrays.asList(new RecordWithStringUUID(), new RecordWithStringUUID());    expected.get(0).uuid = u1.toString();    expected.get(1).uuid = u2.toString();        File test = write(uuidSchema, r1, r2);        Schema uuidStringSchema = SchemaBuilder.record(RecordWithStringUUID.class.getName()).fields().requiredString("uuid").endRecord();    Assert.assertEquals("Should read uuid as String without UUID conversion", expected, read(REFLECT, uuidStringSchema, test));    Assert.assertEquals("Should read uuid as String without UUID logical type", expected, read(ReflectData.get(), uuidStringSchema, test));}
public void parquet-mr_f579_0() throws IOException
{    Schema uuidSchema = SchemaBuilder.record("RecordWithUUID").fields().requiredString("uuid").endRecord();    LogicalTypes.uuid().addToSchema(uuidSchema.getField("uuid").schema());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    RecordWithStringUUID r1 = new RecordWithStringUUID();    r1.uuid = u1.toString();    RecordWithStringUUID r2 = new RecordWithStringUUID();    r2.uuid = u2.toString();    List<GenericData.Record> expected = Arrays.asList(new GenericData.Record(uuidSchema), new GenericData.Record(uuidSchema));    expected.get(0).put("uuid", u1);    expected.get(1).put("uuid", u2);    File test = write(ReflectData.get().getSchema(RecordWithStringUUID.class), r1, r2);    Assert.assertEquals("Should convert Strings to UUIDs", expected, read(REFLECT, uuidSchema, test));        Schema uuidStringSchema = SchemaBuilder.record(RecordWithStringUUID.class.getName()).fields().requiredString("uuid").endRecord();    LogicalTypes.uuid().addToSchema(uuidSchema.getField("uuid").schema());    Assert.assertEquals("Should not convert to UUID if accessor is String", Arrays.asList(r1, r2), read(REFLECT, uuidStringSchema, test));}
public void parquet-mr_f580_0() throws IOException
{    Schema uuidArraySchema = SchemaBuilder.record(RecordWithUUIDArray.class.getName()).fields().name("uuids").type().array().items().stringType().noDefault().endRecord();    LogicalTypes.uuid().addToSchema(uuidArraySchema.getField("uuids").schema().getElementType());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    GenericRecord r = new GenericData.Record(uuidArraySchema);    r.put("uuids", Arrays.asList(u1.toString(), u2.toString()));    RecordWithUUIDArray expected = new RecordWithUUIDArray();    expected.uuids = new UUID[] { u1, u2 };    File test = write(uuidArraySchema, r);    Assert.assertEquals("Should convert Strings to UUIDs", expected, read(REFLECT, uuidArraySchema, test).get(0));}
public void parquet-mr_f581_0() throws IOException
{    Schema uuidArraySchema = SchemaBuilder.record(RecordWithUUIDArray.class.getName()).fields().name("uuids").type().array().items().stringType().noDefault().endRecord();    LogicalTypes.uuid().addToSchema(uuidArraySchema.getField("uuids").schema().getElementType());    Schema stringArraySchema = SchemaBuilder.record("RecordWithUUIDArray").fields().name("uuids").type().array().items().stringType().noDefault().endRecord();    stringArraySchema.getField("uuids").schema().addProp(SpecificData.CLASS_PROP, List.class.getName());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    GenericRecord expected = new GenericData.Record(stringArraySchema);    List<String> uuids = new ArrayList<String>();    uuids.add(u1.toString());    uuids.add(u2.toString());    expected.put("uuids", uuids);    RecordWithUUIDArray r = new RecordWithUUIDArray();    r.uuids = new UUID[] { u1, u2 };    File test = write(REFLECT, uuidArraySchema, r);    Assert.assertEquals("Should read UUIDs as Strings", expected, read(ReflectData.get(), stringArraySchema, test).get(0));}
public void parquet-mr_f582_0() throws IOException
{    Schema uuidListSchema = SchemaBuilder.record(RecordWithUUIDList.class.getName()).fields().name("uuids").type().array().items().stringType().noDefault().endRecord();    uuidListSchema.getField("uuids").schema().addProp(SpecificData.CLASS_PROP, List.class.getName());    LogicalTypes.uuid().addToSchema(uuidListSchema.getField("uuids").schema().getElementType());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    GenericRecord r = new GenericData.Record(uuidListSchema);    r.put("uuids", Arrays.asList(u1.toString(), u2.toString()));    RecordWithUUIDList expected = new RecordWithUUIDList();    expected.uuids = Arrays.asList(u1, u2);    File test = write(uuidListSchema, r);    Assert.assertEquals("Should convert Strings to UUIDs", expected, read(REFLECT, uuidListSchema, test).get(0));}
public void parquet-mr_f583_0() throws IOException
{    Schema uuidListSchema = SchemaBuilder.record(RecordWithUUIDList.class.getName()).fields().name("uuids").type().array().items().stringType().noDefault().endRecord();    uuidListSchema.getField("uuids").schema().addProp(SpecificData.CLASS_PROP, List.class.getName());    LogicalTypes.uuid().addToSchema(uuidListSchema.getField("uuids").schema().getElementType());    Schema stringArraySchema = SchemaBuilder.record("RecordWithUUIDArray").fields().name("uuids").type().array().items().stringType().noDefault().endRecord();    stringArraySchema.getField("uuids").schema().addProp(SpecificData.CLASS_PROP, List.class.getName());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    GenericRecord expected = new GenericData.Record(stringArraySchema);    expected.put("uuids", Arrays.asList(u1.toString(), u2.toString()));    RecordWithUUIDList r = new RecordWithUUIDList();    r.uuids = Arrays.asList(u1, u2);    File test = write(REFLECT, uuidListSchema, r);    Assert.assertEquals("Should read UUIDs as Strings", expected, read(REFLECT, stringArraySchema, test).get(0));}
private File parquet-mr_f584_0(Schema schema, D... data) throws IOException
{    return write(ReflectData.get(), schema, data);}
private File parquet-mr_f585_0(GenericData model, Schema schema, D... data) throws IOException
{    return AvroTestUtil.write(temp, model, schema, data);}
public int parquet-mr_f586_0()
{    return uuid.hashCode();}
public boolean parquet-mr_f587_0(Object obj)
{    if (obj == null) {        return false;    }    if (!(obj instanceof RecordWithUUID)) {        return false;    }    RecordWithUUID that = (RecordWithUUID) obj;    return this.uuid.equals(that.uuid);}
public int parquet-mr_f588_0()
{    return uuid.hashCode();}
public boolean parquet-mr_f589_0(Object obj)
{    if (obj == null) {        return false;    }    if (!(obj instanceof RecordWithStringUUID)) {        return false;    }    RecordWithStringUUID that = (RecordWithStringUUID) obj;    return this.uuid.equals(that.uuid);}
public int parquet-mr_f590_0()
{    return Arrays.hashCode(uuids);}
public boolean parquet-mr_f591_0(Object obj)
{    if (obj == null) {        return false;    }    if (!(obj instanceof RecordWithUUIDArray)) {        return false;    }    RecordWithUUIDArray that = (RecordWithUUIDArray) obj;    return Arrays.equals(this.uuids, that.uuids);}
public int parquet-mr_f592_0()
{    return uuids.hashCode();}
public boolean parquet-mr_f593_0(Object obj)
{    if (obj == null) {        return false;    }    if (!(obj instanceof RecordWithUUIDList)) {        return false;    }    RecordWithUUIDList that = (RecordWithUUIDList) obj;    return this.uuids.equals(that.uuids);}
public void parquet-mr_f594_0() throws IOException
{    Configuration conf = new Configuration(false);    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    AvroReadSupport.setAvroDataSupplier(conf, ReflectDataSupplier.class);    Path path = writePojosToParquetFile(10, CompressionCodecName.UNCOMPRESSED, false);    try (ParquetReader<Pojo> reader = new AvroParquetReader<Pojo>(conf, path)) {        Pojo object = getPojo();        for (int i = 0; i < 10; i++) {            assertEquals(object, reader.read());        }        assertNull(reader.read());    }}
public void parquet-mr_f595_0() throws IOException
{    Configuration conf = new Configuration(false);    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    AvroReadSupport.setAvroDataSupplier(conf, GenericDataSupplier.class);    Path path = writePojosToParquetFile(2, CompressionCodecName.UNCOMPRESSED, false);    try (ParquetReader<GenericRecord> reader = new AvroParquetReader<GenericRecord>(conf, path)) {        GenericRecord object = getGenericPojoUtf8();        for (int i = 0; i < 2; i += 1) {            assertEquals(object, reader.read());        }        assertNull(reader.read());    }}
private GenericRecord parquet-mr_f596_0()
{    Schema schema = ReflectData.get().getSchema(Pojo.class);    GenericData.Record record = new GenericData.Record(schema);    record.put("myboolean", true);    record.put("mybyte", 1);    record.put("myshort", 1);    record.put("myint", 1);    record.put("mylong", 2L);    record.put("myfloat", 3.1f);    record.put("mydouble", 4.1);    record.put("mybytes", ByteBuffer.wrap(new byte[] { 1, 2, 3, 4 }));    record.put("mystring", new Utf8("Hello"));    record.put("myenum", new GenericData.EnumSymbol(schema.getField("myenum").schema(), "A"));    Map<CharSequence, CharSequence> map = new HashMap<CharSequence, CharSequence>();    map.put(new Utf8("a"), new Utf8("1"));    map.put(new Utf8("b"), new Utf8("2"));    record.put("mymap", map);    record.put("myshortarray", new GenericData.Array<Integer>(schema.getField("myshortarray").schema(), Lists.newArrayList(1, 2)));    record.put("myintarray", new GenericData.Array<Integer>(schema.getField("myintarray").schema(), Lists.newArrayList(1, 2)));    record.put("mystringarray", new GenericData.Array<Utf8>(schema.getField("mystringarray").schema(), Lists.newArrayList(new Utf8("a"), new Utf8("b"))));    record.put("mylist", new GenericData.Array<Utf8>(schema.getField("mylist").schema(), Lists.newArrayList(new Utf8("a"), new Utf8("b"), new Utf8("c"))));    record.put("mystringable", new StringableObj("blah blah"));    return record;}
private Pojo parquet-mr_f597_0()
{    Pojo object = new Pojo();    object.myboolean = true;    object.mybyte = 1;    object.myshort = 1;    object.myint = 1;    object.mylong = 2L;    object.myfloat = 3.1f;    object.mydouble = 4.1;    object.mybytes = new byte[] { 1, 2, 3, 4 };    object.mystring = "Hello";    object.myenum = E.A;    Map<String, String> map = new HashMap<String, String>();    map.put("a", "1");    map.put("b", "2");    object.mymap = map;    object.myshortarray = new short[] { 1, 2 };    object.myintarray = new int[] { 1, 2 };    object.mystringarray = new String[] { "a", "b" };    object.mylist = Lists.newArrayList("a", "b", "c");    object.mystringable = new StringableObj("blah blah");    return object;}
private Path parquet-mr_f598_0(int num, CompressionCodecName compression, boolean enableDictionary) throws IOException
{    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path path = new Path(tmp.getPath());    Pojo object = getPojo();    Schema schema = ReflectData.get().getSchema(object.getClass());    try (ParquetWriter<Pojo> writer = AvroParquetWriter.<Pojo>builder(path).withSchema(schema).withCompressionCodec(compression).withDataModel(ReflectData.get()).withDictionaryEncoding(enableDictionary).build()) {        for (int i = 0; i < num; i++) {            writer.write(object);        }    }    return path;}
public String parquet-mr_f599_0()
{    return this.value;}
public boolean parquet-mr_f600_0(Object other)
{    return other instanceof StringableObj && this.value.equals(((StringableObj) other).value);}
public boolean parquet-mr_f601_0(Object o)
{    if (!(o instanceof Pojo))        return false;    Pojo that = (Pojo) o;    return myboolean == that.myboolean && mybyte == that.mybyte && myshort == that.myshort && myint == that.myint && mylong == that.mylong && myfloat == that.myfloat && mydouble == that.mydouble && Arrays.equals(mybytes, that.mybytes) && mystring.equals(that.mystring) && myenum == that.myenum && mymap.equals(that.mymap) && Arrays.equals(myshortarray, that.myshortarray) && Arrays.equals(myintarray, that.myintarray) && Arrays.equals(mystringarray, that.mystringarray) && mylist.equals(that.mylist) && mystringable.equals(that.mystringable);}
public String parquet-mr_f602_0()
{    return "Pojo{" + "myboolean=" + myboolean + ", mybyte=" + mybyte + ", myshort=" + myshort + ", myint=" + myint + ", mylong=" + mylong + ", myfloat=" + myfloat + ", mydouble=" + mydouble + ", mybytes=" + Arrays.toString(mybytes) + ", mystring='" + mystring + '\'' + ", myenum=" + myenum + ", mymap=" + mymap + ", myshortarray=" + Arrays.toString(myshortarray) + ", myintarray=" + Arrays.toString(myintarray) + ", mystringarray=" + Arrays.toString(mystringarray) + ", mylist=" + mylist + ", mystringable=" + mystringable.toString() + '}';}
public static Car parquet-mr_f603_0(int i)
{    String vin = "1VXBR12EXCP000000";    Car.Builder carBuilder = Car.newBuilder().setDoors(2).setMake("Tesla").setModel(String.format("Model X v%d", i % 2)).setVin(new Vin(vin.getBytes())).setYear(2014 + i).setOptionalExtra(LeatherTrim.newBuilder().setColour("black").build()).setRegistration("California");    Engine.Builder engineBuilder = Engine.newBuilder().setCapacity(85.0f).setHasTurboCharger(false);    if (i % 2 == 0) {        engineBuilder.setType(EngineType.ELECTRIC);    } else {        engineBuilder.setType(EngineType.PETROL);    }    carBuilder.setEngine(engineBuilder.build());    if (i % 4 == 0) {        List<Service> serviceList = Lists.newArrayList();        serviceList.add(Service.newBuilder().setDate(1374084640).setMechanic("Elon Musk").build());        carBuilder.setServiceHistory(serviceList);    }    return carBuilder.build();}
public void parquet-mr_f604_0(Context context) throws IOException, InterruptedException
{    for (int i = 0; i < 10; i++) {        context.write(null, nextRecord(i));    }}
protected void parquet-mr_f605_0(Void key, Car car, Context context) throws IOException, InterruptedException
{        if (car != null) {        context.write(null, car);    }}
protected void parquet-mr_f606_0(Void key, ShortCar car, Context context) throws IOException, InterruptedException
{        if (car != null) {        context.write(null, car);    }}
public RecordFilter parquet-mr_f607_0(Iterable<ColumnReader> readers)
{    return filter.bind(readers);}
public void parquet-mr_f608_0() throws Exception
{    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        final Job job = new Job(conf, "write");                TextInputFormat.addInputPath(job, inputPath);        job.setInputFormatClass(TextInputFormat.class);        job.setMapperClass(TestSpecificInputOutputFormat.MyMapper.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(AvroParquetOutputFormat.class);        AvroParquetOutputFormat.setOutputPath(job, parquetPath);        AvroParquetOutputFormat.setSchema(job, Car.SCHEMA$);        waitForJob(job);    }}
public void parquet-mr_f609_0() throws Exception
{    final Job job = new Job(conf, "read");    job.setInputFormatClass(AvroParquetInputFormat.class);    AvroParquetInputFormat.setInputPaths(job, parquetPath);        AvroParquetInputFormat.setUnboundRecordFilter(job, ElectricCarFilter.class);        Schema projection = Schema.createRecord(Car.SCHEMA$.getName(), Car.SCHEMA$.getDoc(), Car.SCHEMA$.getNamespace(), false);    List<Schema.Field> fields = Lists.newArrayList();    for (Schema.Field field : Car.SCHEMA$.getFields()) {        if (!"optionalExtra".equals(field.name())) {            fields.add(new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultVal(), field.order()));        }    }    projection.setFields(fields);    AvroParquetInputFormat.setRequestedProjection(job, projection);    job.setMapperClass(TestSpecificInputOutputFormat.MyMapper2.class);    job.setNumReduceTasks(0);    job.setOutputFormatClass(AvroParquetOutputFormat.class);    AvroParquetOutputFormat.setOutputPath(job, outputPath);    AvroParquetOutputFormat.setSchema(job, Car.SCHEMA$);    waitForJob(job);    final Path mapperOutput = new Path(outputPath.toString(), "part-m-00000.parquet");    try (final AvroParquetReader<Car> out = new AvroParquetReader<>(mapperOutput)) {        Car car;        Car previousCar = null;        int lineNumber = 0;        while ((car = out.read()) != null) {            if (previousCar != null) {                                assertTrue(car.getModel() == previousCar.getModel());            }                        if (car.getEngine().getType() == EngineType.PETROL) {                fail("UnboundRecordFilter failed to remove cars with PETROL engines");            }                        Car expectedCar = nextRecord(lineNumber * 2);                                    expectedCar.setOptionalExtra(null);            assertEquals("line " + lineNumber, expectedCar, car);            ++lineNumber;            previousCar = car;        }    }}
public void parquet-mr_f610_0() throws Exception
{    final Job job = new Job(conf, "read changed/short");    job.setInputFormatClass(AvroParquetInputFormat.class);    AvroParquetInputFormat.setInputPaths(job, parquetPath);        AvroParquetInputFormat.setUnboundRecordFilter(job, ElectricCarFilter.class);            Schema projection = Schema.createRecord(Car.SCHEMA$.getName(), Car.SCHEMA$.getDoc(), Car.SCHEMA$.getNamespace(), false);    List<Schema.Field> fields = Lists.newArrayList();    for (Schema.Field field : Car.SCHEMA$.getFields()) {                if ("engine".equals(field.name()) || "year".equals(field.name()) || "vin".equals(field.name())) {            fields.add(new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultVal(), field.order()));        }    }    projection.setFields(fields);    AvroParquetInputFormat.setRequestedProjection(job, projection);    AvroParquetInputFormat.setAvroReadSchema(job, ShortCar.SCHEMA$);    job.setMapperClass(TestSpecificInputOutputFormat.MyMapperShort.class);    job.setNumReduceTasks(0);    job.setOutputFormatClass(AvroParquetOutputFormat.class);    AvroParquetOutputFormat.setOutputPath(job, outputPath);    AvroParquetOutputFormat.setSchema(job, ShortCar.SCHEMA$);    waitForJob(job);    final Path mapperOutput = new Path(outputPath.toString(), "part-m-00000.parquet");    try (final AvroParquetReader<ShortCar> out = new AvroParquetReader<>(mapperOutput)) {        ShortCar car;        int lineNumber = 0;        while ((car = out.read()) != null) {                                    Car expectedCar = nextRecord(lineNumber * 2);                        assertNull(car.getMake());            assertEquals(car.getEngine(), expectedCar.getEngine());            assertEquals(car.getYear(), expectedCar.getYear());            assertEquals(car.getVin(), expectedCar.getVin());            ++lineNumber;        }    }}
private void parquet-mr_f611_1(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {                sleep(100);    }        if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
public void parquet-mr_f612_0() throws IOException
{    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);}
public static Collection<Object[]> parquet-mr_f613_0()
{    Object[][] data = new Object[][] {     { false },     { true } };    return Arrays.asList(data);}
public void parquet-mr_f614_0() throws IOException
{    Path path = writeCarsToParquetFile(10, CompressionCodecName.UNCOMPRESSED, false);    try (ParquetReader<Car> reader = new AvroParquetReader<>(testConf, path)) {        for (int i = 0; i < 10; i++) {            assertEquals(getVwPolo().toString(), reader.read().toString());            assertEquals(getVwPassat().toString(), reader.read().toString());            assertEquals(getBmwMini().toString(), reader.read().toString());        }        assertNull(reader.read());    }}
public void parquet-mr_f615_0() throws IOException
{    Path path = writeCarsToParquetFile(10, CompressionCodecName.UNCOMPRESSED, true);    try (ParquetReader<Car> reader = new AvroParquetReader<>(testConf, path)) {        for (int i = 0; i < 10; i++) {            assertEquals(getVwPolo().toString(), reader.read().toString());            assertEquals(getVwPassat().toString(), reader.read().toString());            assertEquals(getBmwMini().toString(), reader.read().toString());        }        assertNull(reader.read());    }}
public void parquet-mr_f616_0() throws IOException
{    Path path = writeCarsToParquetFile(10, CompressionCodecName.UNCOMPRESSED, false);    try (ParquetReader<Car> reader = new AvroParquetReader<>(testConf, path, column("make", equalTo("Volkswagen")))) {        for (int i = 0; i < 10; i++) {            assertEquals(getVwPolo().toString(), reader.read().toString());            assertEquals(getVwPassat().toString(), reader.read().toString());        }        assertNull(reader.read());    }}
public void parquet-mr_f617_0() throws IOException
{    Path path = writeCarsToParquetFile(10000, CompressionCodecName.UNCOMPRESSED, false, DEFAULT_BLOCK_SIZE / 64, DEFAULT_PAGE_SIZE / 64);    try (ParquetReader<Car> reader = new AvroParquetReader<>(testConf, path, column("make", equalTo("Volkswagen")))) {        for (int i = 0; i < 10000; i++) {            assertEquals(getVwPolo().toString(), reader.read().toString());            assertEquals(getVwPassat().toString(), reader.read().toString());        }        assertNull(reader.read());    }}
public void parquet-mr_f618_0() throws IOException
{    Path path = writeCarsToParquetFile(10000, CompressionCodecName.UNCOMPRESSED, false, DEFAULT_BLOCK_SIZE / 64, DEFAULT_PAGE_SIZE / 64);    try (ParquetReader<Car> reader = new AvroParquetReader<>(testConf, path, column("make", equalTo("Bogus")))) {        assertNull(reader.read());    }}
public void parquet-mr_f619_0() throws IOException
{    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path path = new Path(tmp.getPath());    Car vwPolo = getVwPolo();    Car vwPassat = getVwPassat();    Car bmwMini = getBmwMini();    try (ParquetWriter<Car> writer = new AvroParquetWriter<Car>(path, Car.SCHEMA$, CompressionCodecName.UNCOMPRESSED, DEFAULT_BLOCK_SIZE / 128, DEFAULT_PAGE_SIZE / 128, false)) {        for (int i = 0; i < 10000; i++) {            writer.write(vwPolo);            writer.write(vwPassat);            writer.write(vwPolo);        }                writer.write(bmwMini);    }    try (ParquetReader<Car> reader = new AvroParquetReader<Car>(testConf, path, column("make", equalTo("BMW")))) {        assertEquals(getBmwMini().toString(), reader.read().toString());        assertNull(reader.read());    }}
public void parquet-mr_f620_0() throws IOException
{    Path path = writeCarsToParquetFile(1, CompressionCodecName.UNCOMPRESSED, true);    try (ParquetReader<Car> reader = new AvroParquetReader<>(testConf, path, column("make", equalTo("Volkswagen")))) {        assertEquals(getVwPolo().toString(), reader.read().toString());        assertEquals(getVwPassat().toString(), reader.read().toString());        assertNull(reader.read());    }}
public void parquet-mr_f621_0() throws IOException
{    Path path = writeCarsToParquetFile(1, CompressionCodecName.UNCOMPRESSED, false);    ParquetReader<Car> reader = new AvroParquetReader<Car>(testConf, path, column("engine.type", equalTo(EngineType.DIESEL)));    assertEquals(reader.read().toString(), getVwPassat().toString());    assertNull(reader.read());    reader = new AvroParquetReader<Car>(testConf, path, column("engine.capacity", equalTo(1.4f)));    assertEquals(getVwPolo().toString(), reader.read().toString());    assertNull(reader.read());    reader = new AvroParquetReader<Car>(testConf, path, column("engine.hasTurboCharger", equalTo(true)));    assertEquals(getBmwMini().toString(), reader.read().toString());    assertNull(reader.read());}
public void parquet-mr_f622_0() throws IOException
{    Path path = writeCarsToParquetFile(1, CompressionCodecName.UNCOMPRESSED, false);    Configuration conf = new Configuration(testConf);    Schema schema = Car.getClassSchema();    List<Schema.Field> fields = schema.getFields();        List<Schema.Field> projectedFields = new ArrayList<Schema.Field>();    for (Schema.Field field : fields) {        String name = field.name();        if ("optionalExtra".equals(name) || "serviceHistory".equals(name)) {            continue;        }                Schema.Field fieldClone = new Schema.Field(name, field.schema(), field.doc(), field.defaultVal());        projectedFields.add(fieldClone);    }    Schema projectedSchema = Schema.createRecord(schema.getName(), schema.getDoc(), schema.getNamespace(), schema.isError());    projectedSchema.setFields(projectedFields);    AvroReadSupport.setRequestedProjection(conf, projectedSchema);    try (ParquetReader<Car> reader = new AvroParquetReader<Car>(conf, path)) {        for (Car car = reader.read(); car != null; car = reader.read()) {            assertTrue(car.getDoors() == 4 || car.getDoors() == 5);            assertNotNull(car.getEngine());            assertNotNull(car.getMake());            assertNotNull(car.getModel());            assertEquals(2010, car.getYear());            assertNotNull(car.getVin());            assertNull(car.getOptionalExtra());            assertNull(car.getServiceHistory());        }    }}
public void parquet-mr_f623_0() throws IOException
{    Path path = writeCarsToParquetFile(1, CompressionCodecName.UNCOMPRESSED, false);    Configuration conf = new Configuration(testConf);    AvroReadSupport.setAvroReadSchema(conf, NewCar.SCHEMA$);    try (ParquetReader<NewCar> reader = new AvroParquetReader<>(conf, path)) {        for (NewCar car = reader.read(); car != null; car = reader.read()) {            assertNotNull(car.getEngine());            assertNotNull(car.getBrand());            assertEquals(2010, car.getYear());            assertNotNull(car.getVin());            assertNull(car.getDescription());            assertEquals(5, car.getOpt());        }    }}
private Path parquet-mr_f624_0(int num, CompressionCodecName compression, boolean enableDictionary) throws IOException
{    return writeCarsToParquetFile(num, compression, enableDictionary, DEFAULT_BLOCK_SIZE, DEFAULT_PAGE_SIZE);}
private Path parquet-mr_f625_0(int num, CompressionCodecName compression, boolean enableDictionary, int blockSize, int pageSize) throws IOException
{    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path path = new Path(tmp.getPath());    Car vwPolo = getVwPolo();    Car vwPassat = getVwPassat();    Car bmwMini = getBmwMini();    try (ParquetWriter<Car> writer = new AvroParquetWriter<>(path, Car.SCHEMA$, compression, blockSize, pageSize, enableDictionary)) {        for (int i = 0; i < num; i++) {            writer.write(vwPolo);            writer.write(vwPassat);            writer.write(bmwMini);        }    }    return path;}
public static Car parquet-mr_f626_0()
{    String vin = "WVWDB4505LK000001";    return Car.newBuilder().setYear(2010).setRegistration("A123 GTR").setMake("Volkswagen").setModel("Polo").setVin(new Vin(vin.getBytes())).setDoors(4).setEngine(Engine.newBuilder().setType(EngineType.PETROL).setCapacity(1.4f).setHasTurboCharger(false).build()).setOptionalExtra(Stereo.newBuilder().setMake("Blaupunkt").setSpeakers(4).build()).setServiceHistory(ImmutableList.of(Service.newBuilder().setDate(1325376000l).setMechanic("Jim").build(), Service.newBuilder().setDate(1356998400l).setMechanic("Mike").build())).build();}
public static Car parquet-mr_f627_0()
{    String vin = "WVWDB4505LK000002";    return Car.newBuilder().setYear(2010).setRegistration("A123 GXR").setMake("Volkswagen").setModel("Passat").setVin(new Vin(vin.getBytes())).setDoors(5).setEngine(Engine.newBuilder().setType(EngineType.DIESEL).setCapacity(2.0f).setHasTurboCharger(false).build()).setOptionalExtra(LeatherTrim.newBuilder().setColour("Black").build()).setServiceHistory(ImmutableList.of(Service.newBuilder().setDate(1325376000l).setMechanic("Jim").build())).build();}
public static Car parquet-mr_f628_0()
{    String vin = "WBABA91060AL00003";    return Car.newBuilder().setYear(2010).setRegistration("A124 GSR").setMake("BMW").setModel("Mini").setVin(new Vin(vin.getBytes())).setDoors(4).setEngine(Engine.newBuilder().setType(EngineType.PETROL).setCapacity(1.6f).setHasTurboCharger(true).build()).setOptionalExtra(null).setServiceHistory(ImmutableList.of(Service.newBuilder().setDate(1356998400l).setMechanic("Mike").build())).build();}
public static void parquet-mr_f629_0() throws IOException
{    TestStringBehavior.SCHEMA = new Schema.Parser().parse(Resources.getResource("stringBehavior.avsc").openStream());}
public void parquet-mr_f630_0() throws IOException
{        GenericRecord record = new GenericRecordBuilder(SCHEMA).set("default_class", "default").set("string_class", "string").set("stringable_class", BIG_DECIMAL.toString()).set("default_map", ImmutableMap.of("default_key", 34)).set("string_map", ImmutableMap.of("string_key", 35)).set("stringable_map", ImmutableMap.of(BIG_DECIMAL.toString(), 36)).build();    File file = temp.newFile("parquet");    file.delete();    file.deleteOnExit();    parquetFile = new Path(file.getPath());    try (ParquetWriter<GenericRecord> parquet = AvroParquetWriter.<GenericRecord>builder(parquetFile).withDataModel(GenericData.get()).withSchema(SCHEMA).build()) {        parquet.write(record);    }    avroFile = temp.newFile("avro");    avroFile.delete();    avroFile.deleteOnExit();    try (DataFileWriter<GenericRecord> avro = new DataFileWriter<GenericRecord>(new GenericDatumWriter<>(SCHEMA)).create(SCHEMA, avroFile)) {        avro.append(record);    }}
public void parquet-mr_f631_0() throws IOException
{    GenericRecord avroRecord;    try (DataFileReader<GenericRecord> avro = new DataFileReader<>(avroFile, new GenericDatumReader<>(SCHEMA))) {        avroRecord = avro.next();    }    GenericRecord parquetRecord;    Configuration conf = new Configuration();    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    AvroReadSupport.setAvroDataSupplier(conf, GenericDataSupplier.class);    AvroReadSupport.setAvroReadSchema(conf, SCHEMA);    try (ParquetReader<GenericRecord> parquet = AvroParquetReader.<GenericRecord>builder(parquetFile).withConf(conf).build()) {        parquetRecord = parquet.read();    }    Assert.assertEquals("Avro default string class should be Utf8", Utf8.class, avroRecord.get("default_class").getClass());    Assert.assertEquals("Parquet default string class should be Utf8", Utf8.class, parquetRecord.get("default_class").getClass());    Assert.assertEquals("Avro avro.java.string=String class should be String", String.class, avroRecord.get("string_class").getClass());    Assert.assertEquals("Parquet avro.java.string=String class should be String", String.class, parquetRecord.get("string_class").getClass());    Assert.assertEquals("Avro stringable class should be Utf8", Utf8.class, avroRecord.get("stringable_class").getClass());    Assert.assertEquals("Parquet stringable class should be Utf8", Utf8.class, parquetRecord.get("stringable_class").getClass());    Assert.assertEquals("Avro map default string class should be Utf8", Utf8.class, keyClass(avroRecord.get("default_map")));    Assert.assertEquals("Parquet map default string class should be Utf8", Utf8.class, keyClass(parquetRecord.get("default_map")));    Assert.assertEquals("Avro map avro.java.string=String class should be String", String.class, keyClass(avroRecord.get("string_map")));    Assert.assertEquals("Parquet map avro.java.string=String class should be String", String.class, keyClass(parquetRecord.get("string_map")));    Assert.assertEquals("Avro map stringable class should be Utf8", Utf8.class, keyClass(avroRecord.get("stringable_map")));    Assert.assertEquals("Parquet map stringable class should be Utf8", Utf8.class, keyClass(parquetRecord.get("stringable_map")));}
public void parquet-mr_f632_0() throws IOException
{    org.apache.parquet.avro.StringBehaviorTest avroRecord;    try (DataFileReader<org.apache.parquet.avro.StringBehaviorTest> avro = new DataFileReader<>(avroFile, new SpecificDatumReader<>(org.apache.parquet.avro.StringBehaviorTest.getClassSchema()))) {        avroRecord = avro.next();    }    org.apache.parquet.avro.StringBehaviorTest parquetRecord;    Configuration conf = new Configuration();    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    AvroReadSupport.setAvroDataSupplier(conf, SpecificDataSupplier.class);    AvroReadSupport.setAvroReadSchema(conf, org.apache.parquet.avro.StringBehaviorTest.getClassSchema());    try (ParquetReader<org.apache.parquet.avro.StringBehaviorTest> parquet = AvroParquetReader.<org.apache.parquet.avro.StringBehaviorTest>builder(parquetFile).withConf(conf).build()) {        parquetRecord = parquet.read();    }    Assert.assertEquals("Avro default string class should be String", Utf8.class, avroRecord.default_class.getClass());    Assert.assertEquals("Parquet default string class should be String", Utf8.class, parquetRecord.default_class.getClass());    Assert.assertEquals("Avro avro.java.string=String class should be String", String.class, avroRecord.string_class.getClass());    Assert.assertEquals("Parquet avro.java.string=String class should be String", String.class, parquetRecord.string_class.getClass());    Assert.assertEquals("Avro stringable class should be BigDecimal", BigDecimal.class, avroRecord.stringable_class.getClass());    Assert.assertEquals("Parquet stringable class should be BigDecimal", BigDecimal.class, parquetRecord.stringable_class.getClass());    Assert.assertEquals("Should have the correct BigDecimal value", BIG_DECIMAL, parquetRecord.stringable_class);    Assert.assertEquals("Avro map default string class should be String", Utf8.class, keyClass(avroRecord.default_map));    Assert.assertEquals("Parquet map default string class should be String", Utf8.class, keyClass(parquetRecord.default_map));    Assert.assertEquals("Avro map avro.java.string=String class should be String", String.class, keyClass(avroRecord.string_map));    Assert.assertEquals("Parquet map avro.java.string=String class should be String", String.class, keyClass(parquetRecord.string_map));    Assert.assertEquals("Avro map stringable class should be BigDecimal", BigDecimal.class, keyClass(avroRecord.stringable_map));    Assert.assertEquals("Parquet map stringable class should be BigDecimal", BigDecimal.class, keyClass(parquetRecord.stringable_map));}
public void parquet-mr_f633_0() throws IOException
{    Schema reflectSchema = ReflectData.get().getSchema(ReflectRecord.class);    ReflectRecord avroRecord;    try (DataFileReader<ReflectRecord> avro = new DataFileReader<>(avroFile, new ReflectDatumReader<>(reflectSchema))) {        avroRecord = avro.next();    }    ReflectRecord parquetRecord;    Configuration conf = new Configuration();    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    AvroReadSupport.setAvroDataSupplier(conf, ReflectDataSupplier.class);    AvroReadSupport.setAvroReadSchema(conf, reflectSchema);    try (ParquetReader<ReflectRecord> parquet = AvroParquetReader.<ReflectRecord>builder(parquetFile).withConf(conf).build()) {        parquetRecord = parquet.read();    }    Assert.assertEquals("Avro default string class should be String", String.class, avroRecord.default_class.getClass());    Assert.assertEquals("Parquet default string class should be String", String.class, parquetRecord.default_class.getClass());    Assert.assertEquals("Avro avro.java.string=String class should be String", String.class, avroRecord.string_class.getClass());    Assert.assertEquals("Parquet avro.java.string=String class should be String", String.class, parquetRecord.string_class.getClass());    Assert.assertEquals("Avro stringable class should be BigDecimal", BigDecimal.class, avroRecord.stringable_class.getClass());    Assert.assertEquals("Parquet stringable class should be BigDecimal", BigDecimal.class, parquetRecord.stringable_class.getClass());    Assert.assertEquals("Should have the correct BigDecimal value", BIG_DECIMAL, parquetRecord.stringable_class);    Assert.assertEquals("Avro map default string class should be String", String.class, keyClass(avroRecord.default_map));    Assert.assertEquals("Parquet map default string class should be String", String.class, keyClass(parquetRecord.default_map));    Assert.assertEquals("Avro map avro.java.string=String class should be String", String.class, keyClass(avroRecord.string_map));    Assert.assertEquals("Parquet map avro.java.string=String class should be String", String.class, keyClass(parquetRecord.string_map));    Assert.assertEquals("Avro map stringable class should be BigDecimal", BigDecimal.class, keyClass(avroRecord.stringable_map));    Assert.assertEquals("Parquet map stringable class should be BigDecimal", BigDecimal.class, keyClass(parquetRecord.stringable_map));}
public void parquet-mr_f634_0() throws IOException
{    Schema reflectSchema = ReflectData.get().getSchema(ReflectRecordJavaClass.class);    System.err.println("Schema: " + reflectSchema.toString(true));    ReflectRecordJavaClass avroRecord;    try (DataFileReader<ReflectRecordJavaClass> avro = new DataFileReader<>(avroFile, new ReflectDatumReader<>(reflectSchema))) {        avroRecord = avro.next();    }    ReflectRecordJavaClass parquetRecord;    Configuration conf = new Configuration();    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    AvroReadSupport.setAvroDataSupplier(conf, ReflectDataSupplier.class);    AvroReadSupport.setAvroReadSchema(conf, reflectSchema);    AvroReadSupport.setRequestedProjection(conf, reflectSchema);    try (ParquetReader<ReflectRecordJavaClass> parquet = AvroParquetReader.<ReflectRecordJavaClass>builder(parquetFile).withConf(conf).build()) {        parquetRecord = parquet.read();    }        Assert.assertEquals("Avro default string class should be String", String.class, avroRecord.default_class.getClass());    Assert.assertEquals("Parquet default string class should be String", String.class, parquetRecord.default_class.getClass());    Assert.assertEquals("Avro stringable class should be BigDecimal", BigDecimal.class, avroRecord.stringable_class.getClass());    Assert.assertEquals("Parquet stringable class should be BigDecimal", BigDecimal.class, parquetRecord.stringable_class.getClass());    Assert.assertEquals("Should have the correct BigDecimal value", BIG_DECIMAL, parquetRecord.stringable_class);}
public static Class<?> parquet-mr_f635_0(Object obj)
{    Assert.assertTrue("Should be a map", obj instanceof Map);    Map<?, ?> map = (Map<?, ?>) obj;    return Iterables.getFirst(map.keySet(), null).getClass();}
public static void parquet-mr_f636_0(Configuration conf, Path path)
{    try {        FileSystem fs = path.getFileSystem(conf);        if (fs.exists(path)) {            if (!fs.delete(path, true)) {                System.err.println("Couldn't delete " + path);            }        }    } catch (IOException e) {        System.err.println("Couldn't delete " + path);        e.printStackTrace();    }}
public static boolean parquet-mr_f637_0(Configuration conf, Path path) throws IOException
{    FileSystem fs = path.getFileSystem(conf);    return fs.exists(path);}
public void parquet-mr_f638_0()
{    try {        generateData(file_1M, configuration, PARQUET_2_0, BLOCK_SIZE_DEFAULT, PAGE_SIZE_DEFAULT, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);                generateData(file_1M_BS256M_PS4M, configuration, PARQUET_2_0, BLOCK_SIZE_256M, PAGE_SIZE_4M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);        generateData(file_1M_BS256M_PS8M, configuration, PARQUET_2_0, BLOCK_SIZE_256M, PAGE_SIZE_8M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);        generateData(file_1M_BS512M_PS4M, configuration, PARQUET_2_0, BLOCK_SIZE_512M, PAGE_SIZE_4M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);        generateData(file_1M_BS512M_PS8M, configuration, PARQUET_2_0, BLOCK_SIZE_512M, PAGE_SIZE_8M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);                        generateData(file_1M_SNAPPY, configuration, PARQUET_2_0, BLOCK_SIZE_DEFAULT, PAGE_SIZE_DEFAULT, FIXED_LEN_BYTEARRAY_SIZE, SNAPPY, ONE_MILLION);        generateData(file_1M_GZIP, configuration, PARQUET_2_0, BLOCK_SIZE_DEFAULT, PAGE_SIZE_DEFAULT, FIXED_LEN_BYTEARRAY_SIZE, GZIP, ONE_MILLION);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public void parquet-mr_f639_0(Path outFile, Configuration configuration, ParquetProperties.WriterVersion version, int blockSize, int pageSize, int fixedLenByteArraySize, CompressionCodecName codec, int nRows) throws IOException
{    if (exists(configuration, outFile)) {        System.out.println("File already exists " + outFile);        return;    }    System.out.println("Generating data @ " + outFile);    MessageType schema = parseMessageType("message test { " + "required binary binary_field; " + "required int32 int32_field; " + "required int64 int64_field; " + "required boolean boolean_field; " + "required float float_field; " + "required double double_field; " + "required fixed_len_byte_array(" + fixedLenByteArraySize + ") flba_field; " + "required int96 int96_field; " + "} ");    GroupWriteSupport.setSchema(schema, configuration);    SimpleGroupFactory f = new SimpleGroupFactory(schema);    ParquetWriter<Group> writer = new ParquetWriter<Group>(outFile, new GroupWriteSupport(), codec, blockSize, pageSize, DICT_PAGE_SIZE, true, false, version, configuration);        char[] chars = new char[fixedLenByteArraySize];    Arrays.fill(chars, '*');    for (int i = 0; i < nRows; i++) {        writer.write(f.newGroup().append("binary_field", randomUUID().toString()).append("int32_field", i).append("int64_field", 64l).append("boolean_field", true).append("float_field", 1.0f).append("double_field", 2.0d).append("flba_field", new String(chars)).append("int96_field", Binary.fromConstantByteArray(new byte[12])));    }    writer.close();}
public void parquet-mr_f640_0()
{    deleteIfExists(configuration, targetDir);}
public static void parquet-mr_f641_0(String[] args)
{    DataGenerator generator = new DataGenerator();    if (args.length < 1) {        System.err.println("Please specify a command (generate|cleanup).");        System.exit(1);    }    String command = args[0];    if (command.equalsIgnoreCase("generate")) {        generator.generateAll();    } else if (command.equalsIgnoreCase("cleanup")) {        generator.cleanup();    } else {        throw new IllegalArgumentException("invalid command " + command);    }}
public String parquet-mr_f642_0()
{    try {        FileSystem fs = file.getFileSystem(new Configuration());        long bytes = fs.getFileStatus(file).getLen();        int exp = (int) (Math.log(bytes) / Math.log(1024));        if (exp == 0) {            return Long.toString(bytes);        }        String suffix = SUFFIXES[exp - 1];        return String.format("%d [%.2f%s]", bytes, bytes / Math.pow(1024, exp), suffix);    } catch (IOException e) {        return "N/A";    }}
 String parquet-mr_f643_0()
{    char[] str = new char[MIN_LENGTH + random.nextInt(MAX_LENGTH - MIN_LENGTH)];    for (int i = 0, n = str.length; i < n; ++i) {        str[i] = ALPHABET.charAt(random.nextInt(ALPHABET.length()));    }    return new String(str);}
public Builder<T> parquet-mr_f644_0(Builder<T> builder)
{    return builder;}
public String parquet-mr_f645_0()
{    return "DEFAULT";}
public org.apache.parquet.hadoop.ParquetWriter.Builder<T, ?> parquet-mr_f646_0(org.apache.parquet.hadoop.ParquetWriter.Builder<T, ?> builder)
{    return builder;}
public String parquet-mr_f647_0()
{    return "DEFAULT";}
public Builder<T> parquet-mr_f648_0(Builder<T> builder)
{    return builder.useColumnIndexFilter(false);}
public Builder<T> parquet-mr_f649_0(Builder<T> builder)
{    return builder.useColumnIndexFilter(true);}
public static void parquet-mr_f650_0(long[] data, int bucketCnt)
{    long bucketSize = (long) (Long.MAX_VALUE / (bucketCnt / 2.0));    long[] bucketBorders = new long[bucketCnt - 1];    for (int i = 0, n = bucketBorders.length; i < n; ++i) {        bucketBorders[i] = Long.MIN_VALUE + (i + 1) * bucketSize;    }    LongList[] buckets = new LongList[bucketCnt];    for (int i = 0; i < bucketCnt; ++i) {        buckets[i] = new LongArrayList(data.length / bucketCnt);    }    for (int i = 0, n = data.length; i < n; ++i) {        long value = data[i];        int bucket = Arrays.binarySearch(bucketBorders, value);        if (bucket < 0) {            bucket = -(bucket + 1);        }        buckets[bucket].add(value);    }    int offset = 0;    int mid = bucketCnt / 2;    for (int i = 0; i < bucketCnt; ++i) {        int bucketIndex;        if (i % 2 == 0) {            bucketIndex = mid + i / 2;        } else {            bucketIndex = mid - i / 2 - 1;        }        LongList bucket = buckets[bucketIndex];        bucket.getElements(0, data, offset, bucket.size());        offset += bucket.size();    }}
 void parquet-mr_f651_0(long[] data)
{    Arrays.parallelSort(data);}
 void parquet-mr_f652_0(long[] data)
{    arrangeToBuckets(data, 9);}
 void parquet-mr_f653_0(long[] data)
{    arrangeToBuckets(data, 5);}
 void parquet-mr_f654_0(long[] data)
{    arrangeToBuckets(data, 3);}
 void parquet-mr_f655_0(long[] data)
{}
public ParquetWriter.Builder<T, ?> parquet-mr_f656_0(ParquetWriter.Builder<T, ?> builder)
{    return builder.withPageSize(    Integer.MAX_VALUE).withPageRowCountLimit(1_000);}
public ParquetWriter.Builder<T, ?> parquet-mr_f657_0(ParquetWriter.Builder<T, ?> builder)
{    return builder.withPageSize(    Integer.MAX_VALUE).withPageRowCountLimit(10_000);}
public ParquetWriter.Builder<T, ?> parquet-mr_f658_0(ParquetWriter.Builder<T, ?> builder)
{    return builder.withPageSize(    Integer.MAX_VALUE).withPageRowCountLimit(50_000);}
public ParquetWriter.Builder<T, ?> parquet-mr_f659_0(ParquetWriter.Builder<T, ?> builder)
{    return builder.withPageSize(    Integer.MAX_VALUE).withPageRowCountLimit(100_000);}
public void parquet-mr_f660_0() throws IOException
{    WriteConfigurator writeConfigurator = getWriteConfigurator();    file = new Path(Files.createTempFile("benchmark-filtering_" + characteristic + '_' + writeConfigurator + '_', ".parquet").toAbsolutePath().toString());    long[] data = generateData();    characteristic.arrangeData(data);    try (ParquetWriter<Group> writer = writeConfigurator.configureBuilder(ExampleParquetWriter.builder(file).config(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, SCHEMA.toString()).withRowGroupSize(    Integer.MAX_VALUE).withWriteMode(OVERWRITE)).build()) {        for (long value : data) {            Group group = new SimpleGroup(SCHEMA);            group.add(0, value);            group.add(1, Binary.fromString(dummyGenerator.nextString()));            group.add(2, Binary.fromString(dummyGenerator.nextString()));            group.add(3, Binary.fromString(dummyGenerator.nextString()));            group.add(4, Binary.fromString(dummyGenerator.nextString()));            group.add(5, Binary.fromString(dummyGenerator.nextString()));            writer.write(group);        }    }}
 WriteConfigurator parquet-mr_f661_0()
{    return WriteConfigurator.DEFAULT;}
 ReadConfigurator parquet-mr_f662_0()
{    return ReadConfigurator.DEFAULT;}
private long[] parquet-mr_f663_0()
{    Random random = new Random(43);    long[] data = new long[RECORD_COUNT];    for (int i = 0, n = data.length; i < n; ++i) {        data[i] = random.nextLong();    }    return data;}
public void parquet-mr_f664_0()
{    random = new Random(42);    dummyGenerator = new StringGenerator();}
public void parquet-mr_f665_0()
{    System.gc();}
public void parquet-mr_f666_1() throws IOException
{        file.getFileSystem(new Configuration()).delete(file, false);}
public ParquetReader.Builder<Group> parquet-mr_f667_0() throws IOException
{    ReadConfigurator readConfigurator = getReadConfigurator();    return readConfigurator.configureBuilder(new ParquetReader.Builder<Group>(HadoopInputFile.fromPath(file, new Configuration())) {        @Override        protected ReadSupport<Group> getReadSupport() {            return new GroupReadSupport();        }    }.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, SCHEMA.toString()));}
protected ReadSupport<Group> parquet-mr_f668_0()
{    return new GroupReadSupport();}
public Random parquet-mr_f669_0()
{    return random;}
 ReadConfigurator parquet-mr_f670_0()
{    return columnIndexUsage;}
 WriteConfigurator parquet-mr_f671_0()
{    return pageRowLimit;}
 ReadConfigurator parquet-mr_f672_0()
{    return ColumnIndexUsage.WITH_COLUMN_INDEX;}
public void parquet-mr_f673_0(Blackhole blackhole, WithOrWithoutColumnIndexContext context) throws Exception
{    benchmark(blackhole, context);}
public void parquet-mr_f674_0(Blackhole blackhole, PageSizeContext context) throws Exception
{    benchmark(blackhole, context);}
private void parquet-mr_f675_0(Blackhole blackhole, BaseContext context) throws Exception
{    FilterPredicate filter = FilterApi.eq(BaseContext.COLUMN, context.getRandom().nextLong());    try (ParquetReader<Group> reader = context.createReaderBuilder().withFilter(FilterCompat.get(filter)).build()) {        blackhole.consume(reader.read());    }}
public boolean parquet-mr_f676_0()
{    return false;}
public long parquet-mr_f677_0()
{    return -1L;}
public PositionOutputStream parquet-mr_f678_0(long blockSizeHint)
{    return create(blockSizeHint);}
public PositionOutputStream parquet-mr_f679_0(long blockSizeHint)
{    return new PositionOutputStream() {        private long pos;        @Override        public long getPos() throws IOException {            return pos;        }        @Override        public void write(int b) throws IOException {            ++pos;        }    };}
public long parquet-mr_f680_0() throws IOException
{    return pos;}
public void parquet-mr_f681_0(int b) throws IOException
{    ++pos;}
public Group parquet-mr_f682_0()
{    if (random.nextDouble() > NULL_RATIO) {        Group group = FACTORY.newGroup();        group.addGroup("int_list").addGroup("list").append("element", random.nextInt());        return group;    } else {        return NULL;    }}
public void parquet-mr_f683_0() throws IOException
{    ValueGenerator generator = new ValueGenerator();    try (ParquetWriter<Group> writer = ExampleParquetWriter.builder(BLACK_HOLE).withWriteMode(Mode.OVERWRITE).withType(SCHEMA).build()) {        for (int i = 0; i < RECORD_COUNT; ++i) {            writer.write(generator.nextValue());        }    }}
public void parquet-mr_f684_0(Path outFile, int nRows, boolean writeChecksums, CompressionCodecName compression) throws IOException
{    if (exists(configuration, outFile)) {        System.out.println("File already exists " + outFile);        return;    }    ParquetWriter<Group> writer = ExampleParquetWriter.builder(outFile).withConf(configuration).withWriteMode(ParquetFileWriter.Mode.OVERWRITE).withCompressionCodec(compression).withDictionaryEncoding(true).withType(SCHEMA).withPageWriteChecksumEnabled(writeChecksums).build();    GroupFactory groupFactory = new SimpleGroupFactory(SCHEMA);    Random rand = new Random(42);    for (int i = 0; i < nRows; i++) {        Group group = groupFactory.newGroup();        group.append("long_field", (long) i).append("binary_field", randomUUID().toString()).addGroup("group").append("int_field", rand.nextInt() % 100).append("int_field", rand.nextInt() % 100).append("int_field", rand.nextInt() % 100).append("int_field", rand.nextInt() % 100);        writer.write(group);    }    writer.close();}
public void parquet-mr_f685_0()
{    try {                        generateData(file_100K_CHECKSUMS_UNCOMPRESSED, 100 * ONE_K, true, UNCOMPRESSED);        generateData(file_100K_CHECKSUMS_GZIP, 100 * ONE_K, true, GZIP);        generateData(file_100K_CHECKSUMS_SNAPPY, 100 * ONE_K, true, SNAPPY);        generateData(file_1M_CHECKSUMS_UNCOMPRESSED, ONE_MILLION, true, UNCOMPRESSED);        generateData(file_1M_CHECKSUMS_GZIP, ONE_MILLION, true, GZIP);        generateData(file_1M_CHECKSUMS_SNAPPY, ONE_MILLION, true, SNAPPY);        generateData(file_10M_CHECKSUMS_UNCOMPRESSED, 10 * ONE_MILLION, true, UNCOMPRESSED);        generateData(file_10M_CHECKSUMS_GZIP, 10 * ONE_MILLION, true, GZIP);        generateData(file_10M_CHECKSUMS_SNAPPY, 10 * ONE_MILLION, true, SNAPPY);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public void parquet-mr_f686_0()
{    pageChecksumDataGenerator.generateAll();}
private void parquet-mr_f687_0(Path file, int nRows, boolean verifyChecksums, Blackhole blackhole) throws IOException
{    try (ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file).withConf(configuration).usePageChecksumVerification(verifyChecksums).build()) {        for (int i = 0; i < nRows; i++) {            Group group = reader.read();            blackhole.consume(group.getLong("long_field", 0));            blackhole.consume(group.getBinary("binary_field", 0));            Group subgroup = group.getGroup("group", 0);            blackhole.consume(subgroup.getInteger("int_field", 0));            blackhole.consume(subgroup.getInteger("int_field", 1));            blackhole.consume(subgroup.getInteger("int_field", 2));            blackhole.consume(subgroup.getInteger("int_field", 3));        }    }}
public void parquet-mr_f688_0(Blackhole blackhole) throws IOException
{    readFile(file_100K_CHECKSUMS_UNCOMPRESSED, 100 * ONE_K, false, blackhole);}
public void parquet-mr_f689_0(Blackhole blackhole) throws IOException
{    readFile(file_100K_CHECKSUMS_UNCOMPRESSED, 100 * ONE_K, true, blackhole);}
public void parquet-mr_f690_0(Blackhole blackhole) throws IOException
{    readFile(file_100K_CHECKSUMS_GZIP, 100 * ONE_K, false, blackhole);}
public void parquet-mr_f691_0(Blackhole blackhole) throws IOException
{    readFile(file_100K_CHECKSUMS_GZIP, 100 * ONE_K, true, blackhole);}
public void parquet-mr_f692_0(Blackhole blackhole) throws IOException
{    readFile(file_100K_CHECKSUMS_SNAPPY, 100 * ONE_K, false, blackhole);}
public void parquet-mr_f693_0(Blackhole blackhole) throws IOException
{    readFile(file_100K_CHECKSUMS_SNAPPY, 100 * ONE_K, true, blackhole);}
public void parquet-mr_f694_0(Blackhole blackhole) throws IOException
{    readFile(file_1M_CHECKSUMS_UNCOMPRESSED, ONE_MILLION, false, blackhole);}
public void parquet-mr_f695_0(Blackhole blackhole) throws IOException
{    readFile(file_1M_CHECKSUMS_UNCOMPRESSED, ONE_MILLION, true, blackhole);}
public void parquet-mr_f696_0(Blackhole blackhole) throws IOException
{    readFile(file_1M_CHECKSUMS_GZIP, ONE_MILLION, false, blackhole);}
public void parquet-mr_f697_0(Blackhole blackhole) throws IOException
{    readFile(file_1M_CHECKSUMS_GZIP, ONE_MILLION, true, blackhole);}
public void parquet-mr_f698_0(Blackhole blackhole) throws IOException
{    readFile(file_1M_CHECKSUMS_SNAPPY, ONE_MILLION, false, blackhole);}
public void parquet-mr_f699_0(Blackhole blackhole) throws IOException
{    readFile(file_1M_CHECKSUMS_SNAPPY, ONE_MILLION, true, blackhole);}
public void parquet-mr_f700_0(Blackhole blackhole) throws IOException
{    readFile(file_10M_CHECKSUMS_UNCOMPRESSED, 10 * ONE_MILLION, false, blackhole);}
public void parquet-mr_f701_0(Blackhole blackhole) throws IOException
{    readFile(file_10M_CHECKSUMS_UNCOMPRESSED, 10 * ONE_MILLION, true, blackhole);}
public void parquet-mr_f702_0(Blackhole blackhole) throws IOException
{    readFile(file_10M_CHECKSUMS_GZIP, 10 * ONE_MILLION, false, blackhole);}
public void parquet-mr_f703_0(Blackhole blackhole) throws IOException
{    readFile(file_10M_CHECKSUMS_GZIP, 10 * ONE_MILLION, true, blackhole);}
public void parquet-mr_f704_0(Blackhole blackhole) throws IOException
{    readFile(file_10M_CHECKSUMS_SNAPPY, 10 * ONE_MILLION, false, blackhole);}
public void parquet-mr_f705_0(Blackhole blackhole) throws IOException
{    readFile(file_10M_CHECKSUMS_SNAPPY, 10 * ONE_MILLION, true, blackhole);}
public void parquet-mr_f706_0()
{    pageChecksumDataGenerator.cleanup();}
public void parquet-mr_f707_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_100K_NOCHECKSUMS_UNCOMPRESSED, 100 * ONE_K, false, UNCOMPRESSED);}
public void parquet-mr_f708_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_100K_CHECKSUMS_UNCOMPRESSED, 100 * ONE_K, true, UNCOMPRESSED);}
public void parquet-mr_f709_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_100K_NOCHECKSUMS_GZIP, 100 * ONE_K, false, GZIP);}
public void parquet-mr_f710_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_100K_CHECKSUMS_GZIP, 100 * ONE_K, true, GZIP);}
public void parquet-mr_f711_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_100K_NOCHECKSUMS_SNAPPY, 100 * ONE_K, false, SNAPPY);}
public void parquet-mr_f712_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_100K_CHECKSUMS_SNAPPY, 100 * ONE_K, true, SNAPPY);}
public void parquet-mr_f713_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_1M_NOCHECKSUMS_UNCOMPRESSED, ONE_MILLION, false, UNCOMPRESSED);}
public void parquet-mr_f714_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_1M_CHECKSUMS_UNCOMPRESSED, ONE_MILLION, true, UNCOMPRESSED);}
public void parquet-mr_f715_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_1M_NOCHECKSUMS_GZIP, ONE_MILLION, false, GZIP);}
public void parquet-mr_f716_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_1M_CHECKSUMS_GZIP, ONE_MILLION, true, GZIP);}
public void parquet-mr_f717_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_1M_NOCHECKSUMS_SNAPPY, ONE_MILLION, false, SNAPPY);}
public void parquet-mr_f718_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_1M_CHECKSUMS_SNAPPY, ONE_MILLION, true, SNAPPY);}
public void parquet-mr_f719_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_10M_NOCHECKSUMS_UNCOMPRESSED, 10 * ONE_MILLION, false, UNCOMPRESSED);}
public void parquet-mr_f720_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_10M_CHECKSUMS_UNCOMPRESSED, 10 * ONE_MILLION, true, UNCOMPRESSED);}
public void parquet-mr_f721_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_10M_NOCHECKSUMS_GZIP, 10 * ONE_MILLION, false, GZIP);}
public void parquet-mr_f722_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_10M_CHECKSUMS_GZIP, 10 * ONE_MILLION, true, GZIP);}
public void parquet-mr_f723_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_10M_NOCHECKSUMS_SNAPPY, 10 * ONE_MILLION, false, SNAPPY);}
public void parquet-mr_f724_0() throws IOException
{    pageChecksumDataGenerator.generateData(file_10M_CHECKSUMS_SNAPPY, 10 * ONE_MILLION, true, SNAPPY);}
private void parquet-mr_f725_0(Path parquetFile, int nRows, Blackhole blackhole) throws IOException
{    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), parquetFile).withConf(configuration).build();    for (int i = 0; i < nRows; i++) {        Group group = reader.read();        blackhole.consume(group.getBinary("binary_field", 0));        blackhole.consume(group.getInteger("int32_field", 0));        blackhole.consume(group.getLong("int64_field", 0));        blackhole.consume(group.getBoolean("boolean_field", 0));        blackhole.consume(group.getFloat("float_field", 0));        blackhole.consume(group.getDouble("double_field", 0));        blackhole.consume(group.getBinary("flba_field", 0));        blackhole.consume(group.getInt96("int96_field", 0));    }    reader.close();}
public void parquet-mr_f726_0()
{    new DataGenerator().generateAll();}
public void parquet-mr_f727_0(Blackhole blackhole) throws IOException
{    read(file_1M, ONE_MILLION, blackhole);}
public void parquet-mr_f728_0(Blackhole blackhole) throws IOException
{    read(file_1M_BS256M_PS4M, ONE_MILLION, blackhole);}
public void parquet-mr_f729_0(Blackhole blackhole) throws IOException
{    read(file_1M_BS256M_PS8M, ONE_MILLION, blackhole);}
public void parquet-mr_f730_0(Blackhole blackhole) throws IOException
{    read(file_1M_BS512M_PS4M, ONE_MILLION, blackhole);}
public void parquet-mr_f731_0(Blackhole blackhole) throws IOException
{    read(file_1M_BS512M_PS8M, ONE_MILLION, blackhole);}
public void parquet-mr_f732_0(Blackhole blackhole) throws IOException
{    read(file_1M_SNAPPY, ONE_MILLION, blackhole);}
public void parquet-mr_f733_0(Blackhole blackhole) throws IOException
{    read(file_1M_GZIP, ONE_MILLION, blackhole);}
public void parquet-mr_f734_0()
{        dataGenerator.cleanup();}
public void parquet-mr_f735_0() throws IOException
{    dataGenerator.generateData(file_1M, configuration, PARQUET_2_0, BLOCK_SIZE_DEFAULT, PAGE_SIZE_DEFAULT, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);}
public void parquet-mr_f736_0() throws IOException
{    dataGenerator.generateData(file_1M_BS256M_PS4M, configuration, PARQUET_2_0, BLOCK_SIZE_256M, PAGE_SIZE_4M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);}
public void parquet-mr_f737_0() throws IOException
{    dataGenerator.generateData(file_1M_BS256M_PS8M, configuration, PARQUET_2_0, BLOCK_SIZE_256M, PAGE_SIZE_8M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);}
public void parquet-mr_f738_0() throws IOException
{    dataGenerator.generateData(file_1M_BS512M_PS4M, configuration, PARQUET_2_0, BLOCK_SIZE_512M, PAGE_SIZE_4M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);}
public void parquet-mr_f739_0() throws IOException
{    dataGenerator.generateData(file_1M_BS512M_PS8M, configuration, PARQUET_2_0, BLOCK_SIZE_512M, PAGE_SIZE_8M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);}
public void parquet-mr_f740_0() throws IOException
{    dataGenerator.generateData(file_1M_SNAPPY, configuration, PARQUET_2_0, BLOCK_SIZE_DEFAULT, PAGE_SIZE_DEFAULT, FIXED_LEN_BYTEARRAY_SIZE, SNAPPY, ONE_MILLION);}
public void parquet-mr_f741_0() throws IOException
{    dataGenerator.generateData(file_1M_GZIP, configuration, PARQUET_2_0, BLOCK_SIZE_DEFAULT, PAGE_SIZE_DEFAULT, FIXED_LEN_BYTEARRAY_SIZE, GZIP, ONE_MILLION);}
public void parquet-mr_f742_0(FlowProcess<JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    super.sourceConfInit(fp, tap, jobConf);    jobConf.setInputFormat(DeprecatedParquetInputFormat.class);    ParquetInputFormat.setReadSupportClass(jobConf, ThriftReadSupport.class);    ThriftReadSupport.setRecordConverterClass(jobConf, TBaseRecordConverter.class);}
public void parquet-mr_f743_0(FlowProcess<JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    if (this.config.getKlass() == null) {        throw new IllegalArgumentException("To use ParquetTBaseScheme as a sink, you must specify a thrift class in the constructor");    }    DeprecatedParquetOutputFormat.setAsOutputFormat(jobConf);    DeprecatedParquetOutputFormat.setWriteSupportClass(jobConf, TBaseWriteSupport.class);    TBaseWriteSupport.<T>setThriftClass(jobConf, this.config.getKlass());}
public void parquet-mr_f744_0(FlowProcess<JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    if (filterPredicate != null) {        ParquetInputFormat.setFilterPredicate(jobConf, filterPredicate);    }    jobConf.setInputFormat(DeprecatedParquetInputFormat.class);    ParquetInputFormat.setReadSupportClass(jobConf, TupleReadSupport.class);    TupleReadSupport.setRequestedFields(jobConf, getSourceFields());}
public Fields parquet-mr_f745_0(FlowProcess<JobConf> flowProcess, Tap tap)
{    MessageType schema = readSchema(flowProcess, tap);    SchemaIntersection intersection = new SchemaIntersection(schema, getSourceFields());    setSourceFields(intersection.getSourceFields());    return getSourceFields();}
private MessageType parquet-mr_f746_0(FlowProcess<JobConf> flowProcess, Tap tap)
{    try {        Hfs hfs;        if (tap instanceof CompositeTap)            hfs = (Hfs) ((CompositeTap) tap).getChildTaps().next();        else            hfs = (Hfs) tap;        List<Footer> footers = getFooters(flowProcess, hfs);        if (footers.isEmpty()) {            throw new TapException("Could not read Parquet metadata at " + hfs.getPath());        } else {            return footers.get(0).getParquetMetadata().getFileMetaData().getSchema();        }    } catch (IOException e) {        throw new TapException(e);    }}
private List<Footer> parquet-mr_f747_0(FlowProcess<JobConf> flowProcess, Hfs hfs) throws IOException
{    JobConf jobConf = flowProcess.getConfigCopy();    DeprecatedParquetInputFormat format = new DeprecatedParquetInputFormat();    format.addInputPath(jobConf, hfs.getPath());    return format.getFooters(jobConf);}
public boolean parquet-mr_f748_0(FlowProcess<JobConf> fp, SourceCall<Object[], RecordReader> sc) throws IOException
{    Container<Tuple> value = (Container<Tuple>) sc.getInput().createValue();    boolean hasNext = sc.getInput().next(null, value);    if (!hasNext) {        return false;    }        if (value == null) {        return true;    }    sc.getIncomingEntry().setTuple(value.get());    return true;}
public void parquet-mr_f749_0(FlowProcess<JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    DeprecatedParquetOutputFormat.setAsOutputFormat(jobConf);    jobConf.set(TupleWriteSupport.PARQUET_CASCADING_SCHEMA, parquetSchema);    ParquetOutputFormat.setWriteSupportClass(jobConf, TupleWriteSupport.class);}
public boolean parquet-mr_f750_0()
{    return parquetSchema != null;}
public void parquet-mr_f751_0(FlowProcess<JobConf> fp, SinkCall<Object[], OutputCollector> sink) throws IOException
{    TupleEntry tuple = sink.getOutgoingEntry();    OutputCollector outputCollector = sink.getOutput();    outputCollector.collect(null, tuple);}
public FilterPredicate parquet-mr_f752_0()
{    return filterPredicate;}
public String parquet-mr_f753_0()
{    return deprecatedProjectionString;}
public String parquet-mr_f754_0()
{    return strictProjectionString;}
public Class<T> parquet-mr_f755_0()
{    return klass;}
public Config<T> parquet-mr_f756_0(FilterPredicate f)
{    return new Config<T>(this.klass, checkNotNull(f, "filterPredicate"), this.deprecatedProjectionString, this.strictProjectionString);}
public Config<T> parquet-mr_f757_0(String p)
{    return new Config<T>(this.klass, this.filterPredicate, checkNotNull(p, "projectionString"), this.strictProjectionString);}
public Config<T> parquet-mr_f758_0(String p)
{    return new Config<T>(this.klass, this.filterPredicate, this.deprecatedProjectionString, checkNotNull(p, "projectionString"));}
public Config<T> parquet-mr_f759_0(Class<T> klass)
{    return new Config<T>(checkNotNull(klass, "recordClass"), this.filterPredicate, this.deprecatedProjectionString, this.strictProjectionString);}
private void parquet-mr_f760_0(JobConf jobConf)
{    if (this.config.deprecatedProjectionString != null) {        ThriftReadSupport.setProjectionPushdown(jobConf, this.config.deprecatedProjectionString);    }}
private void parquet-mr_f761_0(JobConf jobConf)
{    if (this.config.strictProjectionString != null) {        ThriftReadSupport.setStrictFieldProjectionFilter(jobConf, this.config.strictProjectionString);    }}
private void parquet-mr_f762_0(JobConf jobConf)
{    if (this.config.filterPredicate != null) {        ParquetInputFormat.setFilterPredicate(jobConf, this.config.filterPredicate);    }}
public void parquet-mr_f763_0(FlowProcess<JobConf> jobConfFlowProcess, Tap<JobConf, RecordReader, OutputCollector> jobConfRecordReaderOutputCollectorTap, final JobConf jobConf)
{    setPredicatePushdown(jobConf);    setProjectionPushdown(jobConf);    setStrictProjectionPushdown(jobConf);    setRecordClass(jobConf);}
private void parquet-mr_f764_0(JobConf jobConf)
{    if (config.klass != null) {        ParquetThriftInputFormat.setThriftClass(jobConf, config.klass);    }}
public boolean parquet-mr_f765_0(FlowProcess<JobConf> fp, SourceCall<Object[], RecordReader> sc) throws IOException
{    Container<T> value = (Container<T>) sc.getInput().createValue();    boolean hasNext = sc.getInput().next(null, value);    if (!hasNext) {        return false;    }        if (value == null) {        return true;    }    sc.getIncomingEntry().setTuple(new Tuple(value.get()));    return true;}
public void parquet-mr_f766_0(FlowProcess<JobConf> fp, SinkCall<Object[], OutputCollector> sc) throws IOException
{    TupleEntry tuple = sc.getOutgoingEntry();    if (tuple.size() != 1) {        throw new RuntimeException("ParquetValueScheme expects tuples with an arity of exactly 1, but found " + tuple.getFields());    }    T value = (T) tuple.getObject(0);    OutputCollector output = sc.getOutput();    output.collect(null, value);}
public void parquet-mr_f767_0() throws Exception
{    Path path = new Path(parquetOutputPath);    JobConf jobConf = new JobConf();    final FileSystem fs = path.getFileSystem(jobConf);    if (fs.exists(path))        fs.delete(path, true);    Scheme sourceScheme = new TextLine(new Fields("first", "last"));    Tap source = new Hfs(sourceScheme, txtInputPath);    Scheme sinkScheme = new ParquetTBaseScheme(Name.class);    Tap sink = new Hfs(sinkScheme, parquetOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new PackThriftFunction());    HadoopFlowConnector hadoopFlowConnector = new HadoopFlowConnector();    Flow flow = hadoopFlowConnector.connect("namecp", source, sink, assembly);    flow.complete();    assertTrue(fs.exists(new Path(parquetOutputPath)));    assertTrue(fs.exists(new Path(parquetOutputPath + "/_metadata")));    assertTrue(fs.exists(new Path(parquetOutputPath + "/_common_metadata")));}
public void parquet-mr_f768_0() throws Exception
{    doRead(new ParquetTBaseScheme(Name.class));}
public void parquet-mr_f769_0() throws Exception
{    doRead(new ParquetTBaseScheme());}
private void parquet-mr_f770_0(Scheme sourceScheme) throws Exception
{    createFileForRead();    Path path = new Path(txtOutputPath);    final FileSystem fs = path.getFileSystem(new Configuration());    if (fs.exists(path))        fs.delete(path, true);    Tap source = new Hfs(sourceScheme, parquetInputPath);    Scheme sinkScheme = new TextLine(new Fields("first", "last"));    Tap sink = new Hfs(sinkScheme, txtOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new UnpackThriftFunction());    Flow flow = new HadoopFlowConnector().connect("namecp", source, sink, assembly);    flow.complete();    String result = FileUtils.readFileToString(new File(txtOutputPath + "/part-00000"));    assertEquals("Alice\tPractice\nBob\tHope\nCharlie\tHorse\n", result);}
private void parquet-mr_f771_0() throws Exception
{    final Path fileToCreate = new Path(parquetInputPath + "/names.parquet");    final Configuration conf = new Configuration();    final FileSystem fs = fileToCreate.getFileSystem(conf);    if (fs.exists(fileToCreate))        fs.delete(fileToCreate, true);    TProtocolFactory protocolFactory = new TCompactProtocol.Factory();    TaskAttemptID taskId = new TaskAttemptID("local", 0, true, 0, 0);    ThriftToParquetFileWriter w = new ThriftToParquetFileWriter(fileToCreate, ContextUtil.newTaskAttemptContext(conf, taskId), protocolFactory, Name.class);    final ByteArrayOutputStream baos = new ByteArrayOutputStream();    final TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(baos));    Name n1 = new Name();    n1.setFirst_name("Alice");    n1.setLast_name("Practice");    Name n2 = new Name();    n2.setFirst_name("Bob");    n2.setLast_name("Hope");    Name n3 = new Name();    n3.setFirst_name("Charlie");    n3.setLast_name("Horse");    n1.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    baos.reset();    n2.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    baos.reset();    n3.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    w.close();}
public void parquet-mr_f772_0(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Name name = new Name();    name.setFirst_name(arguments.getString(0));    name.setLast_name(arguments.getString(1));    result.add(name);    functionCall.getOutputCollector().add(result);}
public void parquet-mr_f773_0(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Name name = (Name) arguments.get(0);    result.add(name.getFirst_name());    result.add(name.getLast_name());    functionCall.getOutputCollector().add(result);}
private Converter parquet-mr_f774_0(Type type, int i)
{    if (!type.isPrimitive()) {        throw new IllegalArgumentException("cascading can only build tuples from primitive types");    } else {        return new TuplePrimitiveConverter(this, i);    }}
public Converter parquet-mr_f775_0(int fieldIndex)
{    return converters[fieldIndex];}
public final void parquet-mr_f776_0()
{    currentTuple = Tuple.size(converters.length);}
public void parquet-mr_f777_0()
{}
public final Tuple parquet-mr_f778_0()
{    return currentTuple;}
public void parquet-mr_f779_0(Binary value)
{    parent.getCurrentTuple().setString(index, value.toStringUsingUTF8());}
public void parquet-mr_f780_0(boolean value)
{    parent.getCurrentTuple().setBoolean(index, value);}
public void parquet-mr_f781_0(double value)
{    parent.getCurrentTuple().setDouble(index, value);}
public void parquet-mr_f782_0(float value)
{    parent.getCurrentTuple().setFloat(index, value);}
public void parquet-mr_f783_0(int value)
{    parent.getCurrentTuple().setInteger(index, value);}
public void parquet-mr_f784_0(long value)
{    parent.getCurrentTuple().setLong(index, value);}
public Tuple parquet-mr_f785_0()
{    return root.getCurrentTuple();}
public GroupConverter parquet-mr_f786_0()
{    return root;}
public MessageType parquet-mr_f787_0()
{    return requestedSchema;}
public Fields parquet-mr_f788_0()
{    return sourceFields;}
protected static Fields parquet-mr_f789_0(Configuration configuration)
{    String fieldsString = configuration.get(PARQUET_CASCADING_REQUESTED_FIELDS);    if (fieldsString == null)        return Fields.ALL;    String[] parts = StringUtils.split(fieldsString, ":");    if (parts.length == 0)        return Fields.ALL;    else        return new Fields(parts);}
protected static void parquet-mr_f790_0(JobConf configuration, Fields fields)
{    String fieldsString = StringUtils.join(fields.iterator(), ":");    configuration.set(PARQUET_CASCADING_REQUESTED_FIELDS, fieldsString);}
public ReadContext parquet-mr_f791_0(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema)
{    Fields requestedFields = getRequestedFields(configuration);    if (requestedFields == null) {        return new ReadContext(fileSchema);    } else {        SchemaIntersection intersection = new SchemaIntersection(fileSchema, requestedFields);        return new ReadContext(intersection.getRequestedSchema());    }}
public RecordMaterializer<Tuple> parquet-mr_f792_0(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema, ReadContext readContext)
{    MessageType requestedSchema = readContext.getRequestedSchema();    return new TupleRecordMaterializer(requestedSchema);}
public String parquet-mr_f793_0()
{    return "cascading";}
public WriteContext parquet-mr_f794_0(Configuration configuration)
{    String schema = configuration.get(PARQUET_CASCADING_SCHEMA);    rootSchema = MessageTypeParser.parseMessageType(schema);    return new WriteContext(rootSchema, new HashMap<String, String>());}
public void parquet-mr_f795_0(RecordConsumer recordConsumer)
{    this.recordConsumer = recordConsumer;}
public void parquet-mr_f796_0(TupleEntry record)
{    recordConsumer.startMessage();    final List<Type> fields = rootSchema.getFields();    for (int i = 0; i < fields.size(); i++) {        Type field = fields.get(i);        if (record == null || record.getObject(field.getName()) == null) {            continue;        }        recordConsumer.startField(field.getName(), i);        if (field.isPrimitive()) {            writePrimitive(record, field.asPrimitiveType());        } else {            throw new UnsupportedOperationException("Complex type not implemented");        }        recordConsumer.endField(field.getName(), i);    }    recordConsumer.endMessage();}
private void parquet-mr_f797_0(TupleEntry record, PrimitiveType field)
{    switch(field.getPrimitiveTypeName()) {        case BINARY:            recordConsumer.addBinary(Binary.fromString(record.getString(field.getName())));            break;        case BOOLEAN:            recordConsumer.addBoolean(record.getBoolean(field.getName()));            break;        case INT32:            recordConsumer.addInteger(record.getInteger(field.getName()));            break;        case INT64:            recordConsumer.addLong(record.getLong(field.getName()));            break;        case DOUBLE:            recordConsumer.addDouble(record.getDouble(field.getName()));            break;        case FLOAT:            recordConsumer.addFloat(record.getFloat(field.getName()));            break;        case FIXED_LEN_BYTE_ARRAY:            throw new UnsupportedOperationException("Fixed len byte array type not implemented");        case INT96:            throw new UnsupportedOperationException("Int96 type not implemented");        default:            throw new UnsupportedOperationException(field.getName() + " type not implemented");    }}
public void parquet-mr_f798_0() throws Exception
{    String sourceFolder = parquetInputPath;    testReadWrite(sourceFolder);    String sourceGlobPattern = parquetInputPath + "/*";    testReadWrite(sourceGlobPattern);    String multiLevelGlobPattern = "target/test/ParquetTupleIn/**/*";    testReadWrite(multiLevelGlobPattern);}
public void parquet-mr_f799_0() throws Exception
{    createFileForRead();    Path path = new Path(txtOutputPath);    final FileSystem fs = path.getFileSystem(new Configuration());    if (fs.exists(path))        fs.delete(path, true);    Scheme sourceScheme = new ParquetTupleScheme(new Fields("last_name"));    Tap source = new Hfs(sourceScheme, parquetInputPath);    Scheme sinkScheme = new TextLine(new Fields("last_name"));    Tap sink = new Hfs(sinkScheme, txtOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new ProjectedTupleFunction());    Flow flow = new HadoopFlowConnector().connect("namecp", source, sink, assembly);    flow.complete();    String result = FileUtils.readFileToString(new File(txtOutputPath + "/part-00000"));    assertEquals("Practice\nHope\nHorse\n", result);}
public void parquet-mr_f800_0(String inputPath) throws Exception
{    createFileForRead();    Path path = new Path(txtOutputPath);    final FileSystem fs = path.getFileSystem(new Configuration());    if (fs.exists(path))        fs.delete(path, true);    Scheme sourceScheme = new ParquetTupleScheme(new Fields("first_name", "last_name"));    Tap source = new Hfs(sourceScheme, inputPath);    Scheme sinkScheme = new TextLine(new Fields("first", "last"));    Tap sink = new Hfs(sinkScheme, txtOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new UnpackTupleFunction());    Flow flow = new HadoopFlowConnector().connect("namecp", source, sink, assembly);    flow.complete();    String result = FileUtils.readFileToString(new File(txtOutputPath + "/part-00000"));    assertEquals("Alice\tPractice\nBob\tHope\nCharlie\tHorse\n", result);}
private void parquet-mr_f801_0() throws Exception
{    final Path fileToCreate = new Path(parquetInputPath + "/names.parquet");    final Configuration conf = new Configuration();    final FileSystem fs = fileToCreate.getFileSystem(conf);    if (fs.exists(fileToCreate))        fs.delete(fileToCreate, true);    TProtocolFactory protocolFactory = new TCompactProtocol.Factory();    TaskAttemptID taskId = new TaskAttemptID("local", 0, true, 0, 0);    ThriftToParquetFileWriter w = new ThriftToParquetFileWriter(fileToCreate, ContextUtil.newTaskAttemptContext(conf, taskId), protocolFactory, Name.class);    final ByteArrayOutputStream baos = new ByteArrayOutputStream();    final TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(baos));    Name n1 = new Name();    n1.setFirst_name("Alice");    n1.setLast_name("Practice");    Name n2 = new Name();    n2.setFirst_name("Bob");    n2.setLast_name("Hope");    Name n3 = new Name();    n3.setFirst_name("Charlie");    n3.setLast_name("Horse");    n1.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    baos.reset();    n2.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    baos.reset();    n3.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    w.close();}
public void parquet-mr_f802_0(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Tuple name = new Tuple();    name.addString(arguments.getString(0));    name.addString(arguments.getString(1));    result.add(name);    functionCall.getOutputCollector().add(result);}
public void parquet-mr_f803_0(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Tuple name = new Tuple();    name.addString(arguments.getString(0));        result.add(name);    functionCall.getOutputCollector().add(result);}
public void parquet-mr_f804_0(FlowProcess<? extends JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    super.sourceConfInit(fp, tap, jobConf);    jobConf.setInputFormat(DeprecatedParquetInputFormat.class);    ParquetInputFormat.setReadSupportClass(jobConf, ThriftReadSupport.class);    ThriftReadSupport.setRecordConverterClass(jobConf, TBaseRecordConverter.class);}
public void parquet-mr_f805_0(FlowProcess<? extends JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    if (this.config.getKlass() == null) {        throw new IllegalArgumentException("To use ParquetTBaseScheme as a sink, you must specify a thrift class in the constructor");    }    DeprecatedParquetOutputFormat.setAsOutputFormat(jobConf);    DeprecatedParquetOutputFormat.setWriteSupportClass(jobConf, TBaseWriteSupport.class);    TBaseWriteSupport.<T>setThriftClass(jobConf, this.config.getKlass());}
public void parquet-mr_f806_0(FlowProcess<? extends JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    if (filterPredicate != null) {        ParquetInputFormat.setFilterPredicate(jobConf, filterPredicate);    }    jobConf.setInputFormat(DeprecatedParquetInputFormat.class);    ParquetInputFormat.setReadSupportClass(jobConf, TupleReadSupport.class);    TupleReadSupport.setRequestedFields(jobConf, getSourceFields());}
public Fields parquet-mr_f807_0(FlowProcess<? extends JobConf> flowProcess, Tap tap)
{    MessageType schema = readSchema(flowProcess, tap);    SchemaIntersection intersection = new SchemaIntersection(schema, getSourceFields());    setSourceFields(intersection.getSourceFields());    return getSourceFields();}
private MessageType parquet-mr_f808_0(FlowProcess<? extends JobConf> flowProcess, Tap tap)
{    try {        Hfs hfs;        if (tap instanceof CompositeTap)            hfs = (Hfs) ((CompositeTap) tap).getChildTaps().next();        else            hfs = (Hfs) tap;        List<Footer> footers = getFooters(flowProcess, hfs);        if (footers.isEmpty()) {            throw new TapException("Could not read Parquet metadata at " + hfs.getPath());        } else {            return footers.get(0).getParquetMetadata().getFileMetaData().getSchema();        }    } catch (IOException e) {        throw new TapException(e);    }}
private List<Footer> parquet-mr_f809_0(FlowProcess<? extends JobConf> flowProcess, Hfs hfs) throws IOException
{    JobConf jobConf = flowProcess.getConfigCopy();    DeprecatedParquetInputFormat format = new DeprecatedParquetInputFormat();    format.addInputPath(jobConf, hfs.getPath());    return format.getFooters(jobConf);}
public boolean parquet-mr_f810_0(FlowProcess<? extends JobConf> fp, SourceCall<Object[], RecordReader> sc) throws IOException
{    Container<Tuple> value = (Container<Tuple>) sc.getInput().createValue();    boolean hasNext = sc.getInput().next(null, value);    if (!hasNext) {        return false;    }        if (value == null) {        return true;    }    sc.getIncomingEntry().setTuple(value.get());    return true;}
public void parquet-mr_f811_0(FlowProcess<? extends JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    DeprecatedParquetOutputFormat.setAsOutputFormat(jobConf);    jobConf.set(TupleWriteSupport.PARQUET_CASCADING_SCHEMA, parquetSchema);    ParquetOutputFormat.setWriteSupportClass(jobConf, TupleWriteSupport.class);}
public boolean parquet-mr_f812_0()
{    return parquetSchema != null;}
public void parquet-mr_f813_0(FlowProcess<? extends JobConf> fp, SinkCall<Object[], OutputCollector> sink) throws IOException
{    TupleEntry tuple = sink.getOutgoingEntry();    OutputCollector outputCollector = sink.getOutput();    outputCollector.collect(null, tuple);}
public FilterPredicate parquet-mr_f814_0()
{    return filterPredicate;}
public String parquet-mr_f815_0()
{    return deprecatedProjectionString;}
public String parquet-mr_f816_0()
{    return strictProjectionString;}
public Class<T> parquet-mr_f817_0()
{    return klass;}
public Config<T> parquet-mr_f818_0(FilterPredicate f)
{    return new Config<T>(this.klass, checkNotNull(f, "filterPredicate"), this.deprecatedProjectionString, this.strictProjectionString);}
public Config<T> parquet-mr_f819_0(String p)
{    return new Config<T>(this.klass, this.filterPredicate, checkNotNull(p, "projectionString"), this.strictProjectionString);}
public Config<T> parquet-mr_f820_0(String p)
{    return new Config<T>(this.klass, this.filterPredicate, this.deprecatedProjectionString, checkNotNull(p, "projectionString"));}
public Config<T> parquet-mr_f821_0(Class<T> klass)
{    return new Config<T>(checkNotNull(klass, "recordClass"), this.filterPredicate, this.deprecatedProjectionString, this.strictProjectionString);}
private void parquet-mr_f822_0(JobConf jobConf)
{    if (this.config.deprecatedProjectionString != null) {        ThriftReadSupport.setProjectionPushdown(jobConf, this.config.deprecatedProjectionString);    }}
private void parquet-mr_f823_0(JobConf jobConf)
{    if (this.config.strictProjectionString != null) {        ThriftReadSupport.setStrictFieldProjectionFilter(jobConf, this.config.strictProjectionString);    }}
private void parquet-mr_f824_0(JobConf jobConf)
{    if (this.config.filterPredicate != null) {        ParquetInputFormat.setFilterPredicate(jobConf, this.config.filterPredicate);    }}
public void parquet-mr_f825_0(FlowProcess<? extends JobConf> jobConfFlowProcess, Tap<JobConf, RecordReader, OutputCollector> jobConfRecordReaderOutputCollectorTap, JobConf jobConf)
{    setPredicatePushdown(jobConf);    setProjectionPushdown(jobConf);    setStrictProjectionPushdown(jobConf);    setRecordClass(jobConf);}
private void parquet-mr_f826_0(JobConf jobConf)
{    if (config.klass != null) {        ParquetThriftInputFormat.setThriftClass(jobConf, config.klass);    }}
public boolean parquet-mr_f827_0(FlowProcess<? extends JobConf> fp, SourceCall<Object[], RecordReader> sc) throws IOException
{    Container<T> value = (Container<T>) sc.getInput().createValue();    boolean hasNext = sc.getInput().next(null, value);    if (!hasNext) {        return false;    }        if (value == null) {        return true;    }    sc.getIncomingEntry().setTuple(new Tuple(value.get()));    return true;}
public void parquet-mr_f828_0(FlowProcess<? extends JobConf> fp, SinkCall<Object[], OutputCollector> sc) throws IOException
{    TupleEntry tuple = sc.getOutgoingEntry();    if (tuple.size() != 1) {        throw new RuntimeException("ParquetValueScheme expects tuples with an arity of exactly 1, but found " + tuple.getFields());    }    T value = (T) tuple.getObject(0);    OutputCollector output = sc.getOutput();    output.collect(null, value);}
public void parquet-mr_f829_0() throws Exception
{    Path path = new Path(parquetOutputPath);    JobConf jobConf = new JobConf();    final FileSystem fs = path.getFileSystem(jobConf);    if (fs.exists(path))        fs.delete(path, true);    Scheme sourceScheme = new TextLine(new Fields("first", "last"));    Tap source = new Hfs(sourceScheme, txtInputPath);    Scheme sinkScheme = new ParquetTBaseScheme(Name.class);    Tap sink = new Hfs(sinkScheme, parquetOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new PackThriftFunction());    HadoopFlowConnector hadoopFlowConnector = new HadoopFlowConnector();    Flow flow = hadoopFlowConnector.connect("namecp", source, sink, assembly);    flow.complete();    assertTrue(fs.exists(new Path(parquetOutputPath)));    assertTrue(fs.exists(new Path(parquetOutputPath + "/_metadata")));    assertTrue(fs.exists(new Path(parquetOutputPath + "/_common_metadata")));}
public void parquet-mr_f830_0() throws Exception
{    doRead(new ParquetTBaseScheme(Name.class));}
public void parquet-mr_f831_0() throws Exception
{    doRead(new ParquetTBaseScheme());}
private void parquet-mr_f832_0(Scheme sourceScheme) throws Exception
{    createFileForRead();    Path path = new Path(txtOutputPath);    final FileSystem fs = path.getFileSystem(new Configuration());    if (fs.exists(path))        fs.delete(path, true);    Tap source = new Hfs(sourceScheme, parquetInputPath);    Scheme sinkScheme = new TextLine(new Fields("first", "last"));    Tap sink = new Hfs(sinkScheme, txtOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new UnpackThriftFunction());    Flow flow = new HadoopFlowConnector().connect("namecp", source, sink, assembly);    flow.complete();    String result = FileUtils.readFileToString(new File(txtOutputPath + "/part-00000"));    assertEquals("Alice\tPractice\nBob\tHope\nCharlie\tHorse\n", result);}
private void parquet-mr_f833_0() throws Exception
{    final Path fileToCreate = new Path(parquetInputPath + "/names.parquet");    final Configuration conf = new Configuration();    final FileSystem fs = fileToCreate.getFileSystem(conf);    if (fs.exists(fileToCreate))        fs.delete(fileToCreate, true);    TProtocolFactory protocolFactory = new TCompactProtocol.Factory();    TaskAttemptID taskId = new TaskAttemptID("local", 0, true, 0, 0);    ThriftToParquetFileWriter w = new ThriftToParquetFileWriter(fileToCreate, ContextUtil.newTaskAttemptContext(conf, taskId), protocolFactory, Name.class);    final ByteArrayOutputStream baos = new ByteArrayOutputStream();    final TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(baos));    Name n1 = new Name();    n1.setFirst_name("Alice");    n1.setLast_name("Practice");    Name n2 = new Name();    n2.setFirst_name("Bob");    n2.setLast_name("Hope");    Name n3 = new Name();    n3.setFirst_name("Charlie");    n3.setLast_name("Horse");    n1.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    baos.reset();    n2.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    baos.reset();    n3.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    w.close();}
public void parquet-mr_f834_0(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Name name = new Name();    name.setFirst_name(arguments.getString(0));    name.setLast_name(arguments.getString(1));    result.add(name);    functionCall.getOutputCollector().add(result);}
public void parquet-mr_f835_0(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Name name = (Name) arguments.getObject(0);    result.add(name.getFirst_name());    result.add(name.getLast_name());    functionCall.getOutputCollector().add(result);}
public FileSystem parquet-mr_f836_0() throws IOException
{    if (localFS == null) {        this.localFS = FileSystem.getLocal(getConf());    }    return localFS;}
public void parquet-mr_f837_0(String content, Logger console, String filename) throws IOException
{    if (filename == null || "-".equals(filename)) {        console.info(content);    } else {        FSDataOutputStream outgoing = create(filename);        try {            outgoing.write(content.getBytes(StandardCharsets.UTF_8));        } finally {            outgoing.close();        }    }}
public FSDataOutputStream parquet-mr_f838_0(String filename) throws IOException
{    return create(filename, true);}
public FSDataOutputStream parquet-mr_f839_0(String filename) throws IOException
{    return create(filename, false);}
private FSDataOutputStream parquet-mr_f840_0(String filename, boolean noChecksum) throws IOException
{    Path filePath = qualifiedPath(filename);        FileSystem fs = filePath.getFileSystem(getConf());    if (noChecksum && fs instanceof ChecksumFileSystem) {        fs = ((ChecksumFileSystem) fs).getRawFileSystem();    }    return fs.create(filePath, true);}
public Path parquet-mr_f841_0(String filename) throws IOException
{    Path cwd = defaultFS().makeQualified(new Path("."));    return new Path(filename).makeQualified(defaultFS().getUri(), cwd);}
public URI parquet-mr_f842_0(String filename) throws IOException
{    try {        URI fileURI = new URI(filename);        if (RESOURCE_URI_SCHEME.equals(fileURI.getScheme())) {            return fileURI;        }    } catch (URISyntaxException ignore) {    }    return qualifiedPath(filename).toUri();}
public InputStream parquet-mr_f843_0(String filename) throws IOException
{    if (STDIN_AS_SOURCE.equals(filename)) {        return System.in;    }    URI uri = qualifiedURI(filename);    if (RESOURCE_URI_SCHEME.equals(uri.getScheme())) {        return Resources.getResource(uri.getRawSchemeSpecificPart()).openStream();    } else {        Path filePath = new Path(uri);                FileSystem fs = filePath.getFileSystem(getConf());        return fs.open(filePath);    }}
public SeekableInput parquet-mr_f844_0(String filename) throws IOException
{    Path path = qualifiedPath(filename);        FileSystem fs = path.getFileSystem(getConf());    return new SeekableFSDataInputStream(fs, path);}
public void parquet-mr_f845_0(Configuration conf)
{    this.conf = conf;    HadoopFileSystemURLStreamHandler.setDefaultConf(conf);}
public Configuration parquet-mr_f846_0()
{        return null != conf ? conf : new Configuration();}
protected static ClassLoader parquet-mr_f847_0(List<String> jars, List<String> paths) throws MalformedURLException
{    return AccessController.doPrivileged(new GetClassLoader(urls(jars, paths)));}
protected static ClassLoader parquet-mr_f848_0(List<String> jars) throws MalformedURLException
{    return AccessController.doPrivileged(new GetClassLoader(urls(jars, null)));}
protected static ClassLoader parquet-mr_f849_0(List<String> paths) throws MalformedURLException
{    return AccessController.doPrivileged(new GetClassLoader(urls(null, paths)));}
private static List<URL> parquet-mr_f850_0(List<String> jars, List<String> dirs) throws MalformedURLException
{        final List<URL> urls = Lists.newArrayList();    if (dirs != null) {        for (String lib : dirs) {                        File path = lib.endsWith("/") ? new File(lib) : new File(lib + "/");            Preconditions.checkArgument(path.exists(), "Lib directory does not exist: " + lib);            Preconditions.checkArgument(path.isDirectory(), "Not a directory: " + lib);            Preconditions.checkArgument(path.canRead() && path.canExecute(), "Insufficient permissions to access lib directory: " + lib);            urls.add(path.toURI().toURL());        }    }    if (jars != null) {        for (String jar : jars) {            File path = new File(jar);            Preconditions.checkArgument(path.exists(), "Jar files does not exist: " + jar);            Preconditions.checkArgument(path.isFile(), "Not a file: " + jar);            Preconditions.checkArgument(path.canRead(), "Cannot read jar file: " + jar);            urls.add(path.toURI().toURL());        }    }    return urls;}
protected Iterable<D> parquet-mr_f851_0(final String source, Schema projection) throws IOException
{    Formats.Format format = Formats.detectFormat(open(source));    switch(format) {        case PARQUET:            Configuration conf = new Configuration(getConf());                        AvroReadSupport.setRequestedProjection(conf, projection);            AvroReadSupport.setAvroReadSchema(conf, projection);            final ParquetReader<D> parquet = AvroParquetReader.<D>builder(qualifiedPath(source)).disableCompatibility().withDataModel(GenericData.get()).withConf(conf).build();            return new Iterable<D>() {                @Override                public Iterator<D> iterator() {                    return new Iterator<D>() {                        private boolean hasNext = false;                        private D next = advance();                        @Override                        public boolean hasNext() {                            return hasNext;                        }                        @Override                        public D next() {                            if (!hasNext) {                                throw new NoSuchElementException();                            }                            D toReturn = next;                            this.next = advance();                            return toReturn;                        }                        private D advance() {                            try {                                D next = parquet.read();                                this.hasNext = (next != null);                                return next;                            } catch (IOException e) {                                throw new RuntimeException("Failed while reading Parquet file: " + source, e);                            }                        }                        @Override                        public void remove() {                            throw new UnsupportedOperationException("Remove is not supported");                        }                    };                }            };        case AVRO:            Iterable<D> avroReader = (Iterable<D>) DataFileReader.openReader(openSeekable(source), new GenericDatumReader<>(projection));            return avroReader;        default:            if (source.endsWith("json")) {                return new AvroJsonReader<>(open(source), projection);            } else {                Preconditions.checkArgument(projection == null, "Cannot select columns from text files");                Iterable text = CharStreams.readLines(new InputStreamReader(open(source)));                return text;            }    }}
public Iterator<D> parquet-mr_f852_0()
{    return new Iterator<D>() {        private boolean hasNext = false;        private D next = advance();        @Override        public boolean hasNext() {            return hasNext;        }        @Override        public D next() {            if (!hasNext) {                throw new NoSuchElementException();            }            D toReturn = next;            this.next = advance();            return toReturn;        }        private D advance() {            try {                D next = parquet.read();                this.hasNext = (next != null);                return next;            } catch (IOException e) {                throw new RuntimeException("Failed while reading Parquet file: " + source, e);            }        }        @Override        public void remove() {            throw new UnsupportedOperationException("Remove is not supported");        }    };}
public boolean parquet-mr_f853_0()
{    return hasNext;}
public D parquet-mr_f854_0()
{    if (!hasNext) {        throw new NoSuchElementException();    }    D toReturn = next;    this.next = advance();    return toReturn;}
private D parquet-mr_f855_0()
{    try {        D next = parquet.read();        this.hasNext = (next != null);        return next;    } catch (IOException e) {        throw new RuntimeException("Failed while reading Parquet file: " + source, e);    }}
public void parquet-mr_f856_0()
{    throw new UnsupportedOperationException("Remove is not supported");}
protected Schema parquet-mr_f857_0(String source) throws IOException
{    Formats.Format format;    try (SeekableInput in = openSeekable(source)) {        format = Formats.detectFormat((InputStream) in);        in.seek(0);        switch(format) {            case PARQUET:                return Schemas.fromParquet(getConf(), qualifiedURI(source));            case AVRO:                return Schemas.fromAvro(open(source));            case TEXT:                if (source.endsWith("avsc")) {                    return Schemas.fromAvsc(open(source));                } else if (source.endsWith("json")) {                    return Schemas.fromJSON("json", open(source));                }            default:        }        throw new IllegalArgumentException(String.format("Could not determine file format of %s.", source));    }}
public int parquet-mr_f858_0() throws IOException
{    Preconditions.checkArgument(sourceFiles != null && !sourceFiles.isEmpty(), "Missing file name");    Preconditions.checkArgument(sourceFiles.size() == 1, "Only one file can be given");    final String source = sourceFiles.get(0);    Schema schema = getAvroSchema(source);    Schema projection = Expressions.filterSchema(schema, columns);    Iterable<Object> reader = openDataFile(source, projection);    boolean threw = true;    long count = 0;    try {        for (Object record : reader) {            if (numRecords > 0 && count >= numRecords) {                break;            }            if (columns == null || columns.size() != 1) {                console.info(String.valueOf(record));            } else {                console.info(String.valueOf(select(projection, record, columns.get(0))));            }            count += 1;        }        threw = false;    } catch (RuntimeException e) {        throw new RuntimeException("Failed on record " + count, e);    } finally {        if (reader instanceof Closeable) {            Closeables.close((Closeable) reader, threw);        }    }    return 0;}
public List<String> parquet-mr_f859_0()
{    return Lists.newArrayList("# Show the first 10 records in file \"data.avro\":", "data.avro", "# Show the first 50 records in file \"data.parquet\":", "data.parquet -n 50");}
public int parquet-mr_f860_0() throws IOException
{    boolean badFiles = false;    for (String file : files) {        String problem = check(file);        if (problem != null) {            badFiles = true;            console.info("{} has corrupt stats: {}", file, problem);        } else {            console.info("{} has no corrupt stats", file);        }    }    return badFiles ? 1 : 0;}
private String parquet-mr_f861_0(String file) throws IOException
{    Path path = qualifiedPath(file);    ParquetMetadata footer = ParquetFileReader.readFooter(getConf(), path, ParquetMetadataConverter.NO_FILTER);    FileMetaData meta = footer.getFileMetaData();    String createdBy = meta.getCreatedBy();    if (CorruptStatistics.shouldIgnoreStatistics(createdBy, BINARY)) {                FileMetaData fakeMeta = new FileMetaData(meta.getSchema(), meta.getKeyValueMetaData(), Version.FULL_VERSION);                List<ColumnDescriptor> columns = Lists.newArrayList();        Iterables.addAll(columns, Iterables.filter(meta.getSchema().getColumns(), new Predicate<ColumnDescriptor>() {            @Override            public boolean apply(@Nullable ColumnDescriptor input) {                return input != null && input.getType() == BINARY;            }        }));                ParquetFileReader reader = new ParquetFileReader(getConf(), fakeMeta, path, footer.getBlocks(), columns);        try {            PageStatsValidator validator = new PageStatsValidator();            for (PageReadStore pages = reader.readNextRowGroup(); pages != null; pages = reader.readNextRowGroup()) {                validator.validate(columns, pages);            }        } catch (BadStatsException e) {            return e.getMessage();        }    }    return null;}
public boolean parquet-mr_f862_0(@Nullable ColumnDescriptor input)
{    return input != null && input.getType() == BINARY;}
public List<String> parquet-mr_f863_0()
{    return Arrays.asList("# Check file1.parquet for corrupt page and column stats", "file1.parquet");}
public DictionaryPage parquet-mr_f864_0()
{    return dict;}
public long parquet-mr_f865_0()
{    return data.getValueCount();}
public DataPage parquet-mr_f866_0()
{    return data;}
private static Statistics<T> parquet-mr_f867_0(DataPage page)
{    return page.accept(new DataPage.Visitor<Statistics<T>>() {        @Override        @SuppressWarnings("unchecked")        public Statistics<T> visit(DataPageV1 dataPageV1) {            return (Statistics<T>) dataPageV1.getStatistics();        }        @Override        @SuppressWarnings("unchecked")        public Statistics<T> visit(DataPageV2 dataPageV2) {            return (Statistics<T>) dataPageV2.getStatistics();        }    });}
public Statistics<T> parquet-mr_f868_0(DataPageV1 dataPageV1)
{    return (Statistics<T>) dataPageV1.getStatistics();}
public Statistics<T> parquet-mr_f869_0(DataPageV2 dataPageV2)
{    return (Statistics<T>) dataPageV2.getStatistics();}
public void parquet-mr_f870_0(T value)
{    if (hasNonNull) {        if (comparator.compare(min, value) > 0) {            throw new BadStatsException("Min should be <= all values.");        }        if (comparator.compare(max, value) < 0) {            throw new BadStatsException("Max should be >= all values.");        }    }}
private PrimitiveConverter parquet-mr_f871_0(final DataPage page, PrimitiveTypeName type)
{    return type.convert(new PrimitiveTypeNameConverter<PrimitiveConverter, RuntimeException>() {        @Override        public PrimitiveConverter convertFLOAT(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Float> validator = new StatsValidator<Float>(page);            return new PrimitiveConverter() {                @Override                public void addFloat(float value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertDOUBLE(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Double> validator = new StatsValidator<Double>(page);            return new PrimitiveConverter() {                @Override                public void addDouble(double value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertINT32(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Integer> validator = new StatsValidator<Integer>(page);            return new PrimitiveConverter() {                @Override                public void addInt(int value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertINT64(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Long> validator = new StatsValidator<Long>(page);            return new PrimitiveConverter() {                @Override                public void addLong(long value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertBOOLEAN(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Boolean> validator = new StatsValidator<Boolean>(page);            return new PrimitiveConverter() {                @Override                public void addBoolean(boolean value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertINT96(PrimitiveTypeName primitiveTypeName) {            return convertBINARY(primitiveTypeName);        }        @Override        public PrimitiveConverter convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) {            return convertBINARY(primitiveTypeName);        }        @Override        public PrimitiveConverter convertBINARY(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Binary> validator = new StatsValidator<Binary>(page);            return new PrimitiveConverter() {                @Override                public void addBinary(Binary value) {                    validator.validate(value);                }            };        }    });}
public PrimitiveConverter parquet-mr_f872_0(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Float> validator = new StatsValidator<Float>(page);    return new PrimitiveConverter() {        @Override        public void addFloat(float value) {            validator.validate(value);        }    };}
public void parquet-mr_f873_0(float value)
{    validator.validate(value);}
public PrimitiveConverter parquet-mr_f874_0(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Double> validator = new StatsValidator<Double>(page);    return new PrimitiveConverter() {        @Override        public void addDouble(double value) {            validator.validate(value);        }    };}
public void parquet-mr_f875_0(double value)
{    validator.validate(value);}
public PrimitiveConverter parquet-mr_f876_0(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Integer> validator = new StatsValidator<Integer>(page);    return new PrimitiveConverter() {        @Override        public void addInt(int value) {            validator.validate(value);        }    };}
public void parquet-mr_f877_0(int value)
{    validator.validate(value);}
public PrimitiveConverter parquet-mr_f878_0(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Long> validator = new StatsValidator<Long>(page);    return new PrimitiveConverter() {        @Override        public void addLong(long value) {            validator.validate(value);        }    };}
public void parquet-mr_f879_0(long value)
{    validator.validate(value);}
public PrimitiveConverter parquet-mr_f880_0(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Boolean> validator = new StatsValidator<Boolean>(page);    return new PrimitiveConverter() {        @Override        public void addBoolean(boolean value) {            validator.validate(value);        }    };}
public void parquet-mr_f881_0(boolean value)
{    validator.validate(value);}
public PrimitiveConverter parquet-mr_f882_0(PrimitiveTypeName primitiveTypeName)
{    return convertBINARY(primitiveTypeName);}
public PrimitiveConverter parquet-mr_f883_0(PrimitiveTypeName primitiveTypeName)
{    return convertBINARY(primitiveTypeName);}
public PrimitiveConverter parquet-mr_f884_0(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Binary> validator = new StatsValidator<Binary>(page);    return new PrimitiveConverter() {        @Override        public void addBinary(Binary value) {            validator.validate(value);        }    };}
public void parquet-mr_f885_0(Binary value)
{    validator.validate(value);}
public void parquet-mr_f886_0(List<ColumnDescriptor> columns, PageReadStore store)
{    for (ColumnDescriptor desc : columns) {        PageReader reader = store.getPageReader(desc);        DictionaryPage dict = reader.readDictionaryPage();        DictionaryPage reusableDict = null;        if (dict != null) {            try {                reusableDict = new DictionaryPage(BytesInput.from(dict.getBytes().toByteArray()), dict.getDictionarySize(), dict.getEncoding());            } catch (IOException e) {                throw new ParquetDecodingException("Cannot read dictionary", e);            }        }        DataPage page;        while ((page = reader.readPage()) != null) {            validateStatsForPage(page, reusableDict, desc);        }    }}
private void parquet-mr_f887_0(DataPage page, DictionaryPage dict, ColumnDescriptor desc)
{    SingletonPageReader reader = new SingletonPageReader(dict, page);    PrimitiveConverter converter = getValidatingConverter(page, desc.getType());    Statistics stats = getStatisticsFromPageHeader(page);    long numNulls = 0;    ColumnReader column = COL_READER_CTOR.newInstance(desc, reader, converter, null);    for (int i = 0; i < reader.getTotalValueCount(); i += 1) {        if (column.getCurrentDefinitionLevel() >= desc.getMaxDefinitionLevel()) {            column.writeCurrentValueToConverter();        } else {            numNulls += 1;        }        column.consume();    }    if (numNulls != stats.getNumNulls()) {        throw new BadStatsException("Number of nulls doesn't match.");    }    console.debug(String.format("Validated stats min=%s max=%s nulls=%d for page=%s col=%s", stats.minAsString(), stats.maxAsString(), stats.getNumNulls(), page, Arrays.toString(desc.getPath())));}
public int parquet-mr_f888_0() throws IOException
{    Preconditions.checkArgument(targets != null && targets.size() == 1, "A data file is required.");    String source = targets.get(0);    CompressionCodecName codec = Codecs.parquetCodec(compressionCodecName);    Schema schema;    if (avroSchemaFile != null) {        schema = Schemas.fromAvsc(open(avroSchemaFile));    } else {        schema = getAvroSchema(source);    }    Schema projection = filterSchema(schema, columns);    Path outPath = qualifiedPath(outputPath);    FileSystem outFS = outPath.getFileSystem(getConf());    if (overwrite && outFS.exists(outPath)) {        console.debug("Deleting output file {} (already exists)", outPath);        outFS.delete(outPath);    }    Iterable<Record> reader = openDataFile(source, projection);    boolean threw = true;    long count = 0;    try {        try (ParquetWriter<Record> writer = AvroParquetWriter.<Record>builder(qualifiedPath(outputPath)).withWriterVersion(v2 ? PARQUET_2_0 : PARQUET_1_0).withConf(getConf()).withCompressionCodec(codec).withRowGroupSize(rowGroupSize).withDictionaryPageSize(dictionaryPageSize < 64 ? 64 : dictionaryPageSize).withDictionaryEncoding(dictionaryPageSize != 0).withPageSize(pageSize).withDataModel(GenericData.get()).withSchema(projection).build()) {            for (Record record : reader) {                writer.write(record);                count += 1;            }        }        threw = false;    } catch (RuntimeException e) {        throw new RuntimeException("Failed on record " + count, e);    } finally {        if (reader instanceof Closeable) {            Closeables.close((Closeable) reader, threw);        }    }    return 0;}
public List<String> parquet-mr_f889_0()
{    return Lists.newArrayList("# Create a Parquet file from an Avro file", "sample.avro -o sample.parquet", "# Create a Parquet file in S3 from a local Avro file", "path/to/sample.avro -o s3:/user/me/sample.parquet", "# Create a Parquet file from Avro data in S3", "s3:/data/path/sample.avro -o sample.parquet");}
public int parquet-mr_f890_0() throws IOException
{    Preconditions.checkArgument(targets != null && targets.size() == 1, "CSV path is required.");    if (header != null) {                noHeader = true;    }    CSVProperties props = new CSVProperties.Builder().delimiter(delimiter).escape(escape).quote(quote).header(header).hasHeader(!noHeader).linesToSkip(linesToSkip).charset(charsetName).build();    String source = targets.get(0);    Schema csvSchema;    if (avroSchemaFile != null) {        csvSchema = Schemas.fromAvsc(open(avroSchemaFile));    } else {        Set<String> required = ImmutableSet.of();        if (requiredFields != null) {            required = ImmutableSet.copyOf(requiredFields);        }        String filename = new File(source).getName();        String recordName;        if (filename.contains(".")) {            recordName = filename.substring(0, filename.indexOf("."));        } else {            recordName = filename;        }        csvSchema = AvroCSV.inferNullableSchema(recordName, open(source), props, required);    }    long count = 0;    try (AvroCSVReader<Record> reader = new AvroCSVReader<>(open(source), props, csvSchema, Record.class, true)) {        CompressionCodecName codec = Codecs.parquetCodec(compressionCodecName);        try (ParquetWriter<Record> writer = AvroParquetWriter.<Record>builder(qualifiedPath(outputPath)).withWriterVersion(v2 ? PARQUET_2_0 : PARQUET_1_0).withWriteMode(overwrite ? ParquetFileWriter.Mode.OVERWRITE : ParquetFileWriter.Mode.CREATE).withCompressionCodec(codec).withDictionaryEncoding(true).withDictionaryPageSize(dictionaryPageSize).withPageSize(pageSize).withRowGroupSize(rowGroupSize).withDataModel(GenericData.get()).withConf(getConf()).withSchema(csvSchema).build()) {            for (Record record : reader) {                writer.write(record);            }        } catch (RuntimeException e) {            throw new RuntimeException("Failed on record " + count, e);        }    }    return 0;}
public List<String> parquet-mr_f891_0()
{    return Lists.newArrayList("# Create a Parquet file from a CSV file", "sample.csv sample.parquet --schema schema.avsc", "# Create a Parquet file in HDFS from local CSV", "path/to/sample.csv hdfs:/user/me/sample.parquet --schema schema.avsc", "# Create an Avro file from CSV data in S3", "s3:/data/path/sample.csv sample.avro --format avro --schema s3:/schemas/schema.avsc");}
public int parquet-mr_f892_0() throws IOException
{    Preconditions.checkArgument(samplePaths != null && !samplePaths.isEmpty(), "Sample CSV path is required");    Preconditions.checkArgument(samplePaths.size() == 1, "Only one CSV sample can be given");    if (header != null) {                noHeader = true;    }    CSVProperties props = new CSVProperties.Builder().delimiter(delimiter).escape(escape).quote(quote).header(header).hasHeader(!noHeader).linesToSkip(linesToSkip).charset(charsetName).build();    Set<String> required = ImmutableSet.of();    if (requiredFields != null) {        required = ImmutableSet.copyOf(requiredFields);    }        String sampleSchema = AvroCSV.inferNullableSchema(recordName, open(samplePaths.get(0)), props, required).toString(!minimize);    output(sampleSchema, console, outputPath);    return 0;}
public List<String> parquet-mr_f893_0()
{    return Lists.newArrayList("# Print the schema for samples.csv to standard out:", "samples.csv --record-name Sample", "# Write schema to sample.avsc:", "samples.csv -o sample.avsc --record-name Sample");}
public int parquet-mr_f894_0() throws IOException
{    Preconditions.checkArgument(targets != null && targets.size() >= 1, "A Parquet file is required.");    Preconditions.checkArgument(targets.size() == 1, "Cannot process multiple Parquet files.");    String source = targets.get(0);    ParquetMetadata footer = ParquetFileReader.readFooter(getConf(), qualifiedPath(source), ParquetMetadataConverter.NO_FILTER);    console.info("\nFile path:  {}", source);    console.info("Created by: {}", footer.getFileMetaData().getCreatedBy());    Map<String, String> kv = footer.getFileMetaData().getKeyValueMetaData();    if (kv != null && !kv.isEmpty()) {        console.info("Properties:");        String format = "  %" + maxSize(kv.keySet()) + "s: %s";        for (Map.Entry<String, String> entry : kv.entrySet()) {            console.info(String.format(format, entry.getKey(), entry.getValue()));        }    } else {        console.info("Properties: (none)");    }    MessageType schema = footer.getFileMetaData().getSchema();    console.info("Schema:\n{}", schema);    List<BlockMetaData> rowGroups = footer.getBlocks();    for (int index = 0, n = rowGroups.size(); index < n; index += 1) {        printRowGroup(console, index, rowGroups.get(index), schema);    }    console.info("");    return 0;}
public List<String> parquet-mr_f895_0()
{    return Lists.newArrayList();}
private int parquet-mr_f896_0(Iterable<String> strings)
{    int size = 0;    for (String s : strings) {        size = Math.max(size, s.length());    }    return size;}
private void parquet-mr_f897_0(Logger console, int index, BlockMetaData rowGroup, MessageType schema)
{    long start = rowGroup.getStartingPos();    long rowCount = rowGroup.getRowCount();    long compressedSize = rowGroup.getCompressedSize();    long uncompressedSize = rowGroup.getTotalByteSize();    String filePath = rowGroup.getPath();    console.info(String.format("\nRow group %d:  count: %d  %s records  start: %d  total: %s%s\n%s", index, rowCount, humanReadable(((float) compressedSize) / rowCount), start, humanReadable(compressedSize), filePath != null ? " path: " + filePath : "", StringUtils.leftPad("", 80, '-')));    int size = maxSize(Iterables.transform(rowGroup.getColumns(), new Function<ColumnChunkMetaData, String>() {        @Override        public String apply(@Nullable ColumnChunkMetaData input) {            return input == null ? "" : input.getPath().toDotString();        }    }));    console.info(String.format("%-" + size + "s  %-9s %-9s %-9s %-10s %-7s %s", "", "type", "encodings", "count", "avg size", "nulls", "min / max"));    for (ColumnChunkMetaData column : rowGroup.getColumns()) {        printColumnChunk(console, size, column, schema);    }}
public String parquet-mr_f898_0(@Nullable ColumnChunkMetaData input)
{    return input == null ? "" : input.getPath().toDotString();}
private void parquet-mr_f899_0(Logger console, int width, ColumnChunkMetaData column, MessageType schema)
{    String[] path = column.getPath().toArray();    PrimitiveType type = primitive(schema, path);    Preconditions.checkNotNull(type);    ColumnDescriptor desc = schema.getColumnDescription(path);    long size = column.getTotalSize();    long count = column.getValueCount();    float perValue = ((float) size) / count;    CompressionCodecName codec = column.getCodec();    Set<Encoding> encodings = column.getEncodings();    EncodingStats encodingStats = column.getEncodingStats();    String encodingSummary = encodingStats == null ? encodingsAsString(encodings, desc) : encodingStatsAsString(encodingStats);    Statistics stats = column.getStatistics();    String name = column.getPath().toDotString();    PrimitiveType.PrimitiveTypeName typeName = type.getPrimitiveTypeName();    if (typeName == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {        console.info(String.format("%-" + width + "s  FIXED[%d] %s %-7s %-9d %-8s %-7s %s", name, type.getTypeLength(), shortCodec(codec), encodingSummary, count, humanReadable(perValue), stats == null || !stats.isNumNullsSet() ? "" : String.valueOf(stats.getNumNulls()), minMaxAsString(stats)));    } else {        console.info(String.format("%-" + width + "s  %-9s %s %-7s %-9d %-10s %-7s %s", name, typeName, shortCodec(codec), encodingSummary, count, humanReadable(perValue), stats == null || !stats.isNumNullsSet() ? "" : String.valueOf(stats.getNumNulls()), minMaxAsString(stats)));    }}
public int parquet-mr_f900_0() throws IOException
{    Preconditions.checkArgument(targets != null && targets.size() == 1, "Parquet file is required.");    if (targets.size() > 1) {        Preconditions.checkArgument(outputPath == null, "Cannot output multiple schemas to file " + outputPath);        for (String source : targets) {            console.info("{}: {}", source, getSchema(source));        }    } else {        String source = targets.get(0);        if (outputPath != null) {            Path outPath = qualifiedPath(outputPath);            FileSystem outFS = outPath.getFileSystem(getConf());            if (overwrite && outFS.exists(outPath)) {                console.debug("Deleting output file {} (already exists)", outPath);                outFS.delete(outPath);            }            try (OutputStream out = create(outputPath)) {                out.write(getSchema(source).getBytes(StandardCharsets.UTF_8));            }        } else {            console.info(getSchema(source));        }    }    return 0;}
public List<String> parquet-mr_f901_0()
{    return Lists.newArrayList("# Print the Avro schema for a Parquet file", "sample.parquet", "# Print the Avro schema for an Avro file", "sample.avro", "# Print the Avro schema for a JSON file", "sample.json");}
private String parquet-mr_f902_0(String source) throws IOException
{    if (parquetSchema) {        return getParquetSchema(source);    } else {        return getAvroSchema(source).toString(true);    }}
private String parquet-mr_f903_0(String source) throws IOException
{    Formats.Format format;    try (SeekableInput in = openSeekable(source)) {        format = Formats.detectFormat((InputStream) in);        in.seek(0);        switch(format) {            case PARQUET:                return new ParquetFileReader(getConf(), qualifiedPath(source), ParquetMetadataConverter.NO_FILTER).getFileMetaData().getSchema().toString();            default:                throw new IllegalArgumentException(String.format("Could not get a Parquet schema for format %s: %s", format, source));        }    }}
public List<String> parquet-mr_f904_0()
{    return Lists.newArrayList("# Show only column indexes for column 'col' from a Parquet file", "-c col -i sample.parquet");}
public int parquet-mr_f905_0() throws IOException
{    Preconditions.checkArgument(files != null && files.size() >= 1, "A Parquet file is required.");    Preconditions.checkArgument(files.size() == 1, "Cannot process multiple Parquet files.");    InputFile in = HadoopInputFile.fromPath(qualifiedPath(files.get(0)), getConf());    if (!showColumnIndex && !showOffsetIndex) {        showColumnIndex = true;        showOffsetIndex = true;    }    Set<String> rowGroupIndexSet = new HashSet<>();    if (rowGroupIndexes != null) {        rowGroupIndexSet.addAll(rowGroupIndexes);    }    try (ParquetFileReader reader = ParquetFileReader.open(in)) {        boolean firstBlock = true;        int rowGroupIndex = 0;        for (BlockMetaData block : reader.getFooter().getBlocks()) {            if (!rowGroupIndexSet.isEmpty() && !rowGroupIndexSet.contains(Integer.toString(rowGroupIndex))) {                ++rowGroupIndex;                continue;            }            if (!firstBlock) {                console.info("");            }            firstBlock = false;            console.info("row-group {}:", rowGroupIndex);            for (ColumnChunkMetaData column : getColumns(block)) {                String path = column.getPath().toDotString();                if (showColumnIndex) {                    console.info("column index for column {}:", path);                    ColumnIndex columnIndex = reader.readColumnIndex(column);                    if (columnIndex == null) {                        console.info("NONE");                    } else {                        console.info(columnIndex.toString());                    }                }                if (showOffsetIndex) {                    console.info("offset index for column {}:", path);                    OffsetIndex offsetIndex = reader.readOffsetIndex(column);                    if (offsetIndex == null) {                        console.info("NONE");                    } else {                        console.info(offsetIndex.toString());                    }                }            }            ++rowGroupIndex;        }    }    return 0;}
private List<ColumnChunkMetaData> parquet-mr_f906_0(BlockMetaData block)
{    List<ColumnChunkMetaData> columns = block.getColumns();    if (ColumnPaths == null || ColumnPaths.isEmpty()) {        return columns;    }    Map<String, ColumnChunkMetaData> pathMap = new HashMap<>();    for (ColumnChunkMetaData column : columns) {        pathMap.put(column.getPath().toDotString(), column);    }    List<ColumnChunkMetaData> filtered = new ArrayList<>();    for (String path : ColumnPaths) {        ColumnChunkMetaData column = pathMap.get(path);        if (column != null) {            filtered.add(column);        }    }    return filtered;}
public int parquet-mr_f907_0() throws IOException
{    Preconditions.checkArgument(targets != null && targets.size() >= 1, "A Parquet file is required.");    Preconditions.checkArgument(targets.size() == 1, "Cannot process multiple Parquet files.");    String source = targets.get(0);    ParquetFileReader reader = ParquetFileReader.open(getConf(), qualifiedPath(source));    MessageType schema = reader.getFileMetaData().getSchema();    ColumnDescriptor descriptor = Util.descriptor(column, schema);    PrimitiveType type = Util.primitive(column, schema);    Preconditions.checkNotNull(type);    DictionaryPageReadStore dictionaryReader;    int rowGroup = 0;    while ((dictionaryReader = reader.getNextDictionaryReader()) != null) {        DictionaryPage page = dictionaryReader.readDictionaryPage(descriptor);        Dictionary dict = page.getEncoding().initDictionary(descriptor, page);        console.info("\nRow group {} dictionary for \"{}\":", rowGroup, column, page.getCompressedSize());        for (int i = 0; i <= dict.getMaxId(); i += 1) {            switch(type.getPrimitiveTypeName()) {                case BINARY:                    if (type.getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation) {                        console.info("{}: {}", String.format("%6d", i), Util.humanReadable(dict.decodeToBinary(i).toStringUsingUTF8(), 70));                    } else {                        console.info("{}: {}", String.format("%6d", i), Util.humanReadable(dict.decodeToBinary(i).getBytesUnsafe(), 70));                    }                    break;                case INT32:                    console.info("{}: {}", String.format("%6d", i), dict.decodeToInt(i));                    break;                case INT64:                    console.info("{}: {}", String.format("%6d", i), dict.decodeToLong(i));                    break;                case FLOAT:                    console.info("{}: {}", String.format("%6d", i), dict.decodeToFloat(i));                    break;                case DOUBLE:                    console.info("{}: {}", String.format("%6d", i), dict.decodeToDouble(i));                    break;                default:                    throw new IllegalArgumentException("Unknown dictionary type: " + type.getPrimitiveTypeName());            }        }        reader.skipNextRowGroup();        rowGroup += 1;    }    console.info("");    return 0;}
public List<String> parquet-mr_f908_0()
{    return Lists.newArrayList("# Show the dictionary for column 'col' from a Parquet file", "-c col sample.parquet");}
public int parquet-mr_f909_0() throws IOException
{    Preconditions.checkArgument(targets != null && targets.size() >= 1, "A Parquet file is required.");    Preconditions.checkArgument(targets.size() == 1, "Cannot process multiple Parquet files.");    String source = targets.get(0);    ParquetFileReader reader = ParquetFileReader.open(getConf(), qualifiedPath(source));    MessageType schema = reader.getFileMetaData().getSchema();    Map<ColumnDescriptor, PrimitiveType> columns = Maps.newLinkedHashMap();    if (this.columns == null || this.columns.isEmpty()) {        for (ColumnDescriptor descriptor : schema.getColumns()) {            columns.put(descriptor, primitive(schema, descriptor.getPath()));        }    } else {        for (String column : this.columns) {            columns.put(descriptor(column, schema), primitive(column, schema));        }    }    CompressionCodecName codec = reader.getRowGroups().get(0).getColumns().get(0).getCodec();        Map<String, List<String>> formatted = Maps.newLinkedHashMap();    PageFormatter formatter = new PageFormatter();    PageReadStore pageStore;    int rowGroupNum = 0;    while ((pageStore = reader.readNextRowGroup()) != null) {        for (ColumnDescriptor descriptor : columns.keySet()) {            List<String> lines = formatted.get(columnName(descriptor));            if (lines == null) {                lines = Lists.newArrayList();                formatted.put(columnName(descriptor), lines);            }            formatter.setContext(rowGroupNum, columns.get(descriptor), codec);            PageReader pages = pageStore.getPageReader(descriptor);            DictionaryPage dict = pages.readDictionaryPage();            if (dict != null) {                lines.add(formatter.format(dict));            }            DataPage page;            while ((page = pages.readPage()) != null) {                lines.add(formatter.format(page));            }        }        rowGroupNum += 1;    }        for (String columnName : formatted.keySet()) {        console.info(String.format("\nColumn: %s\n%s", columnName, StringUtils.leftPad("", 80, '-')));        console.info(formatter.getHeader());        for (String line : formatted.get(columnName)) {            console.info(line);        }        console.info("");    }    return 0;}
public List<String> parquet-mr_f910_0()
{    return Lists.newArrayList("# Show pages for column 'col' from a Parquet file", "-c col sample.parquet");}
 String parquet-mr_f911_0()
{    return String.format("  %-6s %-5s %-4s %-7s %-10s %-10s %-8s %-7s %s", "page", "type", "enc", "count", "avg size", "size", "rows", "nulls", "min / max");}
 void parquet-mr_f912_0(int rowGroupNum, PrimitiveType type, CompressionCodecName codec)
{    this.rowGroupNum = rowGroupNum;    this.pageNum = 0;    this.type = type;    this.shortCodec = shortCodec(codec);}
 String parquet-mr_f913_0(Page page)
{    String formatted = "";    if (page instanceof DictionaryPage) {        formatted = printDictionaryPage((DictionaryPage) page);    } else if (page instanceof DataPage) {        formatted = ((DataPage) page).accept(this);    }    pageNum += 1;    return formatted;}
private String parquet-mr_f914_0(DictionaryPage dict)
{        dict.getUncompressedSize();    long totalSize = dict.getCompressedSize();    int count = dict.getDictionarySize();    float perValue = ((float) totalSize) / count;    String enc = encodingAsString(dict.getEncoding(), true);    if (pageNum == 0) {        return String.format("%3d-D    %-5s %s %-2s %-7d %-10s %-10s", rowGroupNum, "dict", shortCodec, enc, count, humanReadable(perValue), humanReadable(totalSize));    } else {        return String.format("%3d-%-3d  %-5s %s %-2s %-7d %-10s %-10s", rowGroupNum, pageNum, "dict", shortCodec, enc, count, humanReadable(perValue), humanReadable(totalSize));    }}
public String parquet-mr_f915_0(DataPageV1 page)
{    String enc = encodingAsString(page.getValueEncoding(), false);    long totalSize = page.getCompressedSize();    int count = page.getValueCount();    String numNulls = page.getStatistics().isNumNullsSet() ? Long.toString(page.getStatistics().getNumNulls()) : "";    float perValue = ((float) totalSize) / count;    String minMax = minMaxAsString(page.getStatistics());    return String.format("%3d-%-3d  %-5s %s %-2s %-7d %-10s %-10s %-8s %-7s %s", rowGroupNum, pageNum, "data", shortCodec, enc, count, humanReadable(perValue), humanReadable(totalSize), "", numNulls, minMax);}
public String parquet-mr_f916_0(DataPageV2 page)
{    String enc = encodingAsString(page.getDataEncoding(), false);    long totalSize = page.getCompressedSize();    int count = page.getValueCount();    int numRows = page.getRowCount();    int numNulls = page.getNullCount();    float perValue = ((float) totalSize) / count;    String minMax = minMaxAsString(page.getStatistics());    String compression = (page.isCompressed() ? shortCodec : "_");    return String.format("%3d-%-3d  %-5s %s %-2s %-7d %-10s %-10s %-8d %-7s %s", rowGroupNum, pageNum, "data", compression, enc, count, humanReadable(perValue), humanReadable(totalSize), numRows, numNulls, minMax);}
public int parquet-mr_f917_0() throws IOException
{    Preconditions.checkArgument(targets != null && targets.size() == 1, "A data file is required.");    String source = targets.get(0);    CodecFactory codecFactory = Codecs.avroCodec(compressionCodecName);    final Schema schema;    if (avroSchemaFile != null) {        schema = Schemas.fromAvsc(open(avroSchemaFile));    } else {        schema = getAvroSchema(source);    }    final Schema projection = filterSchema(schema, columns);    Path outPath = qualifiedPath(outputPath);    try (FileSystem outFS = outPath.getFileSystem(getConf())) {        if (overwrite && outFS.exists(outPath)) {            console.debug("Deleting output file {} (already exists)", outPath);            outFS.delete(outPath);        }    }    Iterable<Record> reader = openDataFile(source, projection);    boolean threw = true;    long count = 0;    DatumWriter<Record> datumWriter = new GenericDatumWriter<>(schema);    try (DataFileWriter<Record> fileWriter = new DataFileWriter<>(datumWriter)) {        fileWriter.setCodec(codecFactory);        try (DataFileWriter<Record> writer = fileWriter.create(projection, create(outputPath))) {            for (Record record : reader) {                writer.append(record);                count += 1;            }        }        threw = false;    } catch (RuntimeException e) {        throw new RuntimeException("Failed on record " + count, e);    } finally {        if (reader instanceof Closeable) {            Closeables.close((Closeable) reader, threw);        }    }    return 0;}
public List<String> parquet-mr_f918_0()
{    return Lists.newArrayList("# Create an Avro file from a Parquet file", "sample.parquet sample.avro", "# Create an Avro file in HDFS from a local JSON file", "path/to/sample.json hdfs:/user/me/sample.parquet", "# Create an Avro file from data in S3", "s3:/data/path/sample.parquet sample.avro");}
 static CSVReader parquet-mr_f919_0(InputStream incoming, CSVProperties props)
{    return new CSVReader(new InputStreamReader(incoming, Charset.forName(props.charset)), props.delimiter.charAt(0), props.quote.charAt(0), props.escape.charAt(0), props.linesToSkip, false, /* strict quotes off: don't ignore unquoted strings */    true);}
 static CSVParser parquet-mr_f920_0(CSVProperties props)
{    return new CSVParser(props.delimiter.charAt(0), props.quote.charAt(0), props.escape.charAt(0), false, /* strict quotes off: don't ignore unquoted strings */    true);}
public static Schema parquet-mr_f921_0(String name, InputStream incoming, CSVProperties props) throws IOException
{    return inferSchemaInternal(name, incoming, props, NO_REQUIRED_FIELDS, true);}
public static Schema parquet-mr_f922_0(String name, InputStream incoming, CSVProperties props, Set<String> requiredFields) throws IOException
{    return inferSchemaInternal(name, incoming, props, requiredFields, true);}
public static Schema parquet-mr_f923_0(String name, InputStream incoming, CSVProperties props) throws IOException
{    return inferSchemaInternal(name, incoming, props, NO_REQUIRED_FIELDS, false);}
public static Schema parquet-mr_f924_0(String name, InputStream incoming, CSVProperties props, Set<String> requiredFields) throws IOException
{    return inferSchemaInternal(name, incoming, props, requiredFields, false);}
private static Schema parquet-mr_f925_0(String name, InputStream incoming, CSVProperties props, Set<String> requiredFields, boolean makeNullable) throws IOException
{    CSVReader reader = newReader(incoming, props);    String[] header;    String[] line;    if (props.useHeader) {                header = reader.readNext();        line = reader.readNext();        Preconditions.checkNotNull(line, "No content to infer schema");    } else if (props.header != null) {        header = newParser(props).parseLine(props.header);        line = reader.readNext();        Preconditions.checkNotNull(line, "No content to infer schema");    } else {                line = reader.readNext();        Preconditions.checkNotNull(line, "No content to infer schema");        header = new String[line.length];        for (int i = 0; i < line.length; i += 1) {            header[i] = "field_" + String.valueOf(i);        }    }    Schema.Type[] types = new Schema.Type[header.length];    String[] values = new String[header.length];    boolean[] nullable = new boolean[header.length];    boolean[] empty = new boolean[header.length];    for (int processed = 0; processed < DEFAULT_INFER_LINES; processed += 1) {        if (line == null) {            break;        }        for (int i = 0; i < header.length; i += 1) {            if (i < line.length) {                if (types[i] == null) {                    types[i] = inferFieldType(line[i]);                    if (types[i] != null) {                                                values[i] = line[i];                    }                }                if (line[i] == null) {                    nullable[i] = true;                } else if (line[i].isEmpty()) {                    empty[i] = true;                }            } else {                                nullable[i] = true;            }        }        line = reader.readNext();    }    SchemaBuilder.FieldAssembler<Schema> fieldAssembler = SchemaBuilder.record(name).fields();        for (int i = 0; i < header.length; i += 1) {        if (header[i] == null) {            throw new RuntimeException("Bad header for field " + i + ": null");        }        String fieldName = header[i].trim();        if (fieldName.isEmpty()) {            throw new RuntimeException("Bad header for field " + i + ": \"" + fieldName + "\"");        } else if (!isAvroCompatibleName(fieldName)) {            throw new RuntimeException("Bad header for field, should start with a character " + "or _ and can contain only alphanumerics and _ " + i + ": \"" + fieldName + "\"");        }                boolean foundNull = (nullable[i] || (empty[i] && types[i] != Schema.Type.STRING));        if (requiredFields.contains(fieldName)) {            if (foundNull) {                throw new RuntimeException("Found null value for required field: " + fieldName + " (" + types[i] + ")");            }            fieldAssembler = fieldAssembler.name(fieldName).doc("Type inferred from '" + sample(values[i]) + "'").type(schema(types[i], false)).noDefault();        } else {            SchemaBuilder.GenericDefault<Schema> defaultBuilder = fieldAssembler.name(fieldName).doc("Type inferred from '" + sample(values[i]) + "'").type(schema(types[i], makeNullable || foundNull));            if (makeNullable || foundNull) {                fieldAssembler = defaultBuilder.withDefault(null);            } else {                fieldAssembler = defaultBuilder.noDefault();            }        }    }    return fieldAssembler.endRecord();}
private static String parquet-mr_f926_0(String value)
{    if (value != null) {        return NON_PRINTABLE.replaceFrom(value.subSequence(0, min(50, value.length())), '.');    } else {        return "null";    }}
private static Schema parquet-mr_f927_0(Schema.Type type, boolean makeNullable)
{    Schema schema = Schema.create(type == null ? Schema.Type.STRING : type);    if (makeNullable || type == null) {        schema = Schema.createUnion(Lists.newArrayList(Schema.create(Schema.Type.NULL), schema));    }    return schema;}
private static Schema.Type parquet-mr_f928_0(String example)
{    if (example == null || example.isEmpty()) {                return null;    } else if (LONG.matcher(example).matches()) {        return Schema.Type.LONG;    } else if (DOUBLE.matcher(example).matches()) {        return Schema.Type.DOUBLE;    } else if (FLOAT.matcher(example).matches()) {        return Schema.Type.FLOAT;    }    return Schema.Type.STRING;}
private static boolean parquet-mr_f929_0(String name)
{    return AVRO_COMPATIBLE.matcher(name).matches();}
public boolean parquet-mr_f930_0()
{    return hasNext;}
public E parquet-mr_f931_0()
{    if (!hasNext) {        throw new NoSuchElementException();    }    try {        if (reuseRecords) {            this.record = builder.makeRecord(next, record);            return record;        } else {            return builder.makeRecord(next, null);        }    } finally {        this.hasNext = advance();    }}
private boolean parquet-mr_f932_0()
{    try {        next = reader.readNext();    } catch (IOException ex) {        throw new RuntimeIOException("Could not read record", ex);    }    return (next != null);}
public void parquet-mr_f933_0()
{    try {        reader.close();    } catch (IOException e) {        throw new RuntimeIOException("Cannot close reader", e);    }}
public void parquet-mr_f934_0()
{    throw new UnsupportedOperationException("Remove is not implemented.");}
public Iterator<E> parquet-mr_f935_0()
{    return this;}
public Builder parquet-mr_f936_0(String charset)
{    this.charset = charset;    return this;}
public Builder parquet-mr_f937_0(String delimiter)
{    this.delimiter = StringEscapeUtils.unescapeJava(delimiter);    return this;}
public Builder parquet-mr_f938_0(String quote)
{    this.quote = StringEscapeUtils.unescapeJava(quote);    return this;}
public Builder parquet-mr_f939_0(String escape)
{    this.escape = StringEscapeUtils.unescapeJava(escape);    return this;}
public Builder parquet-mr_f940_0(String header)
{    this.header = header;    return this;}
public Builder parquet-mr_f941_0()
{    this.useHeader = true;    return this;}
public Builder parquet-mr_f942_0(boolean hasHeader)
{    this.useHeader = hasHeader;    return this;}
public Builder parquet-mr_f943_0(int linesToSkip)
{    this.linesToSkip = linesToSkip;    return this;}
public CSVProperties parquet-mr_f944_0()
{    return new CSVProperties(charset, delimiter, quote, escape, header, useHeader, linesToSkip);}
public E parquet-mr_f945_0(String[] fields, E reuse)
{    E record = reuse;    if (record == null) {        record = newRecordInstance();    }    if (record instanceof IndexedRecord) {        fillIndexed((IndexedRecord) record, fields);    } else {        fillReflect(record, fields);    }    return record;}
private E parquet-mr_f946_0()
{    if (recordClass != GenericData.Record.class && !recordClass.isInterface()) {        E record = (E) ReflectData.newInstance(recordClass, schema);        if (record != null) {            return record;        }    }    return (E) new GenericData.Record(schema);}
private void parquet-mr_f947_0(IndexedRecord record, String[] data)
{    for (int i = 0; i < indexes.length; i += 1) {        int index = indexes[i];        record.put(i, makeValue(index < data.length ? data[index] : null, fields[i]));    }}
private void parquet-mr_f948_0(Object record, String[] data)
{    for (int i = 0; i < indexes.length; i += 1) {        Schema.Field field = fields[i];        int index = indexes[i];        Object value = makeValue(index < data.length ? data[index] : null, field);        ReflectData.get().setField(record, field.name(), i, value);    }}
private static Object parquet-mr_f949_0(String string, Schema.Field field)
{    try {        Object value = makeValue(string, field.schema());        if (value != null || Schemas.nullOk(field.schema())) {            return value;        } else {                        return ReflectData.get().getDefaultValue(field);        }    } catch (RecordException e) {                throw new RecordException(String.format("Cannot convert field %s", field.name()), e);    } catch (NumberFormatException e) {        throw new RecordException(String.format("Field %s: value not a %s: '%s'", field.name(), field.schema(), string), e);    } catch (AvroRuntimeException e) {        throw new RecordException(String.format("Field %s: cannot make %s value: '%s'", field.name(), field.schema(), string), e);    }}
private static Object parquet-mr_f950_0(String string, Schema schema)
{    if (string == null) {        return null;    }    try {        switch(schema.getType()) {            case BOOLEAN:                return Boolean.valueOf(string);            case STRING:                return string;            case FLOAT:                return Float.valueOf(string);            case DOUBLE:                return Double.valueOf(string);            case INT:                return Integer.valueOf(string);            case LONG:                return Long.valueOf(string);            case ENUM:                                if (schema.hasEnumSymbol(string)) {                    return string;                } else {                    try {                        return schema.getEnumSymbols().get(Integer.parseInt(string));                    } catch (IndexOutOfBoundsException ex) {                        return null;                    }                }            case UNION:                Object value = null;                for (Schema possible : schema.getTypes()) {                    value = makeValue(string, possible);                    if (value != null) {                        return value;                    }                }                return null;            case NULL:                return null;            default:                                throw new RecordException("Unsupported field type:" + schema.getType());        }    } catch (NumberFormatException e) {                if (string.isEmpty()) {            return null;        } else {            throw e;        }    }}
public static Configuration parquet-mr_f951_0()
{    return defaultConf;}
public static void parquet-mr_f952_0(Configuration defaultConf)
{    HadoopFileSystemURLStreamHandler.defaultConf = defaultConf;}
public void parquet-mr_f953_0(Configuration conf)
{    this.conf = conf;}
public Configuration parquet-mr_f954_0()
{    return conf;}
protected URLConnection parquet-mr_f955_0(URL url) throws IOException
{    return new HadoopFileSystemURLConnection(url);}
public void parquet-mr_f956_0() throws IOException
{}
public InputStream parquet-mr_f957_0() throws IOException
{    Path path = new Path(url.toExternalForm());    FileSystem fileSystem = path.getFileSystem(conf);    return fileSystem.open(path);}
public void parquet-mr_f958_0(String programName)
{    this.programName = programName;}
public int parquet-mr_f959_0()
{    if (helpCommands.isEmpty()) {        printGenericHelp();    } else {        for (String cmd : helpCommands) {            JCommander commander = jc.getCommands().get(cmd);            if (commander == null) {                console.error("\nUnknown command: {}\n", cmd);                printGenericHelp();                return 1;            }            boolean hasRequired = false;            console.info("\nUsage: {} [general options] {} {} [command options]", new Object[] { programName, cmd, commander.getMainParameterDescription() });            console.info("\n  Description:");            console.info("\n    {}", jc.getCommandDescription(cmd));            if (!commander.getParameters().isEmpty()) {                console.info("\n  Command options:\n");                for (ParameterDescription param : commander.getParameters()) {                    hasRequired = printOption(console, param) || hasRequired;                }                if (hasRequired) {                    console.info("\n  * = required");                }            }            List<String> examples = ((Command) commander.getObjects().get(0)).getExamples();            if (examples != null) {                console.info("\n  Examples:");                for (String example : examples) {                    if (example.startsWith("#")) {                                                console.info("\n    {}", example);                    } else {                        console.info("    {} {} {}", new Object[] { programName, cmd, example });                    }                }            }                        console.info("");        }    }    return 0;}
public void parquet-mr_f960_0()
{    boolean hasRequired = false;    console.info("\nUsage: {} [options] [command] [command options]", programName);    console.info("\n  Options:\n");    for (ParameterDescription param : jc.getParameters()) {        hasRequired = printOption(console, param) || hasRequired;    }    if (hasRequired) {        console.info("\n  * = required");    }    console.info("\n  Commands:\n");    for (String command : jc.getCommands().keySet()) {        console.info("    {}\n\t{}", command, jc.getCommandDescription(command));    }    console.info("\n  Examples:");    console.info("\n    # print information for create\n    {} help create", programName);    console.info("\n  See '{} help <command>' for more information on a " + "specific command.", programName);}
private boolean parquet-mr_f961_0(Logger console, ParameterDescription param)
{    boolean required = param.getParameter().required();    if (!param.getParameter().hidden()) {        console.info("  {} {}\n\t{}{}", new Object[] { required ? "*" : " ", param.getNames().trim(), param.getDescription(), formatDefault(param) });    }    return required;}
private String parquet-mr_f962_0(ParameterDescription param)
{    Object defaultValue = param.getDefault();    if (defaultValue == null || param.getParameter().arity() < 1) {        return "";    }    return " (default: " + ((defaultValue instanceof String) ? "\"" + defaultValue + "\"" : defaultValue.toString()) + ")";}
public List<String> parquet-mr_f963_0()
{    return null;}
public static Iterator<JsonNode> parquet-mr_f964_0(final InputStream stream)
{    try {                JsonParser parser = FACTORY.createParser(stream);        return parser.readValuesAs(JsonNode.class);    } catch (IOException e) {        throw new RuntimeIOException("Cannot read from stream", e);    }}
public static JsonNode parquet-mr_f965_0(String json)
{    return parse(json, JsonNode.class);}
public static T parquet-mr_f966_0(String json, Class<T> returnType)
{    try {        return MAPPER.readValue(json, returnType);    } catch (JsonParseException e) {        throw new IllegalArgumentException("Invalid JSON", e);    } catch (JsonMappingException e) {        throw new IllegalArgumentException("Invalid JSON", e);    } catch (IOException e) {        throw new RuntimeIOException("Cannot initialize JSON parser", e);    }}
public static JsonNode parquet-mr_f967_0(InputStream json)
{    return parse(json, JsonNode.class);}
public static T parquet-mr_f968_0(InputStream json, Class<T> returnType)
{    try {        return MAPPER.readValue(json, returnType);    } catch (JsonParseException e) {        throw new IllegalArgumentException("Invalid JSON stream", e);    } catch (JsonMappingException e) {        throw new IllegalArgumentException("Invalid JSON stream", e);    } catch (IOException e) {        throw new RuntimeIOException("Cannot initialize JSON parser", e);    }}
public static Object parquet-mr_f969_0(GenericData model, JsonNode datum, Schema schema)
{    if (datum == null) {        return null;    }    switch(schema.getType()) {        case RECORD:            RecordException.check(datum.isObject(), "Cannot convert non-object to record: %s", datum);            Object record = model.newRecord(null, schema);            for (Schema.Field field : schema.getFields()) {                model.setField(record, field.name(), field.pos(), convertField(model, datum.get(field.name()), field));            }            return record;        case MAP:            RecordException.check(datum.isObject(), "Cannot convert non-object to map: %s", datum);            Map<String, Object> map = Maps.newLinkedHashMap();            Iterator<Map.Entry<String, JsonNode>> iter = datum.fields();            while (iter.hasNext()) {                Map.Entry<String, JsonNode> entry = iter.next();                map.put(entry.getKey(), convertToAvro(model, entry.getValue(), schema.getValueType()));            }            return map;        case ARRAY:            RecordException.check(datum.isArray(), "Cannot convert to array: %s", datum);            List<Object> list = Lists.newArrayListWithExpectedSize(datum.size());            for (JsonNode element : datum) {                list.add(convertToAvro(model, element, schema.getElementType()));            }            return list;        case UNION:            return convertToAvro(model, datum, resolveUnion(datum, schema.getTypes()));        case BOOLEAN:            RecordException.check(datum.isBoolean(), "Cannot convert to boolean: %s", datum);            return datum.booleanValue();        case FLOAT:            RecordException.check(datum.isFloat() || datum.isInt(), "Cannot convert to float: %s", datum);            return datum.floatValue();        case DOUBLE:            RecordException.check(datum.isDouble() || datum.isFloat() || datum.isLong() || datum.isInt(), "Cannot convert to double: %s", datum);            return datum.doubleValue();        case INT:            RecordException.check(datum.isInt(), "Cannot convert to int: %s", datum);            return datum.intValue();        case LONG:            RecordException.check(datum.isLong() || datum.isInt(), "Cannot convert to long: %s", datum);            return datum.longValue();        case STRING:            RecordException.check(datum.isTextual(), "Cannot convert to string: %s", datum);            return datum.textValue();        case ENUM:            RecordException.check(datum.isTextual(), "Cannot convert to string: %s", datum);            return model.createEnum(datum.textValue(), schema);        case BYTES:            RecordException.check(datum.isBinary(), "Cannot convert to binary: %s", datum);            try {                return ByteBuffer.wrap(datum.binaryValue());            } catch (IOException e) {                throw new RecordException("Failed to read JSON binary", e);            }        case FIXED:            RecordException.check(datum.isBinary(), "Cannot convert to fixed: %s", datum);            byte[] bytes;            try {                bytes = datum.binaryValue();            } catch (IOException e) {                throw new RecordException("Failed to read JSON binary", e);            }            RecordException.check(bytes.length < schema.getFixedSize(), "Binary data is too short: %s bytes for %s", bytes.length, schema);            return model.createFixed(null, bytes, schema);        case NULL:            return null;        default:                        throw new IllegalArgumentException("Unknown schema type: " + schema);    }}
private static Object parquet-mr_f970_0(GenericData model, JsonNode datum, Schema.Field field)
{    try {        Object value = convertToAvro(model, datum, field.schema());        if (value != null || Schemas.nullOk(field.schema())) {            return value;        } else {            return model.getDefaultValue(field);        }    } catch (RecordException e) {                throw new RecordException(String.format("Cannot convert field %s", field.name()), e);    } catch (AvroRuntimeException e) {        throw new RecordException(String.format("Field %s: cannot make %s value: '%s'", field.name(), field.schema(), String.valueOf(datum)), e);    }}
private static Schema parquet-mr_f971_0(JsonNode datum, Collection<Schema> schemas)
{    Set<Schema.Type> primitives = Sets.newHashSet();    List<Schema> others = Lists.newArrayList();    for (Schema schema : schemas) {        if (PRIMITIVES.containsKey(schema.getType())) {            primitives.add(schema.getType());        } else {            others.add(schema);        }    }        Schema primitiveSchema = null;    if (datum == null || datum.isNull()) {        primitiveSchema = closestPrimitive(primitives, Schema.Type.NULL);    } else if (datum.isShort() || datum.isInt()) {        primitiveSchema = closestPrimitive(primitives, Schema.Type.INT, Schema.Type.LONG, Schema.Type.FLOAT, Schema.Type.DOUBLE);    } else if (datum.isLong()) {        primitiveSchema = closestPrimitive(primitives, Schema.Type.LONG, Schema.Type.DOUBLE);    } else if (datum.isFloat()) {        primitiveSchema = closestPrimitive(primitives, Schema.Type.FLOAT, Schema.Type.DOUBLE);    } else if (datum.isDouble()) {        primitiveSchema = closestPrimitive(primitives, Schema.Type.DOUBLE);    } else if (datum.isBoolean()) {        primitiveSchema = closestPrimitive(primitives, Schema.Type.BOOLEAN);    }    if (primitiveSchema != null) {        return primitiveSchema;    }        for (Schema schema : others) {        if (matches(datum, schema)) {            return schema;        }    }    throw new RecordException(String.format("Cannot resolve union: %s not in %s", datum, schemas));}
private static Schema parquet-mr_f972_0(Set<Schema.Type> possible, Schema.Type... types)
{    for (Schema.Type type : types) {        if (possible.contains(type) && PRIMITIVES.containsKey(type)) {            return PRIMITIVES.get(type);        }    }    return null;}
private static boolean parquet-mr_f973_0(JsonNode datum, Schema schema)
{    switch(schema.getType()) {        case RECORD:            if (datum.isObject()) {                                boolean missingField = false;                for (Schema.Field field : schema.getFields()) {                    if (!datum.has(field.name()) && field.defaultVal() == null) {                        missingField = true;                        break;                    }                }                if (!missingField) {                    return true;                }            }            break;        case UNION:            if (resolveUnion(datum, schema.getTypes()) != null) {                return true;            }            break;        case MAP:            if (datum.isObject()) {                return true;            }            break;        case ARRAY:            if (datum.isArray()) {                return true;            }            break;        case BOOLEAN:            if (datum.isBoolean()) {                return true;            }            break;        case FLOAT:            if (datum.isFloat() || datum.isInt()) {                return true;            }            break;        case DOUBLE:            if (datum.isDouble() || datum.isFloat() || datum.isLong() || datum.isInt()) {                return true;            }            break;        case INT:            if (datum.isInt()) {                return true;            }            break;        case LONG:            if (datum.isLong() || datum.isInt()) {                return true;            }            break;        case STRING:            if (datum.isTextual()) {                return true;            }            break;        case ENUM:            if (datum.isTextual() && schema.hasEnumSymbol(datum.textValue())) {                return true;            }            break;        case BYTES:        case FIXED:            if (datum.isBinary()) {                return true;            }            break;        case NULL:            if (datum == null || datum.isNull()) {                return true;            }            break;        default:                        throw new IllegalArgumentException("Unsupported schema: " + schema);    }    return false;}
public static Schema parquet-mr_f974_0(InputStream incoming, final String name, int numRecords)
{    Iterator<Schema> schemas = Iterators.transform(parser(incoming), new Function<JsonNode, Schema>() {        @Override        public Schema apply(JsonNode node) {            return inferSchema(node, name);        }    });    if (!schemas.hasNext()) {        return null;    }    Schema result = schemas.next();    for (int i = 1; schemas.hasNext() && i < numRecords; i += 1) {        result = Schemas.merge(result, schemas.next());    }    return result;}
public Schema parquet-mr_f975_0(JsonNode node)
{    return inferSchema(node, name);}
public static Schema parquet-mr_f976_0(JsonNode node, String name)
{    return visit(node, new JsonSchemaVisitor(name));}
public static Schema parquet-mr_f977_0(JsonNode node, String name)
{    return visit(node, new JsonSchemaVisitor(name).useMaps());}
public JsonSchemaVisitor parquet-mr_f978_0()
{    this.objectsToRecords = false;    return this;}
public Schema parquet-mr_f979_0(ObjectNode object, Map<String, Schema> fields)
{    if (objectsToRecords || recordLevels.size() < 1) {        List<Schema.Field> recordFields = Lists.newArrayListWithExpectedSize(fields.size());        for (Map.Entry<String, Schema> entry : fields.entrySet()) {            recordFields.add(new Schema.Field(entry.getKey(), entry.getValue(), "Type inferred from '" + object.get(entry.getKey()) + "'", null));        }        Schema recordSchema;        if (recordLevels.size() < 1) {            recordSchema = Schema.createRecord(name, null, null, false);        } else {            recordSchema = Schema.createRecord(DOT.join(recordLevels), null, null, false);        }        recordSchema.setFields(recordFields);        return recordSchema;    } else {                switch(fields.size()) {            case 0:                return Schema.createMap(Schema.create(Schema.Type.NULL));            case 1:                return Schema.createMap(Iterables.getOnlyElement(fields.values()));            default:                return Schema.createMap(Schemas.mergeOrUnion(fields.values()));        }    }}
public Schema parquet-mr_f980_0(ArrayNode ignored, List<Schema> elementSchemas)
{        switch(elementSchemas.size()) {        case 0:            return Schema.createArray(Schema.create(Schema.Type.NULL));        case 1:            return Schema.createArray(Iterables.getOnlyElement(elementSchemas));        default:            return Schema.createArray(Schemas.mergeOrUnion(elementSchemas));    }}
public Schema parquet-mr_f981_0(BinaryNode ignored)
{    return Schema.create(Schema.Type.BYTES);}
public Schema parquet-mr_f982_0(TextNode ignored)
{    return Schema.create(Schema.Type.STRING);}
public Schema parquet-mr_f983_0(NumericNode number)
{    if (number.isInt()) {        return Schema.create(Schema.Type.INT);    } else if (number.isLong()) {        return Schema.create(Schema.Type.LONG);    } else if (number.isFloat()) {        return Schema.create(Schema.Type.FLOAT);    } else if (number.isDouble()) {        return Schema.create(Schema.Type.DOUBLE);    } else {        throw new UnsupportedOperationException(number.getClass().getName() + " is not supported");    }}
public Schema parquet-mr_f984_0(BooleanNode ignored)
{    return Schema.create(Schema.Type.BOOLEAN);}
public Schema parquet-mr_f985_0(NullNode ignored)
{    return Schema.create(Schema.Type.NULL);}
public Schema parquet-mr_f986_0(MissingNode ignored)
{    throw new UnsupportedOperationException("MissingNode is not supported.");}
private static T parquet-mr_f987_0(JsonNode node, JsonTreeVisitor<T> visitor)
{    switch(node.getNodeType()) {        case OBJECT:            Preconditions.checkArgument(node instanceof ObjectNode, "Expected instance of ObjectNode: " + node);                        Map<String, T> fields = Maps.newLinkedHashMap();            Iterator<Map.Entry<String, JsonNode>> iter = node.fields();            while (iter.hasNext()) {                Map.Entry<String, JsonNode> entry = iter.next();                visitor.recordLevels.push(entry.getKey());                fields.put(entry.getKey(), visit(entry.getValue(), visitor));                visitor.recordLevels.pop();            }            return visitor.object((ObjectNode) node, fields);        case ARRAY:            Preconditions.checkArgument(node instanceof ArrayNode, "Expected instance of ArrayNode: " + node);            List<T> elements = Lists.newArrayListWithExpectedSize(node.size());            for (JsonNode element : node) {                elements.add(visit(element, visitor));            }            return visitor.array((ArrayNode) node, elements);        case BINARY:            Preconditions.checkArgument(node instanceof BinaryNode, "Expected instance of BinaryNode: " + node);            return visitor.binary((BinaryNode) node);        case STRING:            Preconditions.checkArgument(node instanceof TextNode, "Expected instance of TextNode: " + node);            return visitor.text((TextNode) node);        case NUMBER:            Preconditions.checkArgument(node instanceof NumericNode, "Expected instance of NumericNode: " + node);            return visitor.number((NumericNode) node);        case BOOLEAN:            Preconditions.checkArgument(node instanceof BooleanNode, "Expected instance of BooleanNode: " + node);            return visitor.bool((BooleanNode) node);        case MISSING:            Preconditions.checkArgument(node instanceof MissingNode, "Expected instance of MissingNode: " + node);            return visitor.missing((MissingNode) node);        case NULL:            Preconditions.checkArgument(node instanceof NullNode, "Expected instance of NullNode: " + node);            return visitor.nullNode((NullNode) node);        default:            throw new IllegalArgumentException("Unknown node type: " + node.getNodeType() + ": " + node);    }}
public T parquet-mr_f988_0(ObjectNode object, Map<String, T> fields)
{    return null;}
public T parquet-mr_f989_0(ArrayNode array, List<T> elements)
{    return null;}
public T parquet-mr_f990_0(BinaryNode binary)
{    return null;}
public T parquet-mr_f991_0(TextNode text)
{    return null;}
public T parquet-mr_f992_0(NumericNode number)
{    return null;}
public T parquet-mr_f993_0(BooleanNode bool)
{    return null;}
public T parquet-mr_f994_0(MissingNode missing)
{    return null;}
public T parquet-mr_f995_0(NullNode nullNode)
{    return null;}
public boolean parquet-mr_f996_0()
{    return iterator.hasNext();}
public E parquet-mr_f997_0()
{    return iterator.next();}
public void parquet-mr_f998_0()
{    iterator = null;    try {        stream.close();    } catch (IOException e) {        throw new RuntimeIOException("Cannot close reader", e);    }}
public void parquet-mr_f999_0()
{    throw new UnsupportedOperationException("Remove is not implemented.");}
public Iterator<E> parquet-mr_f1000_0()
{    return this;}
public int parquet-mr_f1001_0(String[] args) throws Exception
{    try {        jc.parse(args);    } catch (MissingCommandException e) {        console.error(e.getMessage());        return 1;    } catch (ParameterException e) {        help.setProgramName(programName);        String cmd = jc.getParsedCommand();        if (args.length == 1) {                        help.helpCommands.add(cmd);            help.run();            return 1;        } else {                        for (String arg : args) {                if (HELP_ARGS.contains(arg)) {                    help.helpCommands.add(cmd);                    help.run();                    return 0;                }            }        }        console.error(e.getMessage());        return 1;    }    help.setProgramName(programName);        if (debug) {        org.apache.log4j.Logger console = org.apache.log4j.Logger.getLogger(Main.class);        console.setLevel(Level.DEBUG);    }    String parsed = jc.getParsedCommand();    if (parsed == null) {        help.run();        return 1;    } else if ("help".equals(parsed)) {        return help.run();    }    Command command = (Command) jc.getCommands().get(parsed).getObjects().get(0);    if (command == null) {        help.run();        return 1;    }    try {        if (command instanceof Configurable) {            ((Configurable) command).setConf(getConf());        }        return command.run();    } catch (IllegalArgumentException e) {        if (debug) {            console.error("Argument error", e);        } else {            console.error("Argument error: {}", e.getMessage());        }        return 1;    } catch (IllegalStateException e) {        if (debug) {            console.error("State error", e);        } else {            console.error("State error: {}", e.getMessage());        }        return 1;    } catch (Exception e) {        console.error("Unknown error", e);        return 1;    }}
public static void parquet-mr_f1002_0(String[] args) throws Exception
{        PropertyConfigurator.configure(Main.class.getResource("/cli-logging.properties"));    Logger console = LoggerFactory.getLogger(Main.class);        LogFactory.getFactory().setAttribute("org.apache.commons.logging.Log", "org.apache.commons.logging.impl.Log4JLogger");    int rc = ToolRunner.run(new Configuration(), new Main(console), args);    System.exit(rc);}
public static CompressionCodecName parquet-mr_f1003_0(String codec)
{    try {        return CompressionCodecName.valueOf(codec.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {        throw new IllegalArgumentException("Unknown compression codec: " + codec);    }}
public static CodecFactory parquet-mr_f1004_0(String codec)
{    CompressionCodecName parquetCodec = parquetCodec(codec);    switch(parquetCodec) {        case UNCOMPRESSED:            return CodecFactory.nullCodec();        case SNAPPY:            return CodecFactory.snappyCodec();        case GZIP:            return CodecFactory.deflateCodec(9);        case ZSTD:            return CodecFactory.zstandardCodec(CodecFactory.DEFAULT_ZSTANDARD_LEVEL);        default:            throw new IllegalArgumentException("Codec incompatible with Avro: " + codec);    }}
public static Object parquet-mr_f1005_0(Schema schema, Object datum, String path)
{    return select(schema, datum, Lists.newArrayList(parse(path)));}
private static Object parquet-mr_f1006_0(Schema schema, Object datum, List<PathExpr> tokens)
{    if (tokens.isEmpty()) {        return datum;    }    Preconditions.checkArgument(tokens.size() == 1, "Cannot return multiple values");    PathExpr token = tokens.get(0);    switch(schema.getType()) {        case RECORD:            if (!(datum instanceof GenericRecord) && "json".equals(schema.getName())) {                                return select(schema.getField("value").schema(), datum, tokens);            }            Preconditions.checkArgument(token.type == PathExpr.Type.FIELD, "Cannot dereference records");            Preconditions.checkArgument(datum instanceof GenericRecord, "Not a record: %s", datum);            GenericRecord record = (GenericRecord) datum;            Schema.Field field = schema.getField(token.value);            Preconditions.checkArgument(field != null, "No such field '%s' in schema: %s", token.value, schema);            return select(field.schema(), record.get(token.value), token.children);        case MAP:            Preconditions.checkArgument(datum instanceof Map, "Not a map: %s", datum);            Map<Object, Object> map = (Map<Object, Object>) datum;            Object value = map.get(token.value);            if (value == null) {                                value = map.get(new Utf8(token.value));            }            return select(schema.getValueType(), value, token.children);        case ARRAY:            Preconditions.checkArgument(token.type == PathExpr.Type.DEREF, "Cannot access fields of an array");            Preconditions.checkArgument(datum instanceof Collection, "Not an array: %s", datum);            Preconditions.checkArgument(NUMERIC_RE.matcher(token.value).matches(), "Not an array index: %s", token.value);            List<Object> list = (List<Object>) datum;            return select(schema.getElementType(), list.get(Integer.parseInt(token.value)), token.children);        case UNION:            int branch = GenericData.get().resolveUnion(schema, datum);            return select(schema.getTypes().get(branch), datum, tokens);        default:            throw new IllegalArgumentException("Cannot access child of primitive value: " + datum);    }}
public static Schema parquet-mr_f1007_0(Schema schema, String... fieldPaths)
{    return filterSchema(schema, Lists.newArrayList(fieldPaths));}
public static Schema parquet-mr_f1008_0(Schema schema, List<String> fieldPaths)
{    if (fieldPaths == null) {        return schema;    }    List<PathExpr> paths = merge(Lists.newArrayList(fieldPaths));    return filter(schema, paths);}
private static PathExpr parquet-mr_f1009_0(String path)
{    PathExpr expr = null;    PathExpr last = null;    boolean inDeref = false;    boolean afterDeref = false;    int valueStart = 0;    for (int i = 0; i < path.length(); i += 1) {        switch(path.charAt(i)) {            case '.':                Preconditions.checkState(valueStart != i || afterDeref, "Empty reference: ''");                if (!inDeref) {                    if (valueStart != i) {                        PathExpr current = PathExpr.field(path.substring(valueStart, i));                        if (last != null) {                            last.children.add(current);                        } else {                            expr = current;                        }                        last = current;                    }                    valueStart = i + 1;                    afterDeref = false;                }                break;            case '[':                Preconditions.checkState(!inDeref, "Cannot nest [ within []");                Preconditions.checkState(valueStart != i || afterDeref, "Empty reference: ''");                if (valueStart != i) {                    PathExpr current = PathExpr.field(path.substring(valueStart, i));                    if (last != null) {                        last.children.add(current);                    } else {                        expr = current;                    }                    last = current;                }                valueStart = i + 1;                inDeref = true;                afterDeref = false;                break;            case ']':                Preconditions.checkState(inDeref, "Cannot use ] without a starting [");                Preconditions.checkState(valueStart != i, "Empty reference: ''");                PathExpr current = PathExpr.deref(path.substring(valueStart, i));                if (last != null) {                    last.children.add(current);                } else {                    expr = current;                }                last = current;                valueStart = i + 1;                inDeref = false;                afterDeref = true;                break;            default:                Preconditions.checkState(!afterDeref, "Fields after [] must start with .");        }    }    Preconditions.checkState(!inDeref, "Fields after [ must end with ]");    if (valueStart < path.length()) {        PathExpr current = PathExpr.field(path.substring(valueStart, path.length()));        if (last != null) {            last.children.add(current);        } else {            expr = current;        }    }    return expr;}
private static List<PathExpr> parquet-mr_f1010_0(List<String> fields)
{    List<PathExpr> paths = Lists.newArrayList();    for (String field : fields) {        merge(paths, parse(field));    }    return paths;}
private static List<PathExpr> parquet-mr_f1011_0(List<PathExpr> tokens, PathExpr toAdd)
{    boolean merged = false;    for (PathExpr token : tokens) {        if ((token.type == toAdd.type) && (token.type == PathExpr.Type.DEREF || token.value.equals(toAdd.value))) {            for (PathExpr child : toAdd.children) {                merge(token.children, child);            }            merged = true;        }    }    if (!merged) {        tokens.add(toAdd);    }    return tokens;}
private static Schema parquet-mr_f1012_0(Schema schema, List<PathExpr> exprs)
{    if (exprs.isEmpty()) {        return schema;    }    switch(schema.getType()) {        case RECORD:            List<Schema.Field> fields = Lists.newArrayList();            for (PathExpr expr : exprs) {                Schema.Field field = schema.getField(expr.value);                Preconditions.checkArgument(field != null, "Cannot find field '%s' in schema: %s", expr.value, schema);                fields.add(new Schema.Field(expr.value, filter(field.schema(), expr.children), field.doc(), field.defaultVal(), field.order()));            }            return Schema.createRecord(schema.getName(), schema.getDoc(), schema.getNamespace(), schema.isError(), fields);        case UNION:                        if (schema.getTypes().size() == 2) {                if (schema.getTypes().get(0).getType() == Schema.Type.NULL) {                    return filter(schema.getTypes().get(1), exprs);                } else if (schema.getTypes().get(1).getType() == Schema.Type.NULL) {                    return filter(schema.getTypes().get(0), exprs);                }            }            List<Schema> schemas = Lists.newArrayList();            for (PathExpr expr : exprs) {                schemas.add(filter(schema, expr));            }            if (schemas.size() > 1) {                return Schema.createUnion(schemas);            } else {                return schemas.get(0);            }        case MAP:            Preconditions.checkArgument(exprs.size() == 1, "Cannot find multiple children of map schema: %s", schema);            return filter(schema, exprs.get(0));        case ARRAY:            Preconditions.checkArgument(exprs.size() == 1, "Cannot find multiple children of array schema: %s", schema);            return filter(schema, exprs.get(0));        default:            throw new IllegalArgumentException(String.format("Cannot find child of primitive schema: %s", schema));    }}
private static Schema parquet-mr_f1013_0(Schema schema, PathExpr expr)
{    if (expr == null) {        return schema;    }    switch(schema.getType()) {        case RECORD:            Preconditions.checkArgument(expr.type == PathExpr.Type.FIELD, "Cannot index a record: [%s]", expr.value);            Schema.Field field = schema.getField(expr.value);            if (field != null) {                return filter(field.schema(), expr.children);            } else {                throw new IllegalArgumentException(String.format("Cannot find field '%s' in schema: %s", expr.value, schema.toString(true)));            }        case MAP:            return Schema.createMap(filter(schema.getValueType(), expr.children));        case ARRAY:            Preconditions.checkArgument(expr.type == PathExpr.Type.DEREF, "Cannot find field '%s' in an array", expr.value);            Preconditions.checkArgument(NUMERIC_RE.matcher(expr.value).matches(), "Cannot index array by non-numeric value '%s'", expr.value);            return Schema.createArray(filter(schema.getElementType(), expr.children));        case UNION:                                    Preconditions.checkArgument(expr.type == PathExpr.Type.DEREF, "Cannot find field '%s' in a union", expr.value);            List<Schema> options = schema.getTypes();            if (NUMERIC_RE.matcher(expr.value).matches()) {                                int i = Integer.parseInt(expr.value);                if (i < options.size()) {                    return filter(options.get(i), expr.children);                }            } else {                                for (Schema option : options) {                    if (expr.value.equalsIgnoreCase(option.getName())) {                        return filter(option, expr.children);                    }                }            }            throw new IllegalArgumentException(String.format("Invalid union index '%s' for schema: %s", expr.value, schema));        default:            throw new IllegalArgumentException(String.format("Cannot find '%s' in primitive schema: %s", expr.value, schema));    }}
 static PathExpr parquet-mr_f1014_0(String value)
{    return new PathExpr(Type.DEREF, value);}
 static PathExpr parquet-mr_f1015_0(String value, PathExpr child)
{    return new PathExpr(Type.DEREF, value, Lists.newArrayList(child));}
 static PathExpr parquet-mr_f1016_0(String value)
{    return new PathExpr(Type.FIELD, value);}
 static PathExpr parquet-mr_f1017_0(String value, PathExpr child)
{    return new PathExpr(Type.FIELD, value, Lists.newArrayList(child));}
public boolean parquet-mr_f1018_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    PathExpr pathExpr = (PathExpr) o;    if (type != pathExpr.type)        return false;    if (value != null ? !value.equals(pathExpr.value) : pathExpr.value != null)        return false;    return children != null ? children.equals(pathExpr.children) : pathExpr.children == null;}
public int parquet-mr_f1019_0()
{    int result = type != null ? type.hashCode() : 0;    result = 31 * result + (value != null ? value.hashCode() : 0);    result = 31 * result + (children != null ? children.hashCode() : 0);    return result;}
public String parquet-mr_f1020_0()
{    return MoreObjects.toStringHelper(this).add("type", type).add("value", value).add("children", children).toString();}
public static Format parquet-mr_f1021_0(InputStream stream) throws IOException
{    byte[] first3 = new byte[3];    stream.read(first3);    if (Arrays.equals(first3, new byte[] { 'P', 'A', 'R' })) {        return Format.PARQUET;    } else if (Arrays.equals(first3, new byte[] { 'O', 'b', 'j' })) {        return Format.AVRO;    } else if (Arrays.equals(first3, new byte[] { 'S', 'E', 'Q' })) {        return Format.SEQUENCE;    } else {        return Format.TEXT;    }}
public ClassLoader parquet-mr_f1022_0()
{    return new URLClassLoader(urls, Thread.currentThread().getContextClassLoader());}
public static void parquet-mr_f1023_0(boolean isValid, String message, Object... args)
{    if (!isValid) {        String[] argStrings = new String[args.length];        for (int i = 0; i < args.length; i += 1) {            argStrings[i] = String.valueOf(args[i]);        }        throw new RecordException(String.format(String.valueOf(message), (Object[]) argStrings));    }}
public static Schema parquet-mr_f1024_0(InputStream in) throws IOException
{        return new Schema.Parser().parse(in);}
public static Schema parquet-mr_f1025_0(InputStream in) throws IOException
{    GenericDatumReader<GenericRecord> datumReader = new GenericDatumReader<GenericRecord>();    DataFileStream<GenericRecord> stream = null;    boolean threw = true;    try {        stream = new DataFileStream<>(in, datumReader);        Schema schema = stream.getSchema();        threw = false;        return schema;    } finally {        Closeables.close(stream, threw);    }}
public static Schema parquet-mr_f1026_0(Configuration conf, URI location) throws IOException
{    Path path = new Path(location);    FileSystem fs = path.getFileSystem(conf);    ParquetMetadata footer = ParquetFileReader.readFooter(fs.getConf(), path);    String schemaString = footer.getFileMetaData().getKeyValueMetaData().get("parquet.avro.schema");    if (schemaString == null) {                schemaString = footer.getFileMetaData().getKeyValueMetaData().get("avro.schema");    }    if (schemaString != null) {        return new Schema.Parser().parse(schemaString);    } else {        return new AvroSchemaConverter().convert(footer.getFileMetaData().getSchema());    }}
public static Schema parquet-mr_f1027_0(String name, InputStream in) throws IOException
{    return AvroJson.inferSchema(in, name, 20);}
public static boolean parquet-mr_f1028_0(Schema schema)
{    if (Schema.Type.NULL == schema.getType()) {        return true;    } else if (Schema.Type.UNION == schema.getType()) {        for (Schema possible : schema.getTypes()) {            if (nullOk(possible)) {                return true;            }        }    }    return false;}
public static Schema parquet-mr_f1029_0(Iterable<Schema> schemas)
{    Iterator<Schema> iter = schemas.iterator();    if (!iter.hasNext()) {        return null;    }    Schema result = iter.next();    while (iter.hasNext()) {        result = merge(result, iter.next());    }    return result;}
public static Schema parquet-mr_f1030_0(Iterable<Schema> schemas)
{    Iterator<Schema> iter = schemas.iterator();    if (!iter.hasNext()) {        return null;    }    Schema result = iter.next();    while (iter.hasNext()) {        result = mergeOrUnion(result, iter.next());    }    return result;}
public static Schema parquet-mr_f1031_0(Schema left, Schema right)
{    Schema merged = mergeOnly(left, right);    Preconditions.checkState(merged != null, "Cannot merge %s and %s", left, right);    return merged;}
private static Schema parquet-mr_f1032_0(Schema left, Schema right)
{    Schema merged = mergeOnly(left, right);    if (merged != null) {        return merged;    }    return union(left, right);}
private static Schema parquet-mr_f1033_0(Schema left, Schema right)
{    if (left.getType() == Schema.Type.UNION) {        if (right.getType() == Schema.Type.UNION) {                        Schema combined = left;            for (Schema type : right.getTypes()) {                combined = union(combined, type);            }            return combined;        } else {            boolean notMerged = true;                        List<Schema> types = Lists.newArrayList();            Iterator<Schema> schemas = left.getTypes().iterator();                        while (schemas.hasNext()) {                Schema next = schemas.next();                Schema merged = mergeOnly(next, right);                if (merged != null) {                    types.add(merged);                    notMerged = false;                    break;                } else {                                        types.add(next);                }            }                        while (schemas.hasNext()) {                types.add(schemas.next());            }            if (notMerged) {                types.add(right);            }            return Schema.createUnion(types);        }    } else if (right.getType() == Schema.Type.UNION) {        return union(right, left);    }    return Schema.createUnion(ImmutableList.of(left, right));}
private static Schema parquet-mr_f1034_0(Schema left, Schema right)
{    if (Objects.equal(left, right)) {        return left;    }        switch(left.getType()) {        case INT:            if (right.getType() == Schema.Type.LONG) {                return right;            }            break;        case LONG:            if (right.getType() == Schema.Type.INT) {                return left;            }            break;        case FLOAT:            if (right.getType() == Schema.Type.DOUBLE) {                return right;            }            break;        case DOUBLE:            if (right.getType() == Schema.Type.FLOAT) {                return left;            }    }        if (left.getType() != right.getType()) {        return null;    }    switch(left.getType()) {        case UNION:            return union(left, right);        case RECORD:            if (left.getName() == null && right.getName() == null && fieldSimilarity(left, right) < SIMILARITY_THRESH) {                return null;            } else if (!Objects.equal(left.getName(), right.getName())) {                return null;            }            Schema combinedRecord = Schema.createRecord(coalesce(left.getName(), right.getName()), coalesce(left.getDoc(), right.getDoc()), coalesce(left.getNamespace(), right.getNamespace()), false);            combinedRecord.setFields(mergeFields(left, right));            return combinedRecord;        case MAP:            return Schema.createMap(mergeOrUnion(left.getValueType(), right.getValueType()));        case ARRAY:            return Schema.createArray(mergeOrUnion(left.getElementType(), right.getElementType()));        case ENUM:            if (!Objects.equal(left.getName(), right.getName())) {                return null;            }            Set<String> symbols = Sets.newLinkedHashSet();            symbols.addAll(left.getEnumSymbols());            symbols.addAll(right.getEnumSymbols());            return Schema.createEnum(left.getName(), coalesce(left.getDoc(), right.getDoc()), coalesce(left.getNamespace(), right.getNamespace()), ImmutableList.copyOf(symbols));        default:                        throw new UnsupportedOperationException("Unknown schema type: " + left.getType());    }}
private static Schema parquet-mr_f1035_0(Schema schema)
{    if (schema.getType() == Schema.Type.NULL) {        return schema;    }    if (schema.getType() != Schema.Type.UNION) {        return Schema.createUnion(ImmutableList.of(NULL, schema));    }    if (schema.getTypes().get(0).getType() == Schema.Type.NULL) {        return schema;    }    List<Schema> types = Lists.newArrayList();    types.add(NULL);    for (Schema type : schema.getTypes()) {        if (type.getType() != Schema.Type.NULL) {            types.add(type);        }    }    return Schema.createUnion(types);}
private static List<Schema.Field> parquet-mr_f1036_0(Schema left, Schema right)
{    List<Schema.Field> fields = Lists.newArrayList();    for (Schema.Field leftField : left.getFields()) {        Schema.Field rightField = right.getField(leftField.name());        if (rightField != null) {            fields.add(new Schema.Field(leftField.name(), mergeOrUnion(leftField.schema(), rightField.schema()), coalesce(leftField.doc(), rightField.doc()), coalesce(leftField.defaultVal(), rightField.defaultVal())));        } else {            if (leftField.defaultVal() != null) {                fields.add(copy(leftField));            } else {                fields.add(new Schema.Field(leftField.name(), nullableForDefault(leftField.schema()), leftField.doc(), NULL_DEFAULT));            }        }    }    for (Schema.Field rightField : right.getFields()) {        if (left.getField(rightField.name()) == null) {            if (rightField.defaultVal() != null) {                fields.add(copy(rightField));            } else {                fields.add(new Schema.Field(rightField.name(), nullableForDefault(rightField.schema()), rightField.doc(), NULL_DEFAULT));            }        }    }    return fields;}
public static Schema.Field parquet-mr_f1037_0(Schema.Field field)
{    return new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultVal());}
private static float parquet-mr_f1038_0(Schema left, Schema right)
{        Set<String> leftNames = names(left.getFields());    Set<String> rightNames = names(right.getFields());    int common = Sets.intersection(leftNames, rightNames).size();    float leftRatio = ((float) common) / ((float) leftNames.size());    float rightRatio = ((float) common) / ((float) rightNames.size());    return hmean(leftRatio, rightRatio);}
private static Set<String> parquet-mr_f1039_0(Collection<Schema.Field> fields)
{    Set<String> names = Sets.newHashSet();    for (Schema.Field field : fields) {        names.add(field.name());    }    return names;}
private static float parquet-mr_f1040_0(float left, float right)
{    return (2.0f * left * right) / (left + right);}
private static E parquet-mr_f1041_0(E... objects)
{    for (E object : objects) {        if (object != null) {            return object;        }    }    return null;}
public void parquet-mr_f1042_0(long p) throws IOException
{    in.seek(p);}
public long parquet-mr_f1043_0() throws IOException
{    return in.getPos();}
public long parquet-mr_f1044_0() throws IOException
{    return stat.getLen();}
public int parquet-mr_f1045_0(byte[] b) throws IOException
{    return in.read(b);}
public int parquet-mr_f1046_0() throws IOException
{    return in.read();}
public int parquet-mr_f1047_0(byte[] b, int off, int len) throws IOException
{    return in.read(b, off, len);}
public void parquet-mr_f1048_0() throws IOException
{    in.close();}
public static String parquet-mr_f1049_0(float bytes)
{    if (bytes > TB) {        return String.format("%.03f TB", bytes / TB);    } else if (bytes > GB) {        return String.format("%.03f GB", bytes / GB);    } else if (bytes > MB) {        return String.format("%.03f MB", bytes / MB);    } else if (bytes > KB) {        return String.format("%.03f kB", bytes / KB);    } else {        return String.format("%.02f B", bytes);    }}
public static String parquet-mr_f1050_0(long bytes)
{    if (bytes > TB) {        return String.format("%.03f TB", ((float) bytes) / TB);    } else if (bytes > GB) {        return String.format("%.03f GB", ((float) bytes) / GB);    } else if (bytes > MB) {        return String.format("%.03f MB", ((float) bytes) / MB);    } else if (bytes > KB) {        return String.format("%.03f kB", ((float) bytes) / KB);    } else {        return String.format("%d B", bytes);    }}
public static String parquet-mr_f1051_0(Statistics stats, OriginalType annotation)
{    return minMaxAsString(stats);}
public static String parquet-mr_f1052_0(Statistics stats)
{    if (stats == null) {        return "no stats";    }    if (!stats.hasNonNullValue()) {        return "";    }    return String.format("%s / %s", humanReadable(stats.minAsString(), 30), humanReadable(stats.maxAsString(), 30));}
public static String parquet-mr_f1053_0(Statistics stats, long count, OriginalType annotation)
{    return toString(stats, count);}
public static String parquet-mr_f1054_0(Statistics stats, long count)
{    if (stats == null) {        return "no stats";    }    return String.format("min: %s max: %s nulls: %d/%d", humanReadable(stats.minAsString(), 30), humanReadable(stats.maxAsString(), 30), stats.getNumNulls(), count);}
public static String parquet-mr_f1055_0(String str, int len)
{    if (str == null) {        return "null";    }    StringBuilder sb = new StringBuilder();    sb.append("\"");    if (str.length() > len - 2) {        sb.append(str.substring(0, len - 5)).append("...");    } else {        sb.append(str);    }    sb.append("\"");    return sb.toString();}
public static String parquet-mr_f1056_0(byte[] bytes, int len)
{    Preconditions.checkArgument(len >= 5, "Display length must be minimum 5");    if (bytes == null || bytes.length == 0) {        return "null";    }    final String asString = HashCode.fromBytes(bytes).toString();    return "0x" + Ascii.truncate(asString, len - 2, "...");}
public static String parquet-mr_f1057_0(CompressionCodecName codec)
{    switch(codec) {        case UNCOMPRESSED:            return "_";        case SNAPPY:            return "S";        case GZIP:            return "G";        case LZO:            return "L";        case BROTLI:            return "B";        case LZ4:            return "4";        case ZSTD:            return "Z";        default:            return "?";    }}
public static String parquet-mr_f1058_0(Encoding encoding, boolean isDict)
{    switch(encoding) {        case PLAIN:            return "_";        case PLAIN_DICTIONARY:                        return isDict ? "_" : "R";        case RLE_DICTIONARY:            return "R";        case DELTA_BINARY_PACKED:        case DELTA_LENGTH_BYTE_ARRAY:        case DELTA_BYTE_ARRAY:            return "D";        default:            return "?";    }}
public static String parquet-mr_f1059_0(EncodingStats encodingStats)
{    StringBuilder sb = new StringBuilder();    if (encodingStats.hasDictionaryPages()) {        for (Encoding encoding : encodingStats.getDictionaryEncodings()) {            sb.append(encodingAsString(encoding, true));        }        sb.append(" ");    } else {        sb.append("  ");    }    Set<Encoding> encodings = encodingStats.getDataEncodings();    if (encodings.contains(RLE_DICTIONARY) || encodings.contains(PLAIN_DICTIONARY)) {        sb.append("R");    }    if (encodings.contains(PLAIN)) {        sb.append("_");    }    if (encodings.contains(DELTA_BYTE_ARRAY) || encodings.contains(DELTA_BINARY_PACKED) || encodings.contains(DELTA_LENGTH_BYTE_ARRAY)) {        sb.append("D");    }        if (encodingStats.hasDictionaryEncodedPages() && encodingStats.hasNonDictionaryEncodedPages()) {        sb.append(" F");    }    return sb.toString();}
public static String parquet-mr_f1060_0(Set<Encoding> encodings, ColumnDescriptor desc)
{    StringBuilder sb = new StringBuilder();    if (encodings.contains(RLE) || encodings.contains(BIT_PACKED)) {        sb.append(desc.getMaxDefinitionLevel() == 0 ? "B" : "R");        sb.append(desc.getMaxRepetitionLevel() == 0 ? "B" : "R");        if (encodings.contains(PLAIN_DICTIONARY)) {            sb.append("R");        }        if (encodings.contains(PLAIN)) {            sb.append("_");        }    } else {        sb.append("RR");        if (encodings.contains(RLE_DICTIONARY)) {            sb.append("R");        }        if (encodings.contains(PLAIN)) {            sb.append("_");        }        if (encodings.contains(DELTA_BYTE_ARRAY) || encodings.contains(DELTA_BINARY_PACKED) || encodings.contains(DELTA_LENGTH_BYTE_ARRAY)) {            sb.append("D");        }    }    return sb.toString();}
public static ColumnDescriptor parquet-mr_f1061_0(String column, MessageType schema)
{    String[] path = Iterables.toArray(DOT.split(column), String.class);    Preconditions.checkArgument(schema.containsPath(path), "Schema doesn't have column: " + column);    return schema.getColumnDescription(path);}
public static String parquet-mr_f1062_0(ColumnDescriptor desc)
{    return Joiner.on('.').join(desc.getPath());}
public static PrimitiveType parquet-mr_f1063_0(MessageType schema, String[] path)
{    Type current = schema;    for (String part : path) {        current = current.asGroupType().getType(part);        if (current.isPrimitive()) {            return current.asPrimitiveType();        }    }    return null;}
public static PrimitiveType parquet-mr_f1064_0(String column, MessageType schema)
{    String[] path = Iterables.toArray(DOT.split(column), String.class);    Preconditions.checkArgument(schema.containsPath(path), "Schema doesn't have column: " + column);    return primitive(schema, path);}
public void parquet-mr_f1065_0()
{    this.command = new TestCommand(this.console);}
public void parquet-mr_f1066_0() throws IOException
{    Path path = this.command.qualifiedPath(FILE_PATH);    Assert.assertEquals("test.parquet", path.getName());}
public void parquet-mr_f1067_0() throws IOException
{    URI uri = this.command.qualifiedURI(FILE_PATH);    Assert.assertEquals("/var/tmp/test.parquet", uri.getPath());}
public void parquet-mr_f1068_0() throws IOException
{    URI uri = this.command.qualifiedURI("resource:/a");    Assert.assertEquals("/a", uri.getPath());}
public void parquet-mr_f1069_0() throws IOException
{    Assume.assumeTrue(System.getProperty("os.name").toLowerCase().startsWith("win"));    Path path = this.command.qualifiedPath(WIN_FILE_PATH);    Assert.assertEquals("test.parquet", path.getName());}
public void parquet-mr_f1070_0() throws IOException
{    Assume.assumeTrue(System.getProperty("os.name").toLowerCase().startsWith("win"));    URI uri = this.command.qualifiedURI(WIN_FILE_PATH);    Assert.assertEquals("/C:/Test/Downloads/test.parquet", uri.getPath());}
public int parquet-mr_f1071_0() throws IOException
{    return 0;}
public List<String> parquet-mr_f1072_0()
{    return null;}
protected File parquet-mr_f1073_0(File parquetFile) throws IOException
{    return toAvro(parquetFile, "GZIP");}
protected File parquet-mr_f1074_0(File parquetFile, String compressionCodecName) throws IOException
{    ToAvroCommand command = new ToAvroCommand(createLogger());    command.targets = Arrays.asList(parquetFile.getAbsolutePath());    File output = new File(getTempFolder(), getClass().getSimpleName() + ".avro");    command.outputPath = output.getAbsolutePath();    command.compressionCodecName = compressionCodecName;    command.setConf(new Configuration());    int exitCode = command.run();    assert (exitCode == 0);    return output;}
public void parquet-mr_f1075_0() throws IOException
{    File file = parquetFile();    CatCommand command = new CatCommand(createLogger(), 0);    command.sourceFiles = Arrays.asList(file.getAbsolutePath());    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
public void parquet-mr_f1076_0() throws IOException
{    File file = parquetFile();    CheckParquet251Command command = new CheckParquet251Command(createLogger());    command.files = Arrays.asList(file.getAbsolutePath());    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
public void parquet-mr_f1077_0() throws IOException
{    File file = toAvro(parquetFile());    ConvertCommand command = new ConvertCommand(createLogger());    command.targets = Arrays.asList(file.getAbsolutePath());    File output = new File(getTempFolder(), "converted.avro");    command.outputPath = output.getAbsolutePath();    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());    Assert.assertTrue(output.exists());}
public void parquet-mr_f1078_0() throws IOException
{    File file = csvFile();    ConvertCSVCommand command = new ConvertCSVCommand(createLogger());    command.targets = Arrays.asList(file.getAbsolutePath());    File output = new File(getTempFolder(), getClass().getSimpleName() + ".parquet");    command.outputPath = output.getAbsolutePath();    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());    Assert.assertTrue(output.exists());}
public void parquet-mr_f1079_0() throws IOException
{    createTestCSVFile();}
protected File parquet-mr_f1080_0()
{    File tmpDir = getTempFolder();    return new File(tmpDir, getClass().getSimpleName() + ".csv");}
private void parquet-mr_f1081_0() throws IOException
{    File file = csvFile();    try (BufferedWriter writer = new BufferedWriter(new FileWriter(file))) {        writer.write(String.format("%s,%s,%s\n", INT32_FIELD, INT64_FIELD, BINARY_FIELD));        writer.write(String.format("%d,%d,\"%s\"\n", Integer.MIN_VALUE, Long.MIN_VALUE, COLORS[0]));        writer.write(String.format("%d,%d,\"%s\"\n", Integer.MAX_VALUE, Long.MAX_VALUE, COLORS[1]));    }}
public void parquet-mr_f1082_0() throws IOException
{    File file = csvFile();    CSVSchemaCommand command = new CSVSchemaCommand(createLogger());    command.samplePaths = Arrays.asList(file.getAbsolutePath());    command.recordName = "Test";    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
protected File parquet-mr_f1083_0()
{    return this.tempFolder.getRoot();}
protected static Logger parquet-mr_f1084_0()
{    PropertyConfigurator.configure(ParquetFileTest.class.getResource("/cli-logging.properties"));    Logger console = LoggerFactory.getLogger(ParquetFileTest.class);    LogFactory.getFactory().setAttribute("org.apache.commons.logging.Log", "org.apache.commons.logging.impl.Log4JLogger");    return console;}
public void parquet-mr_f1085_0() throws IOException
{    createTestParquetFile();}
protected File parquet-mr_f1086_0()
{    File tmpDir = getTempFolder();    return new File(tmpDir, getClass().getSimpleName() + ".parquet");}
private static MessageType parquet-mr_f1087_0()
{    return new MessageType("schema", new PrimitiveType(REQUIRED, PrimitiveTypeName.INT32, INT32_FIELD), new PrimitiveType(REQUIRED, PrimitiveTypeName.INT64, INT64_FIELD), new PrimitiveType(REQUIRED, PrimitiveTypeName.FLOAT, FLOAT_FIELD), new PrimitiveType(REQUIRED, PrimitiveTypeName.DOUBLE, DOUBLE_FIELD), new PrimitiveType(REQUIRED, PrimitiveTypeName.BINARY, BINARY_FIELD), new PrimitiveType(REQUIRED, PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, 12, FIXED_LEN_BYTE_ARRAY_FIELD));}
private void parquet-mr_f1088_0() throws IOException
{    File file = parquetFile();    Path fsPath = new Path(file.getPath());    Configuration conf = new Configuration();    MessageType schema = createSchema();    SimpleGroupFactory fact = new SimpleGroupFactory(schema);    GroupWriteSupport.setSchema(schema, conf);    try (ParquetWriter<Group> writer = new ParquetWriter<>(fsPath, new GroupWriteSupport(), CompressionCodecName.UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetProperties.WriterVersion.PARQUET_2_0, conf)) {        for (int i = 0; i < 10; i++) {            final byte[] bytes = new byte[12];            ThreadLocalRandom.current().nextBytes(bytes);            writer.write(fact.newGroup().append(INT32_FIELD, 32 + i).append(INT64_FIELD, 64L + i).append(FLOAT_FIELD, 1.0f + i).append(DOUBLE_FIELD, 2.0d + i).append(BINARY_FIELD, Binary.fromString(COLORS[i % COLORS.length])).append(FIXED_LEN_BYTE_ARRAY_FIELD, Binary.fromConstantByteArray(bytes)));        }    }}
public void parquet-mr_f1089_0() throws IOException
{    File file = parquetFile();    ParquetMetadataCommand command = new ParquetMetadataCommand(createLogger());    command.targets = Arrays.asList(file.getAbsolutePath());    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
public void parquet-mr_f1090_0() throws IOException
{    File file = parquetFile();    SchemaCommand command = new SchemaCommand(createLogger());    command.targets = Arrays.asList(file.getAbsolutePath());    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
public void parquet-mr_f1091_0() throws IOException
{    File file = parquetFile();    ShowColumnIndexCommand command = new ShowColumnIndexCommand(createLogger());    command.files = Arrays.asList(file.getAbsolutePath());    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
public void parquet-mr_f1092_0() throws IOException
{    File file = parquetFile();    ShowDictionaryCommand command = new ShowDictionaryCommand(createLogger());    command.targets = Arrays.asList(file.getAbsolutePath());    command.column = BINARY_FIELD;    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
public void parquet-mr_f1093_0() throws IOException
{    File file = parquetFile();    ShowPagesCommand command = new ShowPagesCommand(createLogger());    command.targets = Arrays.asList(file.getAbsolutePath());    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
public void parquet-mr_f1094_0() throws IOException
{    File avroFile = toAvro(parquetFile());    Assert.assertTrue(avroFile.exists());}
public void parquet-mr_f1095_0() throws IOException
{    final File jsonInputFile = folder.newFile("sample.json");    final File avroOutputFile = folder.newFile("sample.avro");        final String inputJson = "{\"id\": 1, \"name\": \"Alice\"}\n" + "{\"id\": 2, \"name\": \"Bob\"}\n" + "{\"id\": 3, \"name\": \"Carol\"}\n" + "{\"id\": 4, \"name\": \"Dave\"}";    try (BufferedWriter writer = new BufferedWriter(new FileWriter(jsonInputFile))) {        writer.write(inputJson);    }    ToAvroCommand cmd = new ToAvroCommand(null);    JCommander.newBuilder().addObject(cmd).build().parse(jsonInputFile.getAbsolutePath(), "--output", avroOutputFile.getAbsolutePath());    assert (cmd.run() == 0);}
public void parquet-mr_f1096_0() throws IOException
{    File avroFile = toAvro(parquetFile(), "GZIP");    Assert.assertTrue(avroFile.exists());}
public void parquet-mr_f1097_0() throws IOException
{    File avroFile = toAvro(parquetFile(), "SNAPPY");    Assert.assertTrue(avroFile.exists());}
public void parquet-mr_f1098_0() throws IOException
{    File avroFile = toAvro(parquetFile(), "ZSTD");    Assert.assertTrue(avroFile.exists());}
public void parquet-mr_f1099_0() throws IOException
{    toAvro(parquetFile(), "FOO");}
public String[] parquet-mr_f1100_0()
{    return path;}
public int parquet-mr_f1101_0()
{    return maxRep;}
public int parquet-mr_f1102_0()
{    return maxDef;}
public PrimitiveTypeName parquet-mr_f1103_0()
{    return type.getPrimitiveTypeName();}
public int parquet-mr_f1104_0()
{    return type.getTypeLength();}
public PrimitiveType parquet-mr_f1105_0()
{    return type;}
public int parquet-mr_f1106_0()
{    return Arrays.hashCode(path);}
public boolean parquet-mr_f1107_0(Object other)
{    if (other == this)        return true;    if (!(other instanceof ColumnDescriptor))        return false;    ColumnDescriptor descriptor = (ColumnDescriptor) other;    return Arrays.equals(path, descriptor.path);}
public int parquet-mr_f1108_0(ColumnDescriptor o)
{    int length = path.length < o.path.length ? path.length : o.path.length;    for (int i = 0; i < length; i++) {        int compareTo = path[i].compareTo(o.path[i]);        if (compareTo != 0) {            return compareTo;        }    }    return path.length - o.path.length;}
public String parquet-mr_f1109_0()
{    return Arrays.toString(path) + " " + type;}
 boolean parquet-mr_f1110_0()
{    return false;}
public Encoding parquet-mr_f1111_0()
{    return encoding;}
public Binary parquet-mr_f1112_0(int id)
{    throw new UnsupportedOperationException(this.getClass().getName());}
public int parquet-mr_f1113_0(int id)
{    throw new UnsupportedOperationException(this.getClass().getName());}
public long parquet-mr_f1114_0(int id)
{    throw new UnsupportedOperationException(this.getClass().getName());}
public float parquet-mr_f1115_0(int id)
{    throw new UnsupportedOperationException(this.getClass().getName());}
public double parquet-mr_f1116_0(int id)
{    throw new UnsupportedOperationException(this.getClass().getName());}
public boolean parquet-mr_f1117_0(int id)
{    throw new UnsupportedOperationException(this.getClass().getName());}
 int parquet-mr_f1118_0(ColumnDescriptor descriptor, ValuesType valuesType)
{    int maxLevel;    switch(valuesType) {        case REPETITION_LEVEL:            maxLevel = descriptor.getMaxRepetitionLevel();            break;        case DEFINITION_LEVEL:            maxLevel = descriptor.getMaxDefinitionLevel();            break;        case VALUES:            if (descriptor.getType() == BOOLEAN) {                maxLevel = 1;                break;            }        default:            throw new ParquetDecodingException("Unsupported encoding for values: " + this);    }    return maxLevel;}
public boolean parquet-mr_f1119_0()
{    return false;}
public Dictionary parquet-mr_f1120_0(ColumnDescriptor descriptor, DictionaryPage dictionaryPage) throws IOException
{    throw new UnsupportedOperationException(this.name() + " does not support dictionary");}
public ValuesReader parquet-mr_f1121_0(ColumnDescriptor descriptor, ValuesType valuesType)
{    throw new UnsupportedOperationException("Error decoding " + descriptor + ". " + this.name() + " is dictionary based");}
public ValuesReader parquet-mr_f1122_0(ColumnDescriptor descriptor, ValuesType valuesType, Dictionary dictionary)
{    throw new UnsupportedOperationException(this.name() + " is not dictionary based");}
public ValuesReader parquet-mr_f1123_0(ColumnDescriptor descriptor, ValuesType valuesType)
{    switch(descriptor.getType()) {        case BOOLEAN:            return new BooleanPlainValuesReader();        case BINARY:            return new BinaryPlainValuesReader();        case FLOAT:            return new FloatPlainValuesReader();        case DOUBLE:            return new DoublePlainValuesReader();        case INT32:            return new IntegerPlainValuesReader();        case INT64:            return new LongPlainValuesReader();        case INT96:            return new FixedLenByteArrayPlainValuesReader(12);        case FIXED_LEN_BYTE_ARRAY:            return new FixedLenByteArrayPlainValuesReader(descriptor.getTypeLength());        default:            throw new ParquetDecodingException("no plain reader for type " + descriptor.getType());    }}
public Dictionary parquet-mr_f1124_0(ColumnDescriptor descriptor, DictionaryPage dictionaryPage) throws IOException
{    switch(descriptor.getType()) {        case BINARY:            return new PlainBinaryDictionary(dictionaryPage);        case FIXED_LEN_BYTE_ARRAY:            return new PlainBinaryDictionary(dictionaryPage, descriptor.getTypeLength());        case INT96:            return new PlainBinaryDictionary(dictionaryPage, 12);        case INT64:            return new PlainLongDictionary(dictionaryPage);        case DOUBLE:            return new PlainDoubleDictionary(dictionaryPage);        case INT32:            return new PlainIntegerDictionary(dictionaryPage);        case FLOAT:            return new PlainFloatDictionary(dictionaryPage);        default:            throw new ParquetDecodingException("Dictionary encoding not supported for type: " + descriptor.getType());    }}
public ValuesReader parquet-mr_f1125_0(ColumnDescriptor descriptor, ValuesType valuesType)
{    int bitWidth = BytesUtils.getWidthFromMaxInt(getMaxLevel(descriptor, valuesType));    if (bitWidth == 0) {        return new ZeroIntegerValuesReader();    }    return new RunLengthBitPackingHybridValuesReader(bitWidth);}
public ValuesReader parquet-mr_f1126_0(ColumnDescriptor descriptor, ValuesType valuesType)
{    return new ByteBitPackingValuesReader(getMaxLevel(descriptor, valuesType), BIG_ENDIAN);}
public ValuesReader parquet-mr_f1127_0(ColumnDescriptor descriptor, ValuesType valuesType, Dictionary dictionary)
{    return RLE_DICTIONARY.getDictionaryBasedValuesReader(descriptor, valuesType, dictionary);}
public Dictionary parquet-mr_f1128_0(ColumnDescriptor descriptor, DictionaryPage dictionaryPage) throws IOException
{    return PLAIN.initDictionary(descriptor, dictionaryPage);}
public boolean parquet-mr_f1129_0()
{    return true;}
public ValuesReader parquet-mr_f1130_0(ColumnDescriptor descriptor, ValuesType valuesType)
{    if (descriptor.getType() != INT32 && descriptor.getType() != INT64) {        throw new ParquetDecodingException("Encoding DELTA_BINARY_PACKED is only supported for type INT32 and INT64");    }    return new DeltaBinaryPackingValuesReader();}
public ValuesReader parquet-mr_f1131_0(ColumnDescriptor descriptor, ValuesType valuesType)
{    if (descriptor.getType() != BINARY) {        throw new ParquetDecodingException("Encoding DELTA_LENGTH_BYTE_ARRAY is only supported for type BINARY");    }    return new DeltaLengthByteArrayValuesReader();}
public ValuesReader parquet-mr_f1132_0(ColumnDescriptor descriptor, ValuesType valuesType)
{    if (descriptor.getType() != BINARY && descriptor.getType() != FIXED_LEN_BYTE_ARRAY) {        throw new ParquetDecodingException("Encoding DELTA_BYTE_ARRAY is only supported for type BINARY and FIXED_LEN_BYTE_ARRAY");    }    return new DeltaByteArrayReader();}
public ValuesReader parquet-mr_f1133_0(ColumnDescriptor descriptor, ValuesType valuesType, Dictionary dictionary)
{    switch(descriptor.getType()) {        case BINARY:        case FIXED_LEN_BYTE_ARRAY:        case INT96:        case INT64:        case DOUBLE:        case INT32:        case FLOAT:            return new DictionaryValuesReader(dictionary);        default:            throw new ParquetDecodingException("Dictionary encoding not supported for type: " + descriptor.getType());    }}
public boolean parquet-mr_f1134_0()
{    return true;}
public Set<Encoding> parquet-mr_f1135_0()
{    return dictStats.keySet();}
public Set<Encoding> parquet-mr_f1136_0()
{    return dataStats.keySet();}
public int parquet-mr_f1137_0(Encoding enc)
{    if (dictStats.containsKey(enc)) {        return dictStats.get(enc);    } else {        return 0;    }}
public int parquet-mr_f1138_0(Encoding enc)
{    if (dataStats.containsKey(enc)) {        return dataStats.get(enc);    } else {        return 0;    }}
public boolean parquet-mr_f1139_0()
{    return !dictStats.isEmpty();}
public boolean parquet-mr_f1140_0()
{    Set<Encoding> encodings = dataStats.keySet();    return (encodings.contains(RLE_DICTIONARY) || encodings.contains(PLAIN_DICTIONARY));}
public boolean parquet-mr_f1141_0()
{    if (dataStats.isEmpty()) {                return false;    }        Set<Encoding> encodings = new HashSet<Encoding>(dataStats.keySet());    if (!encodings.remove(RLE_DICTIONARY) && !encodings.remove(PLAIN_DICTIONARY)) {                return true;    }    if (encodings.isEmpty()) {        return false;    }        return true;}
public boolean parquet-mr_f1142_0()
{    return usesV2Pages;}
public Builder parquet-mr_f1143_0()
{    this.usesV2Pages = false;    dictStats.clear();    dataStats.clear();    return this;}
public Builder parquet-mr_f1144_0()
{    this.usesV2Pages = true;    return this;}
public Builder parquet-mr_f1145_0(Encoding encoding)
{    return addDictEncoding(encoding, 1);}
public Builder parquet-mr_f1146_0(Encoding encoding, int numPages)
{    Integer pages = dictStats.get(encoding);    dictStats.put(encoding, numPages + (pages != null ? pages : 0));    return this;}
public Builder parquet-mr_f1147_0(Collection<Encoding> encodings)
{    for (Encoding encoding : encodings) {        addDataEncoding(encoding);    }    return this;}
public Builder parquet-mr_f1148_0(Encoding encoding)
{    return addDataEncoding(encoding, 1);}
public Builder parquet-mr_f1149_0(Encoding encoding, int numPages)
{    Integer pages = dataStats.get(encoding);    dataStats.put(encoding, numPages + (pages != null ? pages : 0));    return this;}
public EncodingStats parquet-mr_f1150_0()
{    return new EncodingStats(Collections.unmodifiableMap(new LinkedHashMap<Encoding, Integer>(dictStats)), Collections.unmodifiableMap(new LinkedHashMap<Encoding, Integer>(dataStats)), usesV2Pages);}
public int parquet-mr_f1151_0()
{    throw new UnsupportedOperationException();}
public int parquet-mr_f1152_0()
{    throw new UnsupportedOperationException();}
public boolean parquet-mr_f1153_0()
{    throw new UnsupportedOperationException();}
public long parquet-mr_f1154_0()
{    throw new UnsupportedOperationException();}
public Binary parquet-mr_f1155_0()
{    throw new UnsupportedOperationException();}
public float parquet-mr_f1156_0()
{    throw new UnsupportedOperationException();}
public double parquet-mr_f1157_0()
{    throw new UnsupportedOperationException();}
private void parquet-mr_f1158_0(final Dictionary dictionary)
{    binding = new Binding() {        void read() {            dictionaryId = dataColumn.readValueDictionaryId();        }        public void skip() {            dataColumn.skip();        }        @Override        void skip(int n) {            dataColumn.skip(n);        }        public int getDictionaryId() {            return dictionaryId;        }        void writeValue() {            converter.addValueFromDictionary(dictionaryId);        }        public int getInteger() {            return dictionary.decodeToInt(dictionaryId);        }        public boolean getBoolean() {            return dictionary.decodeToBoolean(dictionaryId);        }        public long getLong() {            return dictionary.decodeToLong(dictionaryId);        }        public Binary getBinary() {            return dictionary.decodeToBinary(dictionaryId);        }        public float getFloat() {            return dictionary.decodeToFloat(dictionaryId);        }        public double getDouble() {            return dictionary.decodeToDouble(dictionaryId);        }    };}
 void parquet-mr_f1159_0()
{    dictionaryId = dataColumn.readValueDictionaryId();}
public void parquet-mr_f1160_0()
{    dataColumn.skip();}
 void parquet-mr_f1161_0(int n)
{    dataColumn.skip(n);}
public int parquet-mr_f1162_0()
{    return dictionaryId;}
 void parquet-mr_f1163_0()
{    converter.addValueFromDictionary(dictionaryId);}
public int parquet-mr_f1164_0()
{    return dictionary.decodeToInt(dictionaryId);}
public boolean parquet-mr_f1165_0()
{    return dictionary.decodeToBoolean(dictionaryId);}
public long parquet-mr_f1166_0()
{    return dictionary.decodeToLong(dictionaryId);}
public Binary parquet-mr_f1167_0()
{    return dictionary.decodeToBinary(dictionaryId);}
public float parquet-mr_f1168_0()
{    return dictionary.decodeToFloat(dictionaryId);}
public double parquet-mr_f1169_0()
{    return dictionary.decodeToDouble(dictionaryId);}
private void parquet-mr_f1170_0(PrimitiveTypeName type)
{    binding = type.convert(new PrimitiveTypeNameConverter<Binding, RuntimeException>() {        @Override        public Binding convertFLOAT(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return new Binding() {                float current;                void read() {                    current = dataColumn.readFloat();                }                public void skip() {                    current = 0;                    dataColumn.skip();                }                @Override                void skip(int n) {                    current = 0;                    dataColumn.skip(n);                }                public float getFloat() {                    return current;                }                void writeValue() {                    converter.addFloat(current);                }            };        }        @Override        public Binding convertDOUBLE(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return new Binding() {                double current;                void read() {                    current = dataColumn.readDouble();                }                public void skip() {                    current = 0;                    dataColumn.skip();                }                @Override                void skip(int n) {                    current = 0;                    dataColumn.skip(n);                }                public double getDouble() {                    return current;                }                void writeValue() {                    converter.addDouble(current);                }            };        }        @Override        public Binding convertINT32(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return new Binding() {                int current;                void read() {                    current = dataColumn.readInteger();                }                public void skip() {                    current = 0;                    dataColumn.skip();                }                @Override                void skip(int n) {                    current = 0;                    dataColumn.skip(n);                }                @Override                public int getInteger() {                    return current;                }                void writeValue() {                    converter.addInt(current);                }            };        }        @Override        public Binding convertINT64(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return new Binding() {                long current;                void read() {                    current = dataColumn.readLong();                }                public void skip() {                    current = 0;                    dataColumn.skip();                }                @Override                void skip(int n) {                    current = 0;                    dataColumn.skip(n);                }                @Override                public long getLong() {                    return current;                }                void writeValue() {                    converter.addLong(current);                }            };        }        @Override        public Binding convertINT96(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return this.convertBINARY(primitiveTypeName);        }        @Override        public Binding convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return this.convertBINARY(primitiveTypeName);        }        @Override        public Binding convertBOOLEAN(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return new Binding() {                boolean current;                void read() {                    current = dataColumn.readBoolean();                }                public void skip() {                    current = false;                    dataColumn.skip();                }                @Override                void skip(int n) {                    current = false;                    dataColumn.skip(n);                }                @Override                public boolean getBoolean() {                    return current;                }                void writeValue() {                    converter.addBoolean(current);                }            };        }        @Override        public Binding convertBINARY(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return new Binding() {                Binary current;                void read() {                    current = dataColumn.readBytes();                }                public void skip() {                    current = null;                    dataColumn.skip();                }                @Override                void skip(int n) {                    current = null;                    dataColumn.skip(n);                }                @Override                public Binary getBinary() {                    return current;                }                void writeValue() {                    converter.addBinary(current);                }            };        }    });}
public Binding parquet-mr_f1171_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return new Binding() {        float current;        void read() {            current = dataColumn.readFloat();        }        public void skip() {            current = 0;            dataColumn.skip();        }        @Override        void skip(int n) {            current = 0;            dataColumn.skip(n);        }        public float getFloat() {            return current;        }        void writeValue() {            converter.addFloat(current);        }    };}
 void parquet-mr_f1172_0()
{    current = dataColumn.readFloat();}
public void parquet-mr_f1173_0()
{    current = 0;    dataColumn.skip();}
 void parquet-mr_f1174_0(int n)
{    current = 0;    dataColumn.skip(n);}
public float parquet-mr_f1175_0()
{    return current;}
 void parquet-mr_f1176_0()
{    converter.addFloat(current);}
public Binding parquet-mr_f1177_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return new Binding() {        double current;        void read() {            current = dataColumn.readDouble();        }        public void skip() {            current = 0;            dataColumn.skip();        }        @Override        void skip(int n) {            current = 0;            dataColumn.skip(n);        }        public double getDouble() {            return current;        }        void writeValue() {            converter.addDouble(current);        }    };}
 void parquet-mr_f1178_0()
{    current = dataColumn.readDouble();}
public void parquet-mr_f1179_0()
{    current = 0;    dataColumn.skip();}
 void parquet-mr_f1180_0(int n)
{    current = 0;    dataColumn.skip(n);}
public double parquet-mr_f1181_0()
{    return current;}
 void parquet-mr_f1182_0()
{    converter.addDouble(current);}
public Binding parquet-mr_f1183_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return new Binding() {        int current;        void read() {            current = dataColumn.readInteger();        }        public void skip() {            current = 0;            dataColumn.skip();        }        @Override        void skip(int n) {            current = 0;            dataColumn.skip(n);        }        @Override        public int getInteger() {            return current;        }        void writeValue() {            converter.addInt(current);        }    };}
 void parquet-mr_f1184_0()
{    current = dataColumn.readInteger();}
public void parquet-mr_f1185_0()
{    current = 0;    dataColumn.skip();}
 void parquet-mr_f1186_0(int n)
{    current = 0;    dataColumn.skip(n);}
public int parquet-mr_f1187_0()
{    return current;}
 void parquet-mr_f1188_0()
{    converter.addInt(current);}
public Binding parquet-mr_f1189_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return new Binding() {        long current;        void read() {            current = dataColumn.readLong();        }        public void skip() {            current = 0;            dataColumn.skip();        }        @Override        void skip(int n) {            current = 0;            dataColumn.skip(n);        }        @Override        public long getLong() {            return current;        }        void writeValue() {            converter.addLong(current);        }    };}
 void parquet-mr_f1190_0()
{    current = dataColumn.readLong();}
public void parquet-mr_f1191_0()
{    current = 0;    dataColumn.skip();}
 void parquet-mr_f1192_0(int n)
{    current = 0;    dataColumn.skip(n);}
public long parquet-mr_f1193_0()
{    return current;}
 void parquet-mr_f1194_0()
{    converter.addLong(current);}
public Binding parquet-mr_f1195_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return this.convertBINARY(primitiveTypeName);}
public Binding parquet-mr_f1196_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return this.convertBINARY(primitiveTypeName);}
public Binding parquet-mr_f1197_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return new Binding() {        boolean current;        void read() {            current = dataColumn.readBoolean();        }        public void skip() {            current = false;            dataColumn.skip();        }        @Override        void skip(int n) {            current = false;            dataColumn.skip(n);        }        @Override        public boolean getBoolean() {            return current;        }        void writeValue() {            converter.addBoolean(current);        }    };}
 void parquet-mr_f1198_0()
{    current = dataColumn.readBoolean();}
public void parquet-mr_f1199_0()
{    current = false;    dataColumn.skip();}
 void parquet-mr_f1200_0(int n)
{    current = false;    dataColumn.skip(n);}
public boolean parquet-mr_f1201_0()
{    return current;}
 void parquet-mr_f1202_0()
{    converter.addBoolean(current);}
public Binding parquet-mr_f1203_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return new Binding() {        Binary current;        void read() {            current = dataColumn.readBytes();        }        public void skip() {            current = null;            dataColumn.skip();        }        @Override        void skip(int n) {            current = null;            dataColumn.skip(n);        }        @Override        public Binary getBinary() {            return current;        }        void writeValue() {            converter.addBinary(current);        }    };}
 void parquet-mr_f1204_0()
{    current = dataColumn.readBytes();}
public void parquet-mr_f1205_0()
{    current = null;    dataColumn.skip();}
 void parquet-mr_f1206_0(int n)
{    current = null;    dataColumn.skip(n);}
public Binary parquet-mr_f1207_0()
{    return current;}
 void parquet-mr_f1208_0()
{    converter.addBinary(current);}
 boolean parquet-mr_f1209_0()
{    return readValues >= totalValueCount;}
public void parquet-mr_f1210_0()
{    readValue();    this.binding.writeValue();}
public int parquet-mr_f1211_0()
{    readValue();    return binding.getDictionaryId();}
public int parquet-mr_f1212_0()
{    readValue();    return this.binding.getInteger();}
public boolean parquet-mr_f1213_0()
{    readValue();    return this.binding.getBoolean();}
public long parquet-mr_f1214_0()
{    readValue();    return this.binding.getLong();}
public Binary parquet-mr_f1215_0()
{    readValue();    return this.binding.getBinary();}
public float parquet-mr_f1216_0()
{    readValue();    return this.binding.getFloat();}
public double parquet-mr_f1217_0()
{    readValue();    return this.binding.getDouble();}
public int parquet-mr_f1218_0()
{    return repetitionLevel;}
public ColumnDescriptor parquet-mr_f1219_0()
{    return path;}
public void parquet-mr_f1220_0()
{    try {        if (!valueRead) {            binding.read();            valueRead = true;        }    } catch (RuntimeException e) {        if (CorruptDeltaByteArrays.requiresSequentialReads(writerVersion, currentEncoding) && e instanceof ArrayIndexOutOfBoundsException) {                        throw new ParquetDecodingException("Read failure possibly due to " + "PARQUET-246: try setting parquet.split.files to false", new ParquetDecodingException(format("Can't read value in column %s at value %d out of %d, " + "%d out of %d in currentPage. repetition level: " + "%d, definition level: %d", path, readValues, totalValueCount, readValues - (endOfPageValueCount - pageValueCount), pageValueCount, repetitionLevel, definitionLevel), e));        }        throw new ParquetDecodingException(format("Can't read value in column %s at value %d out of %d, " + "%d out of %d in currentPage. repetition level: " + "%d, definition level: %d", path, readValues, totalValueCount, readValues - (endOfPageValueCount - pageValueCount), pageValueCount, repetitionLevel, definitionLevel), e);    }}
public void parquet-mr_f1221_0()
{    if (!valueRead) {        binding.skip();        valueRead = true;    }}
public int parquet-mr_f1222_0()
{    return definitionLevel;}
private void parquet-mr_f1223_1()
{    int rl, dl;    int skipValues = 0;    for (; ; ) {        if (isPageFullyConsumed()) {            if (isFullyConsumed()) {                                                repetitionLevel = 0;                return;            }            readPage();            skipValues = 0;        }        rl = repetitionLevelColumn.nextInt();        dl = definitionLevelColumn.nextInt();        ++readValues;        if (!skipRL(rl)) {            break;        }        if (dl == maxDefinitionLevel) {            ++skipValues;        }    }    binding.skip(skipValues);    repetitionLevel = rl;    definitionLevel = dl;}
private void parquet-mr_f1224_1()
{        DataPage page = pageReader.readPage();    page.accept(new DataPage.Visitor<Void>() {        @Override        public Void visit(DataPageV1 dataPageV1) {            readPageV1(dataPageV1);            return null;        }        @Override        public Void visit(DataPageV2 dataPageV2) {            readPageV2(dataPageV2);            return null;        }    });}
public Void parquet-mr_f1225_0(DataPageV1 dataPageV1)
{    readPageV1(dataPageV1);    return null;}
public Void parquet-mr_f1226_0(DataPageV2 dataPageV2)
{    readPageV2(dataPageV2);    return null;}
private void parquet-mr_f1227_0(Encoding dataEncoding, ByteBufferInputStream in, int valueCount)
{    ValuesReader previousReader = this.dataColumn;    this.currentEncoding = dataEncoding;    this.pageValueCount = valueCount;    this.endOfPageValueCount = readValues + pageValueCount;    if (dataEncoding.usesDictionary()) {        if (dictionary == null) {            throw new ParquetDecodingException("could not read page in col " + path + " as the dictionary was missing for encoding " + dataEncoding);        }        this.dataColumn = dataEncoding.getDictionaryBasedValuesReader(path, VALUES, dictionary);    } else {        this.dataColumn = dataEncoding.getValuesReader(path, VALUES);    }    if (dataEncoding.usesDictionary() && converter.hasDictionarySupport()) {        bindToDictionary(dictionary);    } else {        bind(path.getType());    }    try {        dataColumn.initFromPage(pageValueCount, in);    } catch (IOException e) {        throw new ParquetDecodingException("could not read page in col " + path, e);    }    if (CorruptDeltaByteArrays.requiresSequentialReads(writerVersion, dataEncoding) && previousReader != null && previousReader instanceof RequiresPreviousReader) {                ((RequiresPreviousReader) dataColumn).setPreviousReader(previousReader);    }}
private void parquet-mr_f1228_1(DataPageV1 page)
{    ValuesReader rlReader = page.getRlEncoding().getValuesReader(path, REPETITION_LEVEL);    ValuesReader dlReader = page.getDlEncoding().getValuesReader(path, DEFINITION_LEVEL);    this.repetitionLevelColumn = new ValuesReaderIntIterator(rlReader);    this.definitionLevelColumn = new ValuesReaderIntIterator(dlReader);    int valueCount = page.getValueCount();    try {        BytesInput bytes = page.getBytes();                        ByteBufferInputStream in = bytes.toInputStream();        rlReader.initFromPage(valueCount, in);                dlReader.initFromPage(valueCount, in);                initDataReader(page.getValueEncoding(), in, valueCount);    } catch (IOException e) {        throw new ParquetDecodingException("could not read page " + page + " in col " + path, e);    }    newPageInitialized(page);}
private void parquet-mr_f1229_1(DataPageV2 page)
{    this.repetitionLevelColumn = newRLEIterator(path.getMaxRepetitionLevel(), page.getRepetitionLevels());    this.definitionLevelColumn = newRLEIterator(path.getMaxDefinitionLevel(), page.getDefinitionLevels());    int valueCount = page.getValueCount();        try {        initDataReader(page.getDataEncoding(), page.getData().toInputStream(), valueCount);    } catch (IOException e) {        throw new ParquetDecodingException("could not read page " + page + " in col " + path, e);    }    newPageInitialized(page);}
 final int parquet-mr_f1230_0()
{    return pageValueCount;}
private IntIterator parquet-mr_f1231_0(int maxLevel, BytesInput bytes)
{    try {        if (maxLevel == 0) {            return new NullIntIterator();        }        return new RLEIntIterator(new RunLengthBitPackingHybridDecoder(BytesUtils.getWidthFromMaxInt(maxLevel), bytes.toInputStream()));    } catch (IOException e) {        throw new ParquetDecodingException("could not read levels in page for col " + path, e);    }}
 boolean parquet-mr_f1232_0()
{    return readValues >= endOfPageValueCount;}
public void parquet-mr_f1233_0()
{    checkRead();    valueRead = false;}
public long parquet-mr_f1234_0()
{    return totalValueCount;}
 int parquet-mr_f1235_0()
{    return delegate.readInteger();}
 int parquet-mr_f1236_0()
{    try {        return delegate.readInt();    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
 int parquet-mr_f1237_0()
{    return 0;}
 boolean parquet-mr_f1238_0(int rl)
{    return false;}
 void parquet-mr_f1239_0(DataPage page)
{}
public ColumnReader parquet-mr_f1240_0(ColumnDescriptor path)
{    PrimitiveConverter converter = getPrimitiveConverter(path);    PageReader pageReader = pageReadStore.getPageReader(path);    Optional<PrimitiveIterator.OfLong> rowIndexes = pageReadStore.getRowIndexes();    if (rowIndexes.isPresent()) {        return new SynchronizingColumnReader(path, pageReader, converter, writerVersion, rowIndexes.get());    } else {        return new ColumnReaderImpl(path, pageReader, converter, writerVersion);    }}
private ColumnReaderImpl parquet-mr_f1241_0(ColumnDescriptor path, PageReader pageReader)
{    PrimitiveConverter converter = getPrimitiveConverter(path);    return new ColumnReaderImpl(path, pageReader, converter, writerVersion);}
private PrimitiveConverter parquet-mr_f1242_0(ColumnDescriptor path)
{    Type currentType = schema;    Converter currentConverter = recordConverter;    for (String fieldName : path.getPath()) {        final GroupType groupType = currentType.asGroupType();        int fieldIndex = groupType.getFieldIndex(fieldName);        currentType = groupType.getType(fieldName);        currentConverter = currentConverter.asGroupConverter().getConverter(fieldIndex);    }    PrimitiveConverter converter = currentConverter.asPrimitiveConverter();    return converter;}
private void parquet-mr_f1243_1(Object value, int r, int d)
{    }
private void parquet-mr_f1244_0()
{    this.statistics = Statistics.createStats(path.getPrimitiveType());}
private void parquet-mr_f1245_0(int definitionLevel)
{    definitionLevelColumn.writeInteger(definitionLevel);}
private void parquet-mr_f1246_0(int repetitionLevel)
{    repetitionLevelColumn.writeInteger(repetitionLevel);    assert pageRowCount == 0 ? repetitionLevel == 0 : true : "Every page shall start on record boundaries";    if (repetitionLevel == 0) {        ++pageRowCount;    }}
public void parquet-mr_f1247_0(int repetitionLevel, int definitionLevel)
{    if (DEBUG)        log(null, repetitionLevel, definitionLevel);    repetitionLevel(repetitionLevel);    definitionLevel(definitionLevel);    statistics.incrementNumNulls();    ++valueCount;}
public void parquet-mr_f1248_0()
{        repetitionLevelColumn.close();    definitionLevelColumn.close();    dataColumn.close();}
public long parquet-mr_f1249_0()
{    return repetitionLevelColumn.getBufferedSize() + definitionLevelColumn.getBufferedSize() + dataColumn.getBufferedSize() + pageWriter.getMemSize();}
public void parquet-mr_f1250_0(double value, int repetitionLevel, int definitionLevel)
{    if (DEBUG)        log(value, repetitionLevel, definitionLevel);    repetitionLevel(repetitionLevel);    definitionLevel(definitionLevel);    dataColumn.writeDouble(value);    statistics.updateStats(value);    ++valueCount;}
public void parquet-mr_f1251_0(float value, int repetitionLevel, int definitionLevel)
{    if (DEBUG)        log(value, repetitionLevel, definitionLevel);    repetitionLevel(repetitionLevel);    definitionLevel(definitionLevel);    dataColumn.writeFloat(value);    statistics.updateStats(value);    ++valueCount;}
public void parquet-mr_f1252_0(Binary value, int repetitionLevel, int definitionLevel)
{    if (DEBUG)        log(value, repetitionLevel, definitionLevel);    repetitionLevel(repetitionLevel);    definitionLevel(definitionLevel);    dataColumn.writeBytes(value);    statistics.updateStats(value);    ++valueCount;}
public void parquet-mr_f1253_0(boolean value, int repetitionLevel, int definitionLevel)
{    if (DEBUG)        log(value, repetitionLevel, definitionLevel);    repetitionLevel(repetitionLevel);    definitionLevel(definitionLevel);    dataColumn.writeBoolean(value);    statistics.updateStats(value);    ++valueCount;}
public void parquet-mr_f1254_0(int value, int repetitionLevel, int definitionLevel)
{    if (DEBUG)        log(value, repetitionLevel, definitionLevel);    repetitionLevel(repetitionLevel);    definitionLevel(definitionLevel);    dataColumn.writeInteger(value);    statistics.updateStats(value);    ++valueCount;}
public void parquet-mr_f1255_0(long value, int repetitionLevel, int definitionLevel)
{    if (DEBUG)        log(value, repetitionLevel, definitionLevel);    repetitionLevel(repetitionLevel);    definitionLevel(definitionLevel);    dataColumn.writeLong(value);    statistics.updateStats(value);    ++valueCount;}
 void parquet-mr_f1256_1()
{    final DictionaryPage dictionaryPage = dataColumn.toDictPageAndClose();    if (dictionaryPage != null) {        if (DEBUG)                    try {            pageWriter.writeDictionaryPage(dictionaryPage);        } catch (IOException e) {            throw new ParquetEncodingException("could not write dictionary page for " + path, e);        }        dataColumn.resetDictionary();    }}
 long parquet-mr_f1257_0()
{    return repetitionLevelColumn.getBufferedSize() + definitionLevelColumn.getBufferedSize() + dataColumn.getBufferedSize();}
 long parquet-mr_f1258_0()
{    return repetitionLevelColumn.getBufferedSize() + definitionLevelColumn.getBufferedSize() + dataColumn.getBufferedSize() + pageWriter.getMemSize();}
 long parquet-mr_f1259_0()
{    return repetitionLevelColumn.getAllocatedSize() + definitionLevelColumn.getAllocatedSize() + dataColumn.getAllocatedSize() + pageWriter.allocatedSize();}
 String parquet-mr_f1260_0(String indent)
{    StringBuilder b = new StringBuilder(indent).append(path).append(" {\n");    b.append(indent).append(" r:").append(repetitionLevelColumn.getAllocatedSize()).append(" bytes\n");    b.append(indent).append(" d:").append(definitionLevelColumn.getAllocatedSize()).append(" bytes\n");    b.append(dataColumn.memUsageString(indent + "  data:")).append("\n");    b.append(pageWriter.memUsageString(indent + "  pages:")).append("\n");    b.append(indent).append(String.format("  total: %,d/%,d", getTotalBufferedSize(), allocatedSize())).append("\n");    b.append(indent).append("}\n");    return b.toString();}
 long parquet-mr_f1261_0()
{    return this.rowsWrittenSoFar;}
 void parquet-mr_f1262_1()
{    if (valueCount == 0) {        throw new ParquetEncodingException("writing empty page");    }    this.rowsWrittenSoFar += pageRowCount;    if (DEBUG)            try {        writePage(pageRowCount, valueCount, statistics, repetitionLevelColumn, definitionLevelColumn, dataColumn);    } catch (IOException e) {        throw new ParquetEncodingException("could not write page for " + path, e);    }    repetitionLevelColumn.reset();    definitionLevelColumn.reset();    dataColumn.reset();    valueCount = 0;    resetStatistics();    pageRowCount = 0;}
 ValuesWriter parquet-mr_f1263_0(ParquetProperties props, ColumnDescriptor path)
{    return props.newRepetitionLevelWriter(path);}
 ValuesWriter parquet-mr_f1264_0(ParquetProperties props, ColumnDescriptor path)
{    return props.newDefinitionLevelWriter(path);}
 void parquet-mr_f1265_0(int rowCount, int valueCount, Statistics<?> statistics, ValuesWriter repetitionLevels, ValuesWriter definitionLevels, ValuesWriter values) throws IOException
{    pageWriter.writePage(concat(repetitionLevels.getBytes(), definitionLevels.getBytes(), values.getBytes()), valueCount, rowCount, statistics, repetitionLevels.getEncoding(), definitionLevels.getEncoding(), values.getEncoding());}
public BytesInput parquet-mr_f1266_0()
{    try {        return encoder.toBytes();    } catch (IOException e) {        throw new ParquetEncodingException(e);    }}
 ValuesWriter parquet-mr_f1267_0(ParquetProperties props, ColumnDescriptor path)
{    return path.getMaxRepetitionLevel() == 0 ? NULL_WRITER : new RLEWriterForV2(props.newRepetitionLevelEncoder(path));}
 ValuesWriter parquet-mr_f1268_0(ParquetProperties props, ColumnDescriptor path)
{    return path.getMaxDefinitionLevel() == 0 ? NULL_WRITER : new RLEWriterForV2(props.newDefinitionLevelEncoder(path));}
 void parquet-mr_f1269_0(int rowCount, int valueCount, Statistics<?> statistics, ValuesWriter repetitionLevels, ValuesWriter definitionLevels, ValuesWriter values) throws IOException
{        BytesInput bytes = values.getBytes();    Encoding encoding = values.getEncoding();    pageWriter.writePageV2(rowCount, Math.toIntExact(statistics.getNumNulls()), valueCount, repetitionLevels.getBytes(), definitionLevels.getBytes(), encoding, bytes, statistics);}
public ColumnWriter parquet-mr_f1270_0(ColumnDescriptor path)
{    ColumnWriterBase column = columns.get(path);    if (column == null) {        column = createColumnWriter(path, pageWriteStore.getPageWriter(path), props);        columns.put(path, column);    }    return column;}
public ColumnWriter parquet-mr_f1271_0(ColumnDescriptor path)
{    return columns.get(path);}
public ColumnWriter parquet-mr_f1272_0(ColumnDescriptor path)
{    return columnWriterProvider.getColumnWriter(path);}
public Set<ColumnDescriptor> parquet-mr_f1273_0()
{    return columns.keySet();}
public String parquet-mr_f1274_0()
{    StringBuilder sb = new StringBuilder();    for (Entry<ColumnDescriptor, ColumnWriterBase> entry : columns.entrySet()) {        sb.append(Arrays.toString(entry.getKey().getPath())).append(": ");        sb.append(entry.getValue().getTotalBufferedSize()).append(" bytes");        sb.append("\n");    }    return sb.toString();}
public long parquet-mr_f1275_0()
{    long total = 0;    for (ColumnWriterBase memColumn : columns.values()) {        total += memColumn.allocatedSize();    }    return total;}
public long parquet-mr_f1276_0()
{    long total = 0;    for (ColumnWriterBase memColumn : columns.values()) {        total += memColumn.getTotalBufferedSize();    }    return total;}
public void parquet-mr_f1277_0()
{    for (ColumnWriterBase memColumn : columns.values()) {        long rows = rowCount - memColumn.getRowsWrittenSoFar();        if (rows > 0) {            memColumn.writePage();        }        memColumn.finalizeColumnChunk();    }}
public String parquet-mr_f1278_0()
{    StringBuilder b = new StringBuilder("Store {\n");    for (ColumnWriterBase memColumn : columns.values()) {        b.append(memColumn.memUsageString(" "));    }    b.append("}\n");    return b.toString();}
public long parquet-mr_f1279_0()
{    long max = 0;    for (ColumnWriterBase memColumn : columns.values()) {        max = Math.max(max, memColumn.getBufferedSizeInMemory());    }    return max;}
public void parquet-mr_f1280_0()
{        flush();    for (ColumnWriterBase memColumn : columns.values()) {        memColumn.close();    }}
public void parquet-mr_f1281_0()
{    ++rowCount;    if (rowCount >= rowCountForNextSizeCheck) {        sizeCheck();    }}
private void parquet-mr_f1282_0()
{    long minRecordToWait = Long.MAX_VALUE;    int pageRowCountLimit = props.getPageRowCountLimit();    long rowCountForNextRowCountCheck = rowCount + pageRowCountLimit;    for (ColumnWriterBase writer : columns.values()) {        long usedMem = writer.getCurrentPageBufferedSize();        long rows = rowCount - writer.getRowsWrittenSoFar();        long remainingMem = props.getPageSizeThreshold() - usedMem;        if (remainingMem <= thresholdTolerance || rows >= pageRowCountLimit) {            writer.writePage();            remainingMem = props.getPageSizeThreshold();        } else {            rowCountForNextRowCountCheck = min(rowCountForNextRowCountCheck, writer.getRowsWrittenSoFar() + pageRowCountLimit);        }        long rowsToFillPage = usedMem == 0 ? props.getMaxRowCountForPageSizeCheck() : (long) ((float) rows) / usedMem * remainingMem;        if (rowsToFillPage < minRecordToWait) {            minRecordToWait = rowsToFillPage;        }    }    if (minRecordToWait == Long.MAX_VALUE) {        minRecordToWait = props.getMinRowCountForPageSizeCheck();    }    if (props.estimateNextSizeCheck()) {                rowCountForNextSizeCheck = rowCount + min(max(minRecordToWait / 2, props.getMinRowCountForPageSizeCheck()), props.getMaxRowCountForPageSizeCheck());    } else {        rowCountForNextSizeCheck = rowCount + props.getMinRowCountForPageSizeCheck();    }        if (rowCountForNextRowCountCheck < rowCountForNextSizeCheck) {        rowCountForNextSizeCheck = rowCountForNextRowCountCheck;    }}
public boolean parquet-mr_f1283_0()
{    return rowCount + 1 >= rowCountForNextSizeCheck;}
 ColumnWriterBase parquet-mr_f1284_0(ColumnDescriptor path, PageWriter pageWriter, ParquetProperties props)
{    return new ColumnWriterV1(path, pageWriter, props);}
 ColumnWriterBase parquet-mr_f1285_0(ColumnDescriptor path, PageWriter pageWriter, ParquetProperties props)
{    return new ColumnWriterV2(path, pageWriter, props);}
 boolean parquet-mr_f1286_0()
{    return getPageValueCount() <= valuesReadFromPage || lastRowInPage < targetRow;}
 boolean parquet-mr_f1287_0()
{    return !rowIndexes.hasNext();}
 boolean parquet-mr_f1288_0(int rl)
{    ++valuesReadFromPage;    if (rl == 0) {        ++currentRow;        if (currentRow > targetRow) {            targetRow = rowIndexes.hasNext() ? rowIndexes.nextLong() : Long.MAX_VALUE;        }    }    return currentRow < targetRow;}
protected void parquet-mr_f1289_0(DataPage page)
{    long firstRowIndex = page.getFirstRowIndex().orElseThrow(() -> new IllegalArgumentException("Missing firstRowIndex for synchronizing values"));    int rowCount = page.getIndexRowCount().orElseThrow(() -> new IllegalArgumentException("Missing rowCount for synchronizing values"));    currentRow = firstRowIndex - 1;    lastRowInPage = firstRowIndex + rowCount - 1;    valuesReadFromPage = 0;}
public int parquet-mr_f1290_0()
{    return valueCount;}
public Optional<Long> parquet-mr_f1291_0()
{    return firstRowIndex < 0 ? Optional.empty() : Optional.of(firstRowIndex);}
public BytesInput parquet-mr_f1292_0()
{    return bytes;}
public Statistics<?> parquet-mr_f1293_0()
{    return statistics;}
public Encoding parquet-mr_f1294_0()
{    return dlEncoding;}
public Encoding parquet-mr_f1295_0()
{    return rlEncoding;}
public Encoding parquet-mr_f1296_0()
{    return valuesEncoding;}
public String parquet-mr_f1297_0()
{    return "Page [bytes.size=" + bytes.size() + ", valueCount=" + getValueCount() + ", uncompressedSize=" + getUncompressedSize() + "]";}
public T parquet-mr_f1298_0(Visitor<T> visitor)
{    return visitor.visit(this);}
public Optional<Integer> parquet-mr_f1299_0()
{    return indexRowCount < 0 ? Optional.empty() : Optional.of(indexRowCount);}
public static DataPageV2 parquet-mr_f1300_0(int rowCount, int nullCount, int valueCount, BytesInput repetitionLevels, BytesInput definitionLevels, Encoding dataEncoding, BytesInput data, Statistics<?> statistics)
{    return new DataPageV2(rowCount, nullCount, valueCount, repetitionLevels, definitionLevels, dataEncoding, data, Math.toIntExact(repetitionLevels.size() + definitionLevels.size() + data.size()), statistics, false);}
public static DataPageV2 parquet-mr_f1301_0(int rowCount, int nullCount, int valueCount, long firstRowIndex, BytesInput repetitionLevels, BytesInput definitionLevels, Encoding dataEncoding, BytesInput data, Statistics<?> statistics)
{    return new DataPageV2(rowCount, nullCount, valueCount, firstRowIndex, repetitionLevels, definitionLevels, dataEncoding, data, Math.toIntExact(repetitionLevels.size() + definitionLevels.size() + data.size()), statistics, false);}
public static DataPageV2 parquet-mr_f1302_0(int rowCount, int nullCount, int valueCount, BytesInput repetitionLevels, BytesInput definitionLevels, Encoding dataEncoding, BytesInput data, int uncompressedSize, Statistics<?> statistics)
{    return new DataPageV2(rowCount, nullCount, valueCount, repetitionLevels, definitionLevels, dataEncoding, data, uncompressedSize, statistics, true);}
public int parquet-mr_f1303_0()
{    return rowCount;}
public int parquet-mr_f1304_0()
{    return nullCount;}
public BytesInput parquet-mr_f1305_0()
{    return repetitionLevels;}
public BytesInput parquet-mr_f1306_0()
{    return definitionLevels;}
public Encoding parquet-mr_f1307_0()
{    return dataEncoding;}
public BytesInput parquet-mr_f1308_0()
{    return data;}
public Statistics<?> parquet-mr_f1309_0()
{    return statistics;}
public boolean parquet-mr_f1310_0()
{    return isCompressed;}
public Optional<Integer> parquet-mr_f1311_0()
{    return Optional.of(rowCount);}
public T parquet-mr_f1312_0(Visitor<T> visitor)
{    return visitor.visit(this);}
public String parquet-mr_f1313_0()
{    return "Page V2 [" + "dl size=" + definitionLevels.size() + ", " + "rl size=" + repetitionLevels.size() + ", " + "data size=" + data.size() + ", " + "data enc=" + dataEncoding + ", " + "valueCount=" + getValueCount() + ", " + "rowCount=" + getRowCount() + ", " + "is compressed=" + isCompressed + ", " + "uncompressedSize=" + getUncompressedSize() + "]";}
public BytesInput parquet-mr_f1314_0()
{    return bytes;}
public int parquet-mr_f1315_0()
{    return dictionarySize;}
public Encoding parquet-mr_f1316_0()
{    return encoding;}
public DictionaryPage parquet-mr_f1317_0() throws IOException
{    return new DictionaryPage(BytesInput.copy(bytes), getUncompressedSize(), dictionarySize, encoding);}
public String parquet-mr_f1318_0()
{    return "Page [bytes.size=" + bytes.size() + ", entryCount=" + dictionarySize + ", uncompressedSize=" + getUncompressedSize() + ", encoding=" + encoding + "]";}
public int parquet-mr_f1319_0()
{    return compressedSize;}
public int parquet-mr_f1320_0()
{    return uncompressedSize;}
public void parquet-mr_f1321_0(int crc)
{    this.crc = OptionalInt.of(crc);}
public OptionalInt parquet-mr_f1322_0()
{    return crc;}
 Optional<PrimitiveIterator.OfLong> parquet-mr_f1323_0()
{    return Optional.empty();}
public static WriterVersion parquet-mr_f1324_0(String name)
{    for (WriterVersion v : WriterVersion.values()) {        if (v.shortName.equals(name)) {            return v;        }    }        return WriterVersion.valueOf(name);}
public ValuesWriter parquet-mr_f1325_0(ColumnDescriptor path)
{    return newColumnDescriptorValuesWriter(path.getMaxRepetitionLevel());}
public ValuesWriter parquet-mr_f1326_0(ColumnDescriptor path)
{    return newColumnDescriptorValuesWriter(path.getMaxDefinitionLevel());}
private ValuesWriter parquet-mr_f1327_0(int maxLevel)
{    if (maxLevel == 0) {        return new DevNullValuesWriter();    } else {        return new RunLengthBitPackingHybridValuesWriter(getWidthFromMaxInt(maxLevel), MIN_SLAB_SIZE, pageSizeThreshold, allocator);    }}
public RunLengthBitPackingHybridEncoder parquet-mr_f1328_0(ColumnDescriptor path)
{    return newLevelEncoder(path.getMaxRepetitionLevel());}
public RunLengthBitPackingHybridEncoder parquet-mr_f1329_0(ColumnDescriptor path)
{    return newLevelEncoder(path.getMaxDefinitionLevel());}
private RunLengthBitPackingHybridEncoder parquet-mr_f1330_0(int maxLevel)
{    return new RunLengthBitPackingHybridEncoder(getWidthFromMaxInt(maxLevel), MIN_SLAB_SIZE, pageSizeThreshold, allocator);}
public ValuesWriter parquet-mr_f1331_0(ColumnDescriptor path)
{    return valuesWriterFactory.newValuesWriter(path);}
public int parquet-mr_f1332_0()
{    return pageSizeThreshold;}
public int parquet-mr_f1333_0()
{    return initialSlabSize;}
public int parquet-mr_f1334_0()
{    return dictionaryPageSizeThreshold;}
public WriterVersion parquet-mr_f1335_0()
{    return writerVersion;}
public boolean parquet-mr_f1336_0()
{    return enableDictionary;}
public ByteBufferAllocator parquet-mr_f1337_0()
{    return allocator;}
public ColumnWriteStore parquet-mr_f1338_0(MessageType schema, PageWriteStore pageStore)
{    switch(writerVersion) {        case PARQUET_1_0:            return new ColumnWriteStoreV1(schema, pageStore, this);        case PARQUET_2_0:            return new ColumnWriteStoreV2(schema, pageStore, this);        default:            throw new IllegalArgumentException("unknown version " + writerVersion);    }}
public int parquet-mr_f1339_0()
{    return minRowCountForPageSizeCheck;}
public int parquet-mr_f1340_0()
{    return maxRowCountForPageSizeCheck;}
public ValuesWriterFactory parquet-mr_f1341_0()
{    return valuesWriterFactory;}
public int parquet-mr_f1342_0()
{    return columnIndexTruncateLength;}
public boolean parquet-mr_f1343_0()
{    return estimateNextSizeCheck;}
public int parquet-mr_f1344_0()
{    return pageRowCountLimit;}
public boolean parquet-mr_f1345_0()
{    return pageWriteChecksumEnabled;}
public static Builder parquet-mr_f1346_0()
{    return new Builder();}
public static Builder parquet-mr_f1347_0(ParquetProperties toCopy)
{    return new Builder(toCopy);}
public Builder parquet-mr_f1348_0(int pageSize)
{    Preconditions.checkArgument(pageSize > 0, "Invalid page size (negative): %s", pageSize);    this.pageSize = pageSize;    return this;}
public Builder parquet-mr_f1349_0(boolean enableDictionary)
{    this.enableDict = enableDictionary;    return this;}
public Builder parquet-mr_f1350_0(int dictionaryPageSize)
{    Preconditions.checkArgument(dictionaryPageSize > 0, "Invalid dictionary page size (negative): %s", dictionaryPageSize);    this.dictPageSize = dictionaryPageSize;    return this;}
public Builder parquet-mr_f1351_0(WriterVersion version)
{    this.writerVersion = version;    return this;}
public Builder parquet-mr_f1352_0(int min)
{    Preconditions.checkArgument(min > 0, "Invalid row count for page size check (negative): %s", min);    this.minRowCountForPageSizeCheck = min;    return this;}
public Builder parquet-mr_f1353_0(int max)
{    Preconditions.checkArgument(max > 0, "Invalid row count for page size check (negative): %s", max);    this.maxRowCountForPageSizeCheck = max;    return this;}
public Builder parquet-mr_f1354_0(boolean estimateNextSizeCheck)
{    this.estimateNextSizeCheck = estimateNextSizeCheck;    return this;}
public Builder parquet-mr_f1355_0(ByteBufferAllocator allocator)
{    Preconditions.checkNotNull(allocator, "ByteBufferAllocator");    this.allocator = allocator;    return this;}
public Builder parquet-mr_f1356_0(ValuesWriterFactory factory)
{    Preconditions.checkNotNull(factory, "ValuesWriterFactory");    this.valuesWriterFactory = factory;    return this;}
public Builder parquet-mr_f1357_0(int length)
{    Preconditions.checkArgument(length > 0, "Invalid column index min/max truncate length (negative) : %s", length);    this.columnIndexTruncateLength = length;    return this;}
public Builder parquet-mr_f1358_0(int rowCount)
{    Preconditions.checkArgument(rowCount > 0, "Invalid row count limit for pages: " + rowCount);    pageRowCountLimit = rowCount;    return this;}
public Builder parquet-mr_f1359_0(boolean val)
{    this.pageWriteChecksumEnabled = val;    return this;}
public ParquetProperties parquet-mr_f1360_0()
{    ParquetProperties properties = new ParquetProperties(writerVersion, pageSize, dictPageSize, enableDict, minRowCountForPageSizeCheck, maxRowCountForPageSizeCheck, estimateNextSizeCheck, allocator, valuesWriterFactory, columnIndexTruncateLength, pageRowCountLimit, pageWriteChecksumEnabled);                    valuesWriterFactory.initialize(properties);    return properties;}
public void parquet-mr_f1361_0(Binary value)
{    if (!this.hasNonNullValue()) {        initializeStats(value, value);    } else {        updateStats(value, value);    }}
public void parquet-mr_f1362_0(Statistics stats)
{    BinaryStatistics binaryStats = (BinaryStatistics) stats;    if (!this.hasNonNullValue()) {        initializeStats(binaryStats.getMin(), binaryStats.getMax());    } else {        updateStats(binaryStats.getMin(), binaryStats.getMax());    }}
public void parquet-mr_f1363_0(byte[] minBytes, byte[] maxBytes)
{    max = Binary.fromReusedByteArray(maxBytes);    min = Binary.fromReusedByteArray(minBytes);    this.markAsNotEmpty();}
public byte[] parquet-mr_f1364_0()
{    return max == null ? null : max.getBytes();}
public byte[] parquet-mr_f1365_0()
{    return min == null ? null : min.getBytes();}
 String parquet-mr_f1366_0(Binary value)
{    return stringifier.stringify(value);}
public boolean parquet-mr_f1367_0(long size)
{    return !hasNonNullValue() || ((min.length() + max.length()) < size);}
public void parquet-mr_f1368_0(Binary min_value, Binary max_value)
{    if (comparator().compare(min, min_value) > 0) {        min = min_value.copy();    }    if (comparator().compare(max, max_value) < 0) {        max = max_value.copy();    }}
public void parquet-mr_f1369_0(Binary min_value, Binary max_value)
{    min = min_value.copy();    max = max_value.copy();    this.markAsNotEmpty();}
public Binary parquet-mr_f1370_0()
{    return min;}
public Binary parquet-mr_f1371_0()
{    return max;}
public Binary parquet-mr_f1372_0()
{    return max;}
public Binary parquet-mr_f1373_0()
{    return min;}
public void parquet-mr_f1374_0(Binary min, Binary max)
{    this.max = max;    this.min = min;    this.markAsNotEmpty();}
public BinaryStatistics parquet-mr_f1375_0()
{    return new BinaryStatistics(this);}
public void parquet-mr_f1376_0(boolean value)
{    if (!this.hasNonNullValue()) {        initializeStats(value, value);    } else {        updateStats(value, value);    }}
public void parquet-mr_f1377_0(Statistics stats)
{    BooleanStatistics boolStats = (BooleanStatistics) stats;    if (!this.hasNonNullValue()) {        initializeStats(boolStats.getMin(), boolStats.getMax());    } else {        updateStats(boolStats.getMin(), boolStats.getMax());    }}
public void parquet-mr_f1378_0(byte[] minBytes, byte[] maxBytes)
{    max = BytesUtils.bytesToBool(maxBytes);    min = BytesUtils.bytesToBool(minBytes);    this.markAsNotEmpty();}
public byte[] parquet-mr_f1379_0()
{    return BytesUtils.booleanToBytes(max);}
public byte[] parquet-mr_f1380_0()
{    return BytesUtils.booleanToBytes(min);}
 String parquet-mr_f1381_0(Boolean value)
{    return stringifier.stringify(value);}
public boolean parquet-mr_f1382_0(long size)
{    return !hasNonNullValue() || (2 < size);}
public void parquet-mr_f1383_0(boolean min_value, boolean max_value)
{    if (comparator().compare(min, min_value) > 0) {        min = min_value;    }    if (comparator().compare(max, max_value) < 0) {        max = max_value;    }}
public void parquet-mr_f1384_0(boolean min_value, boolean max_value)
{    min = min_value;    max = max_value;    this.markAsNotEmpty();}
public Boolean parquet-mr_f1385_0()
{    return min;}
public Boolean parquet-mr_f1386_0()
{    return max;}
public int parquet-mr_f1387_0(boolean value)
{    return comparator().compare(min, value);}
public int parquet-mr_f1388_0(boolean value)
{    return comparator().compare(max, value);}
public boolean parquet-mr_f1389_0()
{    return max;}
public boolean parquet-mr_f1390_0()
{    return min;}
public void parquet-mr_f1391_0(boolean min, boolean max)
{    this.max = max;    this.min = min;    this.markAsNotEmpty();}
public BooleanStatistics parquet-mr_f1392_0()
{    return new BooleanStatistics(this);}
public void parquet-mr_f1393_0(double value)
{    if (!this.hasNonNullValue()) {        initializeStats(value, value);    } else {        updateStats(value, value);    }}
public void parquet-mr_f1394_0(Statistics stats)
{    DoubleStatistics doubleStats = (DoubleStatistics) stats;    if (!this.hasNonNullValue()) {        initializeStats(doubleStats.getMin(), doubleStats.getMax());    } else {        updateStats(doubleStats.getMin(), doubleStats.getMax());    }}
public void parquet-mr_f1395_0(byte[] minBytes, byte[] maxBytes)
{    max = Double.longBitsToDouble(BytesUtils.bytesToLong(maxBytes));    min = Double.longBitsToDouble(BytesUtils.bytesToLong(minBytes));    this.markAsNotEmpty();}
public byte[] parquet-mr_f1396_0()
{    return BytesUtils.longToBytes(Double.doubleToLongBits(max));}
public byte[] parquet-mr_f1397_0()
{    return BytesUtils.longToBytes(Double.doubleToLongBits(min));}
 String parquet-mr_f1398_0(Double value)
{    return stringifier.stringify(value);}
public boolean parquet-mr_f1399_0(long size)
{    return !hasNonNullValue() || (16 < size);}
public void parquet-mr_f1400_0(double min_value, double max_value)
{    if (comparator().compare(min, min_value) > 0) {        min = min_value;    }    if (comparator().compare(max, max_value) < 0) {        max = max_value;    }}
public void parquet-mr_f1401_0(double min_value, double max_value)
{    min = min_value;    max = max_value;    this.markAsNotEmpty();}
public Double parquet-mr_f1402_0()
{    return min;}
public Double parquet-mr_f1403_0()
{    return max;}
public int parquet-mr_f1404_0(double value)
{    return comparator().compare(min, value);}
public int parquet-mr_f1405_0(double value)
{    return comparator().compare(max, value);}
public double parquet-mr_f1406_0()
{    return max;}
public double parquet-mr_f1407_0()
{    return min;}
public void parquet-mr_f1408_0(double min, double max)
{    this.max = max;    this.min = min;    this.markAsNotEmpty();}
public DoubleStatistics parquet-mr_f1409_0()
{    return new DoubleStatistics(this);}
public void parquet-mr_f1410_0(float value)
{    if (!this.hasNonNullValue()) {        initializeStats(value, value);    } else {        updateStats(value, value);    }}
public void parquet-mr_f1411_0(Statistics stats)
{    FloatStatistics floatStats = (FloatStatistics) stats;    if (!this.hasNonNullValue()) {        initializeStats(floatStats.getMin(), floatStats.getMax());    } else {        updateStats(floatStats.getMin(), floatStats.getMax());    }}
public void parquet-mr_f1412_0(byte[] minBytes, byte[] maxBytes)
{    max = Float.intBitsToFloat(BytesUtils.bytesToInt(maxBytes));    min = Float.intBitsToFloat(BytesUtils.bytesToInt(minBytes));    this.markAsNotEmpty();}
public byte[] parquet-mr_f1413_0()
{    return BytesUtils.intToBytes(Float.floatToIntBits(max));}
public byte[] parquet-mr_f1414_0()
{    return BytesUtils.intToBytes(Float.floatToIntBits(min));}
 String parquet-mr_f1415_0(Float value)
{    return stringifier.stringify(value);}
public boolean parquet-mr_f1416_0(long size)
{    return !hasNonNullValue() || (8 < size);}
public void parquet-mr_f1417_0(float min_value, float max_value)
{    if (comparator().compare(min, min_value) > 0) {        min = min_value;    }    if (comparator().compare(max, max_value) < 0) {        max = max_value;    }}
public void parquet-mr_f1418_0(float min_value, float max_value)
{    min = min_value;    max = max_value;    this.markAsNotEmpty();}
public Float parquet-mr_f1419_0()
{    return min;}
public Float parquet-mr_f1420_0()
{    return max;}
public int parquet-mr_f1421_0(float value)
{    return comparator().compare(min, value);}
public int parquet-mr_f1422_0(float value)
{    return comparator().compare(max, value);}
public float parquet-mr_f1423_0()
{    return max;}
public float parquet-mr_f1424_0()
{    return min;}
public void parquet-mr_f1425_0(float min, float max)
{    this.max = max;    this.min = min;    this.markAsNotEmpty();}
public FloatStatistics parquet-mr_f1426_0()
{    return new FloatStatistics(this);}
public void parquet-mr_f1427_0(int value)
{    if (!this.hasNonNullValue()) {        initializeStats(value, value);    } else {        updateStats(value, value);    }}
public void parquet-mr_f1428_0(Statistics stats)
{    IntStatistics intStats = (IntStatistics) stats;    if (!this.hasNonNullValue()) {        initializeStats(intStats.getMin(), intStats.getMax());    } else {        updateStats(intStats.getMin(), intStats.getMax());    }}
public void parquet-mr_f1429_0(byte[] minBytes, byte[] maxBytes)
{    max = BytesUtils.bytesToInt(maxBytes);    min = BytesUtils.bytesToInt(minBytes);    this.markAsNotEmpty();}
public byte[] parquet-mr_f1430_0()
{    return BytesUtils.intToBytes(max);}
public byte[] parquet-mr_f1431_0()
{    return BytesUtils.intToBytes(min);}
 String parquet-mr_f1432_0(Integer value)
{    return stringifier.stringify(value);}
public boolean parquet-mr_f1433_0(long size)
{    return !hasNonNullValue() || (8 < size);}
public void parquet-mr_f1434_0(int min_value, int max_value)
{    if (comparator().compare(min, min_value) > 0) {        min = min_value;    }    if (comparator().compare(max, max_value) < 0) {        max = max_value;    }}
public void parquet-mr_f1435_0(int min_value, int max_value)
{    min = min_value;    max = max_value;    this.markAsNotEmpty();}
public Integer parquet-mr_f1436_0()
{    return min;}
public Integer parquet-mr_f1437_0()
{    return max;}
public int parquet-mr_f1438_0(int value)
{    return comparator().compare(min, value);}
public int parquet-mr_f1439_0(int value)
{    return comparator().compare(max, value);}
public int parquet-mr_f1440_0()
{    return max;}
public int parquet-mr_f1441_0()
{    return min;}
public void parquet-mr_f1442_0(int min, int max)
{    this.max = max;    this.min = min;    this.markAsNotEmpty();}
public IntStatistics parquet-mr_f1443_0()
{    return new IntStatistics(this);}
public void parquet-mr_f1444_0(long value)
{    if (!this.hasNonNullValue()) {        initializeStats(value, value);    } else {        updateStats(value, value);    }}
public void parquet-mr_f1445_0(Statistics stats)
{    LongStatistics longStats = (LongStatistics) stats;    if (!this.hasNonNullValue()) {        initializeStats(longStats.getMin(), longStats.getMax());    } else {        updateStats(longStats.getMin(), longStats.getMax());    }}
public void parquet-mr_f1446_0(byte[] minBytes, byte[] maxBytes)
{    max = BytesUtils.bytesToLong(maxBytes);    min = BytesUtils.bytesToLong(minBytes);    this.markAsNotEmpty();}
public byte[] parquet-mr_f1447_0()
{    return BytesUtils.longToBytes(max);}
public byte[] parquet-mr_f1448_0()
{    return BytesUtils.longToBytes(min);}
 String parquet-mr_f1449_0(Long value)
{    return stringifier.stringify(value);}
public boolean parquet-mr_f1450_0(long size)
{    return !hasNonNullValue() || (16 < size);}
public void parquet-mr_f1451_0(long min_value, long max_value)
{    if (comparator().compare(min, min_value) > 0) {        min = min_value;    }    if (comparator().compare(max, max_value) < 0) {        max = max_value;    }}
public void parquet-mr_f1452_0(long min_value, long max_value)
{    min = min_value;    max = max_value;    this.markAsNotEmpty();}
public Long parquet-mr_f1453_0()
{    return min;}
public Long parquet-mr_f1454_0()
{    return max;}
public int parquet-mr_f1455_0(long value)
{    return comparator().compare(min, value);}
public int parquet-mr_f1456_0(long value)
{    return comparator().compare(max, value);}
public long parquet-mr_f1457_0()
{    return max;}
public long parquet-mr_f1458_0()
{    return min;}
public void parquet-mr_f1459_0(long min, long max)
{    this.max = max;    this.min = min;    this.markAsNotEmpty();}
public LongStatistics parquet-mr_f1460_0()
{    return new LongStatistics(this);}
public Builder parquet-mr_f1461_0(byte[] min)
{    this.min = min;    return this;}
public Builder parquet-mr_f1462_0(byte[] max)
{    this.max = max;    return this;}
public Builder parquet-mr_f1463_0(long numNulls)
{    this.numNulls = numNulls;    return this;}
public Statistics<?> parquet-mr_f1464_0()
{    Statistics<?> stats = createStats(type);    if (min != null && max != null) {        stats.setMinMaxFromBytes(min, max);    }    stats.num_nulls = this.numNulls;    return stats;}
public Statistics<?> parquet-mr_f1465_0()
{    FloatStatistics stats = (FloatStatistics) super.build();    if (stats.hasNonNullValue()) {        Float min = stats.genericGetMin();        Float max = stats.genericGetMax();                if (min.isNaN() || max.isNaN()) {            stats.setMinMax(0.0f, 0.0f);            ((Statistics<?>) stats).hasNonNullValue = false;        } else {                        if (Float.compare(min, 0.0f) == 0) {                min = -0.0f;                stats.setMinMax(min, max);            }            if (Float.compare(max, -0.0f) == 0) {                max = 0.0f;                stats.setMinMax(min, max);            }        }    }    return stats;}
public Statistics<?> parquet-mr_f1466_0()
{    DoubleStatistics stats = (DoubleStatistics) super.build();    if (stats.hasNonNullValue()) {        Double min = stats.genericGetMin();        Double max = stats.genericGetMax();                if (min.isNaN() || max.isNaN()) {            stats.setMinMax(0.0, 0.0);            ((Statistics<?>) stats).hasNonNullValue = false;        } else {                        if (Double.compare(min, 0.0) == 0) {                min = -0.0;                stats.setMinMax(min, max);            }            if (Double.compare(max, -0.0) == 0) {                max = 0.0;                stats.setMinMax(min, max);            }        }    }    return stats;}
public static Statistics parquet-mr_f1467_0(PrimitiveTypeName type)
{    switch(type) {        case INT32:            return new IntStatistics();        case INT64:            return new LongStatistics();        case FLOAT:            return new FloatStatistics();        case DOUBLE:            return new DoubleStatistics();        case BOOLEAN:            return new BooleanStatistics();        case BINARY:            return new BinaryStatistics();        case INT96:            return new BinaryStatistics();        case FIXED_LEN_BYTE_ARRAY:            return new BinaryStatistics();        default:            throw new UnknownColumnTypeException(type);    }}
public static Statistics<?> parquet-mr_f1468_0(Type type)
{    PrimitiveType primitive = type.asPrimitiveType();    switch(primitive.getPrimitiveTypeName()) {        case INT32:            return new IntStatistics(primitive);        case INT64:            return new LongStatistics(primitive);        case FLOAT:            return new FloatStatistics(primitive);        case DOUBLE:            return new DoubleStatistics(primitive);        case BOOLEAN:            return new BooleanStatistics(primitive);        case BINARY:        case INT96:        case FIXED_LEN_BYTE_ARRAY:            return new BinaryStatistics(primitive);        default:            throw new UnknownColumnTypeException(primitive.getPrimitiveTypeName());    }}
public static Builder parquet-mr_f1469_0(PrimitiveType type)
{    switch(type.getPrimitiveTypeName()) {        case FLOAT:            return new FloatBuilder(type);        case DOUBLE:            return new DoubleBuilder(type);        default:            return new Builder(type);    }}
public void parquet-mr_f1470_0(int value)
{    throw new UnsupportedOperationException();}
public void parquet-mr_f1471_0(long value)
{    throw new UnsupportedOperationException();}
public void parquet-mr_f1472_0(float value)
{    throw new UnsupportedOperationException();}
public void parquet-mr_f1473_0(double value)
{    throw new UnsupportedOperationException();}
public void parquet-mr_f1474_0(boolean value)
{    throw new UnsupportedOperationException();}
public void parquet-mr_f1475_0(Binary value)
{    throw new UnsupportedOperationException();}
public boolean parquet-mr_f1476_0(Object other)
{    if (other == this)        return true;    if (!(other instanceof Statistics))        return false;    Statistics stats = (Statistics) other;    return type.equals(stats.type) && Arrays.equals(stats.getMaxBytes(), this.getMaxBytes()) && Arrays.equals(stats.getMinBytes(), this.getMinBytes()) && stats.getNumNulls() == this.getNumNulls();}
public int parquet-mr_f1477_0()
{    return 31 * type.hashCode() + 31 * Arrays.hashCode(getMaxBytes()) + 17 * Arrays.hashCode(getMinBytes()) + Long.valueOf(this.getNumNulls()).hashCode();}
public void parquet-mr_f1478_0(Statistics stats)
{    if (stats.isEmpty())        return;        if (type.equals(stats.type)) {        incrementNumNulls(stats.getNumNulls());        if (stats.hasNonNullValue()) {            mergeStatisticsMinMax(stats);            markAsNotEmpty();        }    } else {        throw StatisticsClassException.create(this, stats);    }}
public final PrimitiveComparator<T> parquet-mr_f1479_0()
{    return comparator;}
public final int parquet-mr_f1480_0(T value)
{    return comparator.compare(genericGetMin(), value);}
public final int parquet-mr_f1481_0(T value)
{    return comparator.compare(genericGetMax(), value);}
public String parquet-mr_f1482_0()
{    return stringify(genericGetMin());}
public String parquet-mr_f1483_0()
{    return stringify(genericGetMax());}
public String parquet-mr_f1484_0()
{    if (this.hasNonNullValue()) {        if (isNumNullsSet()) {            return String.format("min: %s, max: %s, num_nulls: %d", minAsString(), maxAsString(), this.getNumNulls());        } else {            return String.format("min: %s, max: %s, num_nulls not defined", minAsString(), maxAsString());        }    } else if (!this.isEmpty())        return String.format("num_nulls: %d, min/max not defined", this.getNumNulls());    else        return "no stats for this column";}
public void parquet-mr_f1485_0()
{    num_nulls++;}
public void parquet-mr_f1486_0(long increment)
{    num_nulls += increment;}
public long parquet-mr_f1487_0()
{    return num_nulls;}
public void parquet-mr_f1488_0(long nulls)
{    num_nulls = nulls;}
public boolean parquet-mr_f1489_0()
{    return !hasNonNullValue && !isNumNullsSet();}
public boolean parquet-mr_f1490_0()
{    return hasNonNullValue;}
public boolean parquet-mr_f1491_0()
{    return num_nulls >= 0;}
protected void parquet-mr_f1492_0()
{    hasNonNullValue = true;}
public PrimitiveType parquet-mr_f1493_0()
{    return type;}
 static StatisticsClassException parquet-mr_f1494_0(Statistics<?> stats1, Statistics<?> stats2)
{    if (stats1.getClass() != stats2.getClass()) {        return new StatisticsClassException(stats1.getClass().toString(), stats2.getClass().toString());    }    return new StatisticsClassException("Statistics comparator mismatched: " + stats1.comparator() + " vs. " + stats2.comparator());}
public ColumnDescriptor parquet-mr_f1495_0()
{    return descriptor;}
public PrimitiveTypeName parquet-mr_f1496_0()
{    return this.type;}
public int parquet-mr_f1497_0()
{    try {        return bitPackingReader.read();    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
public void parquet-mr_f1498_1(int valueCount, ByteBufferInputStream stream) throws IOException
{    int effectiveBitLength = valueCount * bitsPerValue;    int length = BytesUtils.paddedByteCountFromBits(effectiveBitLength);        this.in = stream.sliceStream(length);    this.bitPackingReader = createBitPackingReader(bitsPerValue, this.in, valueCount);    updateNextOffset(length);}
public void parquet-mr_f1499_0()
{    readInteger();}
private void parquet-mr_f1500_0()
{    this.bitPackingWriter = getBitPackingWriter(bitsPerValue, out);}
public void parquet-mr_f1501_0(int v)
{    try {        bitPackingWriter.write(v);    } catch (IOException e) {        throw new ParquetEncodingException(e);    }}
public long parquet-mr_f1502_0()
{    return out.size();}
public BytesInput parquet-mr_f1503_0()
{    try {        this.bitPackingWriter.finish();        return BytesInput.from(out);    } catch (IOException e) {        throw new ParquetEncodingException(e);    }}
public void parquet-mr_f1504_0()
{    out.reset();    init();}
public void parquet-mr_f1505_0()
{    out.close();}
public long parquet-mr_f1506_0()
{    return out.getCapacity();}
public String parquet-mr_f1507_0(String prefix)
{    return out.memUsageString(prefix);}
public Encoding parquet-mr_f1508_0()
{    return BIT_PACKED;}
public int parquet-mr_f1509_0()
{    ++decodedPosition;    if (decodedPosition == decoded.length) {        try {            if (in.available() < bitWidth) {                                                byte[] tempEncode = new byte[bitWidth];                in.read(tempEncode, 0, in.available());                packer.unpack8Values(tempEncode, 0, decoded, 0);            } else {                ByteBuffer encoded = in.slice(bitWidth);                packer.unpack8Values(encoded, encoded.position(), decoded, 0);            }        } catch (IOException e) {            throw new ParquetDecodingException("Failed to read packed values", e);        }        decodedPosition = 0;    }    return decoded[decodedPosition];}
public void parquet-mr_f1510_1(int valueCount, ByteBufferInputStream stream) throws IOException
{    int effectiveBitLength = valueCount * bitWidth;        int length = BytesUtils.paddedByteCountFromBits(effectiveBitLength);                    length = Math.min(length, stream.available());    this.in = stream.sliceStream(length);    this.decodedPosition = VALUES_AT_A_TIME - 1;    updateNextOffset(length);}
public void parquet-mr_f1511_0()
{    readInteger();}
public void parquet-mr_f1512_0(int v)
{    try {        this.encoder.writeInt(v);    } catch (IOException e) {        throw new ParquetEncodingException(e);    }}
public Encoding parquet-mr_f1513_0()
{    return BIT_PACKED;}
public BytesInput parquet-mr_f1514_0()
{    try {        return encoder.toBytes();    } catch (IOException e) {        throw new ParquetEncodingException(e);    }}
public void parquet-mr_f1515_0()
{    encoder = new ByteBasedBitPackingEncoder(bitWidth, packer);}
public long parquet-mr_f1516_0()
{    return encoder.getBufferSize();}
public long parquet-mr_f1517_0()
{    return encoder.getAllocatedSize();}
public String parquet-mr_f1518_0(String prefix)
{    return encoder.memUsageString(prefix);}
public long parquet-mr_f1519_0()
{    return 0;}
public void parquet-mr_f1520_0()
{}
public void parquet-mr_f1521_0(int v)
{}
public void parquet-mr_f1522_0(int value)
{}
public void parquet-mr_f1523_0(boolean v)
{}
public void parquet-mr_f1524_0(Binary v)
{}
public void parquet-mr_f1525_0(long v)
{}
public void parquet-mr_f1526_0(double v)
{}
public void parquet-mr_f1527_0(float v)
{}
public BytesInput parquet-mr_f1528_0()
{    return BytesInput.empty();}
public long parquet-mr_f1529_0()
{    return 0;}
public Encoding parquet-mr_f1530_0()
{    return BIT_PACKED;}
public String parquet-mr_f1531_0(String prefix)
{    return prefix + "0";}
public static DeltaBinaryPackingConfig parquet-mr_f1532_0(InputStream in) throws IOException
{    return new DeltaBinaryPackingConfig(BytesUtils.readUnsignedVarInt(in), BytesUtils.readUnsignedVarInt(in));}
public BytesInput parquet-mr_f1533_0()
{    return BytesInput.concat(BytesInput.fromUnsignedVarInt(blockSizeInValues), BytesInput.fromUnsignedVarInt(miniBlockNumInABlock));}
public void parquet-mr_f1534_0(int valueCount, ByteBufferInputStream stream) throws IOException
{    this.in = stream;    long startPos = in.position();    this.config = DeltaBinaryPackingConfig.readConfig(in);    this.totalValueCount = BytesUtils.readUnsignedVarInt(in);    allocateValuesBuffer();    bitWidths = new int[config.miniBlockNumInABlock];        valuesBuffer[valuesBuffered++] = BytesUtils.readZigZagVarLong(in);    while (valuesBuffered < totalValueCount) {                loadNewBlockToBuffer();    }    updateNextOffset((int) (in.position() - startPos));}
private void parquet-mr_f1535_0()
{    int totalMiniBlockCount = (int) Math.ceil((double) totalValueCount / config.miniBlockSizeInValues);        valuesBuffer = new long[totalMiniBlockCount * config.miniBlockSizeInValues + 1];}
public void parquet-mr_f1536_0()
{    checkRead();    valuesRead++;}
public void parquet-mr_f1537_0(int n)
{        valuesRead += n - 1;    checkRead();    ++valuesRead;}
public int parquet-mr_f1538_0()
{        return (int) readLong();}
public long parquet-mr_f1539_0()
{    checkRead();    return valuesBuffer[valuesRead++];}
private void parquet-mr_f1540_0()
{    if (valuesRead >= totalValueCount) {        throw new ParquetDecodingException("no more value to read, total value count is " + totalValueCount);    }}
private void parquet-mr_f1541_0() throws IOException
{    try {        minDeltaInCurrentBlock = BytesUtils.readZigZagVarLong(in);    } catch (IOException e) {        throw new ParquetDecodingException("can not read min delta in current block", e);    }    readBitWidthsForMiniBlocks();        int i;    for (i = 0; i < config.miniBlockNumInABlock && valuesBuffered < totalValueCount; i++) {        BytePackerForLong packer = Packer.LITTLE_ENDIAN.newBytePackerForLong(bitWidths[i]);        unpackMiniBlock(packer);    }        int valueUnpacked = i * config.miniBlockSizeInValues;    for (int j = valuesBuffered - valueUnpacked; j < valuesBuffered; j++) {        int index = j;        valuesBuffer[index] += minDeltaInCurrentBlock + valuesBuffer[index - 1];    }}
private void parquet-mr_f1542_0(BytePackerForLong packer) throws IOException
{    for (int j = 0; j < config.miniBlockSizeInValues; j += 8) {        unpack8Values(packer);    }}
private void parquet-mr_f1543_0(BytePackerForLong packer) throws IOException
{            ByteBuffer buffer = in.slice(packer.getBitWidth());    packer.unpack8Values(buffer, buffer.position(), valuesBuffer, valuesBuffered);    this.valuesBuffered += 8;}
private void parquet-mr_f1544_0()
{    for (int i = 0; i < config.miniBlockNumInABlock; i++) {        try {            bitWidths[i] = BytesUtils.readIntLittleEndianOnOneByte(in);        } catch (IOException e) {            throw new ParquetDecodingException("Can not decode bitwidth in block header", e);        }    }}
public long parquet-mr_f1545_0()
{    return baos.size();}
protected void parquet-mr_f1546_0(int i)
{    try {        BytesUtils.writeIntLittleEndianOnOneByte(baos, bitWidths[i]);    } catch (IOException e) {        throw new ParquetEncodingException("can not write bitwith for miniblock", e);    }}
protected int parquet-mr_f1547_0(double numberCount)
{    return (int) Math.ceil(numberCount / config.miniBlockSizeInValues);}
public Encoding parquet-mr_f1548_0()
{    return Encoding.DELTA_BINARY_PACKED;}
public void parquet-mr_f1549_0()
{    this.totalValueCount = 0;    this.baos.reset();    this.deltaValuesToFlush = 0;}
public void parquet-mr_f1550_0()
{    this.totalValueCount = 0;    this.baos.close();    this.deltaValuesToFlush = 0;}
public long parquet-mr_f1551_0()
{    return baos.getCapacity();}
public String parquet-mr_f1552_0(String prefix)
{    return String.format("%s DeltaBinaryPacking %d bytes", prefix, getAllocatedSize());}
public void parquet-mr_f1553_0(int v)
{    totalValueCount++;    if (totalValueCount == 1) {        firstValue = v;        previousValue = firstValue;        return;    }                int delta = v - previousValue;    previousValue = v;    deltaBlockBuffer[deltaValuesToFlush++] = delta;    if (delta < minDeltaInCurrentBlock) {        minDeltaInCurrentBlock = delta;    }    if (config.blockSizeInValues == deltaValuesToFlush) {        flushBlockBuffer();    }}
private void parquet-mr_f1554_0()
{        for (int i = 0; i < deltaValuesToFlush; i++) {        deltaBlockBuffer[i] = deltaBlockBuffer[i] - minDeltaInCurrentBlock;    }    writeMinDelta();    int miniBlocksToFlush = getMiniBlockCountToFlush(deltaValuesToFlush);    calculateBitWidthsForDeltaBlockBuffer(miniBlocksToFlush);    for (int i = 0; i < config.miniBlockNumInABlock; i++) {        writeBitWidthForMiniBlock(i);    }    for (int i = 0; i < miniBlocksToFlush; i++) {                int currentBitWidth = bitWidths[i];        int blockOffset = 0;        BytePacker packer = Packer.LITTLE_ENDIAN.newBytePacker(currentBitWidth);        int miniBlockStart = i * config.miniBlockSizeInValues;        for (int j = miniBlockStart; j < (i + 1) * config.miniBlockSizeInValues; j += 8) {                                                                        packer.pack8Values(deltaBlockBuffer, j, miniBlockByteBuffer, blockOffset);            blockOffset += currentBitWidth;        }        baos.write(miniBlockByteBuffer, 0, blockOffset);    }    minDeltaInCurrentBlock = Integer.MAX_VALUE;    deltaValuesToFlush = 0;}
private void parquet-mr_f1555_0()
{    try {        BytesUtils.writeZigZagVarInt(minDeltaInCurrentBlock, baos);    } catch (IOException e) {        throw new ParquetEncodingException("can not write min delta for block", e);    }}
private void parquet-mr_f1556_0(int miniBlocksToFlush)
{    for (int miniBlockIndex = 0; miniBlockIndex < miniBlocksToFlush; miniBlockIndex++) {        int mask = 0;        int miniStart = miniBlockIndex * config.miniBlockSizeInValues;                int miniEnd = Math.min((miniBlockIndex + 1) * config.miniBlockSizeInValues, deltaValuesToFlush);        for (int i = miniStart; i < miniEnd; i++) {            mask |= deltaBlockBuffer[i];        }        bitWidths[miniBlockIndex] = 32 - Integer.numberOfLeadingZeros(mask);    }}
public BytesInput parquet-mr_f1557_0()
{        if (deltaValuesToFlush != 0) {        flushBlockBuffer();    }    return BytesInput.concat(config.toBytesInput(), BytesInput.fromUnsignedVarInt(totalValueCount), BytesInput.fromZigZagVarInt(firstValue), BytesInput.from(baos));}
public void parquet-mr_f1558_0()
{    super.reset();    this.minDeltaInCurrentBlock = Integer.MAX_VALUE;}
public void parquet-mr_f1559_0()
{    super.close();    this.minDeltaInCurrentBlock = Integer.MAX_VALUE;}
public void parquet-mr_f1560_0(long v)
{    totalValueCount++;    if (totalValueCount == 1) {        firstValue = v;        previousValue = firstValue;        return;    }                long delta = v - previousValue;    previousValue = v;    deltaBlockBuffer[deltaValuesToFlush++] = delta;    if (delta < minDeltaInCurrentBlock) {        minDeltaInCurrentBlock = delta;    }    if (config.blockSizeInValues == deltaValuesToFlush) {        flushBlockBuffer();    }}
private void parquet-mr_f1561_0()
{        for (int i = 0; i < deltaValuesToFlush; i++) {        deltaBlockBuffer[i] = deltaBlockBuffer[i] - minDeltaInCurrentBlock;    }    writeMinDelta();    int miniBlocksToFlush = getMiniBlockCountToFlush(deltaValuesToFlush);    calculateBitWidthsForDeltaBlockBuffer(miniBlocksToFlush);    for (int i = 0; i < config.miniBlockNumInABlock; i++) {        writeBitWidthForMiniBlock(i);    }    for (int i = 0; i < miniBlocksToFlush; i++) {                int currentBitWidth = bitWidths[i];        int blockOffset = 0;                BytePackerForLong packer = Packer.LITTLE_ENDIAN.newBytePackerForLong(currentBitWidth);        int miniBlockStart = i * config.miniBlockSizeInValues;                for (int j = miniBlockStart; j < (i + 1) * config.miniBlockSizeInValues; j += 8) {                                                            packer.pack8Values(deltaBlockBuffer, j, miniBlockByteBuffer, blockOffset);            blockOffset += currentBitWidth;        }        baos.write(miniBlockByteBuffer, 0, blockOffset);    }    minDeltaInCurrentBlock = Long.MAX_VALUE;    deltaValuesToFlush = 0;}
private void parquet-mr_f1562_0()
{    try {        BytesUtils.writeZigZagVarLong(minDeltaInCurrentBlock, baos);    } catch (IOException e) {        throw new ParquetEncodingException("can not write min delta for block", e);    }}
private void parquet-mr_f1563_0(int miniBlocksToFlush)
{    for (int miniBlockIndex = 0; miniBlockIndex < miniBlocksToFlush; miniBlockIndex++) {        long mask = 0;        int miniStart = miniBlockIndex * config.miniBlockSizeInValues;                int miniEnd = Math.min((miniBlockIndex + 1) * config.miniBlockSizeInValues, deltaValuesToFlush);        for (int i = miniStart; i < miniEnd; i++) {            mask |= deltaBlockBuffer[i];        }        bitWidths[miniBlockIndex] = 64 - Long.numberOfLeadingZeros(mask);    }}
public BytesInput parquet-mr_f1564_0()
{        if (deltaValuesToFlush != 0) {        flushBlockBuffer();    }    return BytesInput.concat(config.toBytesInput(), BytesInput.fromUnsignedVarInt(totalValueCount), BytesInput.fromZigZagVarLong(firstValue), BytesInput.from(baos));}
public void parquet-mr_f1565_0()
{    super.reset();    this.minDeltaInCurrentBlock = Long.MAX_VALUE;}
public void parquet-mr_f1566_0()
{    super.close();    this.minDeltaInCurrentBlock = Long.MAX_VALUE;}
public void parquet-mr_f1567_1(int valueCount, ByteBufferInputStream stream) throws IOException
{        lengthReader.initFromPage(valueCount, stream);    this.in = stream.remainingStream();}
public Binary parquet-mr_f1568_0()
{    int length = lengthReader.readInteger();    try {        return Binary.fromConstantByteBuffer(in.slice(length));    } catch (IOException e) {        throw new ParquetDecodingException("Failed to read " + length + " bytes");    }}
public void parquet-mr_f1569_0()
{    skip(1);}
public void parquet-mr_f1570_0(int n)
{    int length = 0;    for (int i = 0; i < n; ++i) {        length += lengthReader.readInteger();    }    try {        in.skipFully(length);    } catch (IOException e) {        throw new ParquetDecodingException("Failed to skip " + length + " bytes");    }}
public void parquet-mr_f1571_0(Binary v)
{    try {        lengthWriter.writeInteger(v.length());        v.writeTo(out);    } catch (IOException e) {        throw new ParquetEncodingException("could not write bytes", e);    }}
public long parquet-mr_f1572_0()
{    return lengthWriter.getBufferedSize() + arrayOut.size();}
public BytesInput parquet-mr_f1573_1()
{    try {        out.flush();    } catch (IOException e) {        throw new ParquetEncodingException("could not write page", e);    }        return BytesInput.concat(lengthWriter.getBytes(), BytesInput.from(arrayOut));}
public Encoding parquet-mr_f1574_0()
{    return Encoding.DELTA_LENGTH_BYTE_ARRAY;}
public void parquet-mr_f1575_0()
{    lengthWriter.reset();    arrayOut.reset();}
public void parquet-mr_f1576_0()
{    lengthWriter.close();    arrayOut.close();}
public long parquet-mr_f1577_0()
{    return lengthWriter.getAllocatedSize() + arrayOut.getCapacity();}
public String parquet-mr_f1578_0(String prefix)
{    return arrayOut.memUsageString(lengthWriter.memUsageString(prefix) + " DELTA_LENGTH_BYTE_ARRAY");}
public void parquet-mr_f1579_0(int valueCount, ByteBufferInputStream stream) throws IOException
{    prefixLengthReader.initFromPage(valueCount, stream);    suffixReader.initFromPage(valueCount, stream);}
public void parquet-mr_f1580_0()
{        readBytes();}
public Binary parquet-mr_f1581_0()
{    int prefixLength = prefixLengthReader.readInteger();        Binary suffix = suffixReader.readBytes();    int length = prefixLength + suffix.length();        if (prefixLength != 0) {        byte[] out = new byte[length];        System.arraycopy(previous.getBytesUnsafe(), 0, out, 0, prefixLength);        System.arraycopy(suffix.getBytesUnsafe(), 0, out, prefixLength, suffix.length());        previous = Binary.fromConstantByteArray(out);    } else {        previous = suffix;    }    return previous;}
public void parquet-mr_f1582_0(ValuesReader reader)
{    if (reader != null) {        this.previous = ((DeltaByteArrayReader) reader).previous;    }}
public long parquet-mr_f1583_0()
{    return prefixLengthWriter.getBufferedSize() + suffixWriter.getBufferedSize();}
public BytesInput parquet-mr_f1584_0()
{    return BytesInput.concat(prefixLengthWriter.getBytes(), suffixWriter.getBytes());}
public Encoding parquet-mr_f1585_0()
{    return Encoding.DELTA_BYTE_ARRAY;}
public void parquet-mr_f1586_0()
{    prefixLengthWriter.reset();    suffixWriter.reset();    previous = new byte[0];}
public void parquet-mr_f1587_0()
{    prefixLengthWriter.close();    suffixWriter.close();}
public long parquet-mr_f1588_0()
{    return prefixLengthWriter.getAllocatedSize() + suffixWriter.getAllocatedSize();}
public String parquet-mr_f1589_0(String prefix)
{    prefix = prefixLengthWriter.memUsageString(prefix);    return suffixWriter.memUsageString(prefix + "  DELTA_STRINGS");}
public void parquet-mr_f1590_0(Binary v)
{    int i = 0;    byte[] vb = v.getBytes();    int length = previous.length < vb.length ? previous.length : vb.length;        for (i = 0; (i < length) && (previous[i] == vb[i]); i++) ;    prefixLengthWriter.writeInteger(i);    suffixWriter.writeBytes(v.slice(i, vb.length - i));    previous = vb;}
public void parquet-mr_f1591_1(int valueCount, ByteBufferInputStream stream) throws IOException
{    this.in = stream.remainingStream();    if (in.available() > 0) {                int bitWidth = BytesUtils.readIntLittleEndianOnOneByte(in);                decoder = new RunLengthBitPackingHybridDecoder(bitWidth, in);    } else {        decoder = new RunLengthBitPackingHybridDecoder(1, in) {            @Override            public int readInt() throws IOException {                throw new IOException("Attempt to read from empty page");            }        };    }}
public int parquet-mr_f1592_0() throws IOException
{    throw new IOException("Attempt to read from empty page");}
public int parquet-mr_f1593_0()
{    try {        return decoder.readInt();    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
public Binary parquet-mr_f1594_0()
{    try {        return dictionary.decodeToBinary(decoder.readInt());    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
public float parquet-mr_f1595_0()
{    try {        return dictionary.decodeToFloat(decoder.readInt());    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
public double parquet-mr_f1596_0()
{    try {        return dictionary.decodeToDouble(decoder.readInt());    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
public int parquet-mr_f1597_0()
{    try {        return dictionary.decodeToInt(decoder.readInt());    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
public long parquet-mr_f1598_0()
{    try {        return dictionary.decodeToLong(decoder.readInt());    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
public void parquet-mr_f1599_0()
{    try {                decoder.readInt();    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
protected DictionaryPage parquet-mr_f1600_0(ValuesWriter dictPageWriter)
{    DictionaryPage ret = new DictionaryPage(dictPageWriter.getBytes(), lastUsedDictionarySize, encodingForDictionaryPage);    dictPageWriter.close();    return ret;}
public boolean parquet-mr_f1601_0()
{        return dictionaryByteSize > maxDictionaryByteSize || getDictionarySize() > MAX_DICTIONARY_ENTRIES;}
public boolean parquet-mr_f1602_0(long rawSize, long encodedSize)
{    return (encodedSize + dictionaryByteSize) < rawSize;}
public void parquet-mr_f1603_0(ValuesWriter writer)
{    fallBackDictionaryEncodedData(writer);    if (lastUsedDictionarySize == 0) {                        clearDictionaryContent();        dictionaryByteSize = 0;        encodedValues = new IntList();    }}
public long parquet-mr_f1604_0()
{    return encodedValues.size() * 4;}
public long parquet-mr_f1605_0()
{        return encodedValues.size() * 4 + dictionaryByteSize;}
public BytesInput parquet-mr_f1606_1()
{    int maxDicId = getDictionarySize() - 1;        int bitWidth = BytesUtils.getWidthFromMaxInt(maxDicId);    int initialSlabSize = CapacityByteArrayOutputStream.initialSlabSizeHeuristic(MIN_INITIAL_SLAB_SIZE, maxDictionaryByteSize, 10);    RunLengthBitPackingHybridEncoder encoder = new RunLengthBitPackingHybridEncoder(bitWidth, initialSlabSize, maxDictionaryByteSize, this.allocator);    encoders.add(encoder);    IntIterator iterator = encodedValues.iterator();    try {        while (iterator.hasNext()) {            encoder.writeInt(iterator.next());        }                byte[] bytesHeader = new byte[] { (byte) bitWidth };        BytesInput rleEncodedBytes = encoder.toBytes();                BytesInput bytes = concat(BytesInput.from(bytesHeader), rleEncodedBytes);                lastUsedDictionarySize = getDictionarySize();        lastUsedDictionaryByteSize = dictionaryByteSize;        return bytes;    } catch (IOException e) {        throw new ParquetEncodingException("could not encode the values", e);    }}
public Encoding parquet-mr_f1607_0()
{    return encodingForDataPage;}
public void parquet-mr_f1608_0()
{    close();    encodedValues = new IntList();}
public void parquet-mr_f1609_0()
{    encodedValues = null;    for (RunLengthBitPackingHybridEncoder encoder : encoders) {        encoder.close();    }    encoders.clear();}
public void parquet-mr_f1610_0()
{    lastUsedDictionaryByteSize = 0;    lastUsedDictionarySize = 0;    dictionaryTooBig = false;    clearDictionaryContent();}
public String parquet-mr_f1611_0(String prefix)
{    return String.format("%s DictionaryValuesWriter{\n" + "%s\n" + "%s\n" + "%s}\n", prefix, prefix + " dict:" + dictionaryByteSize, prefix + " values:" + String.valueOf(encodedValues.size() * 4), prefix);}
public void parquet-mr_f1612_0(Binary v)
{    int id = binaryDictionaryContent.getInt(v);    if (id == -1) {        id = binaryDictionaryContent.size();        binaryDictionaryContent.put(v.copy(), id);                dictionaryByteSize += 4 + v.length();    }    encodedValues.add(id);}
public DictionaryPage parquet-mr_f1613_0()
{    if (lastUsedDictionarySize > 0) {                PlainValuesWriter dictionaryEncoder = new PlainValuesWriter(lastUsedDictionaryByteSize, maxDictionaryByteSize, allocator);        Iterator<Binary> binaryIterator = binaryDictionaryContent.keySet().iterator();                for (int i = 0; i < lastUsedDictionarySize; i++) {            Binary entry = binaryIterator.next();            dictionaryEncoder.writeBytes(entry);        }        return dictPage(dictionaryEncoder);    }    return null;}
public int parquet-mr_f1614_0()
{    return binaryDictionaryContent.size();}
protected void parquet-mr_f1615_0()
{    binaryDictionaryContent.clear();}
public void parquet-mr_f1616_0(ValuesWriter writer)
{        Binary[] reverseDictionary = new Binary[getDictionarySize()];    for (Object2IntMap.Entry<Binary> entry : binaryDictionaryContent.object2IntEntrySet()) {        reverseDictionary[entry.getIntValue()] = entry.getKey();    }        IntIterator iterator = encodedValues.iterator();    while (iterator.hasNext()) {        int id = iterator.next();        writer.writeBytes(reverseDictionary[id]);    }}
public void parquet-mr_f1617_0(Binary value)
{    int id = binaryDictionaryContent.getInt(value);    if (id == -1) {        id = binaryDictionaryContent.size();        binaryDictionaryContent.put(value.copy(), id);        dictionaryByteSize += length;    }    encodedValues.add(id);}
public DictionaryPage parquet-mr_f1618_0()
{    if (lastUsedDictionarySize > 0) {                FixedLenByteArrayPlainValuesWriter dictionaryEncoder = new FixedLenByteArrayPlainValuesWriter(length, lastUsedDictionaryByteSize, maxDictionaryByteSize, allocator);        Iterator<Binary> binaryIterator = binaryDictionaryContent.keySet().iterator();                for (int i = 0; i < lastUsedDictionarySize; i++) {            Binary entry = binaryIterator.next();            dictionaryEncoder.writeBytes(entry);        }        return dictPage(dictionaryEncoder);    }    return null;}
public void parquet-mr_f1619_0(long v)
{    int id = longDictionaryContent.get(v);    if (id == -1) {        id = longDictionaryContent.size();        longDictionaryContent.put(v, id);        dictionaryByteSize += 8;    }    encodedValues.add(id);}
public DictionaryPage parquet-mr_f1620_0()
{    if (lastUsedDictionarySize > 0) {                PlainValuesWriter dictionaryEncoder = new PlainValuesWriter(lastUsedDictionaryByteSize, maxDictionaryByteSize, allocator);        LongIterator longIterator = longDictionaryContent.keySet().iterator();                for (int i = 0; i < lastUsedDictionarySize; i++) {            dictionaryEncoder.writeLong(longIterator.nextLong());        }        return dictPage(dictionaryEncoder);    }    return null;}
public int parquet-mr_f1621_0()
{    return longDictionaryContent.size();}
protected void parquet-mr_f1622_0()
{    longDictionaryContent.clear();}
public void parquet-mr_f1623_0(ValuesWriter writer)
{        long[] reverseDictionary = new long[getDictionarySize()];    ObjectIterator<Long2IntMap.Entry> entryIterator = longDictionaryContent.long2IntEntrySet().iterator();    while (entryIterator.hasNext()) {        Long2IntMap.Entry entry = entryIterator.next();        reverseDictionary[entry.getIntValue()] = entry.getLongKey();    }        IntIterator iterator = encodedValues.iterator();    while (iterator.hasNext()) {        int id = iterator.next();        writer.writeLong(reverseDictionary[id]);    }}
public void parquet-mr_f1624_0(double v)
{    int id = doubleDictionaryContent.get(v);    if (id == -1) {        id = doubleDictionaryContent.size();        doubleDictionaryContent.put(v, id);        dictionaryByteSize += 8;    }    encodedValues.add(id);}
public DictionaryPage parquet-mr_f1625_0()
{    if (lastUsedDictionarySize > 0) {                PlainValuesWriter dictionaryEncoder = new PlainValuesWriter(lastUsedDictionaryByteSize, maxDictionaryByteSize, allocator);        DoubleIterator doubleIterator = doubleDictionaryContent.keySet().iterator();                for (int i = 0; i < lastUsedDictionarySize; i++) {            dictionaryEncoder.writeDouble(doubleIterator.nextDouble());        }        return dictPage(dictionaryEncoder);    }    return null;}
public int parquet-mr_f1626_0()
{    return doubleDictionaryContent.size();}
protected void parquet-mr_f1627_0()
{    doubleDictionaryContent.clear();}
public void parquet-mr_f1628_0(ValuesWriter writer)
{        double[] reverseDictionary = new double[getDictionarySize()];    ObjectIterator<Double2IntMap.Entry> entryIterator = doubleDictionaryContent.double2IntEntrySet().iterator();    while (entryIterator.hasNext()) {        Double2IntMap.Entry entry = entryIterator.next();        reverseDictionary[entry.getIntValue()] = entry.getDoubleKey();    }        IntIterator iterator = encodedValues.iterator();    while (iterator.hasNext()) {        int id = iterator.next();        writer.writeDouble(reverseDictionary[id]);    }}
public void parquet-mr_f1629_0(int v)
{    int id = intDictionaryContent.get(v);    if (id == -1) {        id = intDictionaryContent.size();        intDictionaryContent.put(v, id);        dictionaryByteSize += 4;    }    encodedValues.add(id);}
public DictionaryPage parquet-mr_f1630_0()
{    if (lastUsedDictionarySize > 0) {                PlainValuesWriter dictionaryEncoder = new PlainValuesWriter(lastUsedDictionaryByteSize, maxDictionaryByteSize, allocator);        it.unimi.dsi.fastutil.ints.IntIterator intIterator = intDictionaryContent.keySet().iterator();                for (int i = 0; i < lastUsedDictionarySize; i++) {            dictionaryEncoder.writeInteger(intIterator.nextInt());        }        return dictPage(dictionaryEncoder);    }    return null;}
public int parquet-mr_f1631_0()
{    return intDictionaryContent.size();}
protected void parquet-mr_f1632_0()
{    intDictionaryContent.clear();}
public void parquet-mr_f1633_0(ValuesWriter writer)
{        int[] reverseDictionary = new int[getDictionarySize()];    ObjectIterator<Int2IntMap.Entry> entryIterator = intDictionaryContent.int2IntEntrySet().iterator();    while (entryIterator.hasNext()) {        Int2IntMap.Entry entry = entryIterator.next();        reverseDictionary[entry.getIntValue()] = entry.getIntKey();    }        IntIterator iterator = encodedValues.iterator();    while (iterator.hasNext()) {        int id = iterator.next();        writer.writeInteger(reverseDictionary[id]);    }}
public void parquet-mr_f1634_0(float v)
{    int id = floatDictionaryContent.get(v);    if (id == -1) {        id = floatDictionaryContent.size();        floatDictionaryContent.put(v, id);        dictionaryByteSize += 4;    }    encodedValues.add(id);}
public DictionaryPage parquet-mr_f1635_0()
{    if (lastUsedDictionarySize > 0) {                PlainValuesWriter dictionaryEncoder = new PlainValuesWriter(lastUsedDictionaryByteSize, maxDictionaryByteSize, allocator);        FloatIterator floatIterator = floatDictionaryContent.keySet().iterator();                for (int i = 0; i < lastUsedDictionarySize; i++) {            dictionaryEncoder.writeFloat(floatIterator.nextFloat());        }        return dictPage(dictionaryEncoder);    }    return null;}
public int parquet-mr_f1636_0()
{    return floatDictionaryContent.size();}
protected void parquet-mr_f1637_0()
{    floatDictionaryContent.clear();}
public void parquet-mr_f1638_0(ValuesWriter writer)
{        float[] reverseDictionary = new float[getDictionarySize()];    ObjectIterator<Float2IntMap.Entry> entryIterator = floatDictionaryContent.float2IntEntrySet().iterator();    while (entryIterator.hasNext()) {        Float2IntMap.Entry entry = entryIterator.next();        reverseDictionary[entry.getIntValue()] = entry.getFloatKey();    }        IntIterator iterator = encodedValues.iterator();    while (iterator.hasNext()) {        int id = iterator.next();        writer.writeFloat(reverseDictionary[id]);    }}
 int parquet-mr_f1639_0()
{    return currentSlabSize;}
public boolean parquet-mr_f1640_0()
{    return current < count;}
public int parquet-mr_f1641_0()
{    final int result = slabs[currentRow][currentCol];    incrementPosition();    return result;}
private void parquet-mr_f1642_0()
{    current++;    currentCol++;    if (currentCol >= slabs[currentRow].length) {        currentCol = 0;        currentRow++;    }}
private void parquet-mr_f1643_0()
{    currentSlab = new int[currentSlabSize];    currentSlabPos = 0;}
private void parquet-mr_f1644_0()
{    if (currentSlabSize < MAX_SLAB_SIZE) {        currentSlabSize *= 2;        if (currentSlabSize > MAX_SLAB_SIZE) {            currentSlabSize = MAX_SLAB_SIZE;        }    }}
public void parquet-mr_f1645_0(int i)
{    if (currentSlab == null) {        allocateSlab();    } else if (currentSlabPos == currentSlab.length) {        slabs.add(currentSlab);        updateCurrentSlabSize();        allocateSlab();    }    currentSlab[currentSlabPos] = i;    ++currentSlabPos;}
public IntIterator parquet-mr_f1646_0()
{    if (currentSlab == null) {        allocateSlab();    }    int[][] itSlabs = slabs.toArray(new int[slabs.size() + 1][]);    itSlabs[slabs.size()] = currentSlab;    return new IntIterator(itSlabs, size());}
public int parquet-mr_f1647_0()
{    int size = currentSlabPos;    for (int[] slab : slabs) {        size += slab.length;    }    return size;}
public Binary parquet-mr_f1648_0(int id)
{    return binaryDictionaryContent[id];}
public String parquet-mr_f1649_0()
{    StringBuilder sb = new StringBuilder("PlainBinaryDictionary {\n");    for (int i = 0; i < binaryDictionaryContent.length; i++) {        sb.append(i).append(" => ").append(binaryDictionaryContent[i]).append("\n");    }    return sb.append("}").toString();}
public int parquet-mr_f1650_0()
{    return binaryDictionaryContent.length - 1;}
public long parquet-mr_f1651_0(int id)
{    return longDictionaryContent[id];}
public String parquet-mr_f1652_0()
{    StringBuilder sb = new StringBuilder("PlainLongDictionary {\n");    for (int i = 0; i < longDictionaryContent.length; i++) {        sb.append(i).append(" => ").append(longDictionaryContent[i]).append("\n");    }    return sb.append("}").toString();}
public int parquet-mr_f1653_0()
{    return longDictionaryContent.length - 1;}
public double parquet-mr_f1654_0(int id)
{    return doubleDictionaryContent[id];}
public String parquet-mr_f1655_0()
{    StringBuilder sb = new StringBuilder("PlainDoubleDictionary {\n");    for (int i = 0; i < doubleDictionaryContent.length; i++) {        sb.append(i).append(" => ").append(doubleDictionaryContent[i]).append("\n");    }    return sb.append("}").toString();}
public int parquet-mr_f1656_0()
{    return doubleDictionaryContent.length - 1;}
public int parquet-mr_f1657_0(int id)
{    return intDictionaryContent[id];}
public String parquet-mr_f1658_0()
{    StringBuilder sb = new StringBuilder("PlainIntegerDictionary {\n");    for (int i = 0; i < intDictionaryContent.length; i++) {        sb.append(i).append(" => ").append(intDictionaryContent[i]).append("\n");    }    return sb.append("}").toString();}
public int parquet-mr_f1659_0()
{    return intDictionaryContent.length - 1;}
public float parquet-mr_f1660_0(int id)
{    return floatDictionaryContent[id];}
public String parquet-mr_f1661_0()
{    StringBuilder sb = new StringBuilder("PlainFloatDictionary {\n");    for (int i = 0; i < floatDictionaryContent.length; i++) {        sb.append(i).append(" => ").append(floatDictionaryContent[i]).append("\n");    }    return sb.append("}").toString();}
public int parquet-mr_f1662_0()
{    return floatDictionaryContent.length - 1;}
public void parquet-mr_f1663_0(ParquetProperties properties)
{    this.parquetProperties = properties;}
private Encoding parquet-mr_f1664_0()
{    return PLAIN_DICTIONARY;}
private Encoding parquet-mr_f1665_0()
{    return PLAIN_DICTIONARY;}
public ValuesWriter parquet-mr_f1666_0(ColumnDescriptor descriptor)
{    switch(descriptor.getType()) {        case BOOLEAN:            return getBooleanValuesWriter();        case FIXED_LEN_BYTE_ARRAY:            return getFixedLenByteArrayValuesWriter(descriptor);        case BINARY:            return getBinaryValuesWriter(descriptor);        case INT32:            return getInt32ValuesWriter(descriptor);        case INT64:            return getInt64ValuesWriter(descriptor);        case INT96:            return getInt96ValuesWriter(descriptor);        case DOUBLE:            return getDoubleValuesWriter(descriptor);        case FLOAT:            return getFloatValuesWriter(descriptor);        default:            throw new IllegalArgumentException("Unknown type " + descriptor.getType());    }}
private ValuesWriter parquet-mr_f1667_0()
{        return new BooleanPlainValuesWriter();}
private ValuesWriter parquet-mr_f1668_0(ColumnDescriptor path)
{        return new FixedLenByteArrayPlainValuesWriter(path.getTypeLength(), parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());}
private ValuesWriter parquet-mr_f1669_0(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new PlainValuesWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
private ValuesWriter parquet-mr_f1670_0(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new PlainValuesWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
private ValuesWriter parquet-mr_f1671_0(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new PlainValuesWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
private ValuesWriter parquet-mr_f1672_0(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new FixedLenByteArrayPlainValuesWriter(12, parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
private ValuesWriter parquet-mr_f1673_0(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new PlainValuesWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
private ValuesWriter parquet-mr_f1674_0(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new PlainValuesWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
public void parquet-mr_f1675_0(ParquetProperties properties)
{    this.parquetProperties = properties;}
private Encoding parquet-mr_f1676_0()
{    return RLE_DICTIONARY;}
private Encoding parquet-mr_f1677_0()
{    return PLAIN;}
public ValuesWriter parquet-mr_f1678_0(ColumnDescriptor descriptor)
{    switch(descriptor.getType()) {        case BOOLEAN:            return getBooleanValuesWriter();        case FIXED_LEN_BYTE_ARRAY:            return getFixedLenByteArrayValuesWriter(descriptor);        case BINARY:            return getBinaryValuesWriter(descriptor);        case INT32:            return getInt32ValuesWriter(descriptor);        case INT64:            return getInt64ValuesWriter(descriptor);        case INT96:            return getInt96ValuesWriter(descriptor);        case DOUBLE:            return getDoubleValuesWriter(descriptor);        case FLOAT:            return getFloatValuesWriter(descriptor);        default:            throw new IllegalArgumentException("Unknown type " + descriptor.getType());    }}
private ValuesWriter parquet-mr_f1679_0()
{        return new RunLengthBitPackingHybridValuesWriter(1, parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());}
private ValuesWriter parquet-mr_f1680_0(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new DeltaByteArrayWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
private ValuesWriter parquet-mr_f1681_0(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new DeltaByteArrayWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
private ValuesWriter parquet-mr_f1682_0(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new DeltaBinaryPackingValuesWriterForInteger(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
private ValuesWriter parquet-mr_f1683_0(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new DeltaBinaryPackingValuesWriterForLong(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
private ValuesWriter parquet-mr_f1684_0(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new FixedLenByteArrayPlainValuesWriter(12, parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
private ValuesWriter parquet-mr_f1685_0(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new PlainValuesWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
private ValuesWriter parquet-mr_f1686_0(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new PlainValuesWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
public void parquet-mr_f1687_0(ParquetProperties properties)
{    if (properties.getWriterVersion() == WriterVersion.PARQUET_1_0) {        delegateFactory = DEFAULT_V1_WRITER_FACTORY;    } else {        delegateFactory = DEFAULT_V2_WRITER_FACTORY;    }    delegateFactory.initialize(properties);}
public ValuesWriter parquet-mr_f1688_0(ColumnDescriptor descriptor)
{    return delegateFactory.newValuesWriter(descriptor);}
 static DictionaryValuesWriter parquet-mr_f1689_0(ColumnDescriptor path, ParquetProperties properties, Encoding dictPageEncoding, Encoding dataPageEncoding)
{    switch(path.getType()) {        case BOOLEAN:            throw new IllegalArgumentException("no dictionary encoding for BOOLEAN");        case BINARY:            return new DictionaryValuesWriter.PlainBinaryDictionaryValuesWriter(properties.getDictionaryPageSizeThreshold(), dataPageEncoding, dictPageEncoding, properties.getAllocator());        case INT32:            return new DictionaryValuesWriter.PlainIntegerDictionaryValuesWriter(properties.getDictionaryPageSizeThreshold(), dataPageEncoding, dictPageEncoding, properties.getAllocator());        case INT64:            return new DictionaryValuesWriter.PlainLongDictionaryValuesWriter(properties.getDictionaryPageSizeThreshold(), dataPageEncoding, dictPageEncoding, properties.getAllocator());        case INT96:            return new DictionaryValuesWriter.PlainFixedLenArrayDictionaryValuesWriter(properties.getDictionaryPageSizeThreshold(), 12, dataPageEncoding, dictPageEncoding, properties.getAllocator());        case DOUBLE:            return new DictionaryValuesWriter.PlainDoubleDictionaryValuesWriter(properties.getDictionaryPageSizeThreshold(), dataPageEncoding, dictPageEncoding, properties.getAllocator());        case FLOAT:            return new DictionaryValuesWriter.PlainFloatDictionaryValuesWriter(properties.getDictionaryPageSizeThreshold(), dataPageEncoding, dictPageEncoding, properties.getAllocator());        case FIXED_LEN_BYTE_ARRAY:            return new DictionaryValuesWriter.PlainFixedLenArrayDictionaryValuesWriter(properties.getDictionaryPageSizeThreshold(), path.getTypeLength(), dataPageEncoding, dictPageEncoding, properties.getAllocator());        default:            throw new IllegalArgumentException("Unknown type " + path.getType());    }}
 static ValuesWriter parquet-mr_f1690_0(ColumnDescriptor path, ParquetProperties parquetProperties, Encoding dictPageEncoding, Encoding dataPageEncoding, ValuesWriter writerToFallBackTo)
{    if (parquetProperties.isEnableDictionary()) {        return FallbackValuesWriter.of(dictionaryWriter(path, parquetProperties, dictPageEncoding, dataPageEncoding), writerToFallBackTo);    } else {        return writerToFallBackTo;    }}
public static FallbackValuesWriter<I, F> parquet-mr_f1691_0(I initialWriter, F fallBackWriter)
{    return new FallbackValuesWriter<I, F>(initialWriter, fallBackWriter);}
public long parquet-mr_f1692_0()
{        return rawDataByteSize;}
public BytesInput parquet-mr_f1693_0()
{    if (!fellBackAlready && firstPage) {                BytesInput bytes = initialWriter.getBytes();        if (!initialWriter.isCompressionSatisfying(rawDataByteSize, bytes.size())) {            fallBack();        } else {            return bytes;        }    }    return currentWriter.getBytes();}
public Encoding parquet-mr_f1694_0()
{    Encoding encoding = currentWriter.getEncoding();    if (!fellBackAlready && !initialUsedAndHadDictionary) {        initialUsedAndHadDictionary = encoding.usesDictionary();    }    return encoding;}
public void parquet-mr_f1695_0()
{    rawDataByteSize = 0;    firstPage = false;    currentWriter.reset();}
public void parquet-mr_f1696_0()
{    initialWriter.close();    fallBackWriter.close();}
public DictionaryPage parquet-mr_f1697_0()
{    if (initialUsedAndHadDictionary) {        return initialWriter.toDictPageAndClose();    } else {        return currentWriter.toDictPageAndClose();    }}
public void parquet-mr_f1698_0()
{    if (initialUsedAndHadDictionary) {        initialWriter.resetDictionary();    } else {        currentWriter.resetDictionary();    }    currentWriter = initialWriter;    fellBackAlready = false;    initialUsedAndHadDictionary = false;    firstPage = true;}
public long parquet-mr_f1699_0()
{    return currentWriter.getAllocatedSize();}
public String parquet-mr_f1700_0(String prefix)
{    return String.format("%s FallbackValuesWriter{\n" + "%s\n" + "%s\n" + "%s}\n", prefix, initialWriter.memUsageString(prefix + " initial:"), fallBackWriter.memUsageString(prefix + " fallback:"), prefix);}
private void parquet-mr_f1701_0()
{    if (!fellBackAlready && initialWriter.shouldFallBack()) {        fallBack();    }}
private void parquet-mr_f1702_0()
{    fellBackAlready = true;    initialWriter.fallBackAllValuesTo(fallBackWriter);    currentWriter = fallBackWriter;}
public void parquet-mr_f1703_0(int value)
{    rawDataByteSize += 1;    currentWriter.writeByte(value);    checkFallback();}
public void parquet-mr_f1704_0(Binary v)
{        rawDataByteSize += v.length() + 4;    currentWriter.writeBytes(v);    checkFallback();}
public void parquet-mr_f1705_0(int v)
{    rawDataByteSize += 4;    currentWriter.writeInteger(v);    checkFallback();}
public void parquet-mr_f1706_0(long v)
{    rawDataByteSize += 8;    currentWriter.writeLong(v);    checkFallback();}
public void parquet-mr_f1707_0(float v)
{    rawDataByteSize += 4;    currentWriter.writeFloat(v);    checkFallback();}
public void parquet-mr_f1708_0(double v)
{    rawDataByteSize += 8;    currentWriter.writeDouble(v);    checkFallback();}
public Binary parquet-mr_f1709_0()
{    try {        int length = BytesUtils.readIntLittleEndian(in);        return Binary.fromConstantByteBuffer(in.slice(length));    } catch (IOException e) {        throw new ParquetDecodingException("could not read bytes at offset " + in.position(), e);    } catch (RuntimeException e) {        throw new ParquetDecodingException("could not read bytes at offset " + in.position(), e);    }}
public void parquet-mr_f1710_0()
{    try {        int length = BytesUtils.readIntLittleEndian(in);        in.skipFully(length);    } catch (IOException e) {        throw new ParquetDecodingException("could not skip bytes at offset " + in.position(), e);    } catch (RuntimeException e) {        throw new ParquetDecodingException("could not skip bytes at offset " + in.position(), e);    }}
public void parquet-mr_f1711_1(int valueCount, ByteBufferInputStream stream) throws IOException
{        this.in = stream.remainingStream();}
public boolean parquet-mr_f1712_0()
{    return in.readInteger() == 0 ? false : true;}
public void parquet-mr_f1713_0()
{    in.readInteger();}
public void parquet-mr_f1714_1(int valueCount, ByteBufferInputStream stream) throws IOException
{        this.in.initFromPage(valueCount, stream);}
public int parquet-mr_f1715_0()
{    return in.getNextOffset();}
public final void parquet-mr_f1716_0(boolean v)
{    bitPackingWriter.writeInteger(v ? 1 : 0);}
public long parquet-mr_f1717_0()
{    return bitPackingWriter.getBufferedSize();}
public BytesInput parquet-mr_f1718_0()
{    return bitPackingWriter.getBytes();}
public void parquet-mr_f1719_0()
{    bitPackingWriter.reset();}
public void parquet-mr_f1720_0()
{    bitPackingWriter.close();}
public long parquet-mr_f1721_0()
{    return bitPackingWriter.getAllocatedSize();}
public Encoding parquet-mr_f1722_0()
{    return PLAIN;}
public String parquet-mr_f1723_0(String prefix)
{    return bitPackingWriter.memUsageString(prefix);}
public Binary parquet-mr_f1724_0()
{    try {        return Binary.fromConstantByteBuffer(in.slice(length));    } catch (IOException | RuntimeException e) {        throw new ParquetDecodingException("could not read bytes at offset " + in.position(), e);    }}
public void parquet-mr_f1725_0()
{    skip(1);}
public void parquet-mr_f1726_0(int n)
{    try {        in.skipFully(n * length);    } catch (IOException | RuntimeException e) {        throw new ParquetDecodingException("could not skip bytes at offset " + in.position(), e);    }}
public void parquet-mr_f1727_1(int valueCount, ByteBufferInputStream stream) throws IOException
{        this.in = stream.remainingStream();}
public final void parquet-mr_f1728_0(Binary v)
{    if (v.length() != length) {        throw new IllegalArgumentException("Fixed Binary size " + v.length() + " does not match field type length " + length);    }    try {        v.writeTo(out);    } catch (IOException e) {        throw new ParquetEncodingException("could not write fixed bytes", e);    }}
public long parquet-mr_f1729_0()
{    return arrayOut.size();}
public BytesInput parquet-mr_f1730_1()
{    try {        out.flush();    } catch (IOException e) {        throw new ParquetEncodingException("could not write page", e);    }        return BytesInput.from(arrayOut);}
public void parquet-mr_f1731_0()
{    arrayOut.reset();}
public void parquet-mr_f1732_0()
{    arrayOut.close();}
public long parquet-mr_f1733_0()
{    return arrayOut.getCapacity();}
public Encoding parquet-mr_f1734_0()
{    return Encoding.PLAIN;}
public String parquet-mr_f1735_0(String prefix)
{    return arrayOut.memUsageString(prefix + " PLAIN");}
public void parquet-mr_f1736_1(int valueCount, ByteBufferInputStream stream) throws IOException
{        this.in = new LittleEndianDataInputStream(stream.remainingStream());}
public void parquet-mr_f1737_0()
{    skip(1);}
 void parquet-mr_f1738_0(int n) throws IOException
{    int skipped = 0;    while (skipped < n) {        skipped += in.skipBytes(n - skipped);    }}
public void parquet-mr_f1739_0(int n)
{    try {        skipBytesFully(n * 8);    } catch (IOException e) {        throw new ParquetDecodingException("could not skip " + n + " double values", e);    }}
public double parquet-mr_f1740_0()
{    try {        return in.readDouble();    } catch (IOException e) {        throw new ParquetDecodingException("could not read double", e);    }}
public void parquet-mr_f1741_0(int n)
{    try {        skipBytesFully(n * 4);    } catch (IOException e) {        throw new ParquetDecodingException("could not skip " + n + " floats", e);    }}
public float parquet-mr_f1742_0()
{    try {        return in.readFloat();    } catch (IOException e) {        throw new ParquetDecodingException("could not read float", e);    }}
public void parquet-mr_f1743_0(int n)
{    try {        in.skipBytes(n * 4);    } catch (IOException e) {        throw new ParquetDecodingException("could not skip " + n + " ints", e);    }}
public int parquet-mr_f1744_0()
{    try {        return in.readInt();    } catch (IOException e) {        throw new ParquetDecodingException("could not read int", e);    }}
public void parquet-mr_f1745_0(int n)
{    try {        in.skipBytes(n * 8);    } catch (IOException e) {        throw new ParquetDecodingException("could not skip " + n + " longs", e);    }}
public long parquet-mr_f1746_0()
{    try {        return in.readLong();    } catch (IOException e) {        throw new ParquetDecodingException("could not read long", e);    }}
public final void parquet-mr_f1747_0(Binary v)
{    try {        out.writeInt(v.length());        v.writeTo(out);    } catch (IOException e) {        throw new ParquetEncodingException("could not write bytes", e);    }}
public final void parquet-mr_f1748_0(int v)
{    try {        out.writeInt(v);    } catch (IOException e) {        throw new ParquetEncodingException("could not write int", e);    }}
public final void parquet-mr_f1749_0(long v)
{    try {        out.writeLong(v);    } catch (IOException e) {        throw new ParquetEncodingException("could not write long", e);    }}
public final void parquet-mr_f1750_0(float v)
{    try {        out.writeFloat(v);    } catch (IOException e) {        throw new ParquetEncodingException("could not write float", e);    }}
public final void parquet-mr_f1751_0(double v)
{    try {        out.writeDouble(v);    } catch (IOException e) {        throw new ParquetEncodingException("could not write double", e);    }}
public void parquet-mr_f1752_0(int value)
{    try {        out.write(value);    } catch (IOException e) {        throw new ParquetEncodingException("could not write byte", e);    }}
public long parquet-mr_f1753_0()
{    return arrayOut.size();}
public BytesInput parquet-mr_f1754_1()
{    try {        out.flush();    } catch (IOException e) {        throw new ParquetEncodingException("could not write page", e);    }    if (LOG.isDebugEnabled())            return BytesInput.from(arrayOut);}
public void parquet-mr_f1755_0()
{    arrayOut.reset();}
public void parquet-mr_f1756_0()
{    arrayOut.close();    out.close();}
public long parquet-mr_f1757_0()
{    return arrayOut.getCapacity();}
public Encoding parquet-mr_f1758_0()
{    return Encoding.PLAIN;}
public String parquet-mr_f1759_0(String prefix)
{    return arrayOut.memUsageString(prefix + " PLAIN");}
public int parquet-mr_f1760_0() throws IOException
{    if (currentCount == 0) {        readNext();    }    --currentCount;    int result;    switch(mode) {        case RLE:            result = currentValue;            break;        case PACKED:            result = currentBuffer[currentBuffer.length - 1 - currentCount];            break;        default:            throw new ParquetDecodingException("not a valid mode " + mode);    }    return result;}
private void parquet-mr_f1761_1() throws IOException
{    Preconditions.checkArgument(in.available() > 0, "Reading past RLE/BitPacking stream.");    final int header = BytesUtils.readUnsignedVarInt(in);    mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;    switch(mode) {        case RLE:            currentCount = header >>> 1;                        currentValue = BytesUtils.readIntLittleEndianPaddedOnBitWidth(in, bitWidth);            break;        case PACKED:            int numGroups = header >>> 1;            currentCount = numGroups * 8;                                    currentBuffer = new int[currentCount];            byte[] bytes = new byte[numGroups * bitWidth];                        int bytesToRead = (int) Math.ceil(currentCount * bitWidth / 8.0);            bytesToRead = Math.min(bytesToRead, in.available());            new DataInputStream(in).readFully(bytes, 0, bytesToRead);            for (int valueIndex = 0, byteIndex = 0; valueIndex < currentCount; valueIndex += 8, byteIndex += bitWidth) {                packer.unpack8Values(bytes, byteIndex, currentBuffer, valueIndex);            }            break;        default:            throw new ParquetDecodingException("not a valid mode " + mode);    }}
private void parquet-mr_f1762_0(boolean resetBaos)
{    if (resetBaos) {        this.baos.reset();    }    this.previousValue = 0;    this.numBufferedValues = 0;    this.repeatCount = 0;    this.bitPackedGroupCount = 0;    this.bitPackedRunHeaderPointer = -1;    this.toBytesCalled = false;}
public void parquet-mr_f1763_0(int value) throws IOException
{    if (value == previousValue) {                        ++repeatCount;        if (repeatCount >= 8) {                        return;        }    } else {                if (repeatCount >= 8) {                        writeRleRun();        }                repeatCount = 1;                previousValue = value;    }            bufferedValues[numBufferedValues] = value;    ++numBufferedValues;    if (numBufferedValues == 8) {                                writeOrAppendBitPackedRun();    }}
private void parquet-mr_f1764_0() throws IOException
{    if (bitPackedGroupCount >= 63) {                        endPreviousBitPackedRun();    }    if (bitPackedRunHeaderPointer == -1) {                                baos.write(0);        bitPackedRunHeaderPointer = baos.getCurrentIndex();    }    packer.pack8Values(bufferedValues, 0, packBuffer, 0);    baos.write(packBuffer);        numBufferedValues = 0;            repeatCount = 0;    ++bitPackedGroupCount;}
private void parquet-mr_f1765_0()
{    if (bitPackedRunHeaderPointer == -1) {                return;    }        byte bitPackHeader = (byte) ((bitPackedGroupCount << 1) | 1);        baos.setByte(bitPackedRunHeaderPointer, bitPackHeader);        bitPackedRunHeaderPointer = -1;        bitPackedGroupCount = 0;}
private void parquet-mr_f1766_0() throws IOException
{                endPreviousBitPackedRun();        BytesUtils.writeUnsignedVarInt(repeatCount << 1, baos);        BytesUtils.writeIntLittleEndianPaddedOnBitWidth(baos, previousValue, bitWidth);        repeatCount = 0;        numBufferedValues = 0;}
public BytesInput parquet-mr_f1767_0() throws IOException
{    Preconditions.checkArgument(!toBytesCalled, "You cannot call toBytes() more than once without calling reset()");        if (repeatCount >= 8) {        writeRleRun();    } else if (numBufferedValues > 0) {        for (int i = numBufferedValues; i < 8; i++) {            bufferedValues[i] = 0;        }        writeOrAppendBitPackedRun();        endPreviousBitPackedRun();    } else {        endPreviousBitPackedRun();    }    toBytesCalled = true;    return BytesInput.from(baos);}
public void parquet-mr_f1768_0()
{    reset(true);}
public void parquet-mr_f1769_0()
{    reset(false);    baos.close();}
public long parquet-mr_f1770_0()
{    return baos.size();}
public long parquet-mr_f1771_0()
{    return baos.getCapacity();}
public void parquet-mr_f1772_0(int valueCountL, ByteBufferInputStream stream) throws IOException
{    int length = BytesUtils.readIntLittleEndian(stream);    this.decoder = new RunLengthBitPackingHybridDecoder(bitWidth, stream.sliceStream(length));        updateNextOffset(length + 4);}
public int parquet-mr_f1773_0()
{    try {        return decoder.readInt();    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
public boolean parquet-mr_f1774_0()
{    return readInteger() == 0 ? false : true;}
public void parquet-mr_f1775_0()
{    readInteger();}
public void parquet-mr_f1776_0(int v)
{    try {        encoder.writeInt(v);    } catch (IOException e) {        throw new ParquetEncodingException(e);    }}
public void parquet-mr_f1777_0(boolean v)
{    writeInteger(v ? 1 : 0);}
public long parquet-mr_f1778_0()
{    return encoder.getBufferedSize();}
public long parquet-mr_f1779_0()
{    return encoder.getAllocatedSize();}
public BytesInput parquet-mr_f1780_0()
{    try {                BytesInput rle = encoder.toBytes();        return BytesInput.concat(BytesInput.fromInt(Math.toIntExact(rle.size())), rle);    } catch (IOException e) {        throw new ParquetEncodingException(e);    }}
public Encoding parquet-mr_f1781_0()
{    return Encoding.RLE;}
public void parquet-mr_f1782_0()
{    encoder.reset();}
public void parquet-mr_f1783_0()
{    encoder.close();}
public String parquet-mr_f1784_0(String prefix)
{    return String.format("%s RunLengthBitPackingHybrid %d bytes", prefix, getAllocatedSize());}
public int parquet-mr_f1785_0()
{    return 0;}
public void parquet-mr_f1786_0(int valueCount, ByteBufferInputStream stream) throws IOException
{    updateNextOffset(0);}
public void parquet-mr_f1787_0()
{}
public void parquet-mr_f1788_0(int n)
{}
public void parquet-mr_f1789_0(int valueCount, ByteBuffer page, int offset) throws IOException
{    if (offset < 0) {        throw new IllegalArgumentException("Illegal offset: " + offset);    }    actualOffset = offset;    ByteBuffer pageWithOffset = page.duplicate();    pageWithOffset.position(offset);    initFromPage(valueCount, ByteBufferInputStream.wrap(pageWithOffset));    actualOffset = -1;}
public void parquet-mr_f1790_0(int valueCount, byte[] page, int offset) throws IOException
{    this.initFromPage(valueCount, ByteBuffer.wrap(page), offset);}
public void parquet-mr_f1791_0(int valueCount, ByteBufferInputStream in) throws IOException
{    if (actualOffset != -1) {        throw new UnsupportedOperationException("Either initFromPage(int, ByteBuffer, int) or initFromPage(int, ByteBufferInputStream) must be implemented in " + getClass().getName());    }    initFromPage(valueCount, in.slice(valueCount), 0);}
public int parquet-mr_f1792_0()
{    if (nextOffset == -1) {        throw new ParquetDecodingException("Unsupported: cannot get offset of the next section.");    } else {        return nextOffset;    }}
protected void parquet-mr_f1793_0(int bytesRead)
{    nextOffset = actualOffset == -1 ? -1 : actualOffset + bytesRead;}
public int parquet-mr_f1794_0()
{    throw new UnsupportedOperationException();}
public boolean parquet-mr_f1795_0()
{    throw new UnsupportedOperationException();}
public Binary parquet-mr_f1796_0()
{    throw new UnsupportedOperationException();}
public float parquet-mr_f1797_0()
{    throw new UnsupportedOperationException();}
public double parquet-mr_f1798_0()
{    throw new UnsupportedOperationException();}
public int parquet-mr_f1799_0()
{    throw new UnsupportedOperationException();}
public long parquet-mr_f1800_0()
{    throw new UnsupportedOperationException();}
public void parquet-mr_f1801_0(int n)
{    for (int i = 0; i < n; ++i) {        skip();    }}
public void parquet-mr_f1802_0()
{}
public DictionaryPage parquet-mr_f1803_0()
{    return null;}
public void parquet-mr_f1804_0()
{}
public void parquet-mr_f1805_0(int value)
{    throw new UnsupportedOperationException(getClass().getName());}
public void parquet-mr_f1806_0(boolean v)
{    throw new UnsupportedOperationException(getClass().getName());}
public void parquet-mr_f1807_0(Binary v)
{    throw new UnsupportedOperationException(getClass().getName());}
public void parquet-mr_f1808_0(int v)
{    throw new UnsupportedOperationException(getClass().getName());}
public void parquet-mr_f1809_0(long v)
{    throw new UnsupportedOperationException(getClass().getName());}
public void parquet-mr_f1810_0(double v)
{    throw new UnsupportedOperationException(getClass().getName());}
public void parquet-mr_f1811_0(float v)
{    throw new UnsupportedOperationException(getClass().getName());}
public static boolean parquet-mr_f1812_1(ParsedVersion version, Encoding encoding)
{    if (encoding != Encoding.DELTA_BYTE_ARRAY) {        return false;    }    if (version == null) {        return true;    }    if (!"parquet-mr".equals(version.application)) {                return false;    }    if (!version.hasSemanticVersion()) {                return true;    }    return requiresSequentialReads(version.getSemanticVersion(), encoding);}
public static boolean parquet-mr_f1813_1(SemanticVersion semver, Encoding encoding)
{    if (encoding != Encoding.DELTA_BYTE_ARRAY) {        return false;    }    if (semver == null) {        return true;    }    if (semver.compareTo(PARQUET_246_FIXED_VERSION) < 0) {                return true;    }        return false;}
public static boolean parquet-mr_f1814_1(String createdBy, Encoding encoding)
{    if (encoding != Encoding.DELTA_BYTE_ARRAY) {        return false;    }    if (Strings.isNullOrEmpty(createdBy)) {                return true;    }    try {        return requiresSequentialReads(VersionParser.parse(createdBy), encoding);    } catch (RuntimeException e) {        warnParseError(createdBy, e);        return true;    } catch (VersionParser.VersionParseException e) {        warnParseError(createdBy, e);        return true;    }}
private static void parquet-mr_f1815_1(String createdBy, Throwable e)
{    }
public static boolean parquet-mr_f1816_0(String createdBy, PrimitiveTypeName columnType)
{    if (columnType != PrimitiveTypeName.BINARY && columnType != PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {                return false;    }    if (Strings.isNullOrEmpty(createdBy)) {                        warnOnce("Ignoring statistics because created_by is null or empty! See PARQUET-251 and PARQUET-297");        return true;    }    try {        ParsedVersion version = VersionParser.parse(createdBy);        if (!"parquet-mr".equals(version.application)) {                        return false;        }        if (Strings.isNullOrEmpty(version.version)) {            warnOnce("Ignoring statistics because created_by did not contain a semver (see PARQUET-251): " + createdBy);            return true;        }        SemanticVersion semver = SemanticVersion.parse(version.version);        if (semver.compareTo(PARQUET_251_FIXED_VERSION) < 0 && !(semver.compareTo(CDH_5_PARQUET_251_FIXED_START) >= 0 && semver.compareTo(CDH_5_PARQUET_251_FIXED_END) < 0)) {            warnOnce("Ignoring statistics because this file was created prior to " + PARQUET_251_FIXED_VERSION + ", see PARQUET-251");            return true;        }                return false;    } catch (RuntimeException e) {                        warnParseErrorOnce(createdBy, e);        return true;    } catch (SemanticVersionParseException e) {                        warnParseErrorOnce(createdBy, e);        return true;    } catch (VersionParseException e) {                        warnParseErrorOnce(createdBy, e);        return true;    }}
private static void parquet-mr_f1817_1(String createdBy, Throwable e)
{    if (!alreadyLogged.getAndSet(true)) {            }}
private static void parquet-mr_f1818_1(String message)
{    if (!alreadyLogged.getAndSet(true)) {            }}
public void parquet-mr_f1819_0(String field, int value)
{    add(getType().getFieldIndex(field), value);}
public void parquet-mr_f1820_0(String field, long value)
{    add(getType().getFieldIndex(field), value);}
public void parquet-mr_f1821_0(String field, float value)
{    add(getType().getFieldIndex(field), value);}
public void parquet-mr_f1822_0(String field, double value)
{    add(getType().getFieldIndex(field), value);}
public void parquet-mr_f1823_0(String field, String value)
{    add(getType().getFieldIndex(field), value);}
public void parquet-mr_f1824_0(String field, NanoTime value)
{    add(getType().getFieldIndex(field), value);}
public void parquet-mr_f1825_0(String field, boolean value)
{    add(getType().getFieldIndex(field), value);}
public void parquet-mr_f1826_0(String field, Binary value)
{    add(getType().getFieldIndex(field), value);}
public void parquet-mr_f1827_0(String field, Group value)
{    add(getType().getFieldIndex(field), value);}
public Group parquet-mr_f1828_1(String field)
{    if (LOG.isDebugEnabled()) {            }    return addGroup(getType().getFieldIndex(field));}
public Group parquet-mr_f1829_0(String field, int index)
{    return getGroup(getType().getFieldIndex(field), index);}
public Group parquet-mr_f1830_0()
{    return this;}
public Group parquet-mr_f1831_0(String fieldName, int value)
{    add(fieldName, value);    return this;}
public Group parquet-mr_f1832_0(String fieldName, float value)
{    add(fieldName, value);    return this;}
public Group parquet-mr_f1833_0(String fieldName, double value)
{    add(fieldName, value);    return this;}
public Group parquet-mr_f1834_0(String fieldName, long value)
{    add(fieldName, value);    return this;}
public Group parquet-mr_f1835_0(String fieldName, NanoTime value)
{    add(fieldName, value);    return this;}
public Group parquet-mr_f1836_0(String fieldName, String value)
{    add(fieldName, Binary.fromString(value));    return this;}
public Group parquet-mr_f1837_0(String fieldName, boolean value)
{    add(fieldName, value);    return this;}
public Group parquet-mr_f1838_0(String fieldName, Binary value)
{    add(fieldName, value);    return this;}
public int parquet-mr_f1839_0(String field)
{    return getFieldRepetitionCount(getType().getFieldIndex(field));}
public GroupValueSource parquet-mr_f1840_0(String field, int index)
{    return getGroup(getType().getFieldIndex(field), index);}
public String parquet-mr_f1841_0(String field, int index)
{    return getString(getType().getFieldIndex(field), index);}
public int parquet-mr_f1842_0(String field, int index)
{    return getInteger(getType().getFieldIndex(field), index);}
public long parquet-mr_f1843_0(String field, int index)
{    return getLong(getType().getFieldIndex(field), index);}
public double parquet-mr_f1844_0(String field, int index)
{    return getDouble(getType().getFieldIndex(field), index);}
public float parquet-mr_f1845_0(String field, int index)
{    return getFloat(getType().getFieldIndex(field), index);}
public boolean parquet-mr_f1846_0(String field, int index)
{    return getBoolean(getType().getFieldIndex(field), index);}
public Binary parquet-mr_f1847_0(String field, int index)
{    return getBinary(getType().getFieldIndex(field), index);}
public Binary parquet-mr_f1848_0(String field, int index)
{    return getInt96(getType().getFieldIndex(field), index);}
public void parquet-mr_f1849_0(Group group)
{    recordConsumer.startMessage();    writeGroup(group, schema);    recordConsumer.endMessage();}
private void parquet-mr_f1850_0(Group group, GroupType type)
{    int fieldCount = type.getFieldCount();    for (int field = 0; field < fieldCount; ++field) {        int valueCount = group.getFieldRepetitionCount(field);        if (valueCount > 0) {            Type fieldType = type.getType(field);            String fieldName = fieldType.getName();            recordConsumer.startField(fieldName, field);            for (int index = 0; index < valueCount; ++index) {                if (fieldType.isPrimitive()) {                    group.writeValue(field, index, recordConsumer);                } else {                    recordConsumer.startGroup();                    writeGroup(group.getGroup(field, index), fieldType.asGroupType());                    recordConsumer.endGroup();                }            }            recordConsumer.endField(fieldName, field);        }    }}
public Binary parquet-mr_f1851_0()
{    return binary;}
public String parquet-mr_f1852_0()
{    return binary.toStringUsingUTF8();}
public void parquet-mr_f1853_0(RecordConsumer recordConsumer)
{    recordConsumer.addBinary(binary);}
public String parquet-mr_f1854_0()
{    return getString();}
public String parquet-mr_f1855_0()
{    return String.valueOf(bool);}
public boolean parquet-mr_f1856_0()
{    return bool;}
public void parquet-mr_f1857_0(RecordConsumer recordConsumer)
{    recordConsumer.addBoolean(bool);}
public void parquet-mr_f1858_0()
{    this.current = simpleGroupFactory.newGroup();}
public void parquet-mr_f1859_0()
{}
public Group parquet-mr_f1860_0()
{    return root.getCurrentRecord();}
public GroupConverter parquet-mr_f1861_0()
{    return root;}
public void parquet-mr_f1862_0()
{    current = parent.getCurrentRecord().addGroup(index);}
public Converter parquet-mr_f1863_0(int fieldIndex)
{    return converters[fieldIndex];}
public void parquet-mr_f1864_0()
{}
public Group parquet-mr_f1865_0()
{    return current;}
public void parquet-mr_f1866_0(Binary value)
{    parent.getCurrentRecord().add(index, value);}
public void parquet-mr_f1867_0(boolean value)
{    parent.getCurrentRecord().add(index, value);}
public void parquet-mr_f1868_0(double value)
{    parent.getCurrentRecord().add(index, value);}
public void parquet-mr_f1869_0(float value)
{    parent.getCurrentRecord().add(index, value);}
public void parquet-mr_f1870_0(int value)
{    parent.getCurrentRecord().add(index, value);}
public void parquet-mr_f1871_0(long value)
{    parent.getCurrentRecord().add(index, value);}
public double parquet-mr_f1872_0()
{    return value;}
public void parquet-mr_f1873_0(RecordConsumer recordConsumer)
{    recordConsumer.addDouble(value);}
public String parquet-mr_f1874_0()
{    return String.valueOf(value);}
public float parquet-mr_f1875_0()
{    return value;}
public void parquet-mr_f1876_0(RecordConsumer recordConsumer)
{    recordConsumer.addFloat(value);}
public String parquet-mr_f1877_0()
{    return String.valueOf(value);}
public Binary parquet-mr_f1878_0()
{    return value;}
public void parquet-mr_f1879_0(RecordConsumer recordConsumer)
{    recordConsumer.addBinary(value);}
public String parquet-mr_f1880_0()
{    return "Int96Value{" + String.valueOf(value) + "}";}
public String parquet-mr_f1881_0()
{    return String.valueOf(value);}
public int parquet-mr_f1882_0()
{    return value;}
public void parquet-mr_f1883_0(RecordConsumer recordConsumer)
{    recordConsumer.addInteger(value);}
public String parquet-mr_f1884_0()
{    return String.valueOf(value);}
public long parquet-mr_f1885_0()
{    return value;}
public void parquet-mr_f1886_0(RecordConsumer recordConsumer)
{    recordConsumer.addLong(value);}
public static NanoTime parquet-mr_f1887_0(Binary bytes)
{    Preconditions.checkArgument(bytes.length() == 12, "Must be 12 bytes");    ByteBuffer buf = bytes.toByteBuffer();    buf.order(ByteOrder.LITTLE_ENDIAN);    long timeOfDayNanos = buf.getLong();    int julianDay = buf.getInt();    return new NanoTime(julianDay, timeOfDayNanos);}
public static NanoTime parquet-mr_f1888_0(Int96Value int96)
{    ByteBuffer buf = int96.getInt96().toByteBuffer();    return new NanoTime(buf.getInt(), buf.getLong());}
public int parquet-mr_f1889_0()
{    return julianDay;}
public long parquet-mr_f1890_0()
{    return timeOfDayNanos;}
public Binary parquet-mr_f1891_0()
{    ByteBuffer buf = ByteBuffer.allocate(12);    buf.order(ByteOrder.LITTLE_ENDIAN);    buf.putLong(timeOfDayNanos);    buf.putInt(julianDay);    buf.flip();    return Binary.fromConstantByteBuffer(buf);}
public Int96Value parquet-mr_f1892_0()
{    return new Int96Value(toBinary());}
public void parquet-mr_f1893_0(RecordConsumer recordConsumer)
{    recordConsumer.addBinary(toBinary());}
public String parquet-mr_f1894_0()
{    return "NanoTime{julianDay=" + julianDay + ", timeOfDayNanos=" + timeOfDayNanos + "}";}
public String parquet-mr_f1895_0()
{    throw new UnsupportedOperationException();}
public int parquet-mr_f1896_0()
{    throw new UnsupportedOperationException();}
public long parquet-mr_f1897_0()
{    throw new UnsupportedOperationException();}
public boolean parquet-mr_f1898_0()
{    throw new UnsupportedOperationException();}
public Binary parquet-mr_f1899_0()
{    throw new UnsupportedOperationException();}
public Binary parquet-mr_f1900_0()
{    throw new UnsupportedOperationException();}
public float parquet-mr_f1901_0()
{    throw new UnsupportedOperationException();}
public double parquet-mr_f1902_0()
{    throw new UnsupportedOperationException();}
public String parquet-mr_f1903_0()
{    return toString("");}
public String parquet-mr_f1904_0(String indent)
{    String result = "";    int i = 0;    for (Type field : schema.getFields()) {        String name = field.getName();        List<Object> values = data[i];        ++i;        if (values != null) {            if (values.size() > 0) {                for (Object value : values) {                    result += indent + name;                    if (value == null) {                        result += ": NULL\n";                    } else if (value instanceof Group) {                        result += "\n" + ((SimpleGroup) value).toString(indent + "  ");                    } else {                        result += ": " + value.toString() + "\n";                    }                }            }        }    }    return result;}
public Group parquet-mr_f1905_0(int fieldIndex)
{    SimpleGroup g = new SimpleGroup(schema.getType(fieldIndex).asGroupType());    add(fieldIndex, g);    return g;}
public Group parquet-mr_f1906_0(int fieldIndex, int index)
{    return (Group) getValue(fieldIndex, index);}
private Object parquet-mr_f1907_0(int fieldIndex, int index)
{    List<Object> list;    try {        list = data[fieldIndex];    } catch (IndexOutOfBoundsException e) {        throw new RuntimeException("not found " + fieldIndex + "(" + schema.getFieldName(fieldIndex) + ") in group:\n" + this);    }    try {        return list.get(index);    } catch (IndexOutOfBoundsException e) {        throw new RuntimeException("not found " + fieldIndex + "(" + schema.getFieldName(fieldIndex) + ") element number " + index + " in group:\n" + this);    }}
private void parquet-mr_f1908_0(int fieldIndex, Primitive value)
{    Type type = schema.getType(fieldIndex);    List<Object> list = data[fieldIndex];    if (!type.isRepetition(Type.Repetition.REPEATED) && !list.isEmpty()) {        throw new IllegalStateException("field " + fieldIndex + " (" + type.getName() + ") can not have more than one value: " + list);    }    list.add(value);}
public int parquet-mr_f1909_0(int fieldIndex)
{    List<Object> list = data[fieldIndex];    return list == null ? 0 : list.size();}
public String parquet-mr_f1910_0(int fieldIndex, int index)
{    return String.valueOf(getValue(fieldIndex, index));}
public String parquet-mr_f1911_0(int fieldIndex, int index)
{    return ((BinaryValue) getValue(fieldIndex, index)).getString();}
public int parquet-mr_f1912_0(int fieldIndex, int index)
{    return ((IntegerValue) getValue(fieldIndex, index)).getInteger();}
public long parquet-mr_f1913_0(int fieldIndex, int index)
{    return ((LongValue) getValue(fieldIndex, index)).getLong();}
public double parquet-mr_f1914_0(int fieldIndex, int index)
{    return ((DoubleValue) getValue(fieldIndex, index)).getDouble();}
public float parquet-mr_f1915_0(int fieldIndex, int index)
{    return ((FloatValue) getValue(fieldIndex, index)).getFloat();}
public boolean parquet-mr_f1916_0(int fieldIndex, int index)
{    return ((BooleanValue) getValue(fieldIndex, index)).getBoolean();}
public Binary parquet-mr_f1917_0(int fieldIndex, int index)
{    return ((BinaryValue) getValue(fieldIndex, index)).getBinary();}
public NanoTime parquet-mr_f1918_0(int fieldIndex, int index)
{    return NanoTime.fromInt96((Int96Value) getValue(fieldIndex, index));}
public Binary parquet-mr_f1919_0(int fieldIndex, int index)
{    return ((Int96Value) getValue(fieldIndex, index)).getInt96();}
public void parquet-mr_f1920_0(int fieldIndex, int value)
{    add(fieldIndex, new IntegerValue(value));}
public void parquet-mr_f1921_0(int fieldIndex, long value)
{    add(fieldIndex, new LongValue(value));}
public void parquet-mr_f1922_0(int fieldIndex, String value)
{    add(fieldIndex, new BinaryValue(Binary.fromString(value)));}
public void parquet-mr_f1923_0(int fieldIndex, NanoTime value)
{    add(fieldIndex, value.toInt96());}
public void parquet-mr_f1924_0(int fieldIndex, boolean value)
{    add(fieldIndex, new BooleanValue(value));}
public void parquet-mr_f1925_0(int fieldIndex, Binary value)
{    switch(getType().getType(fieldIndex).asPrimitiveType().getPrimitiveTypeName()) {        case BINARY:        case FIXED_LEN_BYTE_ARRAY:            add(fieldIndex, new BinaryValue(value));            break;        case INT96:            add(fieldIndex, new Int96Value(value));            break;        default:            throw new UnsupportedOperationException(getType().asPrimitiveType().getName() + " not supported for Binary");    }}
public void parquet-mr_f1926_0(int fieldIndex, float value)
{    add(fieldIndex, new FloatValue(value));}
public void parquet-mr_f1927_0(int fieldIndex, double value)
{    add(fieldIndex, new DoubleValue(value));}
public void parquet-mr_f1928_0(int fieldIndex, Group value)
{    data[fieldIndex].add(value);}
public GroupType parquet-mr_f1929_0()
{    return schema;}
public void parquet-mr_f1930_0(int field, int index, RecordConsumer recordConsumer)
{    ((Primitive) getValue(field, index)).writeValue(recordConsumer);}
public Group parquet-mr_f1931_0()
{    return new SimpleGroup(schema);}
public Converter parquet-mr_f1932_0(List<GroupType> path, PrimitiveType primitiveType)
{    return new PrimitiveConverter() {        @Override        public void addBinary(Binary value) {            a = value;        }        @Override        public void addBoolean(boolean value) {            a = value;        }        @Override        public void addDouble(double value) {            a = value;        }        @Override        public void addFloat(float value) {            a = value;        }        @Override        public void addInt(int value) {            a = value;        }        @Override        public void addLong(long value) {            a = value;        }    };}
public void parquet-mr_f1933_0(Binary value)
{    a = value;}
public void parquet-mr_f1934_0(boolean value)
{    a = value;}
public void parquet-mr_f1935_0(double value)
{    a = value;}
public void parquet-mr_f1936_0(float value)
{    a = value;}
public void parquet-mr_f1937_0(int value)
{    a = value;}
public void parquet-mr_f1938_0(long value)
{    a = value;}
public Converter parquet-mr_f1939_0(List<GroupType> path, GroupType groupType, final List<Converter> converters)
{    return new GroupConverter() {        public Converter getConverter(int fieldIndex) {            return converters.get(fieldIndex);        }        public void start() {            a = "start()";        }        public void end() {            a = "end()";        }    };}
public Converter parquet-mr_f1940_0(int fieldIndex)
{    return converters.get(fieldIndex);}
public void parquet-mr_f1941_0()
{    a = "start()";}
public void parquet-mr_f1942_0()
{    a = "end()";}
public Converter parquet-mr_f1943_0(MessageType messageType, List<Converter> children)
{    return convertGroupType(null, messageType, children);}
public Object parquet-mr_f1944_0()
{    return a;}
public GroupConverter parquet-mr_f1945_0()
{    return root;}
public static final UnboundRecordFilter parquet-mr_f1946_0(final UnboundRecordFilter filter1, final UnboundRecordFilter filter2)
{    Preconditions.checkNotNull(filter1, "filter1");    Preconditions.checkNotNull(filter2, "filter2");    return new UnboundRecordFilter() {        @Override        public RecordFilter bind(Iterable<ColumnReader> readers) {            return new AndRecordFilter(filter1.bind(readers), filter2.bind(readers));        }    };}
public RecordFilter parquet-mr_f1947_0(Iterable<ColumnReader> readers)
{    return new AndRecordFilter(filter1.bind(readers), filter2.bind(readers));}
public boolean parquet-mr_f1948_0()
{    return boundFilter1.isMatch() && boundFilter2.isMatch();}
public static Predicate parquet-mr_f1949_0(final String target)
{    Preconditions.checkNotNull(target, "target");    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return target.equals(input.getBinary().toStringUsingUTF8());        }    };}
public boolean parquet-mr_f1950_0(ColumnReader input)
{    return target.equals(input.getBinary().toStringUsingUTF8());}
public static Predicate parquet-mr_f1951_0(final PredicateFunction<String> fn)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return fn.functionToApply(input.getBinary().toStringUsingUTF8());        }    };}
public boolean parquet-mr_f1952_0(ColumnReader input)
{    return fn.functionToApply(input.getBinary().toStringUsingUTF8());}
public static Predicate parquet-mr_f1953_0(final int target)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return input.getInteger() == target;        }    };}
public boolean parquet-mr_f1954_0(ColumnReader input)
{    return input.getInteger() == target;}
public static Predicate parquet-mr_f1955_0(final IntegerPredicateFunction fn)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return fn.functionToApply(input.getInteger());        }    };}
public boolean parquet-mr_f1956_0(ColumnReader input)
{    return fn.functionToApply(input.getInteger());}
public static Predicate parquet-mr_f1957_0(final long target)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return input.getLong() == target;        }    };}
public boolean parquet-mr_f1958_0(ColumnReader input)
{    return input.getLong() == target;}
public static Predicate parquet-mr_f1959_0(final LongPredicateFunction fn)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return fn.functionToApply(input.getLong());        }    };}
public boolean parquet-mr_f1960_0(ColumnReader input)
{    return fn.functionToApply(input.getLong());}
public static Predicate parquet-mr_f1961_0(final float target)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return input.getFloat() == target;        }    };}
public boolean parquet-mr_f1962_0(ColumnReader input)
{    return input.getFloat() == target;}
public static Predicate parquet-mr_f1963_0(final FloatPredicateFunction fn)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return fn.functionToApply(input.getFloat());        }    };}
public boolean parquet-mr_f1964_0(ColumnReader input)
{    return fn.functionToApply(input.getFloat());}
public static Predicate parquet-mr_f1965_0(final double target)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return input.getDouble() == target;        }    };}
public boolean parquet-mr_f1966_0(ColumnReader input)
{    return input.getDouble() == target;}
public static Predicate parquet-mr_f1967_0(final DoublePredicateFunction fn)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return fn.functionToApply(input.getDouble());        }    };}
public boolean parquet-mr_f1968_0(ColumnReader input)
{    return fn.functionToApply(input.getDouble());}
public static Predicate parquet-mr_f1969_0(final boolean target)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return input.getBoolean() == target;        }    };}
public boolean parquet-mr_f1970_0(ColumnReader input)
{    return input.getBoolean() == target;}
public static Predicate parquet-mr_f1971_0(final BooleanPredicateFunction fn)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return fn.functionToApply(input.getBoolean());        }    };}
public boolean parquet-mr_f1972_0(ColumnReader input)
{    return fn.functionToApply(input.getBoolean());}
public static Predicate parquet-mr_f1973_0(final E target)
{    Preconditions.checkNotNull(target, "target");    final String targetAsString = target.name();    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return targetAsString.equals(input.getBinary().toStringUsingUTF8());        }    };}
public boolean parquet-mr_f1974_0(ColumnReader input)
{    return targetAsString.equals(input.getBinary().toStringUsingUTF8());}
public static Predicate parquet-mr_f1975_0(final PredicateFunction<Binary> fn)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return fn.functionToApply(input.getBinary());        }    };}
public boolean parquet-mr_f1976_0(ColumnReader input)
{    return fn.functionToApply(input.getBinary());}
public static final UnboundRecordFilter parquet-mr_f1977_0(final String columnPath, final ColumnPredicates.Predicate predicate)
{    checkNotNull(columnPath, "columnPath");    checkNotNull(predicate, "predicate");    return new UnboundRecordFilter() {        final String[] filterPath = columnPath.split("\\.");        @Override        public RecordFilter bind(Iterable<ColumnReader> readers) {            for (ColumnReader reader : readers) {                if (Arrays.equals(reader.getDescriptor().getPath(), filterPath)) {                    return new ColumnRecordFilter(reader, predicate);                }            }            throw new IllegalArgumentException("Column " + columnPath + " does not exist.");        }    };}
public RecordFilter parquet-mr_f1978_0(Iterable<ColumnReader> readers)
{    for (ColumnReader reader : readers) {        if (Arrays.equals(reader.getDescriptor().getPath(), filterPath)) {            return new ColumnRecordFilter(reader, predicate);        }    }    throw new IllegalArgumentException("Column " + columnPath + " does not exist.");}
public boolean parquet-mr_f1979_0()
{    return filterPredicate.apply(filterOnColumn);}
public static final UnboundRecordFilter parquet-mr_f1980_0(final UnboundRecordFilter filter)
{    Preconditions.checkNotNull(filter, "filter");    return new UnboundRecordFilter() {        @Override        public RecordFilter bind(Iterable<ColumnReader> readers) {            return new NotRecordFilter(filter.bind(readers));        }    };}
public RecordFilter parquet-mr_f1981_0(Iterable<ColumnReader> readers)
{    return new NotRecordFilter(filter.bind(readers));}
public boolean parquet-mr_f1982_0()
{    return !(boundFilter.isMatch());}
public static final UnboundRecordFilter parquet-mr_f1983_0(final UnboundRecordFilter filter1, final UnboundRecordFilter filter2)
{    Preconditions.checkNotNull(filter1, "filter1");    Preconditions.checkNotNull(filter2, "filter2");    return new UnboundRecordFilter() {        @Override        public RecordFilter bind(Iterable<ColumnReader> readers) {            return new OrRecordFilter(filter1.bind(readers), filter2.bind(readers));        }    };}
public RecordFilter parquet-mr_f1984_0(Iterable<ColumnReader> readers)
{    return new OrRecordFilter(filter1.bind(readers), filter2.bind(readers));}
public boolean parquet-mr_f1985_0()
{    return boundFilter1.isMatch() || boundFilter2.isMatch();}
public static final UnboundRecordFilter parquet-mr_f1986_0(final long startPos, final long pageSize)
{    return new UnboundRecordFilter() {        @Override        public RecordFilter bind(Iterable<ColumnReader> readers) {            return new PagedRecordFilter(startPos, pageSize);        }    };}
public RecordFilter parquet-mr_f1987_0(Iterable<ColumnReader> readers)
{    return new PagedRecordFilter(startPos, pageSize);}
public boolean parquet-mr_f1988_0()
{    currentPos++;    return ((currentPos >= startPos) && (currentPos < endPos));}
public static Filter parquet-mr_f1989_1(FilterPredicate filterPredicate)
{    checkNotNull(filterPredicate, "filterPredicate");            FilterPredicate collapsedPredicate = LogicalInverseRewriter.rewrite(filterPredicate);    if (!filterPredicate.equals(collapsedPredicate)) {            }    return new FilterPredicateCompat(collapsedPredicate);}
public static Filter parquet-mr_f1990_0(UnboundRecordFilter unboundRecordFilter)
{    return new UnboundRecordFilterCompat(unboundRecordFilter);}
public static Filter parquet-mr_f1991_0(FilterPredicate filterPredicate, UnboundRecordFilter unboundRecordFilter)
{    checkArgument(filterPredicate == null || unboundRecordFilter == null, "Cannot provide both a FilterPredicate and an UnboundRecordFilter");    if (filterPredicate != null) {        return get(filterPredicate);    }    if (unboundRecordFilter != null) {        return get(unboundRecordFilter);    }    return NOOP;}
public FilterPredicate parquet-mr_f1992_0()
{    return filterPredicate;}
public R parquet-mr_f1993_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public UnboundRecordFilter parquet-mr_f1994_0()
{    return unboundRecordFilter;}
public R parquet-mr_f1995_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public R parquet-mr_f1996_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public static IntColumn parquet-mr_f1997_0(String columnPath)
{    return new IntColumn(ColumnPath.fromDotString(columnPath));}
public static LongColumn parquet-mr_f1998_0(String columnPath)
{    return new LongColumn(ColumnPath.fromDotString(columnPath));}
public static FloatColumn parquet-mr_f1999_0(String columnPath)
{    return new FloatColumn(ColumnPath.fromDotString(columnPath));}
public static DoubleColumn parquet-mr_f2000_0(String columnPath)
{    return new DoubleColumn(ColumnPath.fromDotString(columnPath));}
public static BooleanColumn parquet-mr_f2001_0(String columnPath)
{    return new BooleanColumn(ColumnPath.fromDotString(columnPath));}
public static BinaryColumn parquet-mr_f2002_0(String columnPath)
{    return new BinaryColumn(ColumnPath.fromDotString(columnPath));}
public static Eq<T> parquet-mr_f2003_0(C column, T value)
{    return new Eq<T>(column, value);}
public static NotEq<T> parquet-mr_f2004_0(C column, T value)
{    return new NotEq<T>(column, value);}
public static Lt<T> parquet-mr_f2005_0(C column, T value)
{    return new Lt<T>(column, value);}
public static LtEq<T> parquet-mr_f2006_0(C column, T value)
{    return new LtEq<T>(column, value);}
public static Gt<T> parquet-mr_f2007_0(C column, T value)
{    return new Gt<T>(column, value);}
public static GtEq<T> parquet-mr_f2008_0(C column, T value)
{    return new GtEq<T>(column, value);}
public static UserDefined<T, U> parquet-mr_f2009_0(Column<T> column, Class<U> clazz)
{    return new UserDefinedByClass<T, U>(column, clazz);}
public static UserDefined<T, U> parquet-mr_f2010_0(Column<T> column, U udp)
{    return new UserDefinedByInstance<T, U>(column, udp);}
public static FilterPredicate parquet-mr_f2011_0(FilterPredicate left, FilterPredicate right)
{    return new And(left, right);}
public static FilterPredicate parquet-mr_f2012_0(FilterPredicate left, FilterPredicate right)
{    return new Or(left, right);}
public static FilterPredicate parquet-mr_f2013_0(FilterPredicate predicate)
{    return new Not(predicate);}
public static FilterPredicate parquet-mr_f2014_0(FilterPredicate pred)
{    checkNotNull(pred, "pred");    return pred.accept(INSTANCE);}
public FilterPredicate parquet-mr_f2015_0(Eq<T> eq)
{    return eq;}
public FilterPredicate parquet-mr_f2016_0(NotEq<T> notEq)
{    return notEq;}
public FilterPredicate parquet-mr_f2017_0(Lt<T> lt)
{    return lt;}
public FilterPredicate parquet-mr_f2018_0(LtEq<T> ltEq)
{    return ltEq;}
public FilterPredicate parquet-mr_f2019_0(Gt<T> gt)
{    return gt;}
public FilterPredicate parquet-mr_f2020_0(GtEq<T> gtEq)
{    return gtEq;}
public FilterPredicate parquet-mr_f2021_0(And and)
{    return and(and.getLeft().accept(this), and.getRight().accept(this));}
public FilterPredicate parquet-mr_f2022_0(Or or)
{    return or(or.getLeft().accept(this), or.getRight().accept(this));}
public FilterPredicate parquet-mr_f2023_0(Not not)
{    return LogicalInverter.invert(not.getPredicate().accept(this));}
public FilterPredicate parquet-mr_f2024_0(UserDefined<T, U> udp)
{    return udp;}
public FilterPredicate parquet-mr_f2025_0(LogicalNotUserDefined<T, U> udp)
{    return udp;}
public static FilterPredicate parquet-mr_f2026_0(FilterPredicate pred)
{    checkNotNull(pred, "pred");    return pred.accept(INSTANCE);}
public FilterPredicate parquet-mr_f2027_0(Eq<T> eq)
{    return new NotEq<T>(eq.getColumn(), eq.getValue());}
public FilterPredicate parquet-mr_f2028_0(NotEq<T> notEq)
{    return new Eq<T>(notEq.getColumn(), notEq.getValue());}
public FilterPredicate parquet-mr_f2029_0(Lt<T> lt)
{    return new GtEq<T>(lt.getColumn(), lt.getValue());}
public FilterPredicate parquet-mr_f2030_0(LtEq<T> ltEq)
{    return new Gt<T>(ltEq.getColumn(), ltEq.getValue());}
public FilterPredicate parquet-mr_f2031_0(Gt<T> gt)
{    return new LtEq<T>(gt.getColumn(), gt.getValue());}
public FilterPredicate parquet-mr_f2032_0(GtEq<T> gtEq)
{    return new Lt<T>(gtEq.getColumn(), gtEq.getValue());}
public FilterPredicate parquet-mr_f2033_0(And and)
{    return new Or(and.getLeft().accept(this), and.getRight().accept(this));}
public FilterPredicate parquet-mr_f2034_0(Or or)
{    return new And(or.getLeft().accept(this), or.getRight().accept(this));}
public FilterPredicate parquet-mr_f2035_0(Not not)
{    return not.getPredicate();}
public FilterPredicate parquet-mr_f2036_0(UserDefined<T, U> udp)
{    return new LogicalNotUserDefined<T, U>(udp);}
public FilterPredicate parquet-mr_f2037_0(LogicalNotUserDefined<T, U> udp)
{    return udp.getUserDefined();}
public Class<T> parquet-mr_f2038_0()
{    return columnType;}
public ColumnPath parquet-mr_f2039_0()
{    return columnPath;}
public String parquet-mr_f2040_0()
{    return "column(" + columnPath.toDotString() + ")";}
public boolean parquet-mr_f2041_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Column column = (Column) o;    if (!columnType.equals(column.columnType))        return false;    if (!columnPath.equals(column.columnPath))        return false;    return true;}
public int parquet-mr_f2042_0()
{    int result = columnPath.hashCode();    result = 31 * result + columnType.hashCode();    return result;}
public Column<T> parquet-mr_f2043_0()
{    return column;}
public T parquet-mr_f2044_0()
{    return value;}
public String parquet-mr_f2045_0()
{    return toString;}
public boolean parquet-mr_f2046_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ColumnFilterPredicate that = (ColumnFilterPredicate) o;    if (!column.equals(that.column))        return false;    if (value != null ? !value.equals(that.value) : that.value != null)        return false;    return true;}
public int parquet-mr_f2047_0()
{    int result = column.hashCode();    result = 31 * result + (value != null ? value.hashCode() : 0);    result = 31 * result + getClass().hashCode();    return result;}
public R parquet-mr_f2048_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public R parquet-mr_f2049_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public R parquet-mr_f2050_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public R parquet-mr_f2051_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public R parquet-mr_f2052_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public R parquet-mr_f2053_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public FilterPredicate parquet-mr_f2054_0()
{    return left;}
public FilterPredicate parquet-mr_f2055_0()
{    return right;}
public String parquet-mr_f2056_0()
{    return toString;}
public boolean parquet-mr_f2057_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    BinaryLogicalFilterPredicate that = (BinaryLogicalFilterPredicate) o;    if (!left.equals(that.left))        return false;    if (!right.equals(that.right))        return false;    return true;}
public int parquet-mr_f2058_0()
{    int result = left.hashCode();    result = 31 * result + right.hashCode();    result = 31 * result + getClass().hashCode();    return result;}
public R parquet-mr_f2059_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public R parquet-mr_f2060_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public FilterPredicate parquet-mr_f2061_0()
{    return predicate;}
public String parquet-mr_f2062_0()
{    return toString;}
public R parquet-mr_f2063_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public boolean parquet-mr_f2064_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Not not = (Not) o;    return predicate.equals(not.predicate);}
public int parquet-mr_f2065_0()
{    return predicate.hashCode() * 31 + getClass().hashCode();}
public Column<T> parquet-mr_f2066_0()
{    return column;}
public R parquet-mr_f2067_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public Class<U> parquet-mr_f2068_0()
{    return udpClass;}
public U parquet-mr_f2069_0()
{    try {        return udpClass.newInstance();    } catch (InstantiationException e) {        throw new RuntimeException(String.format(INSTANTIATION_ERROR_MESSAGE, udpClass), e);    } catch (IllegalAccessException e) {        throw new RuntimeException(String.format(INSTANTIATION_ERROR_MESSAGE, udpClass), e);    }}
public String parquet-mr_f2070_0()
{    return toString;}
public boolean parquet-mr_f2071_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    UserDefinedByClass that = (UserDefinedByClass) o;    if (!column.equals(that.column))        return false;    if (!udpClass.equals(that.udpClass))        return false;    return true;}
public int parquet-mr_f2072_0()
{    int result = column.hashCode();    result = 31 * result + udpClass.hashCode();    result = result * 31 + getClass().hashCode();    return result;}
public U parquet-mr_f2073_0()
{    return udpInstance;}
public String parquet-mr_f2074_0()
{    return toString;}
public boolean parquet-mr_f2075_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    UserDefinedByInstance that = (UserDefinedByInstance) o;    if (!column.equals(that.column))        return false;    if (!udpInstance.equals(that.udpInstance))        return false;    return true;}
public int parquet-mr_f2076_0()
{    int result = column.hashCode();    result = 31 * result + udpInstance.hashCode();    result = result * 31 + getClass().hashCode();    return result;}
public UserDefined<T, U> parquet-mr_f2077_0()
{    return udp;}
public R parquet-mr_f2078_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public String parquet-mr_f2079_0()
{    return toString;}
public boolean parquet-mr_f2080_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    LogicalNotUserDefined that = (LogicalNotUserDefined) o;    if (!udp.equals(that.udp))        return false;    return true;}
public int parquet-mr_f2081_0()
{    int result = udp.hashCode();    result = result * 31 + getClass().hashCode();    return result;}
public static Class<?> parquet-mr_f2082_0(Class<?> c)
{    checkArgument(c.isPrimitive(), "Class " + c + " is not primitive!");    return primitiveToBoxed.get(c);}
public static void parquet-mr_f2083_0(FilterPredicate predicate, MessageType schema)
{    checkNotNull(predicate, "predicate");    checkNotNull(schema, "schema");    predicate.accept(new SchemaCompatibilityValidator(schema));}
public Void parquet-mr_f2084_0(Eq<T> pred)
{    validateColumnFilterPredicate(pred);    return null;}
public Void parquet-mr_f2085_0(NotEq<T> pred)
{    validateColumnFilterPredicate(pred);    return null;}
public Void parquet-mr_f2086_0(Lt<T> pred)
{    validateColumnFilterPredicate(pred);    return null;}
public Void parquet-mr_f2087_0(LtEq<T> pred)
{    validateColumnFilterPredicate(pred);    return null;}
public Void parquet-mr_f2088_0(Gt<T> pred)
{    validateColumnFilterPredicate(pred);    return null;}
public Void parquet-mr_f2089_0(GtEq<T> pred)
{    validateColumnFilterPredicate(pred);    return null;}
public Void parquet-mr_f2090_0(And and)
{    and.getLeft().accept(this);    and.getRight().accept(this);    return null;}
public Void parquet-mr_f2091_0(Or or)
{    or.getLeft().accept(this);    or.getRight().accept(this);    return null;}
public Void parquet-mr_f2092_0(Not not)
{    not.getPredicate().accept(this);    return null;}
public Void parquet-mr_f2093_0(UserDefined<T, U> udp)
{    validateColumn(udp.getColumn());    return null;}
public Void parquet-mr_f2094_0(LogicalNotUserDefined<T, U> udp)
{    return udp.getUserDefined().accept(this);}
private void parquet-mr_f2095_0(ColumnFilterPredicate<T> pred)
{    validateColumn(pred.getColumn());}
private void parquet-mr_f2096_0(Column<T> column)
{    ColumnPath path = column.getColumnPath();    Class<?> alreadySeen = columnTypesEncountered.get(path);    if (alreadySeen != null && !alreadySeen.equals(column.getColumnType())) {        throw new IllegalArgumentException("Column: " + path.toDotString() + " was provided with different types in the same predicate." + " Found both: (" + alreadySeen + ", " + column.getColumnType() + ")");    }    if (alreadySeen == null) {        columnTypesEncountered.put(path, column.getColumnType());    }    ColumnDescriptor descriptor = getColumnDescriptor(path);    if (descriptor == null) {                return;    }    if (descriptor.getMaxRepetitionLevel() > 0) {        throw new IllegalArgumentException("FilterPredicates do not currently support repeated columns. " + "Column " + path.toDotString() + " is repeated.");    }    ValidTypeMap.assertTypeValid(column, descriptor.getType());}
private ColumnDescriptor parquet-mr_f2097_0(ColumnPath columnPath)
{    return columnsAccordingToSchema.get(columnPath);}
public T parquet-mr_f2098_0()
{    return min;}
public T parquet-mr_f2099_0()
{    return max;}
public Comparator<T> parquet-mr_f2100_0()
{    return comparator;}
public boolean parquet-mr_f2101_0()
{    try {        return keep(null);    } catch (NullPointerException e) {                return false;    }}
private static void parquet-mr_f2102_0(Class<?> c, PrimitiveTypeName p)
{    Set<PrimitiveTypeName> descriptors = classToParquetType.get(c);    if (descriptors == null) {        descriptors = new HashSet<PrimitiveTypeName>();        classToParquetType.put(c, descriptors);    }    descriptors.add(p);    Set<Class<?>> classes = parquetTypeToClass.get(p);    if (classes == null) {        classes = new HashSet<Class<?>>();        parquetTypeToClass.put(p, classes);    }    classes.add(c);}
public static void parquet-mr_f2103_0(Column<T> foundColumn, PrimitiveTypeName primitiveType)
{    Class<T> foundColumnType = foundColumn.getColumnType();    ColumnPath columnPath = foundColumn.getColumnPath();    Set<PrimitiveTypeName> validTypeDescriptors = classToParquetType.get(foundColumnType);    if (validTypeDescriptors == null) {        StringBuilder message = new StringBuilder();        message.append("Column ").append(columnPath.toDotString()).append(" was declared as type: ").append(foundColumnType.getName()).append(" which is not supported in FilterPredicates.");        Set<Class<?>> supportedTypes = parquetTypeToClass.get(primitiveType);        if (supportedTypes != null) {            message.append(" Supported types for this column are: ").append(supportedTypes);        } else {            message.append(" There are no supported types for columns of " + primitiveType);        }        throw new IllegalArgumentException(message.toString());    }    if (!validTypeDescriptors.contains(primitiveType)) {        StringBuilder message = new StringBuilder();        message.append("FilterPredicate column: ").append(columnPath.toDotString()).append("'s declared type (").append(foundColumnType.getName()).append(") does not match the schema found in file metadata. Column ").append(columnPath.toDotString()).append(" is of type: ").append(primitiveType).append("\nValid types for this column are: ").append(parquetTypeToClass.get(primitiveType));        throw new IllegalArgumentException(message.toString());    }}
public Converter parquet-mr_f2104_0(int fieldIndex)
{        Converter delegateConverter = checkNotNull(delegate.getConverter(fieldIndex), "delegate converter");            List<Integer> newIndexFieldPath = new ArrayList<Integer>(indexFieldPath.size() + 1);    newIndexFieldPath.addAll(indexFieldPath);    newIndexFieldPath.add(fieldIndex);    if (delegateConverter.isPrimitive()) {        PrimitiveColumnIO columnIO = getColumnIO(newIndexFieldPath);        ColumnPath columnPath = ColumnPath.get(columnIO.getColumnDescriptor().getPath());        ValueInspector[] valueInspectors = getValueInspectors(columnPath);        return new FilteringPrimitiveConverter(delegateConverter.asPrimitiveConverter(), valueInspectors);    } else {        return new FilteringGroupConverter(delegateConverter.asGroupConverter(), newIndexFieldPath, valueInspectorsByColumn, columnIOsByIndexFieldPath);    }}
private PrimitiveColumnIO parquet-mr_f2105_0(List<Integer> indexFieldPath)
{    PrimitiveColumnIO found = columnIOsByIndexFieldPath.get(indexFieldPath);    checkArgument(found != null, "Did not find PrimitiveColumnIO for index field path" + indexFieldPath);    return found;}
private ValueInspector[] parquet-mr_f2106_0(ColumnPath columnPath)
{    List<ValueInspector> inspectorsList = valueInspectorsByColumn.get(columnPath);    if (inspectorsList == null) {        return new ValueInspector[] {};    } else {        return inspectorsList.toArray(new ValueInspector[inspectorsList.size()]);    }}
public void parquet-mr_f2107_0()
{    delegate.start();}
public void parquet-mr_f2108_0()
{    delegate.end();}
public boolean parquet-mr_f2109_0()
{    return false;}
public void parquet-mr_f2110_0(Dictionary dictionary)
{    throw new UnsupportedOperationException("FilteringPrimitiveConverter doesn't have dictionary support");}
public void parquet-mr_f2111_0(int dictionaryId)
{    throw new UnsupportedOperationException("FilteringPrimitiveConverter doesn't have dictionary support");}
public void parquet-mr_f2112_0(Binary value)
{    for (ValueInspector valueInspector : valueInspectors) {        valueInspector.update(value);    }    delegate.addBinary(value);}
public void parquet-mr_f2113_0(boolean value)
{    for (ValueInspector valueInspector : valueInspectors) {        valueInspector.update(value);    }    delegate.addBoolean(value);}
public void parquet-mr_f2114_0(double value)
{    for (ValueInspector valueInspector : valueInspectors) {        valueInspector.update(value);    }    delegate.addDouble(value);}
public void parquet-mr_f2115_0(float value)
{    for (ValueInspector valueInspector : valueInspectors) {        valueInspector.update(value);    }    delegate.addFloat(value);}
public void parquet-mr_f2116_0(int value)
{    for (ValueInspector valueInspector : valueInspectors) {        valueInspector.update(value);    }    delegate.addInt(value);}
public void parquet-mr_f2117_0(long value)
{    for (ValueInspector valueInspector : valueInspectors) {        valueInspector.update(value);    }    delegate.addLong(value);}
public static List<Integer> parquet-mr_f2118_0(PrimitiveColumnIO c)
{    return intArrayToList(c.getIndexFieldPath());}
public static List<Integer> parquet-mr_f2119_0(int[] arr)
{    List<Integer> list = new ArrayList<Integer>(arr.length);    for (int i : arr) {        list.add(i);    }    return list;}
public T parquet-mr_f2120_0()
{        boolean keep = IncrementallyUpdatedFilterPredicateEvaluator.evaluate(filterPredicate);        IncrementallyUpdatedFilterPredicateResetter.reset(filterPredicate);    if (keep) {        return delegate.getCurrentRecord();    } else {                return null;    }}
public void parquet-mr_f2121_0()
{    delegate.skipCurrentRecord();}
public GroupConverter parquet-mr_f2122_0()
{    return rootConverter;}
public void parquet-mr_f2123_0()
{    throw new UnsupportedOperationException();}
public void parquet-mr_f2124_0(int value)
{    throw new UnsupportedOperationException();}
public void parquet-mr_f2125_0(long value)
{    throw new UnsupportedOperationException();}
public void parquet-mr_f2126_0(double value)
{    throw new UnsupportedOperationException();}
public void parquet-mr_f2127_0(float value)
{    throw new UnsupportedOperationException();}
public void parquet-mr_f2128_0(boolean value)
{    throw new UnsupportedOperationException();}
public void parquet-mr_f2129_0(Binary value)
{    throw new UnsupportedOperationException();}
public final void parquet-mr_f2130_0()
{    isKnown = false;    result = false;}
protected final void parquet-mr_f2131_0(boolean result)
{    if (isKnown) {        throw new IllegalStateException("setResult() called on a ValueInspector whose result is already known!" + " Did you forget to call reset()?");    }    this.result = result;    this.isKnown = true;}
public final boolean parquet-mr_f2132_0()
{    if (!isKnown) {        throw new IllegalStateException("getResult() called on a ValueInspector whose result is not yet known!");    }    return result;}
public final boolean parquet-mr_f2133_0()
{    return isKnown;}
public boolean parquet-mr_f2134_0(Visitor visitor)
{    return visitor.visit(this);}
public final IncrementallyUpdatedFilterPredicate parquet-mr_f2135_0()
{    return left;}
public final IncrementallyUpdatedFilterPredicate parquet-mr_f2136_0()
{    return right;}
public boolean parquet-mr_f2137_0(Visitor visitor)
{    return visitor.visit(this);}
public boolean parquet-mr_f2138_0(Visitor visitor)
{    return visitor.visit(this);}
public final IncrementallyUpdatedFilterPredicate parquet-mr_f2139_0(FilterPredicate pred)
{    checkArgument(!built, "This builder has already been used");    IncrementallyUpdatedFilterPredicate incremental = pred.accept(this);    built = true;    return incremental;}
protected final void parquet-mr_f2140_0(ColumnPath columnPath, ValueInspector valueInspector)
{    List<ValueInspector> valueInspectors = valueInspectorsByColumn.get(columnPath);    if (valueInspectors == null) {        valueInspectors = new ArrayList<ValueInspector>();        valueInspectorsByColumn.put(columnPath, valueInspectors);    }    valueInspectors.add(valueInspector);}
public Map<ColumnPath, List<ValueInspector>> parquet-mr_f2141_0()
{    return valueInspectorsByColumn;}
protected final PrimitiveComparator<T> parquet-mr_f2142_0(ColumnPath path)
{    return (PrimitiveComparator<T>) comparatorsByColumn.get(path);}
public final IncrementallyUpdatedFilterPredicate parquet-mr_f2143_0(And and)
{    return new IncrementallyUpdatedFilterPredicate.And(and.getLeft().accept(this), and.getRight().accept(this));}
public final IncrementallyUpdatedFilterPredicate parquet-mr_f2144_0(Or or)
{    return new IncrementallyUpdatedFilterPredicate.Or(or.getLeft().accept(this), or.getRight().accept(this));}
public final IncrementallyUpdatedFilterPredicate parquet-mr_f2145_0(Not not)
{    throw new IllegalArgumentException("This predicate contains a not! Did you forget to run this predicate through LogicalInverseRewriter? " + not);}
public static boolean parquet-mr_f2146_0(IncrementallyUpdatedFilterPredicate pred)
{    checkNotNull(pred, "pred");    return pred.accept(INSTANCE);}
public boolean parquet-mr_f2147_0(ValueInspector p)
{    if (!p.isKnown()) {        p.updateNull();    }    return p.getResult();}
public boolean parquet-mr_f2148_0(And and)
{    return and.getLeft().accept(this) && and.getRight().accept(this);}
public boolean parquet-mr_f2149_0(Or or)
{    return or.getLeft().accept(this) || or.getRight().accept(this);}
public static void parquet-mr_f2150_0(IncrementallyUpdatedFilterPredicate pred)
{    checkNotNull(pred, "pred");    pred.accept(INSTANCE);}
public boolean parquet-mr_f2151_0(ValueInspector p)
{    p.reset();    return false;}
public boolean parquet-mr_f2152_0(And and)
{    and.getLeft().accept(this);    and.getRight().accept(this);    return false;}
public boolean parquet-mr_f2153_0(Or or)
{    or.getLeft().accept(this);    or.getRight().accept(this);    return false;}
 ByteBuffer parquet-mr_f2154_0(int pageIndex)
{    return convert(minValues[pageIndex]);}
 ByteBuffer parquet-mr_f2155_0(int pageIndex)
{    return convert(maxValues[pageIndex]);}
 String parquet-mr_f2156_0(int pageIndex)
{    return stringifier.stringify(minValues[pageIndex]);}
 String parquet-mr_f2157_0(int pageIndex)
{    return stringifier.stringify(maxValues[pageIndex]);}
 Statistics<T> parquet-mr_f2158_0(int arrayIndex)
{    return (Statistics<T>) new Statistics<Binary>(minValues[arrayIndex], maxValues[arrayIndex], comparator);}
 ValueComparator parquet-mr_f2159_0(Object value)
{    final Binary v = (Binary) value;    return new ValueComparator() {        @Override        int compareValueToMin(int arrayIndex) {            return comparator.compare(v, minValues[arrayIndex]);        }        @Override        int compareValueToMax(int arrayIndex) {            return comparator.compare(v, maxValues[arrayIndex]);        }    };}
 int parquet-mr_f2160_0(int arrayIndex)
{    return comparator.compare(v, minValues[arrayIndex]);}
 int parquet-mr_f2161_0(int arrayIndex)
{    return comparator.compare(v, maxValues[arrayIndex]);}
private static Binary parquet-mr_f2162_0(ByteBuffer buffer)
{    return Binary.fromReusedByteBuffer(buffer);}
private static ByteBuffer parquet-mr_f2163_0(Binary value)
{    return value.toByteBuffer();}
 void parquet-mr_f2164_0(ByteBuffer min, ByteBuffer max)
{    minValues.add(convert(min));    maxValues.add(convert(max));}
 void parquet-mr_f2165_0(Object min, Object max)
{    minValues.add(min == null ? null : truncator.truncateMin((Binary) min, truncateLength));    maxValues.add(max == null ? null : truncator.truncateMax((Binary) max, truncateLength));}
 ColumnIndexBase<Binary> parquet-mr_f2166_0(PrimitiveType type)
{    BinaryColumnIndex columnIndex = new BinaryColumnIndex(type);    columnIndex.minValues = minValues.toArray(new Binary[minValues.size()]);    columnIndex.maxValues = maxValues.toArray(new Binary[maxValues.size()]);    return columnIndex;}
 void parquet-mr_f2167_0()
{    minValues.clear();    maxValues.clear();}
 int parquet-mr_f2168_0(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(minValues.get(index1), minValues.get(index2));}
 int parquet-mr_f2169_0(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(maxValues.get(index1), maxValues.get(index2));}
 int parquet-mr_f2170_0(Object value)
{    return ((Binary) value).length();}
 Validity parquet-mr_f2171_0(ByteBuffer buffer)
{    int pos = buffer.position();    CoderResult result = CoderResult.OVERFLOW;    while (result.isOverflow()) {        dummyBuffer.clear();        result = decoder.decode(buffer, dummyBuffer, true);    }    buffer.position(pos);    if (result.isUnderflow()) {        return Validity.VALID;    } else if (result.isMalformed()) {        return Validity.MALFORMED;    } else {        return Validity.UNMAPPABLE;    }}
 Binary parquet-mr_f2172_0(Binary minValue, int length)
{    return minValue;}
 Binary parquet-mr_f2173_0(Binary maxValue, int length)
{    return maxValue;}
 Binary parquet-mr_f2174_0(Binary minValue, int length)
{    if (minValue.length() <= length) {        return minValue;    }    ByteBuffer buffer = minValue.toByteBuffer();    byte[] array;    if (validator.checkValidity(buffer) == Validity.VALID) {        array = truncateUtf8(buffer, length);    } else {        array = truncate(buffer, length);    }    return array == null ? minValue : Binary.fromConstantByteArray(array);}
 Binary parquet-mr_f2175_0(Binary maxValue, int length)
{    if (maxValue.length() <= length) {        return maxValue;    }    byte[] array;    ByteBuffer buffer = maxValue.toByteBuffer();    if (validator.checkValidity(buffer) == Validity.VALID) {        array = incrementUtf8(truncateUtf8(buffer, length));    } else {        array = increment(truncate(buffer, length));    }    return array == null ? maxValue : Binary.fromConstantByteArray(array);}
private byte[] parquet-mr_f2176_0(ByteBuffer buffer, int length)
{    assert length < buffer.remaining();    byte[] array = new byte[length];    buffer.get(array);    return array;}
private byte[] parquet-mr_f2177_0(byte[] array)
{    for (int i = array.length - 1; i >= 0; --i) {        byte elem = array[i];        ++elem;        array[i] = elem;        if (elem != 0) {                        return array;        }    }    return null;}
private byte[] parquet-mr_f2178_0(ByteBuffer buffer, int length)
{    assert length < buffer.remaining();    ByteBuffer newBuffer = buffer.slice();    newBuffer.limit(newBuffer.position() + length);    while (validator.checkValidity(newBuffer) != Validity.VALID) {        newBuffer.limit(newBuffer.limit() - 1);        if (newBuffer.remaining() == 0) {            return null;        }    }    byte[] array = new byte[newBuffer.remaining()];    newBuffer.get(array);    return array;}
private byte[] parquet-mr_f2179_0(byte[] array)
{    if (array == null) {        return null;    }    ByteBuffer buffer = ByteBuffer.wrap(array);    for (int i = array.length - 1; i >= 0; --i) {        byte prev = array[i];        byte inc = prev;        while (++inc != 0) {                        array[i] = inc;            switch(validator.checkValidity(buffer)) {                case VALID:                    return array;                case UNMAPPABLE:                                        continue;                case MALFORMED:                                        break;            }                        break;        }        array[i] = prev;    }        return null;}
 static BinaryTruncator parquet-mr_f2180_0(PrimitiveType type)
{    if (type == null) {        return NO_OP_TRUNCATOR;    }    switch(type.getPrimitiveTypeName()) {        case INT96:            return NO_OP_TRUNCATOR;        case BINARY:        case FIXED_LEN_BYTE_ARRAY:            LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();            if (logicalTypeAnnotation == null) {                return DEFAULT_UTF8_TRUNCATOR;            }            return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<BinaryTruncator>() {                @Override                public Optional<BinaryTruncator> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType) {                    return Optional.of(DEFAULT_UTF8_TRUNCATOR);                }                @Override                public Optional<BinaryTruncator> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType) {                    return Optional.of(DEFAULT_UTF8_TRUNCATOR);                }                @Override                public Optional<BinaryTruncator> visit(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType) {                    return Optional.of(DEFAULT_UTF8_TRUNCATOR);                }                @Override                public Optional<BinaryTruncator> visit(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType) {                    return Optional.of(DEFAULT_UTF8_TRUNCATOR);                }            }).orElse(NO_OP_TRUNCATOR);        default:            throw new IllegalArgumentException("No truncator is available for the type: " + type);    }}
public Optional<BinaryTruncator> parquet-mr_f2181_0(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    return Optional.of(DEFAULT_UTF8_TRUNCATOR);}
public Optional<BinaryTruncator> parquet-mr_f2182_0(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType)
{    return Optional.of(DEFAULT_UTF8_TRUNCATOR);}
public Optional<BinaryTruncator> parquet-mr_f2183_0(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType)
{    return Optional.of(DEFAULT_UTF8_TRUNCATOR);}
public Optional<BinaryTruncator> parquet-mr_f2184_0(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType)
{    return Optional.of(DEFAULT_UTF8_TRUNCATOR);}
 ByteBuffer parquet-mr_f2185_0(int pageIndex)
{    return convert(minValues[pageIndex]);}
 ByteBuffer parquet-mr_f2186_0(int pageIndex)
{    return convert(maxValues[pageIndex]);}
 String parquet-mr_f2187_0(int pageIndex)
{    return stringifier.stringify(minValues[pageIndex]);}
 String parquet-mr_f2188_0(int pageIndex)
{    return stringifier.stringify(maxValues[pageIndex]);}
 Statistics<T> parquet-mr_f2189_0(int arrayIndex)
{    return (Statistics<T>) new Statistics<Boolean>(minValues[arrayIndex], maxValues[arrayIndex], comparator);}
 ValueComparator parquet-mr_f2190_0(Object value)
{    final boolean v = (boolean) value;    return new ValueComparator() {        @Override        int compareValueToMin(int arrayIndex) {            return comparator.compare(v, minValues[arrayIndex]);        }        @Override        int compareValueToMax(int arrayIndex) {            return comparator.compare(v, maxValues[arrayIndex]);        }    };}
 int parquet-mr_f2191_0(int arrayIndex)
{    return comparator.compare(v, minValues[arrayIndex]);}
 int parquet-mr_f2192_0(int arrayIndex)
{    return comparator.compare(v, maxValues[arrayIndex]);}
private static boolean parquet-mr_f2193_0(ByteBuffer buffer)
{    return buffer.get(0) != 0;}
private static ByteBuffer parquet-mr_f2194_0(boolean value)
{    return ByteBuffer.allocate(1).put(0, value ? (byte) 1 : 0);}
 void parquet-mr_f2195_0(ByteBuffer min, ByteBuffer max)
{    minValues.add(convert(min));    maxValues.add(convert(max));}
 void parquet-mr_f2196_0(Object min, Object max)
{    minValues.add((boolean) min);    maxValues.add((boolean) max);}
 ColumnIndexBase<Boolean> parquet-mr_f2197_0(PrimitiveType type)
{    BooleanColumnIndex columnIndex = new BooleanColumnIndex(type);    columnIndex.minValues = minValues.toBooleanArray();    columnIndex.maxValues = maxValues.toBooleanArray();    return columnIndex;}
 void parquet-mr_f2198_0()
{    minValues.clear();    maxValues.clear();}
 int parquet-mr_f2199_0(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(minValues.get(index1), minValues.get(index2));}
 int parquet-mr_f2200_0(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(maxValues.get(index1), maxValues.get(index2));}
 int parquet-mr_f2201_0(Object value)
{    return 1;}
private static int parquet-mr_f2202_0(int left, int right)
{        return left + ((right - left) / 2);}
private static int parquet-mr_f2203_0(int left, int right)
{        return left + ((right - left + 1) / 2);}
 PrimitiveIterator.OfInt parquet-mr_f2204_0(ColumnIndexBase<?>.ValueComparator comparator)
{    return IndexIterator.filterTranslate(comparator.arrayLength(), arrayIndex -> comparator.compareValueToMin(arrayIndex) >= 0 && comparator.compareValueToMax(arrayIndex) <= 0, comparator::translate);}
 PrimitiveIterator.OfInt parquet-mr_f2205_0(ColumnIndexBase<?>.ValueComparator comparator)
{    return IndexIterator.filterTranslate(comparator.arrayLength(), arrayIndex -> comparator.compareValueToMax(arrayIndex) < 0, comparator::translate);}
 PrimitiveIterator.OfInt parquet-mr_f2206_0(ColumnIndexBase<?>.ValueComparator comparator)
{    return IndexIterator.filterTranslate(comparator.arrayLength(), arrayIndex -> comparator.compareValueToMax(arrayIndex) <= 0, comparator::translate);}
 PrimitiveIterator.OfInt parquet-mr_f2207_0(ColumnIndexBase<?>.ValueComparator comparator)
{    return IndexIterator.filterTranslate(comparator.arrayLength(), arrayIndex -> comparator.compareValueToMin(arrayIndex) > 0, comparator::translate);}
 PrimitiveIterator.OfInt parquet-mr_f2208_0(ColumnIndexBase<?>.ValueComparator comparator)
{    return IndexIterator.filterTranslate(comparator.arrayLength(), arrayIndex -> comparator.compareValueToMin(arrayIndex) >= 0, comparator::translate);}
 PrimitiveIterator.OfInt parquet-mr_f2209_0(ColumnIndexBase<?>.ValueComparator comparator)
{    return IndexIterator.filterTranslate(comparator.arrayLength(), arrayIndex -> comparator.compareValueToMin(arrayIndex) != 0 || comparator.compareValueToMax(arrayIndex) != 0, comparator::translate);}
 OfInt parquet-mr_f2210_0(ColumnIndexBase<?>.ValueComparator comparator)
{    Bounds bounds = findBounds(comparator);    if (bounds == null) {        return IndexIterator.EMPTY;    }    return IndexIterator.rangeTranslate(bounds.lower, bounds.upper, comparator::translate);}
 OfInt parquet-mr_f2211_0(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = 0;    int right = length;    do {        int i = floorMid(left, right);        if (comparator.compareValueToMax(i) >= 0) {            left = i + 1;        } else {            right = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(right, length - 1, comparator::translate);}
 OfInt parquet-mr_f2212_0(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = 0;    int right = length;    do {        int i = floorMid(left, right);        if (comparator.compareValueToMax(i) > 0) {            left = i + 1;        } else {            right = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(right, length - 1, comparator::translate);}
 OfInt parquet-mr_f2213_0(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = -1;    int right = length - 1;    do {        int i = ceilingMid(left, right);        if (comparator.compareValueToMin(i) <= 0) {            right = i - 1;        } else {            left = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(0, left, comparator::translate);}
 OfInt parquet-mr_f2214_0(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = -1;    int right = length - 1;    do {        int i = ceilingMid(left, right);        if (comparator.compareValueToMin(i) < 0) {            right = i - 1;        } else {            left = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(0, left, comparator::translate);}
 OfInt parquet-mr_f2215_0(ColumnIndexBase<?>.ValueComparator comparator)
{    Bounds bounds = findBounds(comparator);    int length = comparator.arrayLength();    if (bounds == null) {        return IndexIterator.all(comparator);    }    return IndexIterator.filterTranslate(length, i -> i < bounds.lower || i > bounds.upper || comparator.compareValueToMin(i) != 0 || comparator.compareValueToMax(i) != 0, comparator::translate);}
private Bounds parquet-mr_f2216_0(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int lowerLeft = 0;    int upperLeft = 0;    int lowerRight = length - 1;    int upperRight = length - 1;    do {        if (lowerLeft > lowerRight) {            return null;        }        int i = floorMid(lowerLeft, lowerRight);        if (comparator.compareValueToMin(i) < 0) {            lowerRight = upperRight = i - 1;        } else if (comparator.compareValueToMax(i) > 0) {            lowerLeft = upperLeft = i + 1;        } else {            lowerRight = upperLeft = i;        }    } while (lowerLeft != lowerRight);    do {        if (upperLeft > upperRight) {            return null;        }        int i = ceilingMid(upperLeft, upperRight);        if (comparator.compareValueToMin(i) < 0) {            upperRight = i - 1;        } else if (comparator.compareValueToMax(i) > 0) {            upperLeft = i + 1;        } else {            upperLeft = i;        }    } while (upperLeft != upperRight);    return new Bounds(lowerLeft, upperRight);}
 OfInt parquet-mr_f2217_0(ColumnIndexBase<?>.ValueComparator comparator)
{    Bounds bounds = findBounds(comparator);    if (bounds == null) {        return IndexIterator.EMPTY;    }    return IndexIterator.rangeTranslate(bounds.lower, bounds.upper, comparator::translate);}
 OfInt parquet-mr_f2218_0(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = -1;    int right = length - 1;    do {        int i = ceilingMid(left, right);        if (comparator.compareValueToMax(i) >= 0) {            right = i - 1;        } else {            left = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(0, left, comparator::translate);}
 OfInt parquet-mr_f2219_0(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = -1;    int right = length - 1;    do {        int i = ceilingMid(left, right);        if (comparator.compareValueToMax(i) > 0) {            right = i - 1;        } else {            left = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(0, left, comparator::translate);}
 OfInt parquet-mr_f2220_0(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = 0;    int right = length;    do {        int i = floorMid(left, right);        if (comparator.compareValueToMin(i) <= 0) {            left = i + 1;        } else {            right = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(right, length - 1, comparator::translate);}
 OfInt parquet-mr_f2221_0(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = 0;    int right = length;    do {        int i = floorMid(left, right);        if (comparator.compareValueToMin(i) < 0) {            left = i + 1;        } else {            right = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(right, length - 1, comparator::translate);}
 OfInt parquet-mr_f2222_0(ColumnIndexBase<?>.ValueComparator comparator)
{    Bounds bounds = findBounds(comparator);    int length = comparator.arrayLength();    if (bounds == null) {        return IndexIterator.all(comparator);    }    return IndexIterator.filterTranslate(length, i -> i < bounds.lower || i > bounds.upper || comparator.compareValueToMin(i) != 0 || comparator.compareValueToMax(i) != 0, comparator::translate);}
private Bounds parquet-mr_f2223_0(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int lowerLeft = 0;    int upperLeft = 0;    int lowerRight = length - 1;    int upperRight = length - 1;    do {        if (lowerLeft > lowerRight) {            return null;        }        int i = floorMid(lowerLeft, lowerRight);        if (comparator.compareValueToMax(i) > 0) {            lowerRight = upperRight = i - 1;        } else if (comparator.compareValueToMin(i) < 0) {            lowerLeft = upperLeft = i + 1;        } else {            lowerRight = upperLeft = i;        }    } while (lowerLeft != lowerRight);    do {        if (upperLeft > upperRight) {            return null;        }        int i = ceilingMid(upperLeft, upperRight);        if (comparator.compareValueToMax(i) > 0) {            upperRight = i - 1;        } else if (comparator.compareValueToMin(i) < 0) {            upperLeft = i + 1;        } else {            upperLeft = i;        }    } while (upperLeft != upperRight);    return new Bounds(lowerLeft, upperRight);}
 int parquet-mr_f2224_0()
{    return pageIndexes.length;}
 int parquet-mr_f2225_0(int arrayIndex)
{    return pageIndexes[arrayIndex];}
 static String parquet-mr_f2226_0(String str)
{    if (str.length() <= MAX_VALUE_LENGTH_FOR_TOSTRING) {        return str;    }    return str.substring(0, TOSTRING_TRUNCATION_START_POS) + TOSTRING_TRUNCATION_MARKER + str.substring(str.length() - TOSTRING_TRUNCATION_END_POS);}
public BoundaryOrder parquet-mr_f2227_0()
{    return boundaryOrder;}
public List<Long> parquet-mr_f2228_0()
{    if (nullCounts == null) {        return null;    }    return LongLists.unmodifiable(LongArrayList.wrap(nullCounts));}
public List<Boolean> parquet-mr_f2229_0()
{    return BooleanLists.unmodifiable(BooleanArrayList.wrap(nullPages));}
public List<ByteBuffer> parquet-mr_f2230_0()
{    List<ByteBuffer> list = new ArrayList<>(getPageCount());    int arrayIndex = 0;    for (int i = 0, n = getPageCount(); i < n; ++i) {        if (isNullPage(i)) {            list.add(EMPTY_BYTE_BUFFER);        } else {            list.add(getMinValueAsBytes(arrayIndex++));        }    }    return list;}
public List<ByteBuffer> parquet-mr_f2231_0()
{    List<ByteBuffer> list = new ArrayList<>(getPageCount());    int arrayIndex = 0;    for (int i = 0, n = getPageCount(); i < n; ++i) {        if (isNullPage(i)) {            list.add(EMPTY_BYTE_BUFFER);        } else {            list.add(getMaxValueAsBytes(arrayIndex++));        }    }    return list;}
public String parquet-mr_f2232_0()
{    try (Formatter formatter = new Formatter()) {        formatter.format("Boudary order: %s\n", boundaryOrder);        String minMaxPart = "  %-" + MAX_VALUE_LENGTH_FOR_TOSTRING + "s  %-" + MAX_VALUE_LENGTH_FOR_TOSTRING + "s\n";        formatter.format("%-10s  %20s" + minMaxPart, "", "null count", "min", "max");        String format = "page-%-5d  %20s" + minMaxPart;        int arrayIndex = 0;        for (int i = 0, n = nullPages.length; i < n; ++i) {            String nullCount = nullCounts == null ? TOSTRING_MISSING_VALUE_MARKER : Long.toString(nullCounts[i]);            String min, max;            if (nullPages[i]) {                min = max = TOSTRING_MISSING_VALUE_MARKER;            } else {                min = truncate(getMinValueAsString(arrayIndex));                max = truncate(getMaxValueAsString(arrayIndex++));            }            formatter.format(format, i, nullCount, min, max);        }        return formatter.toString();    }}
 int parquet-mr_f2233_0()
{    return nullPages.length;}
 boolean parquet-mr_f2234_0(int pageIndex)
{    return nullPages[pageIndex];}
public PrimitiveIterator.OfInt parquet-mr_f2235_0(And and)
{    throw new UnsupportedOperationException("AND shall not be used on column index directly");}
public PrimitiveIterator.OfInt parquet-mr_f2236_0(Not not)
{    throw new UnsupportedOperationException("NOT shall not be used on column index directly");}
public PrimitiveIterator.OfInt parquet-mr_f2237_0(Or or)
{    throw new UnsupportedOperationException("OR shall not be used on column index directly");}
public PrimitiveIterator.OfInt parquet-mr_f2238_0(Eq<T> eq)
{    T value = eq.getValue();    if (value == null) {        if (nullCounts == null) {                        return IndexIterator.all(getPageCount());        } else {            return IndexIterator.filter(getPageCount(), pageIndex -> nullCounts[pageIndex] > 0);        }    }    return getBoundaryOrder().eq(createValueComparator(value));}
public PrimitiveIterator.OfInt parquet-mr_f2239_0(Gt<T> gt)
{    return getBoundaryOrder().gt(createValueComparator(gt.getValue()));}
public PrimitiveIterator.OfInt parquet-mr_f2240_0(GtEq<T> gtEq)
{    return getBoundaryOrder().gtEq(createValueComparator(gtEq.getValue()));}
public PrimitiveIterator.OfInt parquet-mr_f2241_0(Lt<T> lt)
{    return getBoundaryOrder().lt(createValueComparator(lt.getValue()));}
public PrimitiveIterator.OfInt parquet-mr_f2242_0(LtEq<T> ltEq)
{    return getBoundaryOrder().ltEq(createValueComparator(ltEq.getValue()));}
public PrimitiveIterator.OfInt parquet-mr_f2243_0(NotEq<T> notEq)
{    T value = notEq.getValue();    if (value == null) {        return IndexIterator.filter(getPageCount(), pageIndex -> !nullPages[pageIndex]);    }    if (nullCounts == null) {                return IndexIterator.all(getPageCount());    }        IntSet matchingIndexes = new IntOpenHashSet();    getBoundaryOrder().notEq(createValueComparator(value)).forEachRemaining((int index) -> matchingIndexes.add(index));    return IndexIterator.filter(getPageCount(), pageIndex -> nullCounts[pageIndex] > 0 || matchingIndexes.contains(pageIndex));}
public PrimitiveIterator.OfInt parquet-mr_f2244_0(UserDefined<T, U> udp)
{    final UserDefinedPredicate<T> predicate = udp.getUserDefinedPredicate();    final boolean acceptNulls = predicate.acceptsNullValue();    if (acceptNulls && nullCounts == null) {                return IndexIterator.all(getPageCount());    }    return IndexIterator.filter(getPageCount(), new IntPredicate() {        private int arrayIndex = -1;        @Override        public boolean test(int pageIndex) {            if (isNullPage(pageIndex)) {                return acceptNulls;            } else {                ++arrayIndex;                if (acceptNulls && nullCounts[pageIndex] > 0) {                    return true;                }                org.apache.parquet.filter2.predicate.Statistics<T> stats = createStats(arrayIndex);                return !predicate.canDrop(stats);            }        }    });}
public boolean parquet-mr_f2245_0(int pageIndex)
{    if (isNullPage(pageIndex)) {        return acceptNulls;    } else {        ++arrayIndex;        if (acceptNulls && nullCounts[pageIndex] > 0) {            return true;        }        org.apache.parquet.filter2.predicate.Statistics<T> stats = createStats(arrayIndex);        return !predicate.canDrop(stats);    }}
public PrimitiveIterator.OfInt parquet-mr_f2246_0(LogicalNotUserDefined<T, U> udp)
{    final UserDefinedPredicate<T> inversePredicate = udp.getUserDefined().getUserDefinedPredicate();    final boolean acceptNulls = !inversePredicate.acceptsNullValue();    if (acceptNulls && nullCounts == null) {                return IndexIterator.all(getPageCount());    }    return IndexIterator.filter(getPageCount(), new IntPredicate() {        private int arrayIndex = -1;        @Override        public boolean test(int pageIndex) {            if (isNullPage(pageIndex)) {                return acceptNulls;            } else {                ++arrayIndex;                if (acceptNulls && nullCounts[pageIndex] > 0) {                    return true;                }                org.apache.parquet.filter2.predicate.Statistics<T> stats = createStats(arrayIndex);                return !inversePredicate.inverseCanDrop(stats);            }        }    });}
public boolean parquet-mr_f2247_0(int pageIndex)
{    if (isNullPage(pageIndex)) {        return acceptNulls;    } else {        ++arrayIndex;        if (acceptNulls && nullCounts[pageIndex] > 0) {            return true;        }        org.apache.parquet.filter2.predicate.Statistics<T> stats = createStats(arrayIndex);        return !inversePredicate.inverseCanDrop(stats);    }}
public ColumnIndex parquet-mr_f2248_0()
{    return null;}
public void parquet-mr_f2249_0(Statistics<?> stats)
{}
 void parquet-mr_f2250_0(Object min, Object max)
{}
 ColumnIndexBase<?> parquet-mr_f2251_0(PrimitiveType type)
{    return null;}
 void parquet-mr_f2252_0()
{}
 void parquet-mr_f2253_0(ByteBuffer min, ByteBuffer max)
{}
 int parquet-mr_f2254_0(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return 0;}
 int parquet-mr_f2255_0(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return 0;}
 int parquet-mr_f2256_0(Object value)
{    return 0;}
public static ColumnIndexBuilder parquet-mr_f2257_0()
{    return NO_OP_BUILDER;}
public static ColumnIndexBuilder parquet-mr_f2258_0(PrimitiveType type, int truncateLength)
{    ColumnIndexBuilder builder = createNewBuilder(type, truncateLength);    builder.type = type;    return builder;}
private static ColumnIndexBuilder parquet-mr_f2259_0(PrimitiveType type, int truncateLength)
{    switch(type.getPrimitiveTypeName()) {        case BINARY:        case FIXED_LEN_BYTE_ARRAY:        case INT96:            return new BinaryColumnIndexBuilder(type, truncateLength);        case BOOLEAN:            return new BooleanColumnIndexBuilder();        case DOUBLE:            return new DoubleColumnIndexBuilder();        case FLOAT:            return new FloatColumnIndexBuilder();        case INT32:            return new IntColumnIndexBuilder();        case INT64:            return new LongColumnIndexBuilder();        default:            throw new IllegalArgumentException("Unsupported type for column index: " + type);    }}
public static ColumnIndex parquet-mr_f2260_0(PrimitiveType type, BoundaryOrder boundaryOrder, List<Boolean> nullPages, List<Long> nullCounts, List<ByteBuffer> minValues, List<ByteBuffer> maxValues)
{    ColumnIndexBuilder builder = createNewBuilder(type, Integer.MAX_VALUE);    builder.fill(nullPages, nullCounts, minValues, maxValues);    ColumnIndexBase<?> columnIndex = builder.build(type);    columnIndex.boundaryOrder = requireNonNull(boundaryOrder);    return columnIndex;}
public void parquet-mr_f2261_0(Statistics<?> stats)
{    if (stats.hasNonNullValue()) {        nullPages.add(false);        Object min = stats.genericGetMin();        Object max = stats.genericGetMax();        addMinMax(min, max);        pageIndexes.add(nextPageIndex);        minMaxSize += sizeOf(min);        minMaxSize += sizeOf(max);    } else {        nullPages.add(true);    }    nullCounts.add(stats.getNumNulls());    ++nextPageIndex;}
private void parquet-mr_f2262_0(List<Boolean> nullPages, List<Long> nullCounts, List<ByteBuffer> minValues, List<ByteBuffer> maxValues)
{    clear();    int pageCount = nullPages.size();    if ((nullCounts != null && nullCounts.size() != pageCount) || minValues.size() != pageCount || maxValues.size() != pageCount) {        throw new IllegalArgumentException(String.format("Not all sizes are equal (nullPages:%d, nullCounts:%s, minValues:%d, maxValues:%d", nullPages.size(), nullCounts == null ? "null" : nullCounts.size(), minValues.size(), maxValues.size()));    }    this.nullPages.addAll(nullPages);        if (nullCounts != null) {        this.nullCounts.addAll(nullCounts);    }    for (int i = 0; i < pageCount; ++i) {        if (!nullPages.get(i)) {            ByteBuffer min = minValues.get(i);            ByteBuffer max = maxValues.get(i);            addMinMaxFromBytes(min, max);            pageIndexes.add(i);            minMaxSize += min.remaining();            minMaxSize += max.remaining();        }    }}
public ColumnIndex parquet-mr_f2263_0()
{    ColumnIndexBase<?> columnIndex = build(type);    if (columnIndex == null) {        return null;    }    columnIndex.boundaryOrder = calculateBoundaryOrder(type.comparator());    return columnIndex;}
private ColumnIndexBase<?> parquet-mr_f2264_0(PrimitiveType type)
{    if (nullPages.isEmpty()) {        return null;    }    ColumnIndexBase<?> columnIndex = createColumnIndex(type);    if (columnIndex == null) {                return null;    }    columnIndex.nullPages = nullPages.toBooleanArray();        if (!nullCounts.isEmpty()) {        columnIndex.nullCounts = nullCounts.toLongArray();    }    columnIndex.pageIndexes = pageIndexes.toIntArray();    return columnIndex;}
private BoundaryOrder parquet-mr_f2265_0(PrimitiveComparator<Binary> comparator)
{    if (isAscending(comparator)) {        return BoundaryOrder.ASCENDING;    } else if (isDescending(comparator)) {        return BoundaryOrder.DESCENDING;    } else {        return BoundaryOrder.UNORDERED;    }}
private boolean parquet-mr_f2266_0(PrimitiveComparator<Binary> comparator)
{    for (int i = 1, n = pageIndexes.size(); i < n; ++i) {        if (compareMinValues(comparator, i - 1, i) > 0 || compareMaxValues(comparator, i - 1, i) > 0) {            return false;        }    }    return true;}
private boolean parquet-mr_f2267_0(PrimitiveComparator<Binary> comparator)
{    for (int i = 1, n = pageIndexes.size(); i < n; ++i) {        if (compareMinValues(comparator, i - 1, i) < 0 || compareMaxValues(comparator, i - 1, i) < 0) {            return false;        }    }    return true;}
private void parquet-mr_f2268_0()
{    nullPages.clear();    nullCounts.clear();    clearMinMax();    minMaxSize = 0;    nextPageIndex = 0;    pageIndexes.clear();}
public int parquet-mr_f2269_0()
{    return nullPages.size();}
public long parquet-mr_f2270_0()
{    return minMaxSize;}
 ByteBuffer parquet-mr_f2271_0(int pageIndex)
{    return convert(minValues[pageIndex]);}
 ByteBuffer parquet-mr_f2272_0(int pageIndex)
{    return convert(maxValues[pageIndex]);}
 String parquet-mr_f2273_0(int pageIndex)
{    return stringifier.stringify(minValues[pageIndex]);}
 String parquet-mr_f2274_0(int pageIndex)
{    return stringifier.stringify(maxValues[pageIndex]);}
 Statistics<T> parquet-mr_f2275_0(int arrayIndex)
{    return (Statistics<T>) new Statistics<Double>(minValues[arrayIndex], maxValues[arrayIndex], comparator);}
 ValueComparator parquet-mr_f2276_0(Object value)
{    final double v = (double) value;    return new ValueComparator() {        @Override        int compareValueToMin(int arrayIndex) {            return comparator.compare(v, minValues[arrayIndex]);        }        @Override        int compareValueToMax(int arrayIndex) {            return comparator.compare(v, maxValues[arrayIndex]);        }    };}
 int parquet-mr_f2277_0(int arrayIndex)
{    return comparator.compare(v, minValues[arrayIndex]);}
 int parquet-mr_f2278_0(int arrayIndex)
{    return comparator.compare(v, maxValues[arrayIndex]);}
private static double parquet-mr_f2279_0(ByteBuffer buffer)
{    return buffer.order(LITTLE_ENDIAN).getDouble(0);}
private static ByteBuffer parquet-mr_f2280_0(double value)
{    return ByteBuffer.allocate(Double.BYTES).order(LITTLE_ENDIAN).putDouble(0, value);}
 void parquet-mr_f2281_0(ByteBuffer min, ByteBuffer max)
{    minValues.add(convert(min));    maxValues.add(convert(max));}
 void parquet-mr_f2282_0(Object min, Object max)
{    double dMin = (double) min;    double dMax = (double) max;    if (Double.isNaN(dMin) || Double.isNaN(dMax)) {                invalid = true;    }        if (Double.compare(dMin, +0.0) == 0) {        dMin = -0.0;    }    if (Double.compare(dMax, -0.0) == 0) {        dMax = +0.0;    }    minValues.add(dMin);    maxValues.add(dMax);}
 ColumnIndexBase<Double> parquet-mr_f2283_0(PrimitiveType type)
{    if (invalid) {        return null;    }    DoubleColumnIndex columnIndex = new DoubleColumnIndex(type);    columnIndex.minValues = minValues.toDoubleArray();    columnIndex.maxValues = maxValues.toDoubleArray();    return columnIndex;}
 void parquet-mr_f2284_0()
{    minValues.clear();    maxValues.clear();}
 int parquet-mr_f2285_0(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(minValues.get(index1), minValues.get(index2));}
 int parquet-mr_f2286_0(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(maxValues.get(index1), maxValues.get(index2));}
 int parquet-mr_f2287_0(Object value)
{    return Double.BYTES;}
 ByteBuffer parquet-mr_f2288_0(int pageIndex)
{    return convert(minValues[pageIndex]);}
 ByteBuffer parquet-mr_f2289_0(int pageIndex)
{    return convert(maxValues[pageIndex]);}
 String parquet-mr_f2290_0(int pageIndex)
{    return stringifier.stringify(minValues[pageIndex]);}
 String parquet-mr_f2291_0(int pageIndex)
{    return stringifier.stringify(maxValues[pageIndex]);}
 Statistics<T> parquet-mr_f2292_0(int arrayIndex)
{    return (Statistics<T>) new Statistics<Float>(minValues[arrayIndex], maxValues[arrayIndex], comparator);}
 ValueComparator parquet-mr_f2293_0(Object value)
{    final float v = (float) value;    return new ValueComparator() {        @Override        int compareValueToMin(int arrayIndex) {            return comparator.compare(v, minValues[arrayIndex]);        }        @Override        int compareValueToMax(int arrayIndex) {            return comparator.compare(v, maxValues[arrayIndex]);        }    };}
 int parquet-mr_f2294_0(int arrayIndex)
{    return comparator.compare(v, minValues[arrayIndex]);}
 int parquet-mr_f2295_0(int arrayIndex)
{    return comparator.compare(v, maxValues[arrayIndex]);}
private static float parquet-mr_f2296_0(ByteBuffer buffer)
{    return buffer.order(LITTLE_ENDIAN).getFloat(0);}
private static ByteBuffer parquet-mr_f2297_0(float value)
{    return ByteBuffer.allocate(Float.BYTES).order(LITTLE_ENDIAN).putFloat(0, value);}
 void parquet-mr_f2298_0(ByteBuffer min, ByteBuffer max)
{    minValues.add(convert(min));    maxValues.add(convert(max));}
 void parquet-mr_f2299_0(Object min, Object max)
{    float fMin = (float) min;    float fMax = (float) max;    if (Float.isNaN(fMin) || Float.isNaN(fMax)) {                invalid = true;    }        if (Float.compare(fMin, +0.0f) == 0) {        fMin = -0.0f;    }    if (Float.compare(fMax, -0.0f) == 0) {        fMax = +0.0f;    }    minValues.add(fMin);    maxValues.add(fMax);}
 ColumnIndexBase<Float> parquet-mr_f2300_0(PrimitiveType type)
{    if (invalid) {        return null;    }    FloatColumnIndex columnIndex = new FloatColumnIndex(type);    columnIndex.minValues = minValues.toFloatArray();    columnIndex.maxValues = maxValues.toFloatArray();    return columnIndex;}
 void parquet-mr_f2301_0()
{    minValues.clear();    maxValues.clear();}
 int parquet-mr_f2302_0(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(minValues.get(index1), minValues.get(index2));}
 int parquet-mr_f2303_0(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(maxValues.get(index1), maxValues.get(index2));}
 int parquet-mr_f2304_0(Object value)
{    return Float.BYTES;}
public boolean parquet-mr_f2305_0()
{    return false;}
public int parquet-mr_f2306_0()
{    throw new NoSuchElementException();}
 static PrimitiveIterator.OfInt parquet-mr_f2307_0(int pageCount)
{    return new IndexIterator(0, pageCount, i -> true, i -> i);}
 static PrimitiveIterator.OfInt parquet-mr_f2308_0(ColumnIndexBase<?>.ValueComparator comparator)
{    return new IndexIterator(0, comparator.arrayLength(), i -> true, comparator::translate);}
 static PrimitiveIterator.OfInt parquet-mr_f2309_0(int pageCount, IntPredicate filter)
{    return new IndexIterator(0, pageCount, filter, i -> i);}
 static PrimitiveIterator.OfInt parquet-mr_f2310_0(int arrayLength, IntPredicate filter, IntUnaryOperator translator)
{    return new IndexIterator(0, arrayLength, filter, translator);}
 static PrimitiveIterator.OfInt parquet-mr_f2311_0(int from, int to, IntUnaryOperator translator)
{    return new IndexIterator(from, to + 1, i -> true, translator);}
private int parquet-mr_f2312_0(int startIndex)
{    for (int i = startIndex; i < endIndex; ++i) {        if (filter.test(i)) {            return i;        }    }    return -1;}
public boolean parquet-mr_f2313_0()
{    return index >= 0;}
public int parquet-mr_f2314_0()
{    if (hasNext()) {        int ret = index;        index = nextPageIndex(index + 1);        return translator.applyAsInt(ret);    }    throw new NoSuchElementException();}
 ByteBuffer parquet-mr_f2315_0(int pageIndex)
{    return convert(minValues[pageIndex]);}
 ByteBuffer parquet-mr_f2316_0(int pageIndex)
{    return convert(maxValues[pageIndex]);}
 String parquet-mr_f2317_0(int pageIndex)
{    return stringifier.stringify(minValues[pageIndex]);}
 String parquet-mr_f2318_0(int pageIndex)
{    return stringifier.stringify(maxValues[pageIndex]);}
 Statistics<T> parquet-mr_f2319_0(int arrayIndex)
{    return (Statistics<T>) new Statistics<Integer>(minValues[arrayIndex], maxValues[arrayIndex], comparator);}
 ValueComparator parquet-mr_f2320_0(Object value)
{    final int v = (int) value;    return new ValueComparator() {        @Override        int compareValueToMin(int arrayIndex) {            return comparator.compare(v, minValues[arrayIndex]);        }        @Override        int compareValueToMax(int arrayIndex) {            return comparator.compare(v, maxValues[arrayIndex]);        }    };}
 int parquet-mr_f2321_0(int arrayIndex)
{    return comparator.compare(v, minValues[arrayIndex]);}
 int parquet-mr_f2322_0(int arrayIndex)
{    return comparator.compare(v, maxValues[arrayIndex]);}
private static int parquet-mr_f2323_0(ByteBuffer buffer)
{    return buffer.order(LITTLE_ENDIAN).getInt(0);}
private static ByteBuffer parquet-mr_f2324_0(int value)
{    return ByteBuffer.allocate(Integer.BYTES).order(LITTLE_ENDIAN).putInt(0, value);}
 void parquet-mr_f2325_0(ByteBuffer min, ByteBuffer max)
{    minValues.add(convert(min));    maxValues.add(convert(max));}
 void parquet-mr_f2326_0(Object min, Object max)
{    minValues.add((int) min);    maxValues.add((int) max);}
 ColumnIndexBase<Integer> parquet-mr_f2327_0(PrimitiveType type)
{    IntColumnIndex columnIndex = new IntColumnIndex(type);    columnIndex.minValues = minValues.toIntArray();    columnIndex.maxValues = maxValues.toIntArray();    return columnIndex;}
 void parquet-mr_f2328_0()
{    minValues.clear();    maxValues.clear();}
 int parquet-mr_f2329_0(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(minValues.get(index1), minValues.get(index2));}
 int parquet-mr_f2330_0(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(maxValues.get(index1), maxValues.get(index2));}
 int parquet-mr_f2331_0(Object value)
{    return Integer.BYTES;}
 ByteBuffer parquet-mr_f2332_0(int pageIndex)
{    return convert(minValues[pageIndex]);}
 ByteBuffer parquet-mr_f2333_0(int pageIndex)
{    return convert(maxValues[pageIndex]);}
 String parquet-mr_f2334_0(int pageIndex)
{    return stringifier.stringify(minValues[pageIndex]);}
 String parquet-mr_f2335_0(int pageIndex)
{    return stringifier.stringify(maxValues[pageIndex]);}
 Statistics<T> parquet-mr_f2336_0(int arrayIndex)
{    return (Statistics<T>) new Statistics<Long>(minValues[arrayIndex], maxValues[arrayIndex], comparator);}
 ValueComparator parquet-mr_f2337_0(Object value)
{    final long v = (long) value;    return new ValueComparator() {        @Override        int compareValueToMin(int arrayIndex) {            return comparator.compare(v, minValues[arrayIndex]);        }        @Override        int compareValueToMax(int arrayIndex) {            return comparator.compare(v, maxValues[arrayIndex]);        }    };}
 int parquet-mr_f2338_0(int arrayIndex)
{    return comparator.compare(v, minValues[arrayIndex]);}
 int parquet-mr_f2339_0(int arrayIndex)
{    return comparator.compare(v, maxValues[arrayIndex]);}
private static long parquet-mr_f2340_0(ByteBuffer buffer)
{    return buffer.order(LITTLE_ENDIAN).getLong(0);}
private static ByteBuffer parquet-mr_f2341_0(long value)
{    return ByteBuffer.allocate(Long.BYTES).order(LITTLE_ENDIAN).putLong(0, value);}
 void parquet-mr_f2342_0(ByteBuffer min, ByteBuffer max)
{    minValues.add(convert(min));    maxValues.add(convert(max));}
 void parquet-mr_f2343_0(Object min, Object max)
{    minValues.add((long) min);    maxValues.add((long) max);}
 ColumnIndexBase<Long> parquet-mr_f2344_0(PrimitiveType type)
{    LongColumnIndex columnIndex = new LongColumnIndex(type);    columnIndex.minValues = minValues.toLongArray();    columnIndex.maxValues = maxValues.toLongArray();    return columnIndex;}
 void parquet-mr_f2345_0()
{    minValues.clear();    maxValues.clear();}
 int parquet-mr_f2346_0(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(minValues.get(index1), minValues.get(index2));}
 int parquet-mr_f2347_0(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(maxValues.get(index1), maxValues.get(index2));}
 int parquet-mr_f2348_0(Object value)
{    return Long.BYTES;}
public long parquet-mr_f2349_0(int pageIndex, long rowGroupRowCount)
{    int nextPageIndex = pageIndex + 1;    return (nextPageIndex >= getPageCount() ? rowGroupRowCount : getFirstRowIndex(nextPageIndex)) - 1;}
public String parquet-mr_f2350_0()
{    try (Formatter formatter = new Formatter()) {        formatter.format("%-10s  %20s  %16s  %20s\n", "", "offset", "compressed size", "first row index");        for (int i = 0, n = offsets.length; i < n; ++i) {            formatter.format("page-%-5d  %20d  %16d  %20d\n", i, offsets[i], compressedPageSizes[i], firstRowIndexes[i]);        }        return formatter.toString();    }}
public int parquet-mr_f2351_0()
{    return offsets.length;}
public long parquet-mr_f2352_0(int pageIndex)
{    return offsets[pageIndex];}
public int parquet-mr_f2353_0(int pageIndex)
{    return compressedPageSizes[pageIndex];}
public long parquet-mr_f2354_0(int pageIndex)
{    return firstRowIndexes[pageIndex];}
public void parquet-mr_f2355_0(int compressedPageSize, long rowCount)
{}
public void parquet-mr_f2356_0(long offset, int compressedPageSize, long rowCount)
{}
public static OffsetIndexBuilder parquet-mr_f2357_0()
{    return NO_OP_BUILDER;}
public static OffsetIndexBuilder parquet-mr_f2358_0()
{    return new OffsetIndexBuilder();}
public void parquet-mr_f2359_0(int compressedPageSize, long rowCount)
{    add(previousOffset + previousPageSize, compressedPageSize, previousRowIndex + previousRowCount);    previousRowCount = rowCount;}
public void parquet-mr_f2360_0(long offset, int compressedPageSize, long firstRowIndex)
{    previousOffset = offset;    offsets.add(offset);    previousPageSize = compressedPageSize;    compressedPageSizes.add(compressedPageSize);    previousRowIndex = firstRowIndex;    firstRowIndexes.add(firstRowIndex);}
public OffsetIndex parquet-mr_f2361_0()
{    return build(0);}
public OffsetIndex parquet-mr_f2362_0(long firstPageOffset)
{    if (compressedPageSizes.isEmpty()) {        return null;    }    long[] offsets = this.offsets.toLongArray();    if (firstPageOffset != 0) {        for (int i = 0, n = offsets.length; i < n; ++i) {            offsets[i] += firstPageOffset;        }    }    OffsetIndexImpl offsetIndex = new OffsetIndexImpl();    offsetIndex.offsets = offsets;    offsetIndex.compressedPageSizes = compressedPageSizes.toIntArray();    offsetIndex.firstRowIndexes = firstRowIndexes.toLongArray();    return offsetIndex;}
public static RowRanges parquet-mr_f2363_1(FilterCompat.Filter filter, ColumnIndexStore columnIndexStore, Set<ColumnPath> paths, long rowCount)
{    return filter.accept(new FilterCompat.Visitor<RowRanges>() {        @Override        public RowRanges visit(FilterPredicateCompat filterPredicateCompat) {            try {                return filterPredicateCompat.getFilterPredicate().accept(new ColumnIndexFilter(columnIndexStore, paths, rowCount));            } catch (MissingOffsetIndexException e) {                                return RowRanges.createSingle(rowCount);            }        }        @Override        public RowRanges visit(UnboundRecordFilterCompat unboundRecordFilterCompat) {            return RowRanges.createSingle(rowCount);        }        @Override        public RowRanges visit(NoOpFilter noOpFilter) {            return RowRanges.createSingle(rowCount);        }    });}
public RowRanges parquet-mr_f2364_1(FilterPredicateCompat filterPredicateCompat)
{    try {        return filterPredicateCompat.getFilterPredicate().accept(new ColumnIndexFilter(columnIndexStore, paths, rowCount));    } catch (MissingOffsetIndexException e) {                return RowRanges.createSingle(rowCount);    }}
public RowRanges parquet-mr_f2365_0(UnboundRecordFilterCompat unboundRecordFilterCompat)
{    return RowRanges.createSingle(rowCount);}
public RowRanges parquet-mr_f2366_0(NoOpFilter noOpFilter)
{    return RowRanges.createSingle(rowCount);}
private RowRanges parquet-mr_f2367_0()
{    if (allRows == null) {        allRows = RowRanges.createSingle(rowCount);    }    return allRows;}
public RowRanges parquet-mr_f2368_0(Eq<T> eq)
{    return applyPredicate(eq.getColumn(), ci -> ci.visit(eq), eq.getValue() == null ? allRows() : RowRanges.EMPTY);}
public RowRanges parquet-mr_f2369_0(NotEq<T> notEq)
{    return applyPredicate(notEq.getColumn(), ci -> ci.visit(notEq), notEq.getValue() == null ? RowRanges.EMPTY : allRows());}
public RowRanges parquet-mr_f2370_0(Lt<T> lt)
{    return applyPredicate(lt.getColumn(), ci -> ci.visit(lt), RowRanges.EMPTY);}
public RowRanges parquet-mr_f2371_0(LtEq<T> ltEq)
{    return applyPredicate(ltEq.getColumn(), ci -> ci.visit(ltEq), RowRanges.EMPTY);}
public RowRanges parquet-mr_f2372_0(Gt<T> gt)
{    return applyPredicate(gt.getColumn(), ci -> ci.visit(gt), RowRanges.EMPTY);}
public RowRanges parquet-mr_f2373_0(GtEq<T> gtEq)
{    return applyPredicate(gtEq.getColumn(), ci -> ci.visit(gtEq), RowRanges.EMPTY);}
public RowRanges parquet-mr_f2374_0(UserDefined<T, U> udp)
{    return applyPredicate(udp.getColumn(), ci -> ci.visit(udp), udp.getUserDefinedPredicate().acceptsNullValue() ? allRows() : RowRanges.EMPTY);}
public RowRanges parquet-mr_f2375_0(LogicalNotUserDefined<T, U> udp)
{    return applyPredicate(udp.getUserDefined().getColumn(), ci -> ci.visit(udp), udp.getUserDefined().getUserDefinedPredicate().acceptsNullValue() ? RowRanges.EMPTY : allRows());}
public RowRanges parquet-mr_f2377_0(And and)
{    return RowRanges.intersection(and.getLeft().accept(this), and.getRight().accept(this));}
public RowRanges parquet-mr_f2378_0(Or or)
{    return RowRanges.union(or.getLeft().accept(this), or.getRight().accept(this));}
public RowRanges parquet-mr_f2379_0(Not not)
{    throw new IllegalArgumentException("Predicates containing a NOT must be run through LogicalInverseRewriter. " + not);}
private static Range parquet-mr_f2380_0(Range left, Range right)
{    if (left.from <= right.from) {        if (left.to + 1 >= right.from) {            return new Range(left.from, Math.max(left.to, right.to));        }    } else if (right.to + 1 >= left.from) {        return new Range(right.from, Math.max(left.to, right.to));    }    return null;}
private static Range parquet-mr_f2381_0(Range left, Range right)
{    if (left.from <= right.from) {        if (left.to >= right.from) {            return new Range(right.from, Math.min(left.to, right.to));        }    } else if (right.to >= left.from) {        return new Range(left.from, Math.min(left.to, right.to));    }    return null;}
 long parquet-mr_f2382_0()
{    return to - from + 1;}
 boolean parquet-mr_f2383_0(Range other)
{    return to < other.from;}
 boolean parquet-mr_f2384_0(Range other)
{    return from > other.to;}
public String parquet-mr_f2385_0()
{    return "[" + from + ", " + to + ']';}
 static RowRanges parquet-mr_f2386_0(long rowCount)
{    RowRanges ranges = new RowRanges();    ranges.add(new Range(0, rowCount - 1));    return ranges;}
 static RowRanges parquet-mr_f2387_0(long rowCount, PrimitiveIterator.OfInt pageIndexes, OffsetIndex offsetIndex)
{    RowRanges ranges = new RowRanges();    while (pageIndexes.hasNext()) {        int pageIndex = pageIndexes.nextInt();        ranges.add(new Range(offsetIndex.getFirstRowIndex(pageIndex), offsetIndex.getLastRowIndex(pageIndex, rowCount)));    }    return ranges;}
 static RowRanges parquet-mr_f2388_0(RowRanges left, RowRanges right)
{    RowRanges result = new RowRanges();    Iterator<Range> it1 = left.ranges.iterator();    Iterator<Range> it2 = right.ranges.iterator();    if (it2.hasNext()) {        Range range2 = it2.next();        while (it1.hasNext()) {            Range range1 = it1.next();            if (range1.isAfter(range2)) {                result.add(range2);                range2 = range1;                Iterator<Range> tmp = it1;                it1 = it2;                it2 = tmp;            } else {                result.add(range1);            }        }        result.add(range2);    } else {        it2 = it1;    }    while (it2.hasNext()) {        result.add(it2.next());    }    return result;}
 static RowRanges parquet-mr_f2389_0(RowRanges left, RowRanges right)
{    RowRanges result = new RowRanges();    int rightIndex = 0;    for (Range l : left.ranges) {        for (int i = rightIndex, n = right.ranges.size(); i < n; ++i) {            Range r = right.ranges.get(i);            if (l.isBefore(r)) {                break;            } else if (l.isAfter(r)) {                rightIndex = i + 1;                continue;            }            result.add(Range.intersection(l, r));        }    }    return result;}
private void parquet-mr_f2390_0(Range range)
{    Range rangeToAdd = range;    for (int i = ranges.size() - 1; i >= 0; --i) {        Range last = ranges.get(i);        assert !last.isAfter(range);        Range u = Range.union(last, rangeToAdd);        if (u == null) {            break;        }        rangeToAdd = u;        ranges.remove(i);    }    ranges.add(rangeToAdd);}
public long parquet-mr_f2391_0()
{    long cnt = 0;    for (Range range : ranges) {        cnt += range.count();    }    return cnt;}
public PrimitiveIterator.OfLong parquet-mr_f2392_0()
{    return new PrimitiveIterator.OfLong() {        private int currentRangeIndex = -1;        private Range currentRange;        private long next = findNext();        private long findNext() {            if (currentRange == null || next + 1 > currentRange.to) {                if (currentRangeIndex + 1 < ranges.size()) {                    currentRange = ranges.get(++currentRangeIndex);                    next = currentRange.from;                } else {                    return -1;                }            } else {                ++next;            }            return next;        }        @Override        public boolean hasNext() {            return next >= 0;        }        @Override        public long nextLong() {            long ret = next;            if (ret < 0) {                throw new NoSuchElementException();            }            next = findNext();            return ret;        }    };}
private long parquet-mr_f2393_0()
{    if (currentRange == null || next + 1 > currentRange.to) {        if (currentRangeIndex + 1 < ranges.size()) {            currentRange = ranges.get(++currentRangeIndex);            next = currentRange.from;        } else {            return -1;        }    } else {        ++next;    }    return next;}
public boolean parquet-mr_f2394_0()
{    return next >= 0;}
public long parquet-mr_f2395_0()
{    long ret = next;    if (ret < 0) {        throw new NoSuchElementException();    }    next = findNext();    return ret;}
public boolean parquet-mr_f2396_0(long from, long to)
{    return Collections.binarySearch(ranges, new Range(from, to), (r1, r2) -> r1.isBefore(r2) ? -1 : r1.isAfter(r2) ? 1 : 0) >= 0;}
public String parquet-mr_f2397_0()
{    return ranges.toString();}
public boolean parquet-mr_f2398_0(Object obj)
{    if (obj == null) {        return false;    }    if (obj instanceof Binary) {        return equals((Binary) obj);    }    return false;}
public String parquet-mr_f2399_0()
{    return "Binary{" + length() + (isBackingBytesReused ? " reused" : " constant") + " bytes, " + Arrays.toString(getBytesUnsafe()) + "}";}
public Binary parquet-mr_f2400_0()
{    if (isBackingBytesReused) {        return Binary.fromConstantByteArray(getBytes());    } else {        return this;    }}
public boolean parquet-mr_f2401_0()
{    return isBackingBytesReused;}
public String parquet-mr_f2402_0()
{        return StandardCharsets.UTF_8.decode(ByteBuffer.wrap(value, offset, length)).toString();}
public int parquet-mr_f2403_0()
{    return length;}
public void parquet-mr_f2404_0(OutputStream out) throws IOException
{    out.write(value, offset, length);}
public byte[] parquet-mr_f2405_0()
{    return Arrays.copyOfRange(value, offset, offset + length);}
public byte[] parquet-mr_f2406_0()
{        return getBytes();}
public Binary parquet-mr_f2407_0(int start, int length)
{    if (isBackingBytesReused) {        return Binary.fromReusedByteArray(value, offset + start, length);    } else {        return Binary.fromConstantByteArray(value, offset + start, length);    }}
public int parquet-mr_f2408_0()
{    return Binary.hashCode(value, offset, length);}
 boolean parquet-mr_f2409_0(Binary other)
{    return other.equals(value, offset, length);}
 boolean parquet-mr_f2410_0(byte[] other, int otherOffset, int otherLength)
{    return Binary.equals(value, offset, length, other, otherOffset, otherLength);}
 boolean parquet-mr_f2411_0(ByteBuffer bytes, int otherOffset, int otherLength)
{    return Binary.equals(value, offset, length, bytes, otherOffset, otherLength);}
public int parquet-mr_f2412_0(Binary other)
{    return PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR.compare(this, other);}
public ByteBuffer parquet-mr_f2413_0()
{    return ByteBuffer.wrap(value, offset, length);}
public void parquet-mr_f2414_0(DataOutput out) throws IOException
{    out.write(value, offset, length);}
public String parquet-mr_f2415_0()
{    return "Binary{\"" + toStringUsingUTF8() + "\"}";}
private static ByteBuffer parquet-mr_f2416_0(String value)
{    return ByteBuffer.wrap(value.getBytes(StandardCharsets.UTF_8));}
public String parquet-mr_f2417_0()
{    return "Binary{\"" + toStringUsingUTF8() + "\"}";}
private static ByteBuffer parquet-mr_f2418_0(CharSequence value)
{    try {        return ENCODER.get().encode(CharBuffer.wrap(value));    } catch (CharacterCodingException e) {        throw new ParquetEncodingException("UTF-8 not supported.", e);    }}
public static Binary parquet-mr_f2419_0(final byte[] value, final int offset, final int length)
{    return new ByteArraySliceBackedBinary(value, offset, length, true);}
public static Binary parquet-mr_f2420_0(final byte[] value, final int offset, final int length)
{    return new ByteArraySliceBackedBinary(value, offset, length, false);}
public static Binary parquet-mr_f2421_0(final byte[] value, final int offset, final int length)
{        return fromReusedByteArray(value, offset, length);}
public String parquet-mr_f2422_0()
{    return StandardCharsets.UTF_8.decode(ByteBuffer.wrap(value)).toString();}
public int parquet-mr_f2423_0()
{    return value.length;}
public void parquet-mr_f2424_0(OutputStream out) throws IOException
{    out.write(value);}
public byte[] parquet-mr_f2425_0()
{    return Arrays.copyOfRange(value, 0, value.length);}
public byte[] parquet-mr_f2426_0()
{    return value;}
public Binary parquet-mr_f2427_0(int start, int length)
{    if (isBackingBytesReused) {        return Binary.fromReusedByteArray(value, start, length);    } else {        return Binary.fromConstantByteArray(value, start, length);    }}
public int parquet-mr_f2428_0()
{    return Binary.hashCode(value, 0, value.length);}
 boolean parquet-mr_f2429_0(Binary other)
{    return other.equals(value, 0, value.length);}
 boolean parquet-mr_f2430_0(byte[] other, int otherOffset, int otherLength)
{    return Binary.equals(value, 0, value.length, other, otherOffset, otherLength);}
 boolean parquet-mr_f2431_0(ByteBuffer bytes, int otherOffset, int otherLength)
{    return Binary.equals(value, 0, value.length, bytes, otherOffset, otherLength);}
public int parquet-mr_f2432_0(Binary other)
{    return PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR.compare(this, other);}
public ByteBuffer parquet-mr_f2433_0()
{    return ByteBuffer.wrap(value);}
public void parquet-mr_f2434_0(DataOutput out) throws IOException
{    out.write(value);}
public static Binary parquet-mr_f2435_0(final byte[] value)
{    return new ByteArrayBackedBinary(value, true);}
public static Binary parquet-mr_f2436_0(final byte[] value)
{    return new ByteArrayBackedBinary(value, false);}
public static Binary parquet-mr_f2437_0(final byte[] value)
{        return fromReusedByteArray(value);}
public String parquet-mr_f2438_0()
{    String ret;    if (value.hasArray()) {        ret = new String(value.array(), value.arrayOffset() + offset, length, StandardCharsets.UTF_8);    } else {        int limit = value.limit();        value.limit(offset + length);        int position = value.position();        value.position(offset);                                ret = StandardCharsets.UTF_8.decode(value).toString();        value.limit(limit);        value.position(position);    }    return ret;}
public int parquet-mr_f2439_0()
{    return length;}
public void parquet-mr_f2440_0(OutputStream out) throws IOException
{    if (value.hasArray()) {        out.write(value.array(), value.arrayOffset() + offset, length);    } else {        out.write(getBytesUnsafe(), 0, length);    }}
public byte[] parquet-mr_f2441_0()
{    byte[] bytes = new byte[length];    int limit = value.limit();    value.limit(offset + length);    int position = value.position();    value.position(offset);    value.get(bytes);    value.limit(limit);    value.position(position);    if (!isBackingBytesReused) {                cachedBytes = bytes;    }    return bytes;}
public byte[] parquet-mr_f2442_0()
{    return cachedBytes != null ? cachedBytes : getBytes();}
public Binary parquet-mr_f2443_0(int start, int length)
{    return Binary.fromConstantByteArray(getBytesUnsafe(), start, length);}
public int parquet-mr_f2444_0()
{    if (value.hasArray()) {        return Binary.hashCode(value.array(), value.arrayOffset() + offset, length);    } else {        return Binary.hashCode(value, offset, length);    }}
 boolean parquet-mr_f2445_0(Binary other)
{    if (value.hasArray()) {        return other.equals(value.array(), value.arrayOffset() + offset, length);    } else {        return other.equals(value, offset, length);    }}
 boolean parquet-mr_f2446_0(byte[] other, int otherOffset, int otherLength)
{    if (value.hasArray()) {        return Binary.equals(value.array(), value.arrayOffset() + offset, length, other, otherOffset, otherLength);    } else {        return Binary.equals(other, otherOffset, otherLength, value, offset, length);    }}
 boolean parquet-mr_f2447_0(ByteBuffer otherBytes, int otherOffset, int otherLength)
{    return Binary.equals(value, 0, length, otherBytes, otherOffset, otherLength);}
public int parquet-mr_f2448_0(Binary other)
{    return PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR.compare(this, other);}
public ByteBuffer parquet-mr_f2449_0()
{    ByteBuffer ret = value.duplicate();    ret.position(offset);    ret.limit(offset + length);    return ret;}
public void parquet-mr_f2450_0(DataOutput out) throws IOException
{        out.write(getBytesUnsafe());}
private void parquet-mr_f2451_0(java.io.ObjectOutputStream out) throws IOException
{    byte[] bytes = getBytesUnsafe();    out.writeInt(bytes.length);    out.write(bytes);}
private void parquet-mr_f2452_0(java.io.ObjectInputStream in) throws IOException, ClassNotFoundException
{    int length = in.readInt();    byte[] bytes = new byte[length];    in.readFully(bytes, 0, length);    this.value = ByteBuffer.wrap(bytes);    this.offset = 0;    this.length = length;}
private void parquet-mr_f2453_0() throws ObjectStreamException
{    this.value = ByteBuffer.wrap(new byte[0]);}
public static Binary parquet-mr_f2454_0(final ByteBuffer value, int offset, int length)
{    return new ByteBufferBackedBinary(value, offset, length, true);}
public static Binary parquet-mr_f2455_0(final ByteBuffer value, int offset, int length)
{    return new ByteBufferBackedBinary(value, offset, length, false);}
public static Binary parquet-mr_f2456_0(final ByteBuffer value)
{    return new ByteBufferBackedBinary(value, true);}
public static Binary parquet-mr_f2457_0(final ByteBuffer value)
{    return new ByteBufferBackedBinary(value, false);}
public static Binary parquet-mr_f2458_0(final ByteBuffer value)
{        return fromReusedByteBuffer(value);}
public static Binary parquet-mr_f2459_0(String value)
{    return new FromStringBinary(value);}
public static Binary parquet-mr_f2460_0(CharSequence value)
{    return new FromCharSequenceBinary(value);}
private static final int parquet-mr_f2461_0(byte[] array, int offset, int length)
{    int result = 1;    for (int i = offset; i < offset + length; i++) {        byte b = array[i];        result = 31 * result + b;    }    return result;}
private static final int parquet-mr_f2462_0(ByteBuffer buf, int offset, int length)
{    int result = 1;    for (int i = offset; i < offset + length; i++) {        byte b = buf.get(i);        result = 31 * result + b;    }    return result;}
private static final boolean parquet-mr_f2463_0(ByteBuffer buf1, int offset1, int length1, ByteBuffer buf2, int offset2, int length2)
{    if (buf1 == null && buf2 == null)        return true;    if (buf1 == null || buf2 == null)        return false;    if (length1 != length2)        return false;    for (int i = 0; i < length1; i++) {        if (buf1.get(i + offset1) != buf2.get(i + offset2)) {            return false;        }    }    return true;}
private static final boolean parquet-mr_f2464_0(byte[] array1, int offset1, int length1, ByteBuffer buf, int offset2, int length2)
{    if (array1 == null && buf == null)        return true;    if (array1 == null || buf == null)        return false;    if (length1 != length2)        return false;    for (int i = 0; i < length1; i++) {        if (array1[i + offset1] != buf.get(i + offset2)) {            return false;        }    }    return true;}
private static final boolean parquet-mr_f2465_0(byte[] array1, int offset1, int length1, byte[] array2, int offset2, int length2)
{    if (array1 == null && array2 == null)        return true;    if (array1 == null || array2 == null)        return false;    if (length1 != length2)        return false;    if (array1 == array2 && offset1 == offset2)        return true;    for (int i = 0; i < length1; i++) {        if (array1[i + offset1] != array2[i + offset2]) {            return false;        }    }    return true;}
public PrimitiveConverter parquet-mr_f2466_0()
{    throw new ClassCastException("Expected instance of primitive converter but got \"" + getClass().getName() + "\"");}
public GroupConverter parquet-mr_f2467_0()
{    throw new ClassCastException("Expected instance of group converter but got \"" + getClass().getName() + "\"");}
public boolean parquet-mr_f2468_0()
{    return false;}
public GroupConverter parquet-mr_f2469_0()
{    return this;}
public boolean parquet-mr_f2470_0()
{    return true;}
public PrimitiveConverter parquet-mr_f2471_0()
{    return this;}
public boolean parquet-mr_f2472_0()
{    return false;}
public void parquet-mr_f2473_0(Dictionary dictionary)
{    throw new UnsupportedOperationException(getClass().getName());}
public void parquet-mr_f2474_0(int dictionaryId)
{    throw new UnsupportedOperationException(getClass().getName());}
public void parquet-mr_f2475_0(Binary value)
{    throw new UnsupportedOperationException(getClass().getName());}
public void parquet-mr_f2476_0(boolean value)
{    throw new UnsupportedOperationException(getClass().getName());}
public void parquet-mr_f2477_0(double value)
{    throw new UnsupportedOperationException(getClass().getName());}
public void parquet-mr_f2478_0(float value)
{    throw new UnsupportedOperationException(getClass().getName());}
public void parquet-mr_f2479_0(int value)
{    throw new UnsupportedOperationException(getClass().getName());}
public void parquet-mr_f2480_0(long value)
{    throw new UnsupportedOperationException(getClass().getName());}
public void parquet-mr_f2481_0()
{}
public void parquet-mr_f2482_0()
{}
public T parquet-mr_f2483_0()
{    readOneRecord();    return recordMaterializer.getCurrentRecord();}
protected void parquet-mr_f2484_1(int currentLevel)
{    }
protected void parquet-mr_f2485_1(String message)
{    }
protected final int parquet-mr_f2486_0(int state, int currentLevel, int d, int nextR)
{    return caseLookup[state].getCase(currentLevel, d, nextR).getID();}
protected final void parquet-mr_f2487_1()
{        endField = null;        recordConsumer.startMessage();}
protected final void parquet-mr_f2488_1(String field, int index)
{    startField(field, index);        recordConsumer.startGroup();}
private void parquet-mr_f2489_1(String field, int index)
{        if (endField != null && index == endIndex) {                endField = null;    } else {        if (endField != null) {                        recordConsumer.endField(endField, endIndex);            endField = null;        }        recordConsumer.startField(field, index);    }}
protected final void parquet-mr_f2490_1(String field, int index, long value)
{    startField(field, index);        recordConsumer.addLong(value);    endField(field, index);}
private void parquet-mr_f2491_1(String field, int index)
{        if (endField != null) {        recordConsumer.endField(endField, endIndex);    }    endField = field;    endIndex = index;}
protected final void parquet-mr_f2492_1(String field, int index, Binary value)
{    startField(field, index);        recordConsumer.addBinary(value);    endField(field, index);}
protected final void parquet-mr_f2493_1(String field, int index, int value)
{    startField(field, index);        recordConsumer.addInteger(value);    endField(field, index);}
protected final void parquet-mr_f2494_1(String field, int index)
{    if (endField != null) {                recordConsumer.endField(endField, endIndex);        endField = null;    }        recordConsumer.endGroup();    endField(field, index);}
protected final void parquet-mr_f2495_1()
{    if (endField != null) {                recordConsumer.endField(endField, endIndex);        endField = null;    }        recordConsumer.endMessage();}
protected void parquet-mr_f2496_0(String message)
{    throw new ParquetDecodingException(message);}
 String[] parquet-mr_f2497_0()
{    return fieldPath;}
public String parquet-mr_f2498_0(int level)
{    return fieldPath[level];}
public int[] parquet-mr_f2499_0()
{    return indexFieldPath;}
public int parquet-mr_f2500_0(int level)
{    return indexFieldPath[level];}
public int parquet-mr_f2501_0()
{    return this.index;}
public String parquet-mr_f2502_0()
{    return name;}
 int parquet-mr_f2503_0()
{    return repetitionLevel;}
 int parquet-mr_f2504_0()
{    return definitionLevel;}
 void parquet-mr_f2505_0(int repetitionLevel)
{    this.repetitionLevel = repetitionLevel;}
 void parquet-mr_f2506_0(int definitionLevel)
{    this.definitionLevel = definitionLevel;}
 void parquet-mr_f2507_0(String[] fieldPath, int[] indexFieldPath)
{    this.fieldPath = fieldPath;    this.indexFieldPath = indexFieldPath;}
public Type parquet-mr_f2508_0()
{    return type;}
 void parquet-mr_f2509_0(int r, int d, String[] fieldPath, int[] indexFieldPath, List<ColumnIO> repetition, List<ColumnIO> path)
{    setRepetitionLevel(r);    setDefinitionLevel(d);    setFieldPath(fieldPath, indexFieldPath);}
public GroupColumnIO parquet-mr_f2510_0()
{    return parent;}
 ColumnIO parquet-mr_f2511_0(int r)
{    if (getRepetitionLevel() == r && getType().isRepetition(Repetition.REPEATED)) {        return this;    } else if (getParent() != null && getParent().getDefinitionLevel() >= r) {        return getParent().getParent(r);    } else {        throw new InvalidRecordException("no parent(" + r + ") for " + Arrays.toString(this.getFieldPath()));    }}
public String parquet-mr_f2512_0()
{    return this.getClass().getSimpleName() + " " + type.getName() + " r:" + repetitionLevel + " d:" + definitionLevel + " " + Arrays.toString(fieldPath);}
public void parquet-mr_f2513_0(MessageType messageType)
{    columnIO = new MessageColumnIO(requestedSchema, validating, createdBy);    visitChildren(columnIO, messageType, requestedSchema);    columnIO.setLevels();    columnIO.setLeaves(leaves);}
public void parquet-mr_f2514_0(GroupType groupType)
{    if (currentRequestedType.isPrimitive()) {        incompatibleSchema(groupType, currentRequestedType);    }    GroupColumnIO newIO = new GroupColumnIO(groupType, current, currentRequestedIndex);    current.add(newIO);    visitChildren(newIO, groupType, currentRequestedType.asGroupType());}
private void parquet-mr_f2515_0(GroupColumnIO newIO, GroupType groupType, GroupType requestedGroupType)
{    GroupColumnIO oldIO = current;    current = newIO;    for (Type type : groupType.getFields()) {                if (requestedGroupType.containsField(type.getName())) {            currentRequestedIndex = requestedGroupType.getFieldIndex(type.getName());            currentRequestedType = requestedGroupType.getType(currentRequestedIndex);            if (currentRequestedType.getRepetition().isMoreRestrictiveThan(type.getRepetition())) {                incompatibleSchema(type, currentRequestedType);            }            type.accept(this);        }    }    current = oldIO;}
public void parquet-mr_f2516_0(PrimitiveType primitiveType)
{    if (!currentRequestedType.isPrimitive() || (this.strictTypeChecking && currentRequestedType.asPrimitiveType().getPrimitiveTypeName() != primitiveType.getPrimitiveTypeName())) {        incompatibleSchema(primitiveType, currentRequestedType);    }    PrimitiveColumnIO newIO = new PrimitiveColumnIO(primitiveType, current, currentRequestedIndex, leaves.size());    current.add(newIO);    leaves.add(newIO);}
private void parquet-mr_f2517_0(Type fileType, Type requestedType)
{    throw new ParquetDecodingException("The requested schema is not compatible with the file schema. incompatible types: " + requestedType + " != " + fileType);}
public MessageColumnIO parquet-mr_f2518_0()
{    return columnIO;}
public MessageColumnIO parquet-mr_f2519_0(MessageType requestedSchema, MessageType fileSchema)
{    return getColumnIO(requestedSchema, fileSchema, true);}
public MessageColumnIO parquet-mr_f2520_0(MessageType requestedSchema, MessageType fileSchema, boolean strict)
{    ColumnIOCreatorVisitor visitor = new ColumnIOCreatorVisitor(validating, requestedSchema, createdBy, strict);    fileSchema.accept(visitor);    return visitor.getColumnIO();}
public MessageColumnIO parquet-mr_f2521_0(MessageType schema)
{    return this.getColumnIO(schema, schema);}
public T parquet-mr_f2522_0()
{    recordConsumer.start();    recordConsumer.end();    return recordMaterializer.getCurrentRecord();}
public T parquet-mr_f2523_0()
{    skipToMatch();    if (recordsRead == recordCount) {        return null;    }    ++recordsRead;    return super.read();}
public boolean parquet-mr_f2524_0()
{    return false;}
private void parquet-mr_f2525_0()
{    while (recordsRead < recordCount && !recordFilter.isMatch()) {        State currentState = getState(0);        do {            ColumnReader columnReader = currentState.column;                        if (columnReader.getCurrentDefinitionLevel() >= currentState.maxDefinitionLevel) {                columnReader.skip();            }            columnReader.consume();                        int nextR = currentState.maxRepetitionLevel == 0 ? 0 : columnReader.getCurrentRepetitionLevel();            currentState = currentState.getNextState(nextR);        } while (currentState != null);        ++recordsRead;    }}
 void parquet-mr_f2526_0(ColumnIO child)
{    children.add(child);    childrenByName.put(child.getType().getName(), child);    ++childrenSize;}
 void parquet-mr_f2527_0(int r, int d, String[] fieldPath, int[] indexFieldPath, List<ColumnIO> repetition, List<ColumnIO> path)
{    super.setLevels(r, d, fieldPath, indexFieldPath, repetition, path);    for (ColumnIO child : this.children) {        String[] newFieldPath = Arrays.copyOf(fieldPath, fieldPath.length + 1);        int[] newIndexFieldPath = Arrays.copyOf(indexFieldPath, indexFieldPath.length + 1);        newFieldPath[fieldPath.length] = child.getType().getName();        newIndexFieldPath[indexFieldPath.length] = child.getIndex();        List<ColumnIO> newRepetition;        if (child.getType().isRepetition(REPEATED)) {            newRepetition = new ArrayList<ColumnIO>(repetition);            newRepetition.add(child);        } else {            newRepetition = repetition;        }        List<ColumnIO> newPath = new ArrayList<ColumnIO>(path);        newPath.add(child);        child.setLevels(        child.getType().isRepetition(REPEATED) ? r + 1 : r,         !child.getType().isRepetition(REQUIRED) ? d + 1 : d, newFieldPath, newIndexFieldPath, newRepetition, newPath);    }}
 List<String[]> parquet-mr_f2528_0()
{    ArrayList<String[]> result = new ArrayList<String[]>();    for (ColumnIO c : children) {        result.addAll(c.getColumnNames());    }    return result;}
 PrimitiveColumnIO parquet-mr_f2529_0()
{    return children.get(children.size() - 1).getLast();}
 PrimitiveColumnIO parquet-mr_f2530_0()
{    return children.get(0).getFirst();}
public ColumnIO parquet-mr_f2531_0(String name)
{    return childrenByName.get(name);}
public ColumnIO parquet-mr_f2532_0(int fieldIndex)
{    try {        return children.get(fieldIndex);    } catch (IndexOutOfBoundsException e) {        throw new InvalidRecordException("could not get child " + fieldIndex + " from " + children, e);    }}
public int parquet-mr_f2533_0()
{    return childrenSize;}
public List<String[]> parquet-mr_f2534_0()
{    return super.getColumnNames();}
public RecordReader<T> parquet-mr_f2535_0(PageReadStore columns, RecordMaterializer<T> recordMaterializer)
{    return getRecordReader(columns, recordMaterializer, FilterCompat.NOOP);}
public RecordReader<T> parquet-mr_f2536_0(PageReadStore columns, RecordMaterializer<T> recordMaterializer, UnboundRecordFilter filter)
{    return getRecordReader(columns, recordMaterializer, FilterCompat.get(filter));}
public RecordReader<T> parquet-mr_f2537_0(final PageReadStore columns, final RecordMaterializer<T> recordMaterializer, final Filter filter)
{    checkNotNull(columns, "columns");    checkNotNull(recordMaterializer, "recordMaterializer");    checkNotNull(filter, "filter");    if (leaves.isEmpty()) {        return new EmptyRecordReader<T>(recordMaterializer);    }    return filter.accept(new Visitor<RecordReader<T>>() {        @Override        public RecordReader<T> visit(FilterPredicateCompat filterPredicateCompat) {            FilterPredicate predicate = filterPredicateCompat.getFilterPredicate();            IncrementallyUpdatedFilterPredicateBuilder builder = new IncrementallyUpdatedFilterPredicateBuilder(leaves);            IncrementallyUpdatedFilterPredicate streamingPredicate = builder.build(predicate);            RecordMaterializer<T> filteringRecordMaterializer = new FilteringRecordMaterializer<T>(recordMaterializer, leaves, builder.getValueInspectorsByColumn(), streamingPredicate);            return new RecordReaderImplementation<T>(MessageColumnIO.this, filteringRecordMaterializer, validating, new ColumnReadStoreImpl(columns, filteringRecordMaterializer.getRootConverter(), getType(), createdBy));        }        @Override        public RecordReader<T> visit(UnboundRecordFilterCompat unboundRecordFilterCompat) {            return new FilteredRecordReader<T>(MessageColumnIO.this, recordMaterializer, validating, new ColumnReadStoreImpl(columns, recordMaterializer.getRootConverter(), getType(), createdBy), unboundRecordFilterCompat.getUnboundRecordFilter(), columns.getRowCount());        }        @Override        public RecordReader<T> visit(NoOpFilter noOpFilter) {            return new RecordReaderImplementation<T>(MessageColumnIO.this, recordMaterializer, validating, new ColumnReadStoreImpl(columns, recordMaterializer.getRootConverter(), getType(), createdBy));        }    });}
public RecordReader<T> parquet-mr_f2538_0(FilterPredicateCompat filterPredicateCompat)
{    FilterPredicate predicate = filterPredicateCompat.getFilterPredicate();    IncrementallyUpdatedFilterPredicateBuilder builder = new IncrementallyUpdatedFilterPredicateBuilder(leaves);    IncrementallyUpdatedFilterPredicate streamingPredicate = builder.build(predicate);    RecordMaterializer<T> filteringRecordMaterializer = new FilteringRecordMaterializer<T>(recordMaterializer, leaves, builder.getValueInspectorsByColumn(), streamingPredicate);    return new RecordReaderImplementation<T>(MessageColumnIO.this, filteringRecordMaterializer, validating, new ColumnReadStoreImpl(columns, filteringRecordMaterializer.getRootConverter(), getType(), createdBy));}
public RecordReader<T> parquet-mr_f2539_0(UnboundRecordFilterCompat unboundRecordFilterCompat)
{    return new FilteredRecordReader<T>(MessageColumnIO.this, recordMaterializer, validating, new ColumnReadStoreImpl(columns, recordMaterializer.getRootConverter(), getType(), createdBy), unboundRecordFilterCompat.getUnboundRecordFilter(), columns.getRowCount());}
public RecordReader<T> parquet-mr_f2540_0(NoOpFilter noOpFilter)
{    return new RecordReaderImplementation<T>(MessageColumnIO.this, recordMaterializer, validating, new ColumnReadStoreImpl(columns, recordMaterializer.getRootConverter(), getType(), createdBy));}
public String parquet-mr_f2541_0()
{    return "VistedIndex{" + "vistedIndexes=" + vistedIndexes + '}';}
public void parquet-mr_f2542_0(int fieldsCount)
{    this.vistedIndexes.clear(0, fieldsCount);}
public void parquet-mr_f2543_0(int i)
{    vistedIndexes.set(i);}
public boolean parquet-mr_f2544_0(int i)
{    return vistedIndexes.get(i);}
private void parquet-mr_f2545_0(PrimitiveColumnIO primitive, ColumnWriter writer)
{    GroupColumnIO parent = primitive.getParent();    do {        getLeafWriters(parent).add(writer);        parent = parent.getParent();    } while (parent != null);}
private List<ColumnWriter> parquet-mr_f2546_0(GroupColumnIO group)
{    List<ColumnWriter> writers = groupToLeafWriter.get(group);    if (writers == null) {        writers = new ArrayList<ColumnWriter>();        groupToLeafWriter.put(group, writers);    }    return writers;}
private void parquet-mr_f2547_0()
{    if (DEBUG) {        log(currentLevel + ", " + fieldsWritten[currentLevel] + ": " + Arrays.toString(currentColumnIO.getFieldPath()) + " r:" + r[currentLevel]);        if (r[currentLevel] > currentColumnIO.getRepetitionLevel()) {                        throw new InvalidRecordException(r[currentLevel] + "(r) > " + currentColumnIO.getRepetitionLevel() + " ( schema r)");        }    }}
private void parquet-mr_f2548_1(Object message, Object... parameters)
{    if (DEBUG) {        String indent = "";        for (int i = 0; i < currentLevel; ++i) {            indent += "  ";        }            }}
public void parquet-mr_f2549_0()
{    if (DEBUG)        log("< MESSAGE START >");    currentColumnIO = MessageColumnIO.this;    r[0] = 0;    int numberOfFieldsToVisit = ((GroupColumnIO) currentColumnIO).getChildrenCount();    fieldsWritten[0].reset(numberOfFieldsToVisit);    if (DEBUG)        printState();}
public void parquet-mr_f2550_0()
{    writeNullForMissingFieldsAtCurrentLevel();        if (columns.isColumnFlushNeeded()) {        flush();    }    columns.endRecord();    if (DEBUG)        log("< MESSAGE END >");    if (DEBUG)        printState();}
public void parquet-mr_f2551_0(String field, int index)
{    try {        if (DEBUG)            log("startField({}, {})", field, index);        currentColumnIO = ((GroupColumnIO) currentColumnIO).getChild(index);        emptyField = true;        if (DEBUG)            printState();    } catch (RuntimeException e) {        throw new ParquetEncodingException("error starting field " + field + " at " + index, e);    }}
public void parquet-mr_f2552_0(String field, int index)
{    if (DEBUG)        log("endField({}, {})", field, index);    currentColumnIO = currentColumnIO.getParent();    if (emptyField) {        throw new ParquetEncodingException("empty fields are illegal, the field should be ommited completely instead");    }    fieldsWritten[currentLevel].markWritten(index);    r[currentLevel] = currentLevel == 0 ? 0 : r[currentLevel - 1];    if (DEBUG)        printState();}
private void parquet-mr_f2553_0()
{    int currentFieldsCount = ((GroupColumnIO) currentColumnIO).getChildrenCount();    for (int i = 0; i < currentFieldsCount; i++) {        if (!fieldsWritten[currentLevel].isWritten(i)) {            try {                ColumnIO undefinedField = ((GroupColumnIO) currentColumnIO).getChild(i);                int d = currentColumnIO.getDefinitionLevel();                if (DEBUG)                    log(Arrays.toString(undefinedField.getFieldPath()) + ".writeNull(" + r[currentLevel] + "," + d + ")");                writeNull(undefinedField, r[currentLevel], d);            } catch (RuntimeException e) {                throw new ParquetEncodingException("error while writing nulls for fields of indexes " + i + " . current index: " + fieldsWritten[currentLevel], e);            }        }    }}
private void parquet-mr_f2554_0(ColumnIO undefinedField, int r, int d)
{    if (undefinedField.getType().isPrimitive()) {        columnWriter[((PrimitiveColumnIO) undefinedField).getId()].writeNull(r, d);    } else {        GroupColumnIO groupColumnIO = (GroupColumnIO) undefinedField;                cacheNullForGroup(groupColumnIO, r);    }}
private void parquet-mr_f2555_0(GroupColumnIO group, int r)
{    IntArrayList nulls = groupNullCache.get(group);    if (nulls == null) {        nulls = new IntArrayList();        groupNullCache.put(group, nulls);    }    nulls.add(r);}
private void parquet-mr_f2556_0(GroupColumnIO group)
{    IntArrayList nullCache = groupNullCache.get(group);    if (nullCache == null || nullCache.isEmpty())        return;    int parentDefinitionLevel = group.getParent().getDefinitionLevel();    for (ColumnWriter leafWriter : groupToLeafWriter.get(group)) {        for (IntIterator iter = nullCache.iterator(); iter.hasNext(); ) {            int repetitionLevel = iter.nextInt();            leafWriter.writeNull(repetitionLevel, parentDefinitionLevel);        }    }    nullCache.clear();}
private void parquet-mr_f2557_0()
{    r[currentLevel] = currentColumnIO.getRepetitionLevel();    if (DEBUG)        log("r: {}", r[currentLevel]);}
public void parquet-mr_f2558_0()
{    if (DEBUG)        log("startGroup()");    GroupColumnIO group = (GroupColumnIO) currentColumnIO;        if (hasNullCache(group)) {        flushCachedNulls(group);    }    ++currentLevel;    r[currentLevel] = r[currentLevel - 1];    int fieldsCount = ((GroupColumnIO) currentColumnIO).getChildrenCount();    fieldsWritten[currentLevel].reset(fieldsCount);    if (DEBUG)        printState();}
private boolean parquet-mr_f2559_0(GroupColumnIO group)
{    IntArrayList nulls = groupNullCache.get(group);    return nulls != null && !nulls.isEmpty();}
private void parquet-mr_f2560_0(GroupColumnIO group)
{        for (int i = 0; i < group.getChildrenCount(); i++) {        ColumnIO child = group.getChild(i);        if (child instanceof GroupColumnIO) {            flushCachedNulls((GroupColumnIO) child);        }    }        writeNullToLeaves(group);}
public void parquet-mr_f2561_0()
{    if (DEBUG)        log("endGroup()");    emptyField = false;    writeNullForMissingFieldsAtCurrentLevel();    --currentLevel;    setRepetitionLevel();    if (DEBUG)        printState();}
private ColumnWriter parquet-mr_f2562_0()
{    return columnWriter[((PrimitiveColumnIO) currentColumnIO).getId()];}
public void parquet-mr_f2563_0(int value)
{    if (DEBUG)        log("addInt({})", value);    emptyField = false;    getColumnWriter().write(value, r[currentLevel], currentColumnIO.getDefinitionLevel());    setRepetitionLevel();    if (DEBUG)        printState();}
public void parquet-mr_f2564_0(long value)
{    if (DEBUG)        log("addLong({})", value);    emptyField = false;    getColumnWriter().write(value, r[currentLevel], currentColumnIO.getDefinitionLevel());    setRepetitionLevel();    if (DEBUG)        printState();}
public void parquet-mr_f2565_0(boolean value)
{    if (DEBUG)        log("addBoolean({})", value);    emptyField = false;    getColumnWriter().write(value, r[currentLevel], currentColumnIO.getDefinitionLevel());    setRepetitionLevel();    if (DEBUG)        printState();}
public void parquet-mr_f2566_0(Binary value)
{    if (DEBUG)        log("addBinary({} bytes)", value.length());    emptyField = false;    getColumnWriter().write(value, r[currentLevel], currentColumnIO.getDefinitionLevel());    setRepetitionLevel();    if (DEBUG)        printState();}
public void parquet-mr_f2567_0(float value)
{    if (DEBUG)        log("addFloat({})", value);    emptyField = false;    getColumnWriter().write(value, r[currentLevel], currentColumnIO.getDefinitionLevel());    setRepetitionLevel();    if (DEBUG)        printState();}
public void parquet-mr_f2568_0(double value)
{    if (DEBUG)        log("addDouble({})", value);    emptyField = false;    getColumnWriter().write(value, r[currentLevel], currentColumnIO.getDefinitionLevel());    setRepetitionLevel();    if (DEBUG)        printState();}
public void parquet-mr_f2569_0()
{    flushCachedNulls(MessageColumnIO.this);}
public RecordConsumer parquet-mr_f2570_0(ColumnWriteStore columns)
{    RecordConsumer recordWriter = new MessageColumnIORecordConsumer(columns);    if (DEBUG)        recordWriter = new RecordConsumerLoggingWrapper(recordWriter);    return validating ? new ValidatingRecordConsumer(recordWriter, getType()) : recordWriter;}
 void parquet-mr_f2571_0()
{    setLevels(0, 0, new String[0], new int[0], Arrays.<ColumnIO>asList(this), Arrays.<ColumnIO>asList(this));}
 void parquet-mr_f2572_0(List<PrimitiveColumnIO> leaves)
{    this.leaves = leaves;}
public List<PrimitiveColumnIO> parquet-mr_f2573_0()
{    return this.leaves;}
public MessageType parquet-mr_f2574_0()
{    return (MessageType) super.getType();}
 void parquet-mr_f2575_0(int r, int d, String[] fieldPath, int[] fieldIndexPath, List<ColumnIO> repetition, List<ColumnIO> path)
{    super.setLevels(r, d, fieldPath, fieldIndexPath, repetition, path);    PrimitiveType type = getType().asPrimitiveType();    this.columnDescriptor = new ColumnDescriptor(fieldPath, type, getRepetitionLevel(), getDefinitionLevel());    this.path = path.toArray(new ColumnIO[path.size()]);}
 List<String[]> parquet-mr_f2576_0()
{    return Arrays.asList(new String[][] { getFieldPath() });}
public ColumnDescriptor parquet-mr_f2577_0()
{    return columnDescriptor;}
public ColumnIO[] parquet-mr_f2578_0()
{    return path;}
public boolean parquet-mr_f2579_0(int r)
{    return getLast(r) == this;}
private PrimitiveColumnIO parquet-mr_f2580_0(int r)
{    ColumnIO parent = getParent(r);    PrimitiveColumnIO last = parent.getLast();    return last;}
 PrimitiveColumnIO parquet-mr_f2581_0()
{    return this;}
 PrimitiveColumnIO parquet-mr_f2582_0()
{    return this;}
public boolean parquet-mr_f2583_0(int r)
{    return getFirst(r) == this;}
private PrimitiveColumnIO parquet-mr_f2584_0(int r)
{    ColumnIO parent = getParent(r);    return parent.getFirst();}
public PrimitiveTypeName parquet-mr_f2585_0()
{    return getType().asPrimitiveType().getPrimitiveTypeName();}
public int parquet-mr_f2586_0()
{    return id;}
public void parquet-mr_f2587_0(String field, int index)
{    logOpen(field);    delegate.startField(field, index);}
private void parquet-mr_f2588_0(String field)
{    log("<{}>", field);}
private String parquet-mr_f2589_0()
{    StringBuilder result = new StringBuilder();    for (int i = 0; i < indent; i++) {        result.append("  ");    }    return result.toString();}
private void parquet-mr_f2590_1(Object value, Object... parameters)
{    if (LOG.isDebugEnabled()) {            }}
public void parquet-mr_f2591_0()
{    ++indent;    log("<!-- start group -->");    delegate.startGroup();}
public void parquet-mr_f2592_0(int value)
{    log(value);    delegate.addInteger(value);}
public void parquet-mr_f2593_0(long value)
{    log(value);    delegate.addLong(value);}
public void parquet-mr_f2594_0(boolean value)
{    log(value);    delegate.addBoolean(value);}
public void parquet-mr_f2595_0(Binary value)
{    if (LOG.isDebugEnabled())        log(Arrays.toString(value.getBytesUnsafe()));    delegate.addBinary(value);}
public void parquet-mr_f2596_0(float value)
{    log(value);    delegate.addFloat(value);}
public void parquet-mr_f2597_0(double value)
{    log(value);    delegate.addDouble(value);}
public void parquet-mr_f2598_0()
{    log("<!-- flush -->");    delegate.flush();}
public void parquet-mr_f2599_0()
{    log("<!-- end group -->");    --indent;    delegate.endGroup();}
public void parquet-mr_f2600_0(String field, int index)
{    logClose(field);    delegate.endField(field, index);}
private void parquet-mr_f2601_0(String field)
{    log("</{}>", field);}
public void parquet-mr_f2602_0()
{    log("<!-- start message -->");    delegate.startMessage();}
public void parquet-mr_f2603_0()
{    delegate.endMessage();    log("<!-- end message -->");}
public boolean parquet-mr_f2604_0()
{    return false;}
public void parquet-mr_f2605_0(int id)
{    this.id = id;}
public int parquet-mr_f2606_0()
{    int hashCode = 17;    hashCode += 31 * startLevel;    hashCode += 31 * depth;    hashCode += 31 * nextLevel;    hashCode += 31 * nextState;    hashCode += 31 * (defined ? 0 : 1);    return hashCode;}
public boolean parquet-mr_f2607_0(Object obj)
{    if (obj instanceof Case) {        return equals((Case) obj);    }    return false;}
public boolean parquet-mr_f2608_0(Case other)
{    return other != null && startLevel == other.startLevel && depth == other.depth && nextLevel == other.nextLevel && nextState == other.nextState && ((defined && other.defined) || (!defined && !other.defined));}
public int parquet-mr_f2609_0()
{    return id;}
public int parquet-mr_f2610_0()
{    return startLevel;}
public int parquet-mr_f2611_0()
{    return depth;}
public int parquet-mr_f2612_0()
{    return nextLevel;}
public int parquet-mr_f2613_0()
{    return nextState;}
public boolean parquet-mr_f2614_0()
{    return goingUp;}
public boolean parquet-mr_f2615_0()
{    return goingDown;}
public boolean parquet-mr_f2616_0()
{    return defined;}
public String parquet-mr_f2617_0()
{    return "Case " + startLevel + " -> " + depth + " -> " + nextLevel + "; goto sate_" + getNextState();}
public int parquet-mr_f2618_0(int definitionLevel)
{    return definitionLevelToDepth[definitionLevel];}
public List<Case> parquet-mr_f2619_0()
{    return definedCases;}
public List<Case> parquet-mr_f2620_0()
{    return undefinedCases;}
public Case parquet-mr_f2621_0(int currentLevel, int d, int nextR)
{    return caseLookup[currentLevel][d][nextR];}
public State parquet-mr_f2622_0(int nextR)
{    return nextState[nextR];}
public int parquet-mr_f2623_0(Case o1, Case o2)
{    return o1.id - o2.id;}
private RecordConsumer parquet-mr_f2624_0(RecordConsumer recordConsumer, boolean validating, MessageType schema)
{    return validating ? new ValidatingRecordConsumer(recordConsumer, schema) : recordConsumer;}
private RecordConsumer parquet-mr_f2625_0(RecordConsumer recordConsumer)
{    if (LOG.isDebugEnabled()) {        return new RecordConsumerLoggingWrapper(recordConsumer);    }    return recordConsumer;}
public T parquet-mr_f2626_0()
{    int currentLevel = 0;    recordRootConverter.start();    State currentState = states[0];    do {        ColumnReader columnReader = currentState.column;        int d = columnReader.getCurrentDefinitionLevel();                int depth = currentState.definitionLevelToDepth[d];        for (; currentLevel <= depth; ++currentLevel) {            currentState.groupConverterPath[currentLevel].start();        }                if (d >= currentState.maxDefinitionLevel) {                        columnReader.writeCurrentValueToConverter();        }        columnReader.consume();        int nextR = currentState.maxRepetitionLevel == 0 ? 0 : columnReader.getCurrentRepetitionLevel();                int next = currentState.nextLevel[nextR];        for (; currentLevel > next; currentLevel--) {            currentState.groupConverterPath[currentLevel - 1].end();        }        currentState = currentState.nextState[nextR];    } while (currentState != null);    recordRootConverter.end();    T record = recordMaterializer.getCurrentRecord();    shouldSkipCurrentRecord = record == null;    if (shouldSkipCurrentRecord) {        recordMaterializer.skipCurrentRecord();    }    return record;}
public boolean parquet-mr_f2627_0()
{    return shouldSkipCurrentRecord;}
private static void parquet-mr_f2628_1(String string)
{    }
 int parquet-mr_f2629_0(int current, int nextRepetitionLevel)
{    State nextState = states[current].nextState[nextRepetitionLevel];    return nextState == null ? states.length : nextState.id;}
 int parquet-mr_f2630_0(int current, int nextRepetitionLevel)
{    return states[current].nextLevel[nextRepetitionLevel];}
private int parquet-mr_f2631_0(String[] previous, String[] next)
{    int i = 0;    while (i < Math.min(previous.length, next.length) && previous[i].equals(next[i])) {        ++i;    }    return i;}
protected int parquet-mr_f2632_0()
{    return states.length;}
protected State parquet-mr_f2633_0(int i)
{    return states[i];}
protected RecordMaterializer<T> parquet-mr_f2634_0()
{    return recordMaterializer;}
protected Converter parquet-mr_f2635_0()
{    return recordRootConverter;}
protected Iterable<ColumnReader> parquet-mr_f2636_0()
{        return Arrays.asList(columnReaders);}
public void parquet-mr_f2637_0()
{    previousField.push(-1);    delegate.startMessage();}
public void parquet-mr_f2638_0()
{    delegate.endMessage();    validateMissingFields(types.peek().asGroupType().getFieldCount());    previousField.pop();}
public void parquet-mr_f2639_0(String field, int index)
{    if (index <= previousField.peek()) {        throw new InvalidRecordException("fields must be added in order " + field + " index " + index + " is before previous field " + previousField.peek());    }    validateMissingFields(index);    fields.push(index);    fieldValueCount.push(0);    delegate.startField(field, index);}
private void parquet-mr_f2640_0(int index)
{    for (int i = previousField.peek() + 1; i < index; i++) {        Type type = types.peek().asGroupType().getType(i);        if (type.isRepetition(Repetition.REQUIRED)) {            throw new InvalidRecordException("required field is missing " + type);        }    }}
public void parquet-mr_f2641_0(String field, int index)
{    delegate.endField(field, index);    fieldValueCount.pop();    previousField.push(fields.pop());}
public void parquet-mr_f2642_0()
{    previousField.push(-1);    types.push(types.peek().asGroupType().getType(fields.peek()));    delegate.startGroup();}
public void parquet-mr_f2643_0()
{    delegate.endGroup();    validateMissingFields(types.peek().asGroupType().getFieldCount());    types.pop();    previousField.pop();}
public void parquet-mr_f2644_0()
{    delegate.flush();}
private void parquet-mr_f2645_1(PrimitiveTypeName p)
{    Type currentType = types.peek().asGroupType().getType(fields.peek());    int c = fieldValueCount.pop() + 1;    fieldValueCount.push(c);        switch(currentType.getRepetition()) {        case OPTIONAL:        case REQUIRED:            if (c > 1) {                throw new InvalidRecordException("repeated value when the type is not repeated in " + currentType);            }            break;        case REPEATED:            break;        default:            throw new InvalidRecordException("unknown repetition " + currentType.getRepetition() + " in " + currentType);    }    if (!currentType.isPrimitive() || currentType.asPrimitiveType().getPrimitiveTypeName() != p) {        throw new InvalidRecordException("expected type " + p + " but got " + currentType);    }}
private void parquet-mr_f2646_1(PrimitiveTypeName... ptypes)
{    Type currentType = types.peek().asGroupType().getType(fields.peek());    int c = fieldValueCount.pop() + 1;    fieldValueCount.push(c);    if (LOG.isDebugEnabled())            switch(currentType.getRepetition()) {        case OPTIONAL:        case REQUIRED:            if (c > 1) {                throw new InvalidRecordException("repeated value when the type is not repeated in " + currentType);            }            break;        case REPEATED:            break;        default:            throw new InvalidRecordException("unknown repetition " + currentType.getRepetition() + " in " + currentType);    }    if (!currentType.isPrimitive()) {        throw new InvalidRecordException("expected type in " + Arrays.toString(ptypes) + " but got " + currentType);    }    for (PrimitiveTypeName p : ptypes) {        if (currentType.asPrimitiveType().getPrimitiveTypeName() == p) {                        return;        }    }    throw new InvalidRecordException("expected type in " + Arrays.toString(ptypes) + " but got " + currentType);}
public void parquet-mr_f2647_0(int value)
{    validate(INT32);    delegate.addInteger(value);}
public void parquet-mr_f2648_0(long value)
{    validate(INT64);    delegate.addLong(value);}
public void parquet-mr_f2649_0(boolean value)
{    validate(BOOLEAN);    delegate.addBoolean(value);}
public void parquet-mr_f2650_0(Binary value)
{    validate(BINARY, INT96, FIXED_LEN_BYTE_ARRAY);    delegate.addBinary(value);}
public void parquet-mr_f2651_0(float value)
{    validate(FLOAT);    delegate.addFloat(value);}
public void parquet-mr_f2652_0(double value)
{    validate(DOUBLE);    delegate.addDouble(value);}
public static ColumnOrder parquet-mr_f2653_0()
{    return UNDEFINED_COLUMN_ORDER;}
public static ColumnOrder parquet-mr_f2654_0()
{    return TYPE_DEFINED_COLUMN_ORDER;}
public ColumnOrderName parquet-mr_f2655_0()
{    return columnOrderName;}
public boolean parquet-mr_f2656_0(Object obj)
{    if (obj instanceof ColumnOrder) {        return columnOrderName == ((ColumnOrder) obj).columnOrderName;    }    return false;}
public int parquet-mr_f2657_0()
{    return columnOrderName.hashCode();}
public String parquet-mr_f2658_0()
{    return columnOrderName.toString();}
private static GroupType parquet-mr_f2659_0(Repetition repetition, String alias, LogicalTypeAnnotation logicalTypeAnnotation, Type nested)
{    if (!nested.isRepetition(Repetition.REPEATED)) {        throw new IllegalArgumentException("Nested type should be repeated: " + nested);    }    return new GroupType(repetition, alias, logicalTypeAnnotation, nested);}
public static GroupType parquet-mr_f2660_0(Repetition repetition, String alias, Type keyType, Type valueType)
{    return mapType(repetition, alias, "map", keyType, valueType);}
public static GroupType parquet-mr_f2661_0(Repetition repetition, String alias, String mapAlias, Type valueType)
{    return mapType(repetition, alias, mapAlias, new PrimitiveType(Repetition.REQUIRED, PrimitiveTypeName.BINARY, "key", stringType()), valueType);}
public static GroupType parquet-mr_f2662_0(Repetition repetition, String alias, Type valueType)
{    return stringKeyMapType(repetition, alias, "map", valueType);}
public static GroupType parquet-mr_f2663_0(Repetition repetition, String alias, String mapAlias, Type keyType, Type valueType)
{        if (valueType == null) {        return listWrapper(repetition, alias, LogicalTypeAnnotation.mapType(), new GroupType(Repetition.REPEATED, mapAlias, LogicalTypeAnnotation.MapKeyValueTypeAnnotation.getInstance(), keyType));    } else {        if (!valueType.getName().equals("value")) {            throw new RuntimeException(valueType.getName() + " should be value");        }        return listWrapper(repetition, alias, LogicalTypeAnnotation.mapType(), new GroupType(Repetition.REPEATED, mapAlias, LogicalTypeAnnotation.MapKeyValueTypeAnnotation.getInstance(), keyType, valueType));    }}
public static GroupType parquet-mr_f2664_0(Repetition repetition, String alias, Type nestedType)
{    return listWrapper(repetition, alias, LogicalTypeAnnotation.listType(), nestedType);}
public static GroupType parquet-mr_f2665_0(Repetition listRepetition, String name, Type elementType)
{    Preconditions.checkArgument(elementType.getName().equals(ELEMENT_NAME), "List element type must be named 'element'");    return listWrapper(listRepetition, name, LogicalTypeAnnotation.listType(), new GroupType(Repetition.REPEATED, "list", elementType));}
public int parquet-mr_f2666_0()
{    return precision;}
public int parquet-mr_f2667_0()
{    return scale;}
public boolean parquet-mr_f2668_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    DecimalMetadata that = (DecimalMetadata) o;    if (precision != that.precision)        return false;    if (scale != that.scale)        return false;    return true;}
public int parquet-mr_f2669_0()
{    int result = precision;    result = 31 * result + scale;    return result;}
public GroupType parquet-mr_f2670_0(int id)
{    return new GroupType(getRepetition(), getName(), getLogicalTypeAnnotation(), fields, new ID(id));}
public GroupType parquet-mr_f2671_0(List<Type> newFields)
{    return new GroupType(getRepetition(), getName(), getLogicalTypeAnnotation(), newFields, getId());}
public GroupType parquet-mr_f2672_0(Type... newFields)
{    return withNewFields(asList(newFields));}
public String parquet-mr_f2673_0(int index)
{    return fields.get(index).getName();}
public boolean parquet-mr_f2674_0(String name)
{    return indexByName.containsKey(name);}
public int parquet-mr_f2675_0(String name)
{    if (!indexByName.containsKey(name)) {        throw new InvalidRecordException(name + " not found in " + this);    }    return indexByName.get(name);}
public List<Type> parquet-mr_f2676_0()
{    return fields;}
public int parquet-mr_f2677_0()
{    return fields.size();}
public boolean parquet-mr_f2678_0()
{    return false;}
public Type parquet-mr_f2679_0(String fieldName)
{    return getType(getFieldIndex(fieldName));}
public Type parquet-mr_f2680_0(int index)
{    return fields.get(index);}
 void parquet-mr_f2681_0(StringBuilder sb, String indent)
{    for (Type field : fields) {        field.writeToStringBuilder(sb, indent);        if (field.isPrimitive()) {            sb.append(";");        }        sb.append("\n");    }}
public void parquet-mr_f2682_0(StringBuilder sb, String indent)
{    sb.append(indent).append(getRepetition().name().toLowerCase(Locale.ENGLISH)).append(" group ").append(getName()).append(getLogicalTypeAnnotation() == null ? "" : " (" + getLogicalTypeAnnotation().toString() + ")").append(getId() == null ? "" : " = " + getId()).append(" {\n");    membersDisplayString(sb, indent + "  ");    sb.append(indent).append("}");}
public void parquet-mr_f2683_0(TypeVisitor visitor)
{    visitor.visit(this);}
protected int parquet-mr_f2684_0()
{    return hashCode();}
protected boolean parquet-mr_f2685_0(Type other)
{    return equals(other);}
public int parquet-mr_f2686_0()
{    return Objects.hash(getLogicalTypeAnnotation(), getFields());}
protected boolean parquet-mr_f2687_0(Type otherType)
{    return !otherType.isPrimitive() && super.equals(otherType) && Objects.equals(getLogicalTypeAnnotation(), otherType.getLogicalTypeAnnotation()) && getFields().equals(otherType.asGroupType().getFields());}
protected int parquet-mr_f2688_0(String[] path, int depth)
{    int myVal = isRepetition(Repetition.REPEATED) ? 1 : 0;    if (depth == path.length) {        return myVal;    }    return myVal + getType(path[depth]).getMaxRepetitionLevel(path, depth + 1);}
protected int parquet-mr_f2689_0(String[] path, int depth)
{    int myVal = !isRepetition(Repetition.REQUIRED) ? 1 : 0;    if (depth == path.length) {        return myVal;    }    return myVal + getType(path[depth]).getMaxDefinitionLevel(path, depth + 1);}
protected Type parquet-mr_f2690_0(String[] path, int depth)
{    if (depth == path.length) {        return this;    }    return getType(path[depth]).getType(path, depth + 1);}
protected boolean parquet-mr_f2691_0(String[] path, int depth)
{    if (depth == path.length) {        return false;    }    return containsField(path[depth]) && getType(path[depth]).containsPath(path, depth + 1);}
protected List<String[]> parquet-mr_f2692_0(int depth)
{    List<String[]> result = new ArrayList<String[]>();    for (Type field : fields) {        List<String[]> paths = field.getPaths(depth + 1);        for (String[] path : paths) {            path[depth] = field.getName();            result.add(path);        }    }    return result;}
 void parquet-mr_f2693_0(Type subType)
{    super.checkContains(subType);    checkGroupContains(subType);}
 void parquet-mr_f2694_0(Type subType)
{    if (subType.isPrimitive()) {        throw new InvalidRecordException(subType + " found: expected " + this);    }    List<Type> fields = subType.asGroupType().getFields();    for (Type otherType : fields) {        Type thisType = this.getType(otherType.getName());        thisType.checkContains(otherType);    }}
 T parquet-mr_f2695_0(List<GroupType> path, TypeConverter<T> converter)
{    List<GroupType> childrenPath = new ArrayList<GroupType>(path);    childrenPath.add(this);    final List<T> children = convertChildren(childrenPath, converter);    return converter.convertGroupType(path, this, children);}
protected List<T> parquet-mr_f2696_0(List<GroupType> path, TypeConverter<T> converter)
{    List<T> children = new ArrayList<T>(fields.size());    for (Type field : fields) {        children.add(field.convert(path, converter));    }    return children;}
protected Type parquet-mr_f2697_0(Type toMerge)
{    return union(toMerge, true);}
protected Type parquet-mr_f2698_0(Type toMerge, boolean strict)
{    if (toMerge.isPrimitive()) {        throw new IncompatibleSchemaModificationException("can not merge primitive type " + toMerge + " into group type " + this);    }    return new GroupType(toMerge.getRepetition(), getName(), toMerge.getLogicalTypeAnnotation(), mergeFields(toMerge.asGroupType()), getId());}
 List<Type> parquet-mr_f2699_0(GroupType toMerge)
{    return mergeFields(toMerge, true);}
 List<Type> parquet-mr_f2700_0(GroupType toMerge, boolean strict)
{    List<Type> newFields = new ArrayList<Type>();        for (Type type : this.getFields()) {        Type merged;        if (toMerge.containsField(type.getName())) {            Type fieldToMerge = toMerge.getType(type.getName());            if (type.getLogicalTypeAnnotation() != null && !type.getLogicalTypeAnnotation().equals(fieldToMerge.getLogicalTypeAnnotation())) {                throw new IncompatibleSchemaModificationException("cannot merge logical type " + fieldToMerge.getLogicalTypeAnnotation() + " into " + type.getLogicalTypeAnnotation());            }            merged = type.union(fieldToMerge, strict);        } else {            merged = type;        }        newFields.add(merged);    }        for (Type type : toMerge.getFields()) {        if (!this.containsField(type.getName())) {            newFields.add(type);        }    }    return newFields;}
protected LogicalTypeAnnotation parquet-mr_f2701_0(List<String> params)
{    return mapType();}
protected LogicalTypeAnnotation parquet-mr_f2702_0(List<String> params)
{    return listType();}
protected LogicalTypeAnnotation parquet-mr_f2703_0(List<String> params)
{    return stringType();}
protected LogicalTypeAnnotation parquet-mr_f2704_0(List<String> params)
{    return MapKeyValueTypeAnnotation.getInstance();}
protected LogicalTypeAnnotation parquet-mr_f2705_0(List<String> params)
{    return enumType();}
protected LogicalTypeAnnotation parquet-mr_f2706_0(List<String> params)
{    if (params.size() != 2) {        throw new RuntimeException("Expecting 2 parameters for decimal logical type, got " + params.size());    }    return decimalType(Integer.valueOf(params.get(1)), Integer.valueOf(params.get(0)));}
protected LogicalTypeAnnotation parquet-mr_f2707_0(List<String> params)
{    return dateType();}
protected LogicalTypeAnnotation parquet-mr_f2708_0(List<String> params)
{    if (params.size() != 2) {        throw new RuntimeException("Expecting 2 parameters for time logical type, got " + params.size());    }    return timeType(Boolean.parseBoolean(params.get(1)), TimeUnit.valueOf(params.get(0)));}
protected LogicalTypeAnnotation parquet-mr_f2709_0(List<String> params)
{    if (params.size() != 2) {        throw new RuntimeException("Expecting 2 parameters for timestamp logical type, got " + params.size());    }    return timestampType(Boolean.parseBoolean(params.get(1)), TimeUnit.valueOf(params.get(0)));}
protected LogicalTypeAnnotation parquet-mr_f2710_0(List<String> params)
{    if (params.size() != 2) {        throw new RuntimeException("Expecting 2 parameters for integer logical type, got " + params.size());    }    return intType(Integer.valueOf(params.get(0)), Boolean.parseBoolean(params.get(1)));}
protected LogicalTypeAnnotation parquet-mr_f2711_0(List<String> params)
{    return jsonType();}
protected LogicalTypeAnnotation parquet-mr_f2712_0(List<String> params)
{    return bsonType();}
protected LogicalTypeAnnotation parquet-mr_f2713_0(List<String> params)
{    return IntervalLogicalTypeAnnotation.getInstance();}
 String parquet-mr_f2714_0()
{    return "";}
 boolean parquet-mr_f2715_0(ColumnOrder columnOrder)
{    return columnOrder.getColumnOrderName() == UNDEFINED || columnOrder.getColumnOrderName() == TYPE_DEFINED_ORDER;}
public String parquet-mr_f2716_0()
{    StringBuilder sb = new StringBuilder();    sb.append(getType());    sb.append(typeParametersAsString());    return sb.toString();}
 PrimitiveStringifier parquet-mr_f2717_0(PrimitiveType primitiveType)
{    throw new UnsupportedOperationException("Stringifier is not supported for the logical type: " + this);}
public static LogicalTypeAnnotation parquet-mr_f2718_0(OriginalType originalType, DecimalMetadata decimalMetadata)
{    if (originalType == null) {        return null;    }    switch(originalType) {        case UTF8:            return stringType();        case MAP:            return mapType();        case DECIMAL:            int scale = (decimalMetadata == null ? 0 : decimalMetadata.getScale());            int precision = (decimalMetadata == null ? 0 : decimalMetadata.getPrecision());            return decimalType(scale, precision);        case LIST:            return listType();        case DATE:            return dateType();        case INTERVAL:            return IntervalLogicalTypeAnnotation.getInstance();        case TIMESTAMP_MILLIS:            return timestampType(true, LogicalTypeAnnotation.TimeUnit.MILLIS);        case TIMESTAMP_MICROS:            return timestampType(true, LogicalTypeAnnotation.TimeUnit.MICROS);        case TIME_MILLIS:            return timeType(true, LogicalTypeAnnotation.TimeUnit.MILLIS);        case TIME_MICROS:            return timeType(true, LogicalTypeAnnotation.TimeUnit.MICROS);        case UINT_8:            return intType(8, false);        case UINT_16:            return intType(16, false);        case UINT_32:            return intType(32, false);        case UINT_64:            return intType(64, false);        case INT_8:            return intType(8, true);        case INT_16:            return intType(16, true);        case INT_32:            return intType(32, true);        case INT_64:            return intType(64, true);        case ENUM:            return enumType();        case JSON:            return jsonType();        case BSON:            return bsonType();        case MAP_KEY_VALUE:            return MapKeyValueTypeAnnotation.getInstance();        default:            throw new RuntimeException("Can't convert original type to logical type, unknown original type " + originalType);    }}
public static StringLogicalTypeAnnotation parquet-mr_f2719_0()
{    return StringLogicalTypeAnnotation.INSTANCE;}
public static MapLogicalTypeAnnotation parquet-mr_f2720_0()
{    return MapLogicalTypeAnnotation.INSTANCE;}
public static ListLogicalTypeAnnotation parquet-mr_f2721_0()
{    return ListLogicalTypeAnnotation.INSTANCE;}
public static EnumLogicalTypeAnnotation parquet-mr_f2722_0()
{    return EnumLogicalTypeAnnotation.INSTANCE;}
public static DecimalLogicalTypeAnnotation parquet-mr_f2723_0(final int scale, final int precision)
{    return new DecimalLogicalTypeAnnotation(scale, precision);}
public static DateLogicalTypeAnnotation parquet-mr_f2724_0()
{    return DateLogicalTypeAnnotation.INSTANCE;}
public static TimeLogicalTypeAnnotation parquet-mr_f2725_0(final boolean isAdjustedToUTC, final TimeUnit unit)
{    return new TimeLogicalTypeAnnotation(isAdjustedToUTC, unit);}
public static TimestampLogicalTypeAnnotation parquet-mr_f2726_0(final boolean isAdjustedToUTC, final TimeUnit unit)
{    return new TimestampLogicalTypeAnnotation(isAdjustedToUTC, unit);}
public static IntLogicalTypeAnnotation parquet-mr_f2727_0(final int bitWidth, final boolean isSigned)
{    Preconditions.checkArgument(bitWidth == 8 || bitWidth == 16 || bitWidth == 32 || bitWidth == 64, "Invalid bit width for integer logical type, " + bitWidth + " is not allowed, " + "valid bit width values: 8, 16, 32, 64");    return new IntLogicalTypeAnnotation(bitWidth, isSigned);}
public static JsonLogicalTypeAnnotation parquet-mr_f2728_0()
{    return JsonLogicalTypeAnnotation.INSTANCE;}
public static BsonLogicalTypeAnnotation parquet-mr_f2729_0()
{    return BsonLogicalTypeAnnotation.INSTANCE;}
public OriginalType parquet-mr_f2730_0()
{    return OriginalType.UTF8;}
public Optional<T> parquet-mr_f2731_0(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
 LogicalTypeToken parquet-mr_f2732_0()
{    return LogicalTypeToken.STRING;}
public boolean parquet-mr_f2733_0(Object obj)
{    return obj instanceof StringLogicalTypeAnnotation;}
public int parquet-mr_f2734_0()
{        return getClass().hashCode();}
 PrimitiveStringifier parquet-mr_f2735_0(PrimitiveType primitiveType)
{    return PrimitiveStringifier.UTF8_STRINGIFIER;}
public OriginalType parquet-mr_f2736_0()
{    return OriginalType.MAP;}
public Optional<T> parquet-mr_f2737_0(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
 LogicalTypeToken parquet-mr_f2738_0()
{    return LogicalTypeToken.MAP;}
public boolean parquet-mr_f2739_0(Object obj)
{    return obj instanceof MapLogicalTypeAnnotation;}
public int parquet-mr_f2740_0()
{        return getClass().hashCode();}
public OriginalType parquet-mr_f2741_0()
{    return OriginalType.LIST;}
public Optional<T> parquet-mr_f2742_0(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
 LogicalTypeToken parquet-mr_f2743_0()
{    return LogicalTypeToken.LIST;}
public boolean parquet-mr_f2744_0(Object obj)
{    return obj instanceof ListLogicalTypeAnnotation;}
public int parquet-mr_f2745_0()
{        return getClass().hashCode();}
public OriginalType parquet-mr_f2746_0()
{    return OriginalType.ENUM;}
public Optional<T> parquet-mr_f2747_0(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
 LogicalTypeToken parquet-mr_f2748_0()
{    return LogicalTypeToken.ENUM;}
public boolean parquet-mr_f2749_0(Object obj)
{    return obj instanceof EnumLogicalTypeAnnotation;}
public int parquet-mr_f2750_0()
{        return getClass().hashCode();}
 PrimitiveStringifier parquet-mr_f2751_0(PrimitiveType primitiveType)
{    return PrimitiveStringifier.UTF8_STRINGIFIER;}
public int parquet-mr_f2752_0()
{    return precision;}
public int parquet-mr_f2753_0()
{    return scale;}
public OriginalType parquet-mr_f2754_0()
{    return OriginalType.DECIMAL;}
public Optional<T> parquet-mr_f2755_0(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
 LogicalTypeToken parquet-mr_f2756_0()
{    return LogicalTypeToken.DECIMAL;}
protected String parquet-mr_f2757_0()
{    StringBuilder sb = new StringBuilder();    sb.append("(");    sb.append(precision);    sb.append(",");    sb.append(scale);    sb.append(")");    return sb.toString();}
public boolean parquet-mr_f2758_0(Object obj)
{    if (!(obj instanceof DecimalLogicalTypeAnnotation)) {        return false;    }    DecimalLogicalTypeAnnotation other = (DecimalLogicalTypeAnnotation) obj;    return scale == other.scale && precision == other.precision;}
public int parquet-mr_f2759_0()
{    return Objects.hash(scale, precision);}
 PrimitiveStringifier parquet-mr_f2760_0(PrimitiveType primitiveType)
{    return stringifier;}
public OriginalType parquet-mr_f2761_0()
{    return OriginalType.DATE;}
public Optional<T> parquet-mr_f2762_0(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
 LogicalTypeToken parquet-mr_f2763_0()
{    return LogicalTypeToken.DATE;}
public boolean parquet-mr_f2764_0(Object obj)
{    return obj instanceof DateLogicalTypeAnnotation;}
public int parquet-mr_f2765_0()
{        return getClass().hashCode();}
 PrimitiveStringifier parquet-mr_f2766_0(PrimitiveType primitiveType)
{    return PrimitiveStringifier.DATE_STRINGIFIER;}
public OriginalType parquet-mr_f2767_0()
{    if (!isAdjustedToUTC) {        return null;    }    switch(unit) {        case MILLIS:            return OriginalType.TIME_MILLIS;        case MICROS:            return OriginalType.TIME_MICROS;        default:            return null;    }}
public Optional<T> parquet-mr_f2768_0(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
 LogicalTypeToken parquet-mr_f2769_0()
{    return LogicalTypeToken.TIME;}
protected String parquet-mr_f2770_0()
{    StringBuilder sb = new StringBuilder();    sb.append("(");    sb.append(unit);    sb.append(",");    sb.append(isAdjustedToUTC);    sb.append(")");    return sb.toString();}
public TimeUnit parquet-mr_f2771_0()
{    return unit;}
public boolean parquet-mr_f2772_0()
{    return isAdjustedToUTC;}
public boolean parquet-mr_f2773_0(Object obj)
{    if (!(obj instanceof TimeLogicalTypeAnnotation)) {        return false;    }    TimeLogicalTypeAnnotation other = (TimeLogicalTypeAnnotation) obj;    return isAdjustedToUTC == other.isAdjustedToUTC && unit == other.unit;}
public int parquet-mr_f2774_0()
{    return Objects.hash(isAdjustedToUTC, unit);}
 PrimitiveStringifier parquet-mr_f2775_0(PrimitiveType primitiveType)
{    switch(unit) {        case MICROS:        case MILLIS:            return isAdjustedToUTC ? TIME_UTC_STRINGIFIER : TIME_STRINGIFIER;        case NANOS:            return isAdjustedToUTC ? TIME_NANOS_UTC_STRINGIFIER : TIME_NANOS_STRINGIFIER;        default:            return super.valueStringifier(primitiveType);    }}
public OriginalType parquet-mr_f2776_0()
{    if (!isAdjustedToUTC) {        return null;    }    switch(unit) {        case MILLIS:            return OriginalType.TIMESTAMP_MILLIS;        case MICROS:            return OriginalType.TIMESTAMP_MICROS;        default:            return null;    }}
public Optional<T> parquet-mr_f2777_0(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
 LogicalTypeToken parquet-mr_f2778_0()
{    return LogicalTypeToken.TIMESTAMP;}
protected String parquet-mr_f2779_0()
{    StringBuilder sb = new StringBuilder();    sb.append("(");    sb.append(unit);    sb.append(",");    sb.append(isAdjustedToUTC);    sb.append(")");    return sb.toString();}
public TimeUnit parquet-mr_f2780_0()
{    return unit;}
public boolean parquet-mr_f2781_0()
{    return isAdjustedToUTC;}
public boolean parquet-mr_f2782_0(Object obj)
{    if (!(obj instanceof TimestampLogicalTypeAnnotation)) {        return false;    }    TimestampLogicalTypeAnnotation other = (TimestampLogicalTypeAnnotation) obj;    return (isAdjustedToUTC == other.isAdjustedToUTC) && (unit == other.unit);}
public int parquet-mr_f2783_0()
{    return Objects.hash(isAdjustedToUTC, unit);}
 PrimitiveStringifier parquet-mr_f2784_0(PrimitiveType primitiveType)
{    switch(unit) {        case MICROS:            return isAdjustedToUTC ? TIMESTAMP_MICROS_UTC_STRINGIFIER : TIMESTAMP_MICROS_STRINGIFIER;        case MILLIS:            return isAdjustedToUTC ? TIMESTAMP_MILLIS_UTC_STRINGIFIER : TIMESTAMP_MILLIS_STRINGIFIER;        case NANOS:            return isAdjustedToUTC ? TIMESTAMP_NANOS_UTC_STRINGIFIER : TIMESTAMP_NANOS_STRINGIFIER;        default:            return super.valueStringifier(primitiveType);    }}
public OriginalType parquet-mr_f2785_0()
{    switch(bitWidth) {        case 8:            return isSigned ? OriginalType.INT_8 : OriginalType.UINT_8;        case 16:            return isSigned ? OriginalType.INT_16 : OriginalType.UINT_16;        case 32:            return isSigned ? OriginalType.INT_32 : OriginalType.UINT_32;        case 64:            return isSigned ? OriginalType.INT_64 : OriginalType.UINT_64;        default:            return null;    }}
public Optional<T> parquet-mr_f2786_0(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
 LogicalTypeToken parquet-mr_f2787_0()
{    return LogicalTypeToken.INTEGER;}
protected String parquet-mr_f2788_0()
{    StringBuilder sb = new StringBuilder();    sb.append("(");    sb.append(bitWidth);    sb.append(",");    sb.append(isSigned);    sb.append(")");    return sb.toString();}
public int parquet-mr_f2789_0()
{    return bitWidth;}
public boolean parquet-mr_f2790_0()
{    return isSigned;}
public boolean parquet-mr_f2791_0(Object obj)
{    if (!(obj instanceof IntLogicalTypeAnnotation)) {        return false;    }    IntLogicalTypeAnnotation other = (IntLogicalTypeAnnotation) obj;    return (bitWidth == other.bitWidth) && (isSigned == other.isSigned);}
public int parquet-mr_f2792_0()
{    return Objects.hash(bitWidth, isSigned);}
 PrimitiveStringifier parquet-mr_f2793_0(PrimitiveType primitiveType)
{    return isSigned ? PrimitiveStringifier.DEFAULT_STRINGIFIER : PrimitiveStringifier.UNSIGNED_STRINGIFIER;}
public OriginalType parquet-mr_f2794_0()
{    return OriginalType.JSON;}
public Optional<T> parquet-mr_f2795_0(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
 LogicalTypeToken parquet-mr_f2796_0()
{    return LogicalTypeToken.JSON;}
public boolean parquet-mr_f2797_0(Object obj)
{    return obj instanceof JsonLogicalTypeAnnotation;}
public int parquet-mr_f2798_0()
{        return getClass().hashCode();}
 PrimitiveStringifier parquet-mr_f2799_0(PrimitiveType primitiveType)
{    return PrimitiveStringifier.UTF8_STRINGIFIER;}
public OriginalType parquet-mr_f2800_0()
{    return OriginalType.BSON;}
public Optional<T> parquet-mr_f2801_0(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
 LogicalTypeToken parquet-mr_f2802_0()
{    return LogicalTypeToken.BSON;}
public boolean parquet-mr_f2803_0(Object obj)
{    return obj instanceof BsonLogicalTypeAnnotation;}
public int parquet-mr_f2804_0()
{        return getClass().hashCode();}
 PrimitiveStringifier parquet-mr_f2805_0(PrimitiveType primitiveType)
{    return PrimitiveStringifier.DEFAULT_STRINGIFIER;}
public static LogicalTypeAnnotation parquet-mr_f2806_0()
{    return INSTANCE;}
public OriginalType parquet-mr_f2807_0()
{    return OriginalType.INTERVAL;}
public Optional<T> parquet-mr_f2808_0(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
 LogicalTypeToken parquet-mr_f2809_0()
{    return LogicalTypeToken.INTERVAL;}
public boolean parquet-mr_f2810_0(Object obj)
{    return obj instanceof IntervalLogicalTypeAnnotation;}
public int parquet-mr_f2811_0()
{        return getClass().hashCode();}
 PrimitiveStringifier parquet-mr_f2812_0(PrimitiveType primitiveType)
{    return PrimitiveStringifier.INTERVAL_STRINGIFIER;}
 boolean parquet-mr_f2813_0(ColumnOrder columnOrder)
{    return columnOrder.getColumnOrderName() == UNDEFINED;}
public static MapKeyValueTypeAnnotation parquet-mr_f2814_0()
{    return INSTANCE;}
public OriginalType parquet-mr_f2815_0()
{    return OriginalType.MAP_KEY_VALUE;}
public Optional<T> parquet-mr_f2816_0(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
 LogicalTypeToken parquet-mr_f2817_0()
{    return LogicalTypeToken.MAP_KEY_VALUE;}
public boolean parquet-mr_f2818_0(Object obj)
{    return obj instanceof MapKeyValueTypeAnnotation;}
public int parquet-mr_f2819_0()
{        return getClass().hashCode();}
 Optional<T> parquet-mr_f2820_0(StringLogicalTypeAnnotation stringLogicalType)
{    return empty();}
 Optional<T> parquet-mr_f2821_0(MapLogicalTypeAnnotation mapLogicalType)
{    return empty();}
 Optional<T> parquet-mr_f2822_0(ListLogicalTypeAnnotation listLogicalType)
{    return empty();}
 Optional<T> parquet-mr_f2823_0(EnumLogicalTypeAnnotation enumLogicalType)
{    return empty();}
 Optional<T> parquet-mr_f2824_0(DecimalLogicalTypeAnnotation decimalLogicalType)
{    return empty();}
 Optional<T> parquet-mr_f2825_0(DateLogicalTypeAnnotation dateLogicalType)
{    return empty();}
 Optional<T> parquet-mr_f2826_0(TimeLogicalTypeAnnotation timeLogicalType)
{    return empty();}
 Optional<T> parquet-mr_f2827_0(TimestampLogicalTypeAnnotation timestampLogicalType)
{    return empty();}
 Optional<T> parquet-mr_f2828_0(IntLogicalTypeAnnotation intLogicalType)
{    return empty();}
 Optional<T> parquet-mr_f2829_0(JsonLogicalTypeAnnotation jsonLogicalType)
{    return empty();}
 Optional<T> parquet-mr_f2830_0(BsonLogicalTypeAnnotation bsonLogicalType)
{    return empty();}
 Optional<T> parquet-mr_f2831_0(IntervalLogicalTypeAnnotation intervalLogicalType)
{    return empty();}
 Optional<T> parquet-mr_f2832_0(MapKeyValueTypeAnnotation mapKeyValueLogicalType)
{    return empty();}
public void parquet-mr_f2833_0(TypeVisitor visitor)
{    visitor.visit(this);}
public void parquet-mr_f2834_0(StringBuilder sb, String indent)
{    sb.append("message ").append(getName()).append(getLogicalTypeAnnotation() == null ? "" : " (" + getLogicalTypeAnnotation().toString() + ")").append(" {\n");    membersDisplayString(sb, "  ");    sb.append("}\n");}
public int parquet-mr_f2835_0(String... path)
{    return getMaxRepetitionLevel(path, 0) - 1;}
public int parquet-mr_f2836_0(String... path)
{    return getMaxDefinitionLevel(path, 0) - 1;}
public Type parquet-mr_f2837_0(String... path)
{    return getType(path, 0);}
public ColumnDescriptor parquet-mr_f2838_0(String[] path)
{    int maxRep = getMaxRepetitionLevel(path);    int maxDef = getMaxDefinitionLevel(path);    PrimitiveType type = getType(path).asPrimitiveType();    return new ColumnDescriptor(path, type, maxRep, maxDef);}
public List<String[]> parquet-mr_f2839_0()
{    return this.getPaths(0);}
public List<ColumnDescriptor> parquet-mr_f2840_0()
{    List<String[]> paths = this.getPaths(0);    List<ColumnDescriptor> columns = new ArrayList<ColumnDescriptor>(paths.size());    for (String[] path : paths) {                PrimitiveType primitiveType = getType(path).asPrimitiveType();        columns.add(new ColumnDescriptor(path, primitiveType, getMaxRepetitionLevel(path), getMaxDefinitionLevel(path)));    }    return columns;}
public void parquet-mr_f2841_0(Type subType)
{    if (!(subType instanceof MessageType)) {        throw new InvalidRecordException(subType + " found: expected " + this);    }    checkGroupContains(subType);}
public T parquet-mr_f2842_0(TypeConverter<T> converter)
{    final ArrayList<GroupType> path = new ArrayList<GroupType>();    path.add(this);    return converter.convertMessageType(this, convertChildren(path, converter));}
public boolean parquet-mr_f2843_0(String[] path)
{    return containsPath(path, 0);}
public MessageType parquet-mr_f2844_0(MessageType toMerge)
{    return union(toMerge, true);}
public MessageType parquet-mr_f2845_0(MessageType toMerge, boolean strict)
{    return new MessageType(this.getName(), mergeFields(toMerge, strict));}
public String parquet-mr_f2846_0()
{    while (st.hasMoreTokens()) {        String t = st.nextToken();        if (t.equals("\n")) {            ++line;            currentLine.setLength(0);        } else {            currentLine.append(t);        }        if (!isWhitespace(t)) {            return t;        }    }    throw new IllegalArgumentException("unexpected end of schema");}
private boolean parquet-mr_f2847_0(String t)
{    return t.equals(" ") || t.equals("\t") || t.equals("\n");}
public String parquet-mr_f2848_0()
{    return "line " + line + ": " + currentLine.toString();}
public static MessageType parquet-mr_f2849_0(String input)
{    return parse(input);}
private static MessageType parquet-mr_f2850_0(String schemaString)
{    Tokenizer st = new Tokenizer(schemaString, " ;{}()\n\t");    Types.MessageTypeBuilder builder = Types.buildMessage();    String t = st.nextToken();    check(t, "message", "start with 'message'", st);    String name = st.nextToken();    addGroupTypeFields(st.nextToken(), st, builder);    return builder.named(name);}
private static void parquet-mr_f2851_0(String t, Tokenizer st, Types.GroupBuilder builder)
{    check(t, "{", "start of message", st);    while (!(t = st.nextToken()).equals("}")) {        addType(t, st, builder);    }}
private static void parquet-mr_f2852_0(String t, Tokenizer st, Types.GroupBuilder builder)
{    Repetition repetition = asRepetition(t, st);        String type = st.nextToken();    if ("group".equalsIgnoreCase(type)) {        addGroupType(st, repetition, builder);    } else {        addPrimitiveType(st, asPrimitive(type, st), repetition, builder);    }}
private static void parquet-mr_f2853_0(Tokenizer st, Repetition r, GroupBuilder<?> builder)
{    GroupBuilder<?> childBuilder = builder.group(r);    String t;    String name = st.nextToken();        t = st.nextToken();    OriginalType originalType = null;    if (t.equalsIgnoreCase("(")) {        originalType = OriginalType.valueOf(st.nextToken());        childBuilder.as(originalType);        check(st.nextToken(), ")", "original type ended by )", st);        t = st.nextToken();    }    if (t.equals("=")) {        childBuilder.id(Integer.parseInt(st.nextToken()));        t = st.nextToken();    }    try {        addGroupTypeFields(t, st, childBuilder);    } catch (IllegalArgumentException e) {        throw new IllegalArgumentException("problem reading type: type = group, name = " + name + ", original type = " + originalType, e);    }    childBuilder.named(name);}
private static void parquet-mr_f2854_0(Tokenizer st, PrimitiveTypeName type, Repetition r, Types.GroupBuilder<?> builder)
{    PrimitiveBuilder<?> childBuilder = builder.primitive(type, r);    String t;    if (type == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {        t = st.nextToken();                if (!t.equalsIgnoreCase("(")) {            throw new IllegalArgumentException("expecting (length) for field of type fixed_len_byte_array");        }        childBuilder.length(Integer.parseInt(st.nextToken()));        check(st.nextToken(), ")", "type length ended by )", st);    }    String name = st.nextToken();        t = st.nextToken();    OriginalType originalType = null;    if (t.equalsIgnoreCase("(")) {        t = st.nextToken();        if (isLogicalType(t)) {            LogicalTypeAnnotation.LogicalTypeToken logicalType = LogicalTypeAnnotation.LogicalTypeToken.valueOf(t);            t = st.nextToken();            List<String> tokens = new ArrayList<>();            if ("(".equals(t)) {                while (!")".equals(t)) {                    if (!(",".equals(t) || "(".equals(t) || ")".equals(t))) {                        tokens.add(t);                    }                    t = st.nextToken();                }                t = st.nextToken();            }            LogicalTypeAnnotation logicalTypeAnnotation = logicalType.fromString(tokens);            childBuilder.as(logicalTypeAnnotation);        } else {                        originalType = OriginalType.valueOf(t);            childBuilder.as(originalType);            if (OriginalType.DECIMAL == originalType) {                t = st.nextToken();                                if (t.equalsIgnoreCase("(")) {                    childBuilder.precision(Integer.parseInt(st.nextToken()));                    t = st.nextToken();                    if (t.equalsIgnoreCase(",")) {                        childBuilder.scale(Integer.parseInt(st.nextToken()));                        t = st.nextToken();                    }                    check(t, ")", "decimal type ended by )", st);                    t = st.nextToken();                }            } else {                t = st.nextToken();            }        }        check(t, ")", "logical type ended by )", st);        t = st.nextToken();    }    if (t.equals("=")) {        childBuilder.id(Integer.parseInt(st.nextToken()));        t = st.nextToken();    }    check(t, ";", "field ended by ';'", st);    try {        childBuilder.named(name);    } catch (IllegalArgumentException e) {        throw new IllegalArgumentException("problem reading type: type = " + type + ", name = " + name + ", original type = " + originalType, e);    }}
private static boolean parquet-mr_f2855_0(String t)
{    return Arrays.stream(LogicalTypeAnnotation.LogicalTypeToken.values()).anyMatch((type) -> type.name().equals(t));}
private static PrimitiveTypeName parquet-mr_f2856_0(String t, Tokenizer st)
{    try {        return PrimitiveTypeName.valueOf(t.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {        throw new IllegalArgumentException("expected one of " + Arrays.toString(PrimitiveTypeName.values()) + " got " + t + " at " + st.getLocationString(), e);    }}
private static Repetition parquet-mr_f2857_0(String t, Tokenizer st)
{    try {        return Repetition.valueOf(t.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {        throw new IllegalArgumentException("expected one of " + Arrays.toString(Repetition.values()) + " got " + t + " at " + st.getLocationString(), e);    }}
private static void parquet-mr_f2858_0(String t, String expected, String message, Tokenizer tokenizer)
{    if (!t.equalsIgnoreCase(expected)) {        throw new IllegalArgumentException(message + ": expected '" + expected + "' but got '" + t + "' at " + tokenizer.getLocationString());    }}
public int parquet-mr_f2859_0(boolean b1, boolean b2)
{    throw new UnsupportedOperationException("compare(boolean, boolean) was called on a non-boolean comparator: " + toString());}
public int parquet-mr_f2860_0(int i1, int i2)
{    throw new UnsupportedOperationException("compare(int, int) was called on a non-int comparator: " + toString());}
public int parquet-mr_f2861_0(long l1, long l2)
{    throw new UnsupportedOperationException("compare(long, long) was called on a non-long comparator: " + toString());}
public int parquet-mr_f2862_0(float f1, float f2)
{    throw new UnsupportedOperationException("compare(float, float) was called on a non-float comparator: " + toString());}
public int parquet-mr_f2863_0(double d1, double d2)
{    throw new UnsupportedOperationException("compare(double, double) was called on a non-double comparator: " + toString());}
public final int parquet-mr_f2864_0(T o1, T o2)
{    if (o1 == null) {        return o2 == null ? 0 : -1;    }    return o2 == null ? 1 : compareNotNulls(o1, o2);}
 int parquet-mr_f2865_0(Boolean o1, Boolean o2)
{    return compare(o1.booleanValue(), o2.booleanValue());}
public int parquet-mr_f2866_0(boolean b1, boolean b2)
{    return Boolean.compare(b1, b2);}
public String parquet-mr_f2867_0()
{    return "BOOLEAN_COMPARATOR";}
 int parquet-mr_f2868_0(Integer o1, Integer o2)
{    return compare(o1.intValue(), o2.intValue());}
public int parquet-mr_f2869_0(int i1, int i2)
{    return Integer.compare(i1, i2);}
public String parquet-mr_f2870_0()
{    return "SIGNED_INT32_COMPARATOR";}
public int parquet-mr_f2871_0(int i1, int i2)
{        return Integer.compare(i1 ^ Integer.MIN_VALUE, i2 ^ Integer.MIN_VALUE);}
public String parquet-mr_f2872_0()
{    return "UNSIGNED_INT32_COMPARATOR";}
 int parquet-mr_f2873_0(Long o1, Long o2)
{    return compare(o1.longValue(), o2.longValue());}
public int parquet-mr_f2874_0(long l1, long l2)
{    return Long.compare(l1, l2);}
public String parquet-mr_f2875_0()
{    return "SIGNED_INT64_COMPARATOR";}
public int parquet-mr_f2876_0(long l1, long l2)
{        return Long.compare(l1 ^ Long.MIN_VALUE, l2 ^ Long.MIN_VALUE);}
public String parquet-mr_f2877_0()
{    return "UNSIGNED_INT64_COMPARATOR";}
 int parquet-mr_f2878_0(Float o1, Float o2)
{    return compare(o1.floatValue(), o2.floatValue());}
public int parquet-mr_f2879_0(float f1, float f2)
{    return Float.compare(f1, f2);}
public String parquet-mr_f2880_0()
{    return "FLOAT_COMPARATOR";}
 int parquet-mr_f2881_0(Double o1, Double o2)
{    return compare(o1.doubleValue(), o2.doubleValue());}
public int parquet-mr_f2882_0(double d1, double d2)
{    return Double.compare(d1, d2);}
public String parquet-mr_f2883_0()
{    return "DOUBLE_COMPARATOR";}
 int parquet-mr_f2884_0(Binary o1, Binary o2)
{    return compare(o1.toByteBuffer(), o2.toByteBuffer());}
 final int parquet-mr_f2885_0(byte b)
{    return b & 0xFF;}
 int parquet-mr_f2886_0(ByteBuffer b1, ByteBuffer b2)
{    int l1 = b1.remaining();    int l2 = b2.remaining();    int p1 = b1.position();    int p2 = b2.position();    int minL = Math.min(l1, l2);    for (int i = 0; i < minL; ++i) {        int result = unsignedCompare(b1.get(p1 + i), b2.get(p2 + i));        if (result != 0) {            return result;        }    }    return l1 - l2;}
private int parquet-mr_f2887_0(byte b1, byte b2)
{    return toUnsigned(b1) - toUnsigned(b2);}
public String parquet-mr_f2888_0()
{    return "UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR";}
 int parquet-mr_f2889_0(ByteBuffer b1, ByteBuffer b2)
{    int l1 = b1.remaining();    int l2 = b2.remaining();    int p1 = b1.position();    int p2 = b2.position();    boolean isNegative1 = l1 > 0 && b1.get(p1) < 0;    boolean isNegative2 = l2 > 0 && b2.get(p2) < 0;    if (isNegative1 != isNegative2) {        return isNegative1 ? -1 : 1;    }    int result = 0;        if (l1 < l2) {        int lengthDiff = l2 - l1;        result = -compareWithPadding(lengthDiff, b2, p2, isNegative1 ? NEGATIVE_PADDING : POSITIVE_PADDING);        p2 += lengthDiff;    } else if (l1 > l2) {        int lengthDiff = l1 - l2;        result = compareWithPadding(lengthDiff, b1, p1, isNegative2 ? NEGATIVE_PADDING : POSITIVE_PADDING);        p1 += lengthDiff;    }        if (result == 0) {        result = compare(Math.min(l1, l2), b1, p1, b2, p2);    }    return result;}
private int parquet-mr_f2890_0(int length, ByteBuffer b, int p, int paddingByte)
{    for (int i = p, n = p + length; i < n; ++i) {        int result = toUnsigned(b.get(i)) - paddingByte;        if (result != 0) {            return result;        }    }    return 0;}
private int parquet-mr_f2891_0(int length, ByteBuffer b1, int p1, ByteBuffer b2, int p2)
{    for (int i = 0; i < length; ++i) {        int result = toUnsigned(b1.get(p1 + i)) - toUnsigned(b2.get(p2 + i));        if (result != 0) {            return result;        }    }    return 0;}
public String parquet-mr_f2892_0()
{    return "BINARY_AS_SIGNED_INTEGER_COMPARATOR";}
public final String parquet-mr_f2893_0()
{    return name;}
public String parquet-mr_f2894_0(boolean value)
{    throw new UnsupportedOperationException("stringify(boolean) was called on a non-boolean stringifier: " + toString());}
public String parquet-mr_f2895_0(int value)
{    throw new UnsupportedOperationException("stringify(int) was called on a non-int stringifier: " + toString());}
public String parquet-mr_f2896_0(long value)
{    throw new UnsupportedOperationException("stringify(long) was called on a non-long stringifier: " + toString());}
public String parquet-mr_f2897_0(float value)
{    throw new UnsupportedOperationException("stringify(float) was called on a non-float stringifier: " + toString());}
public String parquet-mr_f2898_0(double value)
{    throw new UnsupportedOperationException("stringify(double) was called on a non-double stringifier: " + toString());}
public String parquet-mr_f2899_0(Binary value)
{    throw new UnsupportedOperationException("stringify(Binary) was called on a non-Binary stringifier: " + toString());}
public final String parquet-mr_f2900_0(Binary value)
{    return value == null ? BINARY_NULL : stringifyNotNull(value);}
public String parquet-mr_f2901_0(boolean value)
{    return Boolean.toString(value);}
public String parquet-mr_f2902_0(int value)
{    return Integer.toString(value);}
public String parquet-mr_f2903_0(long value)
{    return Long.toString(value);}
public String parquet-mr_f2904_0(float value)
{    return Float.toString(value);}
public String parquet-mr_f2905_0(double value)
{    return Double.toString(value);}
 String parquet-mr_f2906_0(Binary value)
{    ByteBuffer buffer = value.toByteBuffer();    StringBuilder builder = new StringBuilder(2 + buffer.remaining() * 2);    builder.append(BINARY_HEXA_PREFIX);    for (int i = buffer.position(), n = buffer.limit(); i < n; ++i) {        byte b = buffer.get(i);        builder.append(digits[(b >>> 4) & 0x0F]);        builder.append(digits[b & 0x0F]);    }    return builder.toString();}
public String parquet-mr_f2907_0(int value)
{    return Long.toString(value & INT_MASK);}
public String parquet-mr_f2908_0(long value)
{    if (value == 0) {                return "0";    } else if (value > 0) {        return Long.toString(value);    } else {        char[] buf = new char[64];        int i = buf.length;                                long top = value >>> 32;        long bot = (value & INT_MASK) + ((top % 10) << 32);        top /= 10;        while ((bot > 0) || (top > 0)) {            buf[--i] = Character.forDigit((int) (bot % 10), 10);            bot = (bot / 10) + ((top % 10) << 32);            top /= 10;        }                return new String(buf, i, buf.length - i);    }}
 String parquet-mr_f2909_0(Binary value)
{    return value.toStringUsingUTF8();}
 String parquet-mr_f2910_0(Binary value)
{    if (value.length() != 12) {        return BINARY_INVALID;    }    ByteBuffer buffer = value.toByteBuffer().order(ByteOrder.LITTLE_ENDIAN);    int pos = buffer.position();    String months = UNSIGNED_STRINGIFIER.stringify(buffer.getInt(pos));    String days = UNSIGNED_STRINGIFIER.stringify(buffer.getInt(pos + 4));    String millis = UNSIGNED_STRINGIFIER.stringify(buffer.getInt(pos + 8));    return "interval(" + months + " months, " + days + " days, " + millis + " millis)";}
public String parquet-mr_f2911_0(int value)
{    return toFormattedString(getInstant(value));}
public String parquet-mr_f2912_0(long value)
{    return toFormattedString(getInstant(value));}
private String parquet-mr_f2913_0(Instant instant)
{    return formatter.format(instant);}
 Instant parquet-mr_f2914_0(int value)
{        super.stringify(value);    return null;}
 Instant parquet-mr_f2915_0(long value)
{        super.stringify(value);    return null;}
 Instant parquet-mr_f2916_0(int value)
{    return Instant.ofEpochMilli(TimeUnit.DAYS.toMillis(value));}
 Instant parquet-mr_f2917_0(long value)
{    return Instant.ofEpochMilli(value);}
 Instant parquet-mr_f2918_0(long value)
{    return Instant.ofEpochSecond(MICROSECONDS.toSeconds(value), MICROSECONDS.toNanos(value % SECONDS.toMicros(1)));}
 Instant parquet-mr_f2919_0(long value)
{    return Instant.ofEpochSecond(NANOSECONDS.toSeconds(value), NANOSECONDS.toNanos(value % SECONDS.toNanos(1)));}
 Instant parquet-mr_f2920_0(long value)
{    return Instant.ofEpochMilli(value);}
 Instant parquet-mr_f2921_0(long value)
{    return Instant.ofEpochSecond(MICROSECONDS.toSeconds(value), MICROSECONDS.toNanos(value % SECONDS.toMicros(1)));}
 Instant parquet-mr_f2922_0(long value)
{    return Instant.ofEpochSecond(NANOSECONDS.toSeconds(value), NANOSECONDS.toNanos(value % SECONDS.toNanos(1)));}
protected String parquet-mr_f2923_0(long duration, TimeUnit unit)
{    String additionalFormat = (unit == MILLISECONDS ? "3d" : unit == MICROSECONDS ? "6d" : "9d");    String timeZone = withZone ? "+0000" : "";    String format = "%02d:%02d:%02d.%0" + additionalFormat + timeZone;    return String.format(format, unit.toHours(duration), convert(duration, unit, MINUTES, HOURS), convert(duration, unit, SECONDS, MINUTES), convert(duration, unit, unit, SECONDS));}
protected long parquet-mr_f2924_0(long duration, TimeUnit from, TimeUnit to, TimeUnit higher)
{    return Math.abs(to.convert(duration, from) % to.convert(1, higher));}
public String parquet-mr_f2925_0(int millis)
{    return toTimeString(millis, MILLISECONDS);}
public String parquet-mr_f2926_0(long micros)
{    return toTimeString(micros, MICROSECONDS);}
public String parquet-mr_f2927_0(long nanos)
{    return toTimeString(nanos, NANOSECONDS);}
public String parquet-mr_f2928_0(int millis)
{    return toTimeString(millis, MILLISECONDS);}
public String parquet-mr_f2929_0(long micros)
{    return toTimeString(micros, MICROSECONDS);}
public String parquet-mr_f2930_0(long nanos)
{    return toTimeString(nanos, NANOSECONDS);}
 static PrimitiveStringifier parquet-mr_f2931_0(final int scale)
{    return new BinaryStringifierBase("DECIMAL_STRINGIFIER(scale: " + scale + ")") {        @Override        public String stringify(int value) {            return stringifyWithScale(BigInteger.valueOf(value));        }        @Override        public String stringify(long value) {            return stringifyWithScale(BigInteger.valueOf(value));        }        @Override        String stringifyNotNull(Binary value) {            try {                return stringifyWithScale(new BigInteger(value.getBytesUnsafe()));            } catch (NumberFormatException e) {                return BINARY_INVALID;            }        }        private String stringifyWithScale(BigInteger i) {            return new BigDecimal(i, scale).toString();        }    };}
public String parquet-mr_f2932_0(int value)
{    return stringifyWithScale(BigInteger.valueOf(value));}
public String parquet-mr_f2933_0(long value)
{    return stringifyWithScale(BigInteger.valueOf(value));}
 String parquet-mr_f2934_0(Binary value)
{    try {        return stringifyWithScale(new BigInteger(value.getBytesUnsafe()));    } catch (NumberFormatException e) {        return BINARY_INVALID;    }}
private String parquet-mr_f2935_0(BigInteger i)
{    return new BigDecimal(i, scale).toString();}
public String parquet-mr_f2936_0(ColumnReader columnReader)
{    return String.valueOf(columnReader.getLong());}
public void parquet-mr_f2937_0(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addLong(columnReader.getLong());}
public void parquet-mr_f2938_0(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addLong(columnReader.getLong());}
public T parquet-mr_f2939_0(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertINT64(this);}
 PrimitiveComparator<?> parquet-mr_f2940_0(LogicalTypeAnnotation logicalType)
{    if (logicalType == null) {        return PrimitiveComparator.SIGNED_INT64_COMPARATOR;    }    return logicalType.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<PrimitiveComparator>() {        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {            return intLogicalType.isSigned() ? of(PrimitiveComparator.SIGNED_INT64_COMPARATOR) : of(PrimitiveComparator.UNSIGNED_INT64_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(PrimitiveComparator.SIGNED_INT64_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {            return of(PrimitiveComparator.SIGNED_INT64_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {            return of(PrimitiveComparator.SIGNED_INT64_COMPARATOR);        }    }).orElseThrow(() -> new ShouldNeverHappenException("No comparator logic implemented for INT64 logical type: " + logicalType));}
public Optional<PrimitiveComparator> parquet-mr_f2941_0(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    return intLogicalType.isSigned() ? of(PrimitiveComparator.SIGNED_INT64_COMPARATOR) : of(PrimitiveComparator.UNSIGNED_INT64_COMPARATOR);}
public Optional<PrimitiveComparator> parquet-mr_f2942_0(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(PrimitiveComparator.SIGNED_INT64_COMPARATOR);}
public Optional<PrimitiveComparator> parquet-mr_f2943_0(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    return of(PrimitiveComparator.SIGNED_INT64_COMPARATOR);}
public Optional<PrimitiveComparator> parquet-mr_f2944_0(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    return of(PrimitiveComparator.SIGNED_INT64_COMPARATOR);}
public String parquet-mr_f2945_0(ColumnReader columnReader)
{    return String.valueOf(columnReader.getInteger());}
public void parquet-mr_f2946_0(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addInteger(columnReader.getInteger());}
public void parquet-mr_f2947_0(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addInt(columnReader.getInteger());}
public T parquet-mr_f2948_0(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertINT32(this);}
 PrimitiveComparator<?> parquet-mr_f2949_0(LogicalTypeAnnotation logicalType)
{    if (logicalType == null) {        return PrimitiveComparator.SIGNED_INT32_COMPARATOR;    }    return logicalType.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<PrimitiveComparator>() {        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {            if (intLogicalType.getBitWidth() == 64) {                return empty();            }            return intLogicalType.isSigned() ? of(PrimitiveComparator.SIGNED_INT32_COMPARATOR) : of(PrimitiveComparator.UNSIGNED_INT32_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(PrimitiveComparator.SIGNED_INT32_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {            return of(PrimitiveComparator.SIGNED_INT32_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {            if (timeLogicalType.getUnit() == MILLIS) {                return of(PrimitiveComparator.SIGNED_INT32_COMPARATOR);            }            return empty();        }    }).orElseThrow(() -> new ShouldNeverHappenException("No comparator logic implemented for INT32 logical type: " + logicalType));}
public Optional<PrimitiveComparator> parquet-mr_f2950_0(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    if (intLogicalType.getBitWidth() == 64) {        return empty();    }    return intLogicalType.isSigned() ? of(PrimitiveComparator.SIGNED_INT32_COMPARATOR) : of(PrimitiveComparator.UNSIGNED_INT32_COMPARATOR);}
public Optional<PrimitiveComparator> parquet-mr_f2951_0(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(PrimitiveComparator.SIGNED_INT32_COMPARATOR);}
public Optional<PrimitiveComparator> parquet-mr_f2952_0(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    return of(PrimitiveComparator.SIGNED_INT32_COMPARATOR);}
public Optional<PrimitiveComparator> parquet-mr_f2953_0(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    if (timeLogicalType.getUnit() == MILLIS) {        return of(PrimitiveComparator.SIGNED_INT32_COMPARATOR);    }    return empty();}
public String parquet-mr_f2954_0(ColumnReader columnReader)
{    return String.valueOf(columnReader.getBoolean());}
public void parquet-mr_f2955_0(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addBoolean(columnReader.getBoolean());}
public void parquet-mr_f2956_0(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addBoolean(columnReader.getBoolean());}
public T parquet-mr_f2957_0(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertBOOLEAN(this);}
 PrimitiveComparator<?> parquet-mr_f2958_0(LogicalTypeAnnotation logicalType)
{    return PrimitiveComparator.BOOLEAN_COMPARATOR;}
public String parquet-mr_f2959_0(ColumnReader columnReader)
{    return String.valueOf(columnReader.getBinary());}
public void parquet-mr_f2960_0(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addBinary(columnReader.getBinary());}
public void parquet-mr_f2961_0(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addBinary(columnReader.getBinary());}
public T parquet-mr_f2962_0(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertBINARY(this);}
 PrimitiveComparator<?> parquet-mr_f2963_0(LogicalTypeAnnotation logicalType)
{    if (logicalType == null) {        return PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR;    }    return logicalType.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<PrimitiveComparator>() {        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(PrimitiveComparator.BINARY_AS_SIGNED_INTEGER_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType) {            return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType) {            return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType) {            return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType) {            return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);        }    }).orElseThrow(() -> new ShouldNeverHappenException("No comparator logic implemented for BINARY logical type: " + logicalType));}
public Optional<PrimitiveComparator> parquet-mr_f2964_0(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(PrimitiveComparator.BINARY_AS_SIGNED_INTEGER_COMPARATOR);}
public Optional<PrimitiveComparator> parquet-mr_f2965_0(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);}
public Optional<PrimitiveComparator> parquet-mr_f2966_0(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType)
{    return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);}
public Optional<PrimitiveComparator> parquet-mr_f2967_0(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType)
{    return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);}
public Optional<PrimitiveComparator> parquet-mr_f2968_0(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType)
{    return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);}
public String parquet-mr_f2969_0(ColumnReader columnReader)
{    return String.valueOf(columnReader.getFloat());}
public void parquet-mr_f2970_0(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addFloat(columnReader.getFloat());}
public void parquet-mr_f2971_0(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addFloat(columnReader.getFloat());}
public T parquet-mr_f2972_0(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertFLOAT(this);}
 PrimitiveComparator<?> parquet-mr_f2973_0(LogicalTypeAnnotation logicalType)
{    return PrimitiveComparator.FLOAT_COMPARATOR;}
public String parquet-mr_f2974_0(ColumnReader columnReader)
{    return String.valueOf(columnReader.getDouble());}
public void parquet-mr_f2975_0(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addDouble(columnReader.getDouble());}
public void parquet-mr_f2976_0(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addDouble(columnReader.getDouble());}
public T parquet-mr_f2977_0(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertDOUBLE(this);}
 PrimitiveComparator<?> parquet-mr_f2978_0(LogicalTypeAnnotation logicalType)
{    return PrimitiveComparator.DOUBLE_COMPARATOR;}
public String parquet-mr_f2979_0(ColumnReader columnReader)
{    return Arrays.toString(columnReader.getBinary().getBytesUnsafe());}
public void parquet-mr_f2980_0(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addBinary(columnReader.getBinary());}
public void parquet-mr_f2981_0(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addBinary(columnReader.getBinary());}
public T parquet-mr_f2982_0(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertINT96(this);}
 PrimitiveComparator<?> parquet-mr_f2983_0(LogicalTypeAnnotation logicalType)
{    return PrimitiveComparator.BINARY_AS_SIGNED_INTEGER_COMPARATOR;}
public String parquet-mr_f2984_0(ColumnReader columnReader)
{    return String.valueOf(columnReader.getBinary());}
public void parquet-mr_f2985_0(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addBinary(columnReader.getBinary());}
public void parquet-mr_f2986_0(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addBinary(columnReader.getBinary());}
public T parquet-mr_f2987_0(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertFIXED_LEN_BYTE_ARRAY(this);}
 PrimitiveComparator<?> parquet-mr_f2988_0(LogicalTypeAnnotation logicalType)
{    if (logicalType == null) {        return PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR;    }    return logicalType.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<PrimitiveComparator>() {        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(PrimitiveComparator.BINARY_AS_SIGNED_INTEGER_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType) {            return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);        }    }).orElseThrow(() -> new ShouldNeverHappenException("No comparator logic implemented for FIXED_LEN_BYTE_ARRAY logical type: " + logicalType));}
public Optional<PrimitiveComparator> parquet-mr_f2989_0(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(PrimitiveComparator.BINARY_AS_SIGNED_INTEGER_COMPARATOR);}
public Optional<PrimitiveComparator> parquet-mr_f2990_0(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType)
{    return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);}
private ColumnOrder parquet-mr_f2991_0(ColumnOrder columnOrder)
{    if (primitive == PrimitiveTypeName.INT96) {        Preconditions.checkArgument(columnOrder.getColumnOrderName() == ColumnOrderName.UNDEFINED, "The column order {} is not supported by INT96", columnOrder);    }    if (getLogicalTypeAnnotation() != null) {        Preconditions.checkArgument(getLogicalTypeAnnotation().isValidColumnOrder(columnOrder), "The column order {} is not supported by {} ({})", columnOrder, primitive, getLogicalTypeAnnotation());    }    return columnOrder;}
public PrimitiveType parquet-mr_f2992_0(int id)
{    return new PrimitiveType(getRepetition(), primitive, length, getName(), getLogicalTypeAnnotation(), new ID(id), columnOrder);}
public PrimitiveTypeName parquet-mr_f2993_0()
{    return primitive;}
public int parquet-mr_f2994_0()
{    return length;}
public DecimalMetadata parquet-mr_f2995_0()
{    return decimalMeta;}
public boolean parquet-mr_f2996_0()
{    return true;}
public void parquet-mr_f2997_0(TypeVisitor visitor)
{    visitor.visit(this);}
public void parquet-mr_f2998_0(StringBuilder sb, String indent)
{    sb.append(indent).append(getRepetition().name().toLowerCase(Locale.ENGLISH)).append(" ").append(primitive.name().toLowerCase());    if (primitive == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {        sb.append("(" + length + ")");    }    sb.append(" ").append(getName());    if (getLogicalTypeAnnotation() != null) {                sb.append(" (").append(getLogicalTypeAnnotation().toString()).append(")");    }    if (getId() != null) {        sb.append(" = ").append(getId());    }}
protected int parquet-mr_f2999_0()
{    return hashCode();}
protected boolean parquet-mr_f3000_0(Type other)
{    return equals(other);}
protected boolean parquet-mr_f3001_0(Type other)
{    if (!other.isPrimitive()) {        return false;    }    PrimitiveType otherPrimitive = other.asPrimitiveType();    return super.equals(other) && primitive == otherPrimitive.getPrimitiveTypeName() && length == otherPrimitive.length && columnOrder.equals(otherPrimitive.columnOrder) && eqOrBothNull(decimalMeta, otherPrimitive.decimalMeta);}
public int parquet-mr_f3002_0()
{    int hash = super.hashCode();    hash = hash * 31 + primitive.hashCode();    hash = hash * 31 + length;    hash = hash * 31 + columnOrder.hashCode();    if (decimalMeta != null) {        hash = hash * 31 + decimalMeta.hashCode();    }    return hash;}
public int parquet-mr_f3003_0(String[] path, int i)
{    if (path.length != i) {        throw new InvalidRecordException("Arrived at primitive node, path invalid");    }    return isRepetition(Repetition.REPEATED) ? 1 : 0;}
public int parquet-mr_f3004_0(String[] path, int i)
{    if (path.length != i) {        throw new InvalidRecordException("Arrived at primitive node, path invalid");    }    return isRepetition(Repetition.REQUIRED) ? 0 : 1;}
public Type parquet-mr_f3005_0(String[] path, int i)
{    if (path.length != i) {        throw new InvalidRecordException("Arrived at primitive node at index " + i + " , path invalid: " + Arrays.toString(path));    }    return this;}
protected List<String[]> parquet-mr_f3006_0(int depth)
{    return Arrays.<String[]>asList(new String[depth]);}
 void parquet-mr_f3007_0(Type subType)
{    super.checkContains(subType);    if (!subType.isPrimitive()) {        throw new InvalidRecordException(subType + " found: expected " + this);    }    PrimitiveType primitiveType = subType.asPrimitiveType();    if (this.primitive != primitiveType.primitive) {        throw new InvalidRecordException(subType + " found: expected " + this);    }}
public T parquet-mr_f3008_0(List<GroupType> path, TypeConverter<T> converter)
{    return converter.convertPrimitiveType(path, this);}
protected boolean parquet-mr_f3009_0(String[] path, int depth)
{    return path.length == depth;}
protected Type parquet-mr_f3010_0(Type toMerge)
{    return union(toMerge, true);}
private void parquet-mr_f3011_0(Type toMerge)
{    throw new IncompatibleSchemaModificationException("can not merge type " + toMerge + " into " + this);}
private void parquet-mr_f3012_0(Type toMerge)
{    throw new IncompatibleSchemaModificationException("can not merge type " + toMerge + " with column order " + toMerge.asPrimitiveType().columnOrder() + " into " + this + " with column order " + columnOrder());}
protected Type parquet-mr_f3013_0(Type toMerge, boolean strict)
{    if (!toMerge.isPrimitive()) {        reportSchemaMergeError(toMerge);    }    if (strict) {                if (!primitive.equals(toMerge.asPrimitiveType().getPrimitiveTypeName()) || !Objects.equals(getLogicalTypeAnnotation(), toMerge.getLogicalTypeAnnotation())) {            reportSchemaMergeError(toMerge);        }                int toMergeLength = toMerge.asPrimitiveType().getTypeLength();        if (primitive == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY && length != toMergeLength) {            reportSchemaMergeError(toMerge);        }                if (!columnOrder().equals(toMerge.asPrimitiveType().columnOrder())) {            reportSchemaMergeErrorWithColumnOrder(toMerge);        }    }    Repetition repetition = Repetition.leastRestrictive(this.getRepetition(), toMerge.getRepetition());    Types.PrimitiveBuilder<PrimitiveType> builder = Types.primitive(primitive, repetition);    if (PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY == primitive) {        builder.length(length);    }    return builder.as(getLogicalTypeAnnotation()).named(getName());}
public PrimitiveComparator<T> parquet-mr_f3014_0()
{    return (PrimitiveComparator<T>) getPrimitiveTypeName().comparator(getLogicalTypeAnnotation());}
public ColumnOrder parquet-mr_f3015_0()
{    return columnOrder;}
public PrimitiveStringifier parquet-mr_f3016_0()
{    LogicalTypeAnnotation logicalTypeAnnotation = getLogicalTypeAnnotation();    return logicalTypeAnnotation == null ? PrimitiveStringifier.DEFAULT_STRINGIFIER : logicalTypeAnnotation.valueStringifier(this);}
public int parquet-mr_f3017_0()
{    return id;}
public int parquet-mr_f3018_0()
{    return id;}
public boolean parquet-mr_f3019_0(Object obj)
{    return (obj instanceof ID) && ((ID) obj).id == id;}
public int parquet-mr_f3020_0()
{    return id;}
public String parquet-mr_f3021_0()
{    return String.valueOf(id);}
public static Repetition parquet-mr_f3022_0(Repetition... repetitions)
{    boolean hasOptional = false;    for (Repetition repetition : repetitions) {        if (repetition == REPEATED) {            return REPEATED;        } else if (repetition == OPTIONAL) {            hasOptional = true;        }    }    if (hasOptional) {        return OPTIONAL;    }    return REQUIRED;}
public boolean parquet-mr_f3023_0(Repetition other)
{    return other != REQUIRED;}
public boolean parquet-mr_f3024_0(Repetition other)
{    return other == REPEATED;}
public boolean parquet-mr_f3025_0(Repetition other)
{    return false;}
public String parquet-mr_f3026_0()
{    return name;}
public boolean parquet-mr_f3027_0(Repetition rep)
{    return repetition == rep;}
public Repetition parquet-mr_f3028_0()
{    return repetition;}
public ID parquet-mr_f3029_0()
{    return id;}
public LogicalTypeAnnotation parquet-mr_f3030_0()
{    return logicalTypeAnnotation;}
public OriginalType parquet-mr_f3031_0()
{    return logicalTypeAnnotation == null ? null : logicalTypeAnnotation.toOriginalType();}
public GroupType parquet-mr_f3032_0()
{    if (isPrimitive()) {        throw new ClassCastException(this + " is not a group");    }    return (GroupType) this;}
public PrimitiveType parquet-mr_f3033_0()
{    if (!isPrimitive()) {        throw new ClassCastException(this + " is not primitive");    }    return (PrimitiveType) this;}
public int parquet-mr_f3034_0()
{    int c = repetition.hashCode();    c = 31 * c + name.hashCode();    if (logicalTypeAnnotation != null) {        c = 31 * c + logicalTypeAnnotation.hashCode();    }    if (id != null) {        c = 31 * c + id.hashCode();    }    return c;}
protected boolean parquet-mr_f3035_0(Type other)
{    return name.equals(other.name) && repetition == other.repetition && eqOrBothNull(repetition, other.repetition) && eqOrBothNull(id, other.id) && eqOrBothNull(logicalTypeAnnotation, other.logicalTypeAnnotation);}
public boolean parquet-mr_f3036_0(Object other)
{    if (!(other instanceof Type) || other == null) {        return false;    }    return equals((Type) other);}
protected boolean parquet-mr_f3037_0(Object o1, Object o2)
{    return (o1 == null && o2 == null) || (o1 != null && o1.equals(o2));}
public String parquet-mr_f3038_0()
{    StringBuilder sb = new StringBuilder();    writeToStringBuilder(sb, "");    return sb.toString();}
 void parquet-mr_f3039_0(Type subType)
{    if (!this.name.equals(subType.name) || this.repetition != subType.repetition) {        throw new InvalidRecordException(subType + " found: expected " + this);    }}
protected final THIS parquet-mr_f3040_0(Type.Repetition repetition)
{    Preconditions.checkArgument(!repetitionAlreadySet, "Repetition has already been set");    Preconditions.checkNotNull(repetition, "Repetition cannot be null");    this.repetition = repetition;    this.repetitionAlreadySet = true;    return self();}
public THIS parquet-mr_f3041_0(OriginalType type)
{    this.logicalTypeAnnotation = LogicalTypeAnnotation.fromOriginalType(type, null);    return self();}
public THIS parquet-mr_f3042_0(LogicalTypeAnnotation type)
{    this.logicalTypeAnnotation = type;    this.newLogicalTypeSet = true;    return self();}
public THIS parquet-mr_f3043_0(int id)
{    this.id = new ID(id);    return self();}
public P parquet-mr_f3044_0(String name)
{    Preconditions.checkNotNull(name, "Name is required");    Preconditions.checkNotNull(repetition, "Repetition is required");    Type type = build(name);    if (parent != null) {                if (BaseGroupBuilder.class.isAssignableFrom(parent.getClass())) {            BaseGroupBuilder.class.cast(parent).addField(type);        }        return parent;    } else if (returnClass != null) {                return returnClass.cast(type);    } else {        throw new IllegalStateException("[BUG] Parent and return type are null: must override named");    }}
protected OriginalType parquet-mr_f3045_0()
{    return logicalTypeAnnotation == null ? null : logicalTypeAnnotation.toOriginalType();}
public THIS parquet-mr_f3046_0(int length)
{    this.length = length;    return self();}
public THIS parquet-mr_f3047_0(int precision)
{    this.precision = precision;    precisionAlreadySet = true;    return self();}
public THIS parquet-mr_f3048_0(int scale)
{    this.scale = scale;    scaleAlreadySet = true;    return self();}
public THIS parquet-mr_f3049_0(ColumnOrder columnOrder)
{    this.columnOrder = columnOrder;    return self();}
protected PrimitiveType parquet-mr_f3050_1(String name)
{    if (PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY == primitiveType) {        Preconditions.checkArgument(length > 0, "Invalid FIXED_LEN_BYTE_ARRAY length: " + length);    }    DecimalMetadata meta = decimalMetadata();        if (logicalTypeAnnotation != null) {        logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<Boolean>() {            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType) {                checkBinaryPrimitiveType(stringLogicalType);                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType) {                checkBinaryPrimitiveType(jsonLogicalType);                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType) {                checkBinaryPrimitiveType(bsonLogicalType);                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {                Preconditions.checkState((primitiveType == PrimitiveTypeName.INT32) || (primitiveType == PrimitiveTypeName.INT64) || (primitiveType == PrimitiveTypeName.BINARY) || (primitiveType == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY), "DECIMAL can only annotate INT32, INT64, BINARY, and FIXED");                if (primitiveType == PrimitiveTypeName.INT32) {                    Preconditions.checkState(meta.getPrecision() <= MAX_PRECISION_INT32, "INT32 cannot store " + meta.getPrecision() + " digits " + "(max " + MAX_PRECISION_INT32 + ")");                } else if (primitiveType == PrimitiveTypeName.INT64) {                    Preconditions.checkState(meta.getPrecision() <= MAX_PRECISION_INT64, "INT64 cannot store " + meta.getPrecision() + " digits " + "(max " + MAX_PRECISION_INT64 + ")");                    if (meta.getPrecision() <= MAX_PRECISION_INT32) {                                            }                } else if (primitiveType == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {                    Preconditions.checkState(meta.getPrecision() <= maxPrecision(length), "FIXED(" + length + ") cannot store " + meta.getPrecision() + " digits (max " + maxPrecision(length) + ")");                }                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {                checkInt32PrimitiveType(dateLogicalType);                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {                LogicalTypeAnnotation.TimeUnit unit = timeLogicalType.getUnit();                switch(unit) {                    case MILLIS:                        checkInt32PrimitiveType(timeLogicalType);                        break;                    case MICROS:                    case NANOS:                        checkInt64PrimitiveType(timeLogicalType);                        break;                    default:                        throw new RuntimeException("Invalid time unit: " + unit);                }                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {                int bitWidth = intLogicalType.getBitWidth();                switch(bitWidth) {                    case 8:                    case 16:                    case 32:                        checkInt32PrimitiveType(intLogicalType);                        break;                    case 64:                        checkInt64PrimitiveType(intLogicalType);                        break;                    default:                        throw new RuntimeException("Invalid bit width: " + bitWidth);                }                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {                checkInt64PrimitiveType(timestampLogicalType);                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType) {                Preconditions.checkState((primitiveType == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) && (length == 12), "INTERVAL can only annotate FIXED_LEN_BYTE_ARRAY(12)");                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType) {                Preconditions.checkState(primitiveType == PrimitiveTypeName.BINARY, "ENUM can only annotate binary fields");                return Optional.of(true);            }            private void checkBinaryPrimitiveType(LogicalTypeAnnotation logicalTypeAnnotation) {                Preconditions.checkState(primitiveType == PrimitiveTypeName.BINARY, logicalTypeAnnotation.toString() + " can only annotate binary fields");            }            private void checkInt32PrimitiveType(LogicalTypeAnnotation logicalTypeAnnotation) {                Preconditions.checkState(primitiveType == PrimitiveTypeName.INT32, logicalTypeAnnotation.toString() + " can only annotate INT32");            }            private void checkInt64PrimitiveType(LogicalTypeAnnotation logicalTypeAnnotation) {                Preconditions.checkState(primitiveType == PrimitiveTypeName.INT64, logicalTypeAnnotation.toString() + " can only annotate INT64");            }        }).orElseThrow(() -> new IllegalStateException(logicalTypeAnnotation + " can not be applied to a primitive type"));    }    if (newLogicalTypeSet) {        return new PrimitiveType(repetition, primitiveType, length, name, logicalTypeAnnotation, id, columnOrder);    } else {        return new PrimitiveType(repetition, primitiveType, length, name, getOriginalType(), meta, id, columnOrder);    }}
public Optional<Boolean> parquet-mr_f3051_0(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    checkBinaryPrimitiveType(stringLogicalType);    return Optional.of(true);}
public Optional<Boolean> parquet-mr_f3052_0(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType)
{    checkBinaryPrimitiveType(jsonLogicalType);    return Optional.of(true);}
public Optional<Boolean> parquet-mr_f3053_0(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType)
{    checkBinaryPrimitiveType(bsonLogicalType);    return Optional.of(true);}
public Optional<Boolean> parquet-mr_f3054_1(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    Preconditions.checkState((primitiveType == PrimitiveTypeName.INT32) || (primitiveType == PrimitiveTypeName.INT64) || (primitiveType == PrimitiveTypeName.BINARY) || (primitiveType == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY), "DECIMAL can only annotate INT32, INT64, BINARY, and FIXED");    if (primitiveType == PrimitiveTypeName.INT32) {        Preconditions.checkState(meta.getPrecision() <= MAX_PRECISION_INT32, "INT32 cannot store " + meta.getPrecision() + " digits " + "(max " + MAX_PRECISION_INT32 + ")");    } else if (primitiveType == PrimitiveTypeName.INT64) {        Preconditions.checkState(meta.getPrecision() <= MAX_PRECISION_INT64, "INT64 cannot store " + meta.getPrecision() + " digits " + "(max " + MAX_PRECISION_INT64 + ")");        if (meta.getPrecision() <= MAX_PRECISION_INT32) {                    }    } else if (primitiveType == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {        Preconditions.checkState(meta.getPrecision() <= maxPrecision(length), "FIXED(" + length + ") cannot store " + meta.getPrecision() + " digits (max " + maxPrecision(length) + ")");    }    return Optional.of(true);}
public Optional<Boolean> parquet-mr_f3055_0(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    checkInt32PrimitiveType(dateLogicalType);    return Optional.of(true);}
public Optional<Boolean> parquet-mr_f3056_0(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    LogicalTypeAnnotation.TimeUnit unit = timeLogicalType.getUnit();    switch(unit) {        case MILLIS:            checkInt32PrimitiveType(timeLogicalType);            break;        case MICROS:        case NANOS:            checkInt64PrimitiveType(timeLogicalType);            break;        default:            throw new RuntimeException("Invalid time unit: " + unit);    }    return Optional.of(true);}
public Optional<Boolean> parquet-mr_f3057_0(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    int bitWidth = intLogicalType.getBitWidth();    switch(bitWidth) {        case 8:        case 16:        case 32:            checkInt32PrimitiveType(intLogicalType);            break;        case 64:            checkInt64PrimitiveType(intLogicalType);            break;        default:            throw new RuntimeException("Invalid bit width: " + bitWidth);    }    return Optional.of(true);}
public Optional<Boolean> parquet-mr_f3058_0(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    checkInt64PrimitiveType(timestampLogicalType);    return Optional.of(true);}
public Optional<Boolean> parquet-mr_f3059_0(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType)
{    Preconditions.checkState((primitiveType == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) && (length == 12), "INTERVAL can only annotate FIXED_LEN_BYTE_ARRAY(12)");    return Optional.of(true);}
public Optional<Boolean> parquet-mr_f3060_0(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType)
{    Preconditions.checkState(primitiveType == PrimitiveTypeName.BINARY, "ENUM can only annotate binary fields");    return Optional.of(true);}
private void parquet-mr_f3061_0(LogicalTypeAnnotation logicalTypeAnnotation)
{    Preconditions.checkState(primitiveType == PrimitiveTypeName.BINARY, logicalTypeAnnotation.toString() + " can only annotate binary fields");}
private void parquet-mr_f3062_0(LogicalTypeAnnotation logicalTypeAnnotation)
{    Preconditions.checkState(primitiveType == PrimitiveTypeName.INT32, logicalTypeAnnotation.toString() + " can only annotate INT32");}
private void parquet-mr_f3063_0(LogicalTypeAnnotation logicalTypeAnnotation)
{    Preconditions.checkState(primitiveType == PrimitiveTypeName.INT64, logicalTypeAnnotation.toString() + " can only annotate INT64");}
private static long parquet-mr_f3064_0(int numBytes)
{    return     Math.round(Math.floor(    Math.log10(    Math.pow(2, 8 * numBytes - 1) - 1)));}
protected DecimalMetadata parquet-mr_f3065_0()
{    DecimalMetadata meta = null;    if (logicalTypeAnnotation instanceof LogicalTypeAnnotation.DecimalLogicalTypeAnnotation) {        LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalType = (LogicalTypeAnnotation.DecimalLogicalTypeAnnotation) logicalTypeAnnotation;        if (newLogicalTypeSet) {            if (scaleAlreadySet) {                Preconditions.checkArgument(this.scale == decimalType.getScale(), "Decimal scale should match with the scale of the logical type");            }            if (precisionAlreadySet) {                Preconditions.checkArgument(this.precision == decimalType.getPrecision(), "Decimal precision should match with the precision of the logical type");            }            scale = decimalType.getScale();            precision = decimalType.getPrecision();        }        Preconditions.checkArgument(precision > 0, "Invalid DECIMAL precision: " + precision);        Preconditions.checkArgument(this.scale >= 0, "Invalid DECIMAL scale: " + this.scale);        Preconditions.checkArgument(this.scale <= precision, "Invalid DECIMAL scale: cannot be greater than precision");        meta = new DecimalMetadata(precision, scale);    }    return meta;}
protected PrimitiveBuilder<P> parquet-mr_f3066_0()
{    return this;}
public PrimitiveBuilder<THIS> parquet-mr_f3067_0(PrimitiveTypeName type, Type.Repetition repetition)
{    return new PrimitiveBuilder<THIS>(self(), type).repetition(repetition);}
public PrimitiveBuilder<THIS> parquet-mr_f3068_0(PrimitiveTypeName type)
{    return new PrimitiveBuilder<THIS>(self(), type).repetition(Type.Repetition.REQUIRED);}
public PrimitiveBuilder<THIS> parquet-mr_f3069_0(PrimitiveTypeName type)
{    return new PrimitiveBuilder<THIS>(self(), type).repetition(Type.Repetition.OPTIONAL);}
public PrimitiveBuilder<THIS> parquet-mr_f3070_0(PrimitiveTypeName type)
{    return new PrimitiveBuilder<THIS>(self(), type).repetition(Type.Repetition.REPEATED);}
public GroupBuilder<THIS> parquet-mr_f3071_0(Type.Repetition repetition)
{    return new GroupBuilder<THIS>(self()).repetition(repetition);}
public GroupBuilder<THIS> parquet-mr_f3072_0()
{    return new GroupBuilder<THIS>(self()).repetition(Type.Repetition.REQUIRED);}
public GroupBuilder<THIS> parquet-mr_f3073_0()
{    return new GroupBuilder<THIS>(self()).repetition(Type.Repetition.OPTIONAL);}
public GroupBuilder<THIS> parquet-mr_f3074_0()
{    return new GroupBuilder<THIS>(self()).repetition(Type.Repetition.REPEATED);}
public THIS parquet-mr_f3075_0(Type type)
{    fields.add(type);    return self();}
public THIS parquet-mr_f3076_0(Type... types)
{    Collections.addAll(fields, types);    return self();}
protected GroupType parquet-mr_f3077_0(String name)
{    if (newLogicalTypeSet) {        return new GroupType(repetition, name, logicalTypeAnnotation, fields, id);    } else {        return new GroupType(repetition, name, getOriginalType(), fields, id);    }}
public MapBuilder<THIS> parquet-mr_f3078_0(Type.Repetition repetition)
{    return new MapBuilder<THIS>(self()).repetition(repetition);}
public MapBuilder<THIS> parquet-mr_f3079_0()
{    return new MapBuilder<THIS>(self()).repetition(Type.Repetition.REQUIRED);}
public MapBuilder<THIS> parquet-mr_f3080_0()
{    return new MapBuilder<THIS>(self()).repetition(Type.Repetition.OPTIONAL);}
public ListBuilder<THIS> parquet-mr_f3081_0(Type.Repetition repetition)
{    return new ListBuilder<THIS>(self()).repetition(repetition);}
public ListBuilder<THIS> parquet-mr_f3082_0()
{    return list(Type.Repetition.REQUIRED);}
public ListBuilder<THIS> parquet-mr_f3083_0()
{    return list(Type.Repetition.OPTIONAL);}
protected GroupBuilder<P> parquet-mr_f3084_0()
{    return this;}
public ValueBuilder<MP, M> parquet-mr_f3085_0(PrimitiveTypeName type, Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new ValueBuilder<MP, M>(mapBuilder, type).repetition(repetition);}
public ValueBuilder<MP, M> parquet-mr_f3086_0(PrimitiveTypeName type)
{    return value(type, Type.Repetition.REQUIRED);}
public ValueBuilder<MP, M> parquet-mr_f3087_0(PrimitiveTypeName type)
{    return value(type, Type.Repetition.OPTIONAL);}
public GroupValueBuilder<MP, M> parquet-mr_f3088_0(Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new GroupValueBuilder<MP, M>(mapBuilder).repetition(repetition);}
public GroupValueBuilder<MP, M> parquet-mr_f3089_0()
{    return groupValue(Type.Repetition.REQUIRED);}
public GroupValueBuilder<MP, M> parquet-mr_f3090_0()
{    return groupValue(Type.Repetition.OPTIONAL);}
public MapValueBuilder<MP, M> parquet-mr_f3091_0(Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new MapValueBuilder<MP, M>(mapBuilder).repetition(repetition);}
public MapValueBuilder<MP, M> parquet-mr_f3092_0()
{    return mapValue(Type.Repetition.REQUIRED);}
public MapValueBuilder<MP, M> parquet-mr_f3093_0()
{    return mapValue(Type.Repetition.OPTIONAL);}
public ListValueBuilder<MP, M> parquet-mr_f3094_0(Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new ListValueBuilder<MP, M>(mapBuilder).repetition(repetition);}
public ListValueBuilder<MP, M> parquet-mr_f3095_0()
{    return listValue(Type.Repetition.REQUIRED);}
public ListValueBuilder<MP, M> parquet-mr_f3096_0()
{    return listValue(Type.Repetition.OPTIONAL);}
public M parquet-mr_f3097_0(Type type)
{    mapBuilder.setKeyType(build("key"));    mapBuilder.setValueType(type);    return this.mapBuilder;}
public MP parquet-mr_f3098_0(String name)
{    mapBuilder.setKeyType(build("key"));    return mapBuilder.named(name);}
protected KeyBuilder<MP, M> parquet-mr_f3099_0()
{    return this;}
public MP parquet-mr_f3100_0(String name)
{    mapBuilder.setValueType(build("value"));    return mapBuilder.named(name);}
protected ValueBuilder<MP, M> parquet-mr_f3101_0()
{    return this;}
protected GroupKeyBuilder<MP, M> parquet-mr_f3102_0()
{    return this;}
public ValueBuilder<MP, M> parquet-mr_f3103_0(PrimitiveTypeName type, Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new ValueBuilder<MP, M>(mapBuilder, type).repetition(repetition);}
public ValueBuilder<MP, M> parquet-mr_f3104_0(PrimitiveTypeName type)
{    return value(type, Type.Repetition.REQUIRED);}
public ValueBuilder<MP, M> parquet-mr_f3105_0(PrimitiveTypeName type)
{    return value(type, Type.Repetition.OPTIONAL);}
public GroupValueBuilder<MP, M> parquet-mr_f3106_0(Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new GroupValueBuilder<MP, M>(mapBuilder).repetition(repetition);}
public GroupValueBuilder<MP, M> parquet-mr_f3107_0()
{    return groupValue(Type.Repetition.REQUIRED);}
public GroupValueBuilder<MP, M> parquet-mr_f3108_0()
{    return groupValue(Type.Repetition.OPTIONAL);}
public MapValueBuilder<MP, M> parquet-mr_f3109_0(Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new MapValueBuilder<MP, M>(mapBuilder).repetition(repetition);}
public MapValueBuilder<MP, M> parquet-mr_f3110_0()
{    return mapValue(Type.Repetition.REQUIRED);}
public MapValueBuilder<MP, M> parquet-mr_f3111_0()
{    return mapValue(Type.Repetition.OPTIONAL);}
public ListValueBuilder<MP, M> parquet-mr_f3112_0(Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new ListValueBuilder<MP, M>(mapBuilder).repetition(repetition);}
public ListValueBuilder<MP, M> parquet-mr_f3113_0()
{    return listValue(Type.Repetition.REQUIRED);}
public ListValueBuilder<MP, M> parquet-mr_f3114_0()
{    return listValue(Type.Repetition.OPTIONAL);}
public M parquet-mr_f3115_0(Type type)
{    mapBuilder.setKeyType(build("key"));    mapBuilder.setValueType(type);    return this.mapBuilder;}
public MP parquet-mr_f3116_0(String name)
{    mapBuilder.setKeyType(build("key"));    return mapBuilder.named(name);}
public MP parquet-mr_f3117_0(String name)
{    mapBuilder.setValueType(build("value"));    return mapBuilder.named(name);}
protected GroupValueBuilder<MP, M> parquet-mr_f3118_0()
{    return this;}
public MP parquet-mr_f3119_0(String name)
{    mapBuilder.setValueType(build("value"));    return mapBuilder.named(name);}
protected MapValueBuilder<MP, M> parquet-mr_f3120_0()
{    return this;}
public MP parquet-mr_f3121_0(String name)
{    mapBuilder.setValueType(build("value"));    return mapBuilder.named(name);}
protected ListValueBuilder<MP, M> parquet-mr_f3122_0()
{    return this;}
protected void parquet-mr_f3123_0(Type keyType)
{    Preconditions.checkState(this.keyType == null, "Only one key type can be built with a MapBuilder");    this.keyType = keyType;}
protected void parquet-mr_f3124_0(Type valueType)
{    Preconditions.checkState(this.valueType == null, "Only one key type can be built with a ValueBuilder");    this.valueType = valueType;}
public KeyBuilder<P, THIS> parquet-mr_f3125_0(PrimitiveTypeName type)
{    return new KeyBuilder<P, THIS>(self(), type);}
public THIS parquet-mr_f3126_0(Type type)
{    setKeyType(type);    return self();}
public GroupKeyBuilder<P, THIS> parquet-mr_f3127_0()
{    return new GroupKeyBuilder<P, THIS>(self());}
public ValueBuilder<P, THIS> parquet-mr_f3128_0(PrimitiveTypeName type, Type.Repetition repetition)
{    return new ValueBuilder<P, THIS>(self(), type).repetition(repetition);}
public ValueBuilder<P, THIS> parquet-mr_f3129_0(PrimitiveTypeName type)
{    return value(type, Type.Repetition.REQUIRED);}
public ValueBuilder<P, THIS> parquet-mr_f3130_0(PrimitiveTypeName type)
{    return value(type, Type.Repetition.OPTIONAL);}
public GroupValueBuilder<P, THIS> parquet-mr_f3131_0(Type.Repetition repetition)
{    return new GroupValueBuilder<P, THIS>(self()).repetition(repetition);}
public GroupValueBuilder<P, THIS> parquet-mr_f3132_0()
{    return groupValue(Type.Repetition.REQUIRED);}
public GroupValueBuilder<P, THIS> parquet-mr_f3133_0()
{    return groupValue(Type.Repetition.OPTIONAL);}
public MapValueBuilder<P, THIS> parquet-mr_f3134_0(Type.Repetition repetition)
{    return new MapValueBuilder<P, THIS>(self()).repetition(repetition);}
public MapValueBuilder<P, THIS> parquet-mr_f3135_0()
{    return mapValue(Type.Repetition.REQUIRED);}
public MapValueBuilder<P, THIS> parquet-mr_f3136_0()
{    return mapValue(Type.Repetition.OPTIONAL);}
public ListValueBuilder<P, THIS> parquet-mr_f3137_0(Type.Repetition repetition)
{    return new ListValueBuilder<P, THIS>(self()).repetition(repetition);}
public ListValueBuilder<P, THIS> parquet-mr_f3138_0()
{    return listValue(Type.Repetition.REQUIRED);}
public ListValueBuilder<P, THIS> parquet-mr_f3139_0()
{    return listValue(Type.Repetition.OPTIONAL);}
public THIS parquet-mr_f3140_0(Type type)
{    setValueType(type);    return self();}
protected Type parquet-mr_f3141_0(String name)
{    Preconditions.checkState(logicalTypeAnnotation == null, "MAP is already a logical type and can't be changed.");    if (keyType == null) {        keyType = STRING_KEY;    }    GroupBuilder<GroupType> builder = buildGroup(repetition).as(OriginalType.MAP);    if (id != null) {        builder.id(id.intValue());    }    if (valueType != null) {        return builder.repeatedGroup().addFields(keyType, valueType).named("map").named(name);    } else {        return builder.repeatedGroup().addFields(keyType).named("map").named(name);    }}
protected MapBuilder<P> parquet-mr_f3142_0()
{    return this;}
public THIS parquet-mr_f3143_0(Type elementType)
{    Preconditions.checkState(this.elementType == null, "Only one element can be built with a ListBuilder");    this.elementType = elementType;    return self();}
public LP parquet-mr_f3144_0(String name)
{    listBuilder.setElementType(build("element"));    return listBuilder.named(name);}
protected ElementBuilder<LP, L> parquet-mr_f3145_0()
{    return this;}
public LP parquet-mr_f3146_0(String name)
{    listBuilder.setElementType(build("element"));    return listBuilder.named(name);}
protected GroupElementBuilder<LP, L> parquet-mr_f3147_0()
{    return this;}
protected MapElementBuilder<LP, L> parquet-mr_f3148_0()
{    return this;}
public LP parquet-mr_f3149_0(String name)
{    listBuilder.setElementType(build("element"));    return listBuilder.named(name);}
protected ListElementBuilder<LP, L> parquet-mr_f3150_0()
{    return this;}
public LP parquet-mr_f3151_0(String name)
{    listBuilder.setElementType(build("element"));    return listBuilder.named(name);}
protected Type parquet-mr_f3152_0(String name)
{    Preconditions.checkState(logicalTypeAnnotation == null, "LIST is already the logical type and can't be changed");    Preconditions.checkNotNull(elementType, "List element type");    GroupBuilder<GroupType> builder = buildGroup(repetition).as(OriginalType.LIST);    if (id != null) {        builder.id(id.intValue());    }    return builder.repeatedGroup().addFields(elementType).named("list").named(name);}
public ElementBuilder<P, THIS> parquet-mr_f3153_0(PrimitiveTypeName type, Type.Repetition repetition)
{    return new ElementBuilder<P, THIS>(self(), type).repetition(repetition);}
public ElementBuilder<P, THIS> parquet-mr_f3154_0(PrimitiveTypeName type)
{    return element(type, Type.Repetition.REQUIRED);}
public ElementBuilder<P, THIS> parquet-mr_f3155_0(PrimitiveTypeName type)
{    return element(type, Type.Repetition.OPTIONAL);}
public GroupElementBuilder<P, THIS> parquet-mr_f3156_0(Type.Repetition repetition)
{    return new GroupElementBuilder<P, THIS>(self()).repetition(repetition);}
public GroupElementBuilder<P, THIS> parquet-mr_f3157_0()
{    return groupElement(Type.Repetition.REQUIRED);}
public GroupElementBuilder<P, THIS> parquet-mr_f3158_0()
{    return groupElement(Type.Repetition.OPTIONAL);}
public MapElementBuilder<P, THIS> parquet-mr_f3159_0(Type.Repetition repetition)
{    return new MapElementBuilder<P, THIS>(self()).repetition(repetition);}
public MapElementBuilder<P, THIS> parquet-mr_f3160_0()
{    return mapElement(Type.Repetition.REQUIRED);}
public MapElementBuilder<P, THIS> parquet-mr_f3161_0()
{    return mapElement(Type.Repetition.OPTIONAL);}
public ListElementBuilder<P, THIS> parquet-mr_f3162_0(Type.Repetition repetition)
{    return new ListElementBuilder<P, THIS>(self()).repetition(repetition);}
public ListElementBuilder<P, THIS> parquet-mr_f3163_0()
{    return listElement(Type.Repetition.REQUIRED);}
public ListElementBuilder<P, THIS> parquet-mr_f3164_0()
{    return listElement(Type.Repetition.OPTIONAL);}
public BaseListBuilder<P, THIS> parquet-mr_f3165_0(Type type)
{    setElementType(type);    return self();}
protected ListBuilder<P> parquet-mr_f3166_0()
{    return this;}
public MessageType parquet-mr_f3167_0(String name)
{    Preconditions.checkNotNull(name, "Name is required");    return new MessageType(name, fields);}
public static MessageTypeBuilder parquet-mr_f3168_0()
{    return new MessageTypeBuilder();}
public static PrimitiveBuilder<PrimitiveType> parquet-mr_f3169_0(PrimitiveTypeName type, Type.Repetition repetition)
{    return new PrimitiveBuilder<PrimitiveType>(PrimitiveType.class, type).repetition(repetition);}
public static PrimitiveBuilder<PrimitiveType> parquet-mr_f3170_0(PrimitiveTypeName type)
{    return new PrimitiveBuilder<PrimitiveType>(PrimitiveType.class, type).repetition(Type.Repetition.REQUIRED);}
public static PrimitiveBuilder<PrimitiveType> parquet-mr_f3171_0(PrimitiveTypeName type)
{    return new PrimitiveBuilder<PrimitiveType>(PrimitiveType.class, type).repetition(Type.Repetition.OPTIONAL);}
public static PrimitiveBuilder<PrimitiveType> parquet-mr_f3172_0(PrimitiveTypeName type)
{    return new PrimitiveBuilder<PrimitiveType>(PrimitiveType.class, type).repetition(Type.Repetition.REPEATED);}
public static GroupBuilder<GroupType> parquet-mr_f3173_0(Type.Repetition repetition)
{    return new GroupBuilder<GroupType>(GroupType.class).repetition(repetition);}
public static GroupBuilder<GroupType> parquet-mr_f3174_0()
{    return new GroupBuilder<GroupType>(GroupType.class).repetition(Type.Repetition.REQUIRED);}
public static GroupBuilder<GroupType> parquet-mr_f3175_0()
{    return new GroupBuilder<GroupType>(GroupType.class).repetition(Type.Repetition.OPTIONAL);}
public static GroupBuilder<GroupType> parquet-mr_f3176_0()
{    return new GroupBuilder<GroupType>(GroupType.class).repetition(Type.Repetition.REPEATED);}
public static MapBuilder<GroupType> parquet-mr_f3177_0(Type.Repetition repetition)
{    return new MapBuilder<GroupType>(GroupType.class).repetition(repetition);}
public static MapBuilder<GroupType> parquet-mr_f3178_0()
{    return map(Type.Repetition.REQUIRED);}
public static MapBuilder<GroupType> parquet-mr_f3179_0()
{    return map(Type.Repetition.OPTIONAL);}
public static ListBuilder<GroupType> parquet-mr_f3180_0(Type.Repetition repetition)
{    return new ListBuilder<GroupType>(GroupType.class).repetition(repetition);}
public static ListBuilder<GroupType> parquet-mr_f3181_0()
{    return list(Type.Repetition.REQUIRED);}
public static ListBuilder<GroupType> parquet-mr_f3182_0()
{    return list(Type.Repetition.OPTIONAL);}
public static void parquet-mr_f3183_0(GroupType schema)
{    schema.accept(new TypeVisitor() {        @Override        public void visit(GroupType groupType) {            if (groupType.getFieldCount() <= 0) {                throw new InvalidSchemaException("Cannot write a schema with an empty group: " + groupType);            }            for (Type type : groupType.getFields()) {                type.accept(this);            }        }        @Override        public void visit(MessageType messageType) {            visit((GroupType) messageType);        }        @Override        public void visit(PrimitiveType primitiveType) {        }    });}
public void parquet-mr_f3184_0(GroupType groupType)
{    if (groupType.getFieldCount() <= 0) {        throw new InvalidSchemaException("Cannot write a schema with an empty group: " + groupType);    }    for (Type type : groupType.getFields()) {        type.accept(this);    }}
public void parquet-mr_f3185_0(MessageType messageType)
{    visit((GroupType) messageType);}
public void parquet-mr_f3186_0(PrimitiveType primitiveType)
{}
public void parquet-mr_f3187_0(Binary value)
{    assertEquals("bar" + count % 10, value.toStringUsingUTF8());    ++count;}
public void parquet-mr_f3188_0() throws Exception
{    MessageType schema = MessageTypeParser.parseMessageType("message test { required binary foo; }");    ColumnDescriptor col = schema.getColumns().get(0);    MemPageWriter pageWriter = new MemPageWriter();    ColumnWriterV2 columnWriterV2 = new ColumnWriterV2(col, pageWriter, ParquetProperties.builder().withDictionaryPageSize(1024).withWriterVersion(PARQUET_2_0).withPageSize(2048).build());    for (int i = 0; i < rows; i++) {        columnWriterV2.write(Binary.fromString("bar" + i % 10), 0, 0);        if ((i + 1) % 1000 == 0) {            columnWriterV2.writePage();        }    }    columnWriterV2.writePage();    columnWriterV2.finalizeColumnChunk();    List<DataPage> pages = pageWriter.getPages();    int valueCount = 0;    int rowCount = 0;    for (DataPage dataPage : pages) {        valueCount += dataPage.getValueCount();        rowCount += ((DataPageV2) dataPage).getRowCount();    }    assertEquals(rows, rowCount);    assertEquals(rows, valueCount);    MemPageReader pageReader = new MemPageReader((long) rows, pages.iterator(), pageWriter.getDictionaryPage());    ValidatingConverter converter = new ValidatingConverter();    ColumnReader columnReader = new ColumnReaderImpl(col, pageReader, converter, VersionParser.parse(Version.FULL_VERSION));    for (int i = 0; i < rows; i++) {        assertEquals(0, columnReader.getCurrentRepetitionLevel());        assertEquals(0, columnReader.getCurrentDefinitionLevel());        columnReader.writeCurrentValueToConverter();        columnReader.consume();    }    assertEquals(rows, converter.count);}
public void parquet-mr_f3189_0() throws Exception
{    MessageType schema = MessageTypeParser.parseMessageType("message test { optional binary foo; }");    ColumnDescriptor col = schema.getColumns().get(0);    MemPageWriter pageWriter = new MemPageWriter();    ColumnWriterV2 columnWriterV2 = new ColumnWriterV2(col, pageWriter, ParquetProperties.builder().withDictionaryPageSize(1024).withWriterVersion(PARQUET_2_0).withPageSize(2048).build());    for (int i = 0; i < rows; i++) {        columnWriterV2.writeNull(0, 0);        if ((i + 1) % 1000 == 0) {            columnWriterV2.writePage();        }    }    columnWriterV2.writePage();    columnWriterV2.finalizeColumnChunk();    List<DataPage> pages = pageWriter.getPages();    int valueCount = 0;    int rowCount = 0;    for (DataPage dataPage : pages) {        valueCount += dataPage.getValueCount();        rowCount += ((DataPageV2) dataPage).getRowCount();    }    assertEquals(rows, rowCount);    assertEquals(rows, valueCount);    MemPageReader pageReader = new MemPageReader((long) rows, pages.iterator(), pageWriter.getDictionaryPage());    ValidatingConverter converter = new ValidatingConverter();    ColumnReader columnReader = new ColumnReaderImpl(col, pageReader, converter, VersionParser.parse(Version.FULL_VERSION));    for (int i = 0; i < rows; i++) {        assertEquals(0, columnReader.getCurrentRepetitionLevel());        assertEquals(0, columnReader.getCurrentDefinitionLevel());        columnReader.consume();    }    assertEquals(0, converter.count);}
public void parquet-mr_f3190_0()
{    assertTrue(CorruptDeltaByteArrays.requiresSequentialReads("parquet-mr version 1.6.0 (build abcd)", Encoding.DELTA_BYTE_ARRAY));    assertTrue(CorruptDeltaByteArrays.requiresSequentialReads((String) null, Encoding.DELTA_BYTE_ARRAY));    assertTrue(CorruptDeltaByteArrays.requiresSequentialReads((ParsedVersion) null, Encoding.DELTA_BYTE_ARRAY));    assertTrue(CorruptDeltaByteArrays.requiresSequentialReads((SemanticVersion) null, Encoding.DELTA_BYTE_ARRAY));    assertTrue(CorruptDeltaByteArrays.requiresSequentialReads("parquet-mr version 1.8.0-SNAPSHOT (build abcd)", Encoding.DELTA_BYTE_ARRAY));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads("parquet-mr version 1.6.0 (build abcd)", Encoding.DELTA_BINARY_PACKED));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads((String) null, Encoding.DELTA_LENGTH_BYTE_ARRAY));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads((ParsedVersion) null, Encoding.PLAIN));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads((SemanticVersion) null, Encoding.RLE));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads("parquet-mr version 1.8.0-SNAPSHOT (build abcd)", Encoding.RLE_DICTIONARY));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads("parquet-mr version 1.8.0-SNAPSHOT (build abcd)", Encoding.PLAIN_DICTIONARY));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads("parquet-mr version 1.8.0-SNAPSHOT (build abcd)", Encoding.BIT_PACKED));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads("parquet-mr version 1.8.0 (build abcd)", Encoding.DELTA_BYTE_ARRAY));}
public void parquet-mr_f3191_0()
{    ParsedVersion impala = new ParsedVersion("impala", "1.2.0", "abcd");    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads(impala, Encoding.DELTA_BYTE_ARRAY));    ParsedVersion broken = new ParsedVersion("parquet-mr", "1.8.0-SNAPSHOT", "abcd");    assertTrue(CorruptDeltaByteArrays.requiresSequentialReads(broken, Encoding.DELTA_BYTE_ARRAY));    ParsedVersion fixed = new ParsedVersion("parquet-mr", "1.8.0", "abcd");    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads(fixed, Encoding.DELTA_BYTE_ARRAY));}
private DeltaByteArrayWriter parquet-mr_f3192_0()
{    return new DeltaByteArrayWriter(10, 100, new HeapByteBufferAllocator());}
public void parquet-mr_f3193_0() throws Exception
{    DeltaByteArrayWriter writer = getDeltaByteArrayWriter();    String lastValue = null;    for (int i = 0; i < 10; i += 1) {        lastValue = str(i);        writer.writeBytes(Binary.fromString(lastValue));    }    ByteBuffer firstPageBytes = writer.getBytes().toByteBuffer();        writer.reset();    corruptWriter(writer, lastValue);    for (int i = 10; i < 20; i += 1) {        writer.writeBytes(Binary.fromString(str(i)));    }    ByteBuffer corruptPageBytes = writer.getBytes().toByteBuffer();    DeltaByteArrayReader firstPageReader = new DeltaByteArrayReader();    firstPageReader.initFromPage(10, ByteBufferInputStream.wrap(firstPageBytes));    for (int i = 0; i < 10; i += 1) {        assertEquals(str(i), firstPageReader.readBytes().toStringUsingUTF8());    }    DeltaByteArrayReader corruptPageReader = new DeltaByteArrayReader();    corruptPageReader.initFromPage(10, ByteBufferInputStream.wrap(corruptPageBytes));    try {        corruptPageReader.readBytes();        fail("Corrupt page did not throw an exception when read");    } catch (ArrayIndexOutOfBoundsException e) {        }    DeltaByteArrayReader secondPageReader = new DeltaByteArrayReader();    secondPageReader.initFromPage(10, ByteBufferInputStream.wrap(corruptPageBytes));    secondPageReader.setPreviousReader(firstPageReader);    for (int i = 10; i < 20; i += 1) {        assertEquals(secondPageReader.readBytes().toStringUsingUTF8(), str(i));    }}
public void parquet-mr_f3194_0() throws Exception
{    DeltaByteArrayWriter writer = getDeltaByteArrayWriter();    for (int i = 0; i < 10; i += 1) {        writer.writeBytes(Binary.fromString(str(i)));    }    ByteBuffer firstPageBytes = writer.getBytes().toByteBuffer();        writer.reset();    for (int i = 10; i < 20; i += 1) {        writer.writeBytes(Binary.fromString(str(i)));    }    ByteBuffer secondPageBytes = writer.getBytes().toByteBuffer();    DeltaByteArrayReader firstPageReader = new DeltaByteArrayReader();    firstPageReader.initFromPage(10, ByteBufferInputStream.wrap(firstPageBytes));    for (int i = 0; i < 10; i += 1) {        assertEquals(firstPageReader.readBytes().toStringUsingUTF8(), str(i));    }    DeltaByteArrayReader secondPageReader = new DeltaByteArrayReader();    secondPageReader.initFromPage(10, ByteBufferInputStream.wrap(secondPageBytes));    secondPageReader.setPreviousReader(firstPageReader);    for (int i = 10; i < 20; i += 1) {        assertEquals(secondPageReader.readBytes().toStringUsingUTF8(), str(i));    }}
public void parquet-mr_f3195_0() throws Exception
{    DeltaByteArrayWriter writer = getDeltaByteArrayWriter();    for (int i = 0; i < 10; i += 1) {        writer.writeBytes(Binary.fromString(str(i)));    }    ByteBuffer firstPageBytes = writer.getBytes().toByteBuffer();        writer.reset();    for (int i = 10; i < 20; i += 1) {        writer.writeBytes(Binary.fromString(str(i)));    }    ByteBuffer secondPageBytes = writer.getBytes().toByteBuffer();    DeltaByteArrayReader firstPageReader = new DeltaByteArrayReader();    firstPageReader.initFromPage(10, ByteBufferInputStream.wrap(firstPageBytes));    for (int i = 0; i < 10; i += 1) {        assertEquals(firstPageReader.readBytes().toStringUsingUTF8(), str(i));    }    DeltaByteArrayReader secondPageReader = new DeltaByteArrayReader();    secondPageReader.initFromPage(10, ByteBufferInputStream.wrap(secondPageBytes));    for (int i = 10; i < 20; i += 1) {        assertEquals(secondPageReader.readBytes().toStringUsingUTF8(), str(i));    }}
public void parquet-mr_f3196_0() throws Exception
{    ColumnDescriptor column = new ColumnDescriptor(new String[] { "s" }, PrimitiveType.PrimitiveTypeName.BINARY, 0, 0);    MemPageStore pages = new MemPageStore(0);    PageWriter memWriter = pages.getPageWriter(column);    ParquetProperties parquetProps = ParquetProperties.builder().withDictionaryEncoding(false).build();        ValuesWriter rdValues = parquetProps.newDefinitionLevelWriter(column);    for (int i = 0; i < 10; i += 1) {        rdValues.writeInteger(0);    }        BytesInput rd = BytesInput.from(rdValues.getBytes().toByteArray());    DeltaByteArrayWriter writer = getDeltaByteArrayWriter();    String lastValue = null;    List<String> values = new ArrayList<String>();    for (int i = 0; i < 10; i += 1) {        lastValue = str(i);        writer.writeBytes(Binary.fromString(lastValue));        values.add(lastValue);    }    memWriter.writePage(BytesInput.concat(rd, rd, writer.getBytes()), 10, /* number of values in the page */    new BinaryStatistics(), rdValues.getEncoding(), rdValues.getEncoding(), writer.getEncoding());    pages.addRowCount(10);        writer.reset();    corruptWriter(writer, lastValue);    for (int i = 10; i < 20; i += 1) {        String value = str(i);        writer.writeBytes(Binary.fromString(value));        values.add(value);    }    memWriter.writePage(BytesInput.concat(rd, rd, writer.getBytes()), 10, /* number of values in the page */    new BinaryStatistics(), rdValues.getEncoding(), rdValues.getEncoding(), writer.getEncoding());    pages.addRowCount(10);    final List<String> actualValues = new ArrayList<String>();    PrimitiveConverter converter = new PrimitiveConverter() {        @Override        public void addBinary(Binary value) {            actualValues.add(value.toStringUsingUTF8());        }    };    ColumnReaderImpl columnReader = new ColumnReaderImpl(column, pages.getPageReader(column), converter, new ParsedVersion("parquet-mr", "1.6.0", "abcd"));    while (actualValues.size() < columnReader.getTotalValueCount()) {        columnReader.writeCurrentValueToConverter();        columnReader.consume();    }    Assert.assertEquals(values, actualValues);}
public void parquet-mr_f3197_0(Binary value)
{    actualValues.add(value.toStringUsingUTF8());}
public void parquet-mr_f3198_0(DeltaByteArrayWriter writer, String data) throws Exception
{    Field previous = writer.getClass().getDeclaredField("previous");    previous.setAccessible(true);    previous.set(writer, Binary.fromString(data).getBytesUnsafe());}
public String parquet-mr_f3199_0(int i)
{    char c = 'a';    return "aaaaaaaaaaa" + (char) (c + i);}
public void parquet-mr_f3200_0() throws Exception
{    MessageType schema = MessageTypeParser.parseMessageType("message msg { required group foo { required int64 bar; } }");    ColumnDescriptor path = schema.getColumnDescription(new String[] { "foo", "bar" });    MemPageStore memPageStore = new MemPageStore(10);    ColumnWriteStoreV1 memColumnsStore = newColumnWriteStoreImpl(memPageStore);    ColumnWriter columnWriter = memColumnsStore.getColumnWriter(path);    columnWriter.write(42l, 0, 0);    memColumnsStore.endRecord();    memColumnsStore.flush();    ColumnReader columnReader = getColumnReader(memPageStore, path, schema);    for (int i = 0; i < columnReader.getTotalValueCount(); i++) {        assertEquals(columnReader.getCurrentRepetitionLevel(), 0);        assertEquals(columnReader.getCurrentDefinitionLevel(), 0);        assertEquals(columnReader.getLong(), 42);        columnReader.consume();    }}
private ColumnWriter parquet-mr_f3201_0(ColumnDescriptor path, MemPageStore memPageStore)
{    ColumnWriteStoreV1 memColumnsStore = newColumnWriteStoreImpl(memPageStore);    ColumnWriter columnWriter = memColumnsStore.getColumnWriter(path);    return columnWriter;}
private ColumnReader parquet-mr_f3202_0(MemPageStore memPageStore, ColumnDescriptor path, MessageType schema)
{    return new ColumnReadStoreImpl(memPageStore, new DummyRecordConverter(schema).getRootConverter(), schema, null).getColumnReader(path);}
public void parquet-mr_f3203_0() throws Exception
{    MessageType mt = MessageTypeParser.parseMessageType("message msg { required group foo { required binary bar; } }");    String[] col = new String[] { "foo", "bar" };    MemPageStore memPageStore = new MemPageStore(10);    ColumnWriteStoreV1 memColumnsStore = newColumnWriteStoreImpl(memPageStore);    ColumnDescriptor path1 = mt.getColumnDescription(col);    ColumnDescriptor path = path1;    ColumnWriter columnWriter = memColumnsStore.getColumnWriter(path);    columnWriter.write(Binary.fromString("42"), 0, 0);    memColumnsStore.endRecord();    memColumnsStore.flush();    ColumnReader columnReader = getColumnReader(memPageStore, path, mt);    for (int i = 0; i < columnReader.getTotalValueCount(); i++) {        assertEquals(columnReader.getCurrentRepetitionLevel(), 0);        assertEquals(columnReader.getCurrentDefinitionLevel(), 0);        assertEquals(columnReader.getBinary().toStringUsingUTF8(), "42");        columnReader.consume();    }}
public void parquet-mr_f3204_0() throws Exception
{    MessageType mt = MessageTypeParser.parseMessageType("message msg { required group foo { required int64 bar; } }");    String[] col = new String[] { "foo", "bar" };    MemPageStore memPageStore = new MemPageStore(10);    ColumnWriteStoreV1 memColumnsStore = newColumnWriteStoreImpl(memPageStore);    ColumnDescriptor path1 = mt.getColumnDescription(col);    ColumnDescriptor path = path1;    ColumnWriter columnWriter = memColumnsStore.getColumnWriter(path);    for (int i = 0; i < 2000; i++) {        columnWriter.write(42l, 0, 0);        memColumnsStore.endRecord();    }    memColumnsStore.flush();    ColumnReader columnReader = getColumnReader(memPageStore, path, mt);    for (int i = 0; i < columnReader.getTotalValueCount(); i++) {        assertEquals(columnReader.getCurrentRepetitionLevel(), 0);        assertEquals(columnReader.getCurrentDefinitionLevel(), 0);        assertEquals(columnReader.getLong(), 42);        columnReader.consume();    }}
public void parquet-mr_f3205_1() throws Exception
{    MessageType mt = MessageTypeParser.parseMessageType("message msg { repeated group foo { repeated int64 bar; } }");    String[] col = new String[] { "foo", "bar" };    MemPageStore memPageStore = new MemPageStore(10);    ColumnWriteStoreV1 memColumnsStore = newColumnWriteStoreImpl(memPageStore);    ColumnDescriptor path1 = mt.getColumnDescription(col);    ColumnDescriptor path = path1;    ColumnWriter columnWriter = memColumnsStore.getColumnWriter(path);    int[] rs = { 0, 0, 0, 1, 1, 1, 2, 2, 2 };    int[] ds = { 0, 1, 2, 0, 1, 2, 0, 1, 2 };    for (int i = 0; i < 837; i++) {        int r = rs[i % rs.length];        int d = ds[i % ds.length];                if (i != 0 && r == 0) {            memColumnsStore.endRecord();        }        if (d == 2) {            columnWriter.write((long) i, r, d);        } else {            columnWriter.writeNull(r, d);        }    }    memColumnsStore.endRecord();    memColumnsStore.flush();    ColumnReader columnReader = getColumnReader(memPageStore, path, mt);    int i = 0;    for (int j = 0; j < columnReader.getTotalValueCount(); j++) {        int r = rs[i % rs.length];        int d = ds[i % ds.length];                assertEquals("r row " + i, r, columnReader.getCurrentRepetitionLevel());        assertEquals("d row " + i, d, columnReader.getCurrentDefinitionLevel());        if (d == 2) {            assertEquals("data row " + i, (long) i, columnReader.getLong());        }        columnReader.consume();        ++i;    }}
public void parquet-mr_f3206_1()
{    MessageType schema = Types.buildMessage().requiredList().requiredElement(BINARY).named("binary_col").requiredList().requiredElement(INT32).named("int32_col").named("msg");    System.out.println(schema);    MemPageStore memPageStore = new MemPageStore(123);        ColumnWriteStore writeStore = new ColumnWriteStoreV2(schema, memPageStore, ParquetProperties.builder().withPageSize(    1024).withMinRowCountForPageSizeCheck(    1).withPageRowCountLimit(10).withDictionaryEncoding(    false).build());    ColumnDescriptor binaryCol = schema.getColumnDescription(new String[] { "binary_col", "list", "element" });    ColumnWriter binaryColWriter = writeStore.getColumnWriter(binaryCol);    ColumnDescriptor int32Col = schema.getColumnDescription(new String[] { "int32_col", "list", "element" });    ColumnWriter int32ColWriter = writeStore.getColumnWriter(int32Col);        for (int i = 0; i < 123; ++i) {                for (int j = 0; j < 10; ++j) {            binaryColWriter.write(Binary.fromString("aaaaaaaaaaaa"), j == 0 ? 0 : 2, 2);            int32ColWriter.write(42, j == 0 ? 0 : 2, 2);        }        writeStore.endRecord();    }    writeStore.flush();        {        PageReader binaryColPageReader = memPageStore.getPageReader(binaryCol);        assertEquals(1230, binaryColPageReader.getTotalValueCount());        int pageCnt = 0;        int valueCnt = 0;        while (valueCnt < binaryColPageReader.getTotalValueCount()) {            DataPage page = binaryColPageReader.readPage();            ++pageCnt;            valueCnt += page.getValueCount();                        assertTrue("Compressed size should be less than 1024", page.getCompressedSize() <= 1024);        }    }        {        PageReader int32ColPageReader = memPageStore.getPageReader(int32Col);        assertEquals(1230, int32ColPageReader.getTotalValueCount());        int pageCnt = 0;        int valueCnt = 0;        while (valueCnt < int32ColPageReader.getTotalValueCount()) {            DataPage page = int32ColPageReader.readPage();            ++pageCnt;            valueCnt += page.getValueCount();                        assertTrue("Row count should be less than 10", page.getIndexRowCount().get() <= 10);        }    }}
private ColumnWriteStoreV1 parquet-mr_f3207_0(MemPageStore memPageStore)
{    return new ColumnWriteStoreV1(memPageStore, ParquetProperties.builder().withPageSize(2048).withDictionaryEncoding(false).build());}
public void parquet-mr_f3208_0() throws IOException
{    MemPageStore memPageStore = new MemPageStore(10);    ColumnDescriptor col = new ColumnDescriptor(path, PrimitiveTypeName.INT64, 2, 2);    LongStatistics stats = new LongStatistics();    PageWriter pageWriter = memPageStore.getPageWriter(col);    pageWriter.writePage(BytesInput.from(new byte[735]), 209, stats, BIT_PACKED, BIT_PACKED, PLAIN);    pageWriter.writePage(BytesInput.from(new byte[743]), 209, stats, BIT_PACKED, BIT_PACKED, PLAIN);    pageWriter.writePage(BytesInput.from(new byte[743]), 209, stats, BIT_PACKED, BIT_PACKED, PLAIN);    pageWriter.writePage(BytesInput.from(new byte[735]), 209, stats, BIT_PACKED, BIT_PACKED, PLAIN);    PageReader pageReader = memPageStore.getPageReader(col);    long totalValueCount = pageReader.getTotalValueCount();    System.out.println(totalValueCount);    int total = 0;    do {        DataPage readPage = pageReader.readPage();        total += readPage.getValueCount();        System.out.println(readPage);        } while (total < totalValueCount);}
public long parquet-mr_f3209_0()
{    return totalValueCount;}
public DataPage parquet-mr_f3210_1()
{    if (pages.hasNext()) {        DataPage next = pages.next();                return next;    } else {        throw new ParquetDecodingException("after last page");    }}
public DictionaryPage parquet-mr_f3211_0()
{    return dictionaryPage;}
public PageWriter parquet-mr_f3212_0(ColumnDescriptor path)
{    MemPageWriter pageWriter = pageWriters.get(path);    if (pageWriter == null) {        pageWriter = new MemPageWriter();        pageWriters.put(path, pageWriter);    }    return pageWriter;}
public PageReader parquet-mr_f3213_1(ColumnDescriptor descriptor)
{    MemPageWriter pageWriter = pageWriters.get(descriptor);    if (pageWriter == null) {        throw new UnknownColumnException(descriptor);    }    List<DataPage> pages = new ArrayList<DataPage>(pageWriter.getPages());        return new MemPageReader(pageWriter.getTotalValueCount(), pages.iterator(), pageWriter.getDictionaryPage());}
public long parquet-mr_f3214_0()
{    return rowCount;}
public void parquet-mr_f3215_0(long count)
{    rowCount += count;}
public void parquet-mr_f3216_1(BytesInput bytesInput, int valueCount, Statistics statistics, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{    if (valueCount == 0) {        throw new ParquetEncodingException("illegal page of 0 values");    }    memSize += bytesInput.size();    pages.add(new DataPageV1(BytesInput.copy(bytesInput), valueCount, (int) bytesInput.size(), statistics, rlEncoding, dlEncoding, valuesEncoding));    totalValueCount += valueCount;    }
public void parquet-mr_f3217_0(BytesInput bytesInput, int valueCount, int rowCount, Statistics<?> statistics, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{    writePage(bytesInput, valueCount, statistics, rlEncoding, dlEncoding, valuesEncoding);}
public void parquet-mr_f3218_1(int rowCount, int nullCount, int valueCount, BytesInput repetitionLevels, BytesInput definitionLevels, Encoding dataEncoding, BytesInput data, Statistics<?> statistics) throws IOException
{    if (valueCount == 0) {        throw new ParquetEncodingException("illegal page of 0 values");    }    long size = repetitionLevels.size() + definitionLevels.size() + data.size();    memSize += size;    pages.add(DataPageV2.uncompressed(rowCount, nullCount, valueCount, copy(repetitionLevels), copy(definitionLevels), dataEncoding, copy(data), statistics));    totalValueCount += valueCount;    }
public long parquet-mr_f3219_0()
{    return memSize;}
public List<DataPage> parquet-mr_f3220_0()
{    return pages;}
public DictionaryPage parquet-mr_f3221_0()
{    return dictionaryPage;}
public long parquet-mr_f3222_0()
{    return totalValueCount;}
public long parquet-mr_f3223_0()
{        return memSize;}
public void parquet-mr_f3224_1(DictionaryPage dictionaryPage) throws IOException
{    if (this.dictionaryPage != null) {        throw new ParquetEncodingException("Only one dictionary page per block");    }    this.memSize += dictionaryPage.getBytes().size();    this.dictionaryPage = dictionaryPage.copy();    }
public String parquet-mr_f3225_0(String prefix)
{    return String.format("%s %,d bytes", prefix, memSize);}
public void parquet-mr_f3226_0()
{    IntStatistics stats = new IntStatistics();    assertTrue(stats.isNumNullsSet());    assertEquals(stats.getNumNulls(), 0);    stats.incrementNumNulls();    stats.incrementNumNulls();    stats.incrementNumNulls();    stats.incrementNumNulls();    assertEquals(stats.getNumNulls(), 4);    stats.incrementNumNulls(5);    assertEquals(stats.getNumNulls(), 9);    stats.setNumNulls(22);    assertEquals(stats.getNumNulls(), 22);}
public void parquet-mr_f3227_0()
{        integerArray = new int[] { 1, 3, 14, 54, 66, 8, 0, 23, 54 };    IntStatistics stats = new IntStatistics();    for (int i : integerArray) {        stats.updateStats(i);    }    assertEquals(stats.getMax(), 66);    assertEquals(stats.getMin(), 0);        integerArray = new int[] { -11, 3, -14, 54, -66, 8, 0, -23, 54 };    IntStatistics statsNeg = new IntStatistics();    for (int i : integerArray) {        statsNeg.updateStats(i);    }    assertEquals(statsNeg.getMax(), 54);    assertEquals(statsNeg.getMin(), -66);    assertTrue(statsNeg.compareMaxToValue(55) < 0);    assertTrue(statsNeg.compareMaxToValue(54) == 0);    assertTrue(statsNeg.compareMaxToValue(5) > 0);    assertTrue(statsNeg.compareMinToValue(0) < 0);    assertTrue(statsNeg.compareMinToValue(-66) == 0);    assertTrue(statsNeg.compareMinToValue(-67) > 0);        byte[] intMaxBytes = statsNeg.getMaxBytes();    byte[] intMinBytes = statsNeg.getMinBytes();    assertEquals(ByteBuffer.wrap(intMaxBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getInt(), 54);    assertEquals(ByteBuffer.wrap(intMinBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getInt(), -66);    IntStatistics statsFromBytes = new IntStatistics();    statsFromBytes.setMinMaxFromBytes(intMinBytes, intMaxBytes);    assertEquals(statsFromBytes.getMax(), 54);    assertEquals(statsFromBytes.getMin(), -66);    integerArray = new int[] { Integer.MAX_VALUE, Integer.MIN_VALUE };    IntStatistics minMaxValues = new IntStatistics();    for (int i : integerArray) {        minMaxValues.updateStats(i);    }    assertEquals(minMaxValues.getMax(), Integer.MAX_VALUE);    assertEquals(minMaxValues.getMin(), Integer.MIN_VALUE);        byte[] intMaxBytesMinMax = minMaxValues.getMaxBytes();    byte[] intMinBytesMinMax = minMaxValues.getMinBytes();    assertEquals(ByteBuffer.wrap(intMaxBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getInt(), Integer.MAX_VALUE);    assertEquals(ByteBuffer.wrap(intMinBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getInt(), Integer.MIN_VALUE);    IntStatistics statsFromBytesMinMax = new IntStatistics();    statsFromBytesMinMax.setMinMaxFromBytes(intMinBytesMinMax, intMaxBytesMinMax);    assertEquals(statsFromBytesMinMax.getMax(), Integer.MAX_VALUE);    assertEquals(statsFromBytesMinMax.getMin(), Integer.MIN_VALUE);        assertEquals(stats.toString(), "min: 0, max: 66, num_nulls: 0");}
public void parquet-mr_f3228_0()
{        longArray = new long[] { 9, 39, 99, 3, 0, 12, 1000, 65, 542 };    LongStatistics stats = new LongStatistics();    for (long l : longArray) {        stats.updateStats(l);    }    assertEquals(stats.getMax(), 1000);    assertEquals(stats.getMin(), 0);        longArray = new long[] { -101, 993, -9914, 54, -9, 89, 0, -23, 90 };    LongStatistics statsNeg = new LongStatistics();    for (long l : longArray) {        statsNeg.updateStats(l);    }    assertEquals(statsNeg.getMax(), 993);    assertEquals(statsNeg.getMin(), -9914);    assertTrue(statsNeg.compareMaxToValue(994) < 0);    assertTrue(statsNeg.compareMaxToValue(993) == 0);    assertTrue(statsNeg.compareMaxToValue(-1000) > 0);    assertTrue(statsNeg.compareMinToValue(10000) < 0);    assertTrue(statsNeg.compareMinToValue(-9914) == 0);    assertTrue(statsNeg.compareMinToValue(-9915) > 0);        byte[] longMaxBytes = statsNeg.getMaxBytes();    byte[] longMinBytes = statsNeg.getMinBytes();    assertEquals(ByteBuffer.wrap(longMaxBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getLong(), 993);    assertEquals(ByteBuffer.wrap(longMinBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getLong(), -9914);    LongStatistics statsFromBytes = new LongStatistics();    statsFromBytes.setMinMaxFromBytes(longMinBytes, longMaxBytes);    assertEquals(statsFromBytes.getMax(), 993);    assertEquals(statsFromBytes.getMin(), -9914);    longArray = new long[] { Long.MAX_VALUE, Long.MIN_VALUE };    LongStatistics minMaxValues = new LongStatistics();    for (long l : longArray) {        minMaxValues.updateStats(l);    }    assertEquals(minMaxValues.getMax(), Long.MAX_VALUE);    assertEquals(minMaxValues.getMin(), Long.MIN_VALUE);        byte[] longMaxBytesMinMax = minMaxValues.getMaxBytes();    byte[] longMinBytesMinMax = minMaxValues.getMinBytes();    assertEquals(ByteBuffer.wrap(longMaxBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getLong(), Long.MAX_VALUE);    assertEquals(ByteBuffer.wrap(longMinBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getLong(), Long.MIN_VALUE);    LongStatistics statsFromBytesMinMax = new LongStatistics();    statsFromBytesMinMax.setMinMaxFromBytes(longMinBytesMinMax, longMaxBytesMinMax);    assertEquals(statsFromBytesMinMax.getMax(), Long.MAX_VALUE);    assertEquals(statsFromBytesMinMax.getMin(), Long.MIN_VALUE);        assertEquals(stats.toString(), "min: 0, max: 1000, num_nulls: 0");}
public void parquet-mr_f3229_0()
{        floatArray = new float[] { 1.5f, 44.5f, 412.99f, 0.65f, 5.6f, 100.6f, 0.0001f, 23.0f, 553.6f };    FloatStatistics stats = new FloatStatistics();    for (float f : floatArray) {        stats.updateStats(f);    }    assertEquals(stats.getMax(), 553.6f, 1e-10);    assertEquals(stats.getMin(), 0.0001f, 1e-10);        floatArray = new float[] { -1.5f, -44.5f, -412.99f, 0.65f, -5.6f, -100.6f, 0.0001f, -23.0f, -3.6f };    FloatStatistics statsNeg = new FloatStatistics();    for (float f : floatArray) {        statsNeg.updateStats(f);    }    assertEquals(statsNeg.getMax(), 0.65f, 1e-10);    assertEquals(statsNeg.getMin(), -412.99f, 1e-10);    assertTrue(statsNeg.compareMaxToValue(1) < 0);    assertTrue(statsNeg.compareMaxToValue(0.65F) == 0);    assertTrue(statsNeg.compareMaxToValue(0.649F) > 0);    assertTrue(statsNeg.compareMinToValue(-412.98F) < 0);    assertTrue(statsNeg.compareMinToValue(-412.99F) == 0);    assertTrue(statsNeg.compareMinToValue(-450) > 0);        byte[] floatMaxBytes = statsNeg.getMaxBytes();    byte[] floatMinBytes = statsNeg.getMinBytes();    assertEquals(ByteBuffer.wrap(floatMaxBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getFloat(), 0.65f, 1e-10);    assertEquals(ByteBuffer.wrap(floatMinBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getFloat(), -412.99f, 1e-10);    FloatStatistics statsFromBytes = new FloatStatistics();    statsFromBytes.setMinMaxFromBytes(floatMinBytes, floatMaxBytes);    assertEquals(statsFromBytes.getMax(), 0.65f, 1e-10);    assertEquals(statsFromBytes.getMin(), -412.99f, 1e-10);    floatArray = new float[] { Float.MAX_VALUE, Float.MIN_VALUE };    FloatStatistics minMaxValues = new FloatStatistics();    for (float f : floatArray) {        minMaxValues.updateStats(f);    }    assertEquals(minMaxValues.getMax(), Float.MAX_VALUE, 1e-10);    assertEquals(minMaxValues.getMin(), Float.MIN_VALUE, 1e-10);        byte[] floatMaxBytesMinMax = minMaxValues.getMaxBytes();    byte[] floatMinBytesMinMax = minMaxValues.getMinBytes();    assertEquals(ByteBuffer.wrap(floatMaxBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getFloat(), Float.MAX_VALUE, 1e-10);    assertEquals(ByteBuffer.wrap(floatMinBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getFloat(), Float.MIN_VALUE, 1e-10);    FloatStatistics statsFromBytesMinMax = new FloatStatistics();    statsFromBytesMinMax.setMinMaxFromBytes(floatMinBytesMinMax, floatMaxBytesMinMax);    assertEquals(statsFromBytesMinMax.getMax(), Float.MAX_VALUE, 1e-10);    assertEquals(statsFromBytesMinMax.getMin(), Float.MIN_VALUE, 1e-10);        assertEquals("min: 1.0E-4, max: 553.6, num_nulls: 0", stats.toString());}
public void parquet-mr_f3230_0()
{        doubleArray = new double[] { 81.5d, 944.5f, 2.002d, 334.5d, 5.6d, 0.001d, 0.00001d, 23.0d, 553.6d };    DoubleStatistics stats = new DoubleStatistics();    for (double d : doubleArray) {        stats.updateStats(d);    }    assertEquals(stats.getMax(), 944.5d, 1e-10);    assertEquals(stats.getMin(), 0.00001d, 1e-10);        doubleArray = new double[] { -81.5d, -944.5d, 2.002d, -334.5d, -5.6d, -0.001d, -0.00001d, 23.0d, -3.6d };    DoubleStatistics statsNeg = new DoubleStatistics();    for (double d : doubleArray) {        statsNeg.updateStats(d);    }    assertEquals(statsNeg.getMax(), 23.0d, 1e-10);    assertEquals(statsNeg.getMin(), -944.5d, 1e-10);    assertTrue(statsNeg.compareMaxToValue(23.0001D) < 0);    assertTrue(statsNeg.compareMaxToValue(23D) == 0);    assertTrue(statsNeg.compareMaxToValue(0D) > 0);    assertTrue(statsNeg.compareMinToValue(-400D) < 0);    assertTrue(statsNeg.compareMinToValue(-944.5D) == 0);    assertTrue(statsNeg.compareMinToValue(-944.500001D) > 0);        byte[] doubleMaxBytes = statsNeg.getMaxBytes();    byte[] doubleMinBytes = statsNeg.getMinBytes();    assertEquals(ByteBuffer.wrap(doubleMaxBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getDouble(), 23.0d, 1e-10);    assertEquals(ByteBuffer.wrap(doubleMinBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getDouble(), -944.5d, 1e-10);    DoubleStatistics statsFromBytes = new DoubleStatistics();    statsFromBytes.setMinMaxFromBytes(doubleMinBytes, doubleMaxBytes);    assertEquals(statsFromBytes.getMax(), 23.0d, 1e-10);    assertEquals(statsFromBytes.getMin(), -944.5d, 1e-10);    doubleArray = new double[] { Double.MAX_VALUE, Double.MIN_VALUE };    DoubleStatistics minMaxValues = new DoubleStatistics();    for (double d : doubleArray) {        minMaxValues.updateStats(d);    }    assertEquals(minMaxValues.getMax(), Double.MAX_VALUE, 1e-10);    assertEquals(minMaxValues.getMin(), Double.MIN_VALUE, 1e-10);        byte[] doubleMaxBytesMinMax = minMaxValues.getMaxBytes();    byte[] doubleMinBytesMinMax = minMaxValues.getMinBytes();    assertEquals(ByteBuffer.wrap(doubleMaxBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getDouble(), Double.MAX_VALUE, 1e-10);    assertEquals(ByteBuffer.wrap(doubleMinBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getDouble(), Double.MIN_VALUE, 1e-10);    DoubleStatistics statsFromBytesMinMax = new DoubleStatistics();    statsFromBytesMinMax.setMinMaxFromBytes(doubleMinBytesMinMax, doubleMaxBytesMinMax);    assertEquals(statsFromBytesMinMax.getMax(), Double.MAX_VALUE, 1e-10);    assertEquals(statsFromBytesMinMax.getMin(), Double.MIN_VALUE, 1e-10);        assertEquals("min: 1.0E-5, max: 944.5, num_nulls: 0", stats.toString());}
public void parquet-mr_f3231_0()
{    Statistics<?> floatStats = Statistics.createStats(Types.optional(PrimitiveTypeName.FLOAT).named("test-float"));    floatStats.updateStats(123.456f);    Statistics<?> doubleStats = Statistics.createStats(Types.optional(PrimitiveTypeName.DOUBLE).named("test-double"));    doubleStats.updateStats(12345.6789);    Locale defaultLocale = Locale.getDefault();    try {                Locale.setDefault(Locale.FRENCH);        assertEquals("min: 123.456, max: 123.456, num_nulls: 0", floatStats.toString());        assertEquals("min: 12345.6789, max: 12345.6789, num_nulls: 0", doubleStats.toString());    } finally {        Locale.setDefault(defaultLocale);    }}
public void parquet-mr_f3232_0()
{        booleanArray = new boolean[] { true, true, true };    BooleanStatistics statsTrue = new BooleanStatistics();    for (boolean i : booleanArray) {        statsTrue.updateStats(i);    }    assertTrue(statsTrue.getMax());    assertTrue(statsTrue.getMin());        booleanArray = new boolean[] { false, false, false };    BooleanStatistics statsFalse = new BooleanStatistics();    for (boolean i : booleanArray) {        statsFalse.updateStats(i);    }    assertFalse(statsFalse.getMax());    assertFalse(statsFalse.getMin());    booleanArray = new boolean[] { false, true, false };    BooleanStatistics statsBoth = new BooleanStatistics();    for (boolean i : booleanArray) {        statsBoth.updateStats(i);    }    assertTrue(statsBoth.getMax());    assertFalse(statsBoth.getMin());        byte[] boolMaxBytes = statsBoth.getMaxBytes();    byte[] boolMinBytes = statsBoth.getMinBytes();    assertEquals((int) (boolMaxBytes[0] & 255), 1);    assertEquals((int) (boolMinBytes[0] & 255), 0);    BooleanStatistics statsFromBytes = new BooleanStatistics();    statsFromBytes.setMinMaxFromBytes(boolMinBytes, boolMaxBytes);    assertTrue(statsFromBytes.getMax());    assertFalse(statsFromBytes.getMin());        assertEquals(statsBoth.toString(), "min: false, max: true, num_nulls: 0");}
public void parquet-mr_f3233_0()
{        stringArray = new String[] { "hello", "world", "this", "is", "a", "test", "of", "the", "stats", "class" };    PrimitiveType type = Types.optional(PrimitiveTypeName.BINARY).as(OriginalType.UTF8).named("test_binary_utf8");    BinaryStatistics stats = (BinaryStatistics) Statistics.createStats(type);    for (String s : stringArray) {        stats.updateStats(Binary.fromString(s));    }    assertEquals(stats.genericGetMax(), Binary.fromString("world"));    assertEquals(stats.genericGetMin(), Binary.fromString("a"));        stringArray = new String[] { "", "", "", "", "" };    BinaryStatistics statsEmpty = (BinaryStatistics) Statistics.createStats(type);    for (String s : stringArray) {        statsEmpty.updateStats(Binary.fromString(s));    }    assertEquals(statsEmpty.genericGetMax(), Binary.fromString(""));    assertEquals(statsEmpty.genericGetMin(), Binary.fromString(""));        byte[] stringMaxBytes = stats.getMaxBytes();    byte[] stringMinBytes = stats.getMinBytes();    assertEquals(new String(stringMaxBytes), "world");    assertEquals(new String(stringMinBytes), "a");    BinaryStatistics statsFromBytes = (BinaryStatistics) Statistics.createStats(type);    statsFromBytes.setMinMaxFromBytes(stringMinBytes, stringMaxBytes);    assertEquals(statsFromBytes.genericGetMax(), Binary.fromString("world"));    assertEquals(statsFromBytes.genericGetMin(), Binary.fromString("a"));        assertEquals(stats.toString(), "min: a, max: world, num_nulls: 0");}
public void parquet-mr_f3234_0()
{    BinaryStatistics stats = new BinaryStatistics();    byte[] bytes = new byte[] { 10 };    final Binary value = Binary.fromReusedByteArray(bytes);    stats.updateStats(value);    bytes[0] = 20;    stats.updateStats(value);    bytes[0] = 15;    stats.updateStats(value);    assertArrayEquals(new byte[] { 20 }, stats.getMaxBytes());    assertArrayEquals(new byte[] { 10 }, stats.getMinBytes());}
public void parquet-mr_f3235_0()
{    testMergingIntStats();    testMergingLongStats();    testMergingFloatStats();    testMergingDoubleStats();    testMergingBooleanStats();    testMergingStringStats();}
private void parquet-mr_f3236_0()
{    integerArray = new int[] { 1, 2, 3, 4, 5 };    IntStatistics intStats = new IntStatistics();    for (int s : integerArray) {        intStats.updateStats(s);    }    integerArray = new int[] { 0, 3, 3 };    IntStatistics intStats2 = new IntStatistics();    for (int s : integerArray) {        intStats2.updateStats(s);    }    intStats.mergeStatistics(intStats2);    assertEquals(intStats.getMax(), 5);    assertEquals(intStats.getMin(), 0);    integerArray = new int[] { -1, -100, 100 };    IntStatistics intStats3 = new IntStatistics();    for (int s : integerArray) {        intStats3.updateStats(s);    }    intStats.mergeStatistics(intStats3);    assertEquals(intStats.getMax(), 100);    assertEquals(intStats.getMin(), -100);}
private void parquet-mr_f3237_0()
{    longArray = new long[] { 1l, 2l, 3l, 4l, 5l };    LongStatistics longStats = new LongStatistics();    for (long s : longArray) {        longStats.updateStats(s);    }    longArray = new long[] { 0l, 3l, 3l };    LongStatistics longStats2 = new LongStatistics();    for (long s : longArray) {        longStats2.updateStats(s);    }    longStats.mergeStatistics(longStats2);    assertEquals(longStats.getMax(), 5l);    assertEquals(longStats.getMin(), 0l);    longArray = new long[] { -1l, -100l, 100l };    LongStatistics longStats3 = new LongStatistics();    for (long s : longArray) {        longStats3.updateStats(s);    }    longStats.mergeStatistics(longStats3);    assertEquals(longStats.getMax(), 100l);    assertEquals(longStats.getMin(), -100l);}
private void parquet-mr_f3238_0()
{    floatArray = new float[] { 1.44f, 12.2f, 98.3f, 1.4f, 0.05f };    FloatStatistics floatStats = new FloatStatistics();    for (float s : floatArray) {        floatStats.updateStats(s);    }    floatArray = new float[] { 0.0001f, 9.9f, 3.1f };    FloatStatistics floatStats2 = new FloatStatistics();    for (float s : floatArray) {        floatStats2.updateStats(s);    }    floatStats.mergeStatistics(floatStats2);    assertEquals(floatStats.getMax(), 98.3f, 1e-10);    assertEquals(floatStats.getMin(), 0.0001f, 1e-10);    floatArray = new float[] { -1.91f, -100.9f, 100.54f };    FloatStatistics floatStats3 = new FloatStatistics();    for (float s : floatArray) {        floatStats3.updateStats(s);    }    floatStats.mergeStatistics(floatStats3);    assertEquals(floatStats.getMax(), 100.54f, 1e-10);    assertEquals(floatStats.getMin(), -100.9f, 1e-10);}
private void parquet-mr_f3239_0()
{    doubleArray = new double[] { 1.44d, 12.2d, 98.3d, 1.4d, 0.05d };    DoubleStatistics doubleStats = new DoubleStatistics();    for (double s : doubleArray) {        doubleStats.updateStats(s);    }    doubleArray = new double[] { 0.0001d, 9.9d, 3.1d };    DoubleStatistics doubleStats2 = new DoubleStatistics();    for (double s : doubleArray) {        doubleStats2.updateStats(s);    }    doubleStats.mergeStatistics(doubleStats2);    assertEquals(doubleStats.getMax(), 98.3d, 1e-10);    assertEquals(doubleStats.getMin(), 0.0001d, 1e-10);    doubleArray = new double[] { -1.91d, -100.9d, 100.54d };    DoubleStatistics doubleStats3 = new DoubleStatistics();    for (double s : doubleArray) {        doubleStats3.updateStats(s);    }    doubleStats.mergeStatistics(doubleStats3);    assertEquals(doubleStats.getMax(), 100.54d, 1e-10);    assertEquals(doubleStats.getMin(), -100.9d, 1e-10);}
private void parquet-mr_f3240_0()
{    booleanArray = new boolean[] { true, true, true };    BooleanStatistics booleanStats = new BooleanStatistics();    for (boolean s : booleanArray) {        booleanStats.updateStats(s);    }    booleanArray = new boolean[] { true, false };    BooleanStatistics booleanStats2 = new BooleanStatistics();    for (boolean s : booleanArray) {        booleanStats2.updateStats(s);    }    booleanStats.mergeStatistics(booleanStats2);    assertEquals(booleanStats.getMax(), true);    assertEquals(booleanStats.getMin(), false);    booleanArray = new boolean[] { false, false, false, false };    BooleanStatistics booleanStats3 = new BooleanStatistics();    for (boolean s : booleanArray) {        booleanStats3.updateStats(s);    }    booleanStats.mergeStatistics(booleanStats3);    assertEquals(booleanStats.getMax(), true);    assertEquals(booleanStats.getMin(), false);}
private void parquet-mr_f3241_0()
{    stringArray = new String[] { "hello", "world", "this", "is", "a", "test", "of", "the", "stats", "class" };    BinaryStatistics stats = new BinaryStatistics();    for (String s : stringArray) {        stats.updateStats(Binary.fromString(s));    }    stringArray = new String[] { "zzzz", "asdf", "testing" };    BinaryStatistics stats2 = new BinaryStatistics();    for (String s : stringArray) {        stats2.updateStats(Binary.fromString(s));    }    stats.mergeStatistics(stats2);    assertEquals(stats.getMax(), Binary.fromString("zzzz"));    assertEquals(stats.getMin(), Binary.fromString("a"));    stringArray = new String[] { "", "good", "testing" };    BinaryStatistics stats3 = new BinaryStatistics();    for (String s : stringArray) {        stats3.updateStats(Binary.fromString(s));    }    stats.mergeStatistics(stats3);    assertEquals(stats.getMax(), Binary.fromString("zzzz"));    assertEquals(stats.getMin(), Binary.fromString(""));}
public void parquet-mr_f3242_0()
{    testBuilder(Types.required(BOOLEAN).named("test_boolean"), false, new byte[] { 0 }, true, new byte[] { 1 });    testBuilder(Types.required(INT32).named("test_int32"), -42, intToBytes(-42), 42, intToBytes(42));    testBuilder(Types.required(INT64).named("test_int64"), -42l, longToBytes(-42), 42l, longToBytes(42));    testBuilder(Types.required(FLOAT).named("test_float"), -42.0f, intToBytes(floatToIntBits(-42.0f)), 42.0f, intToBytes(floatToIntBits(42.0f)));    testBuilder(Types.required(DOUBLE).named("test_double"), -42.0, longToBytes(doubleToLongBits(-42.0)), 42.0, longToBytes(Double.doubleToLongBits(42.0f)));    byte[] min = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 };    byte[] max = { 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24 };    testBuilder(Types.required(INT96).named("test_int96"), Binary.fromConstantByteArray(min), min, Binary.fromConstantByteArray(max), max);    testBuilder(Types.required(FIXED_LEN_BYTE_ARRAY).length(12).named("test_fixed"), Binary.fromConstantByteArray(min), min, Binary.fromConstantByteArray(max), max);    testBuilder(Types.required(BINARY).named("test_binary"), Binary.fromConstantByteArray(min), min, Binary.fromConstantByteArray(max), max);}
private void parquet-mr_f3243_0(PrimitiveType type, Object min, byte[] minBytes, Object max, byte[] maxBytes)
{    Statistics.Builder builder = Statistics.getBuilderForReading(type);    Statistics<?> stats = builder.build();    assertTrue(stats.isEmpty());    assertFalse(stats.isNumNullsSet());    assertFalse(stats.hasNonNullValue());    builder = Statistics.getBuilderForReading(type);    stats = builder.withNumNulls(0).withMin(minBytes).build();    assertFalse(stats.isEmpty());    assertTrue(stats.isNumNullsSet());    assertFalse(stats.hasNonNullValue());    assertEquals(0, stats.getNumNulls());    builder = Statistics.getBuilderForReading(type);    stats = builder.withNumNulls(11).withMax(maxBytes).build();    assertFalse(stats.isEmpty());    assertTrue(stats.isNumNullsSet());    assertFalse(stats.hasNonNullValue());    assertEquals(11, stats.getNumNulls());    builder = Statistics.getBuilderForReading(type);    stats = builder.withNumNulls(42).withMin(minBytes).withMax(maxBytes).build();    assertFalse(stats.isEmpty());    assertTrue(stats.isNumNullsSet());    assertTrue(stats.hasNonNullValue());    assertEquals(42, stats.getNumNulls());    assertEquals(min, stats.genericGetMin());    assertEquals(max, stats.genericGetMax());}
public void parquet-mr_f3244_0()
{    PrimitiveType type = Types.required(FLOAT).named("test_float");    Statistics.Builder builder = Statistics.getBuilderForReading(type);    Statistics<?> stats = builder.withMin(intToBytes(floatToIntBits(Float.NaN))).withMax(intToBytes(floatToIntBits(42.0f))).withNumNulls(0).build();    assertTrue(stats.isNumNullsSet());    assertEquals(0, stats.getNumNulls());    assertFalse(stats.hasNonNullValue());    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(intToBytes(floatToIntBits(-42.0f))).withMax(intToBytes(floatToIntBits(Float.NaN))).withNumNulls(11).build();    assertTrue(stats.isNumNullsSet());    assertEquals(11, stats.getNumNulls());    assertFalse(stats.hasNonNullValue());    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(intToBytes(floatToIntBits(Float.NaN))).withMax(intToBytes(floatToIntBits(Float.NaN))).withNumNulls(42).build();    assertTrue(stats.isNumNullsSet());    assertEquals(42, stats.getNumNulls());    assertFalse(stats.hasNonNullValue());    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(intToBytes(floatToIntBits(0.0f))).withMax(intToBytes(floatToIntBits(42.0f))).build();    assertEquals(0, Float.compare(-0.0f, (Float) stats.genericGetMin()));    assertEquals(0, Float.compare(42.0f, (Float) stats.genericGetMax()));    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(intToBytes(floatToIntBits(-42.0f))).withMax(intToBytes(floatToIntBits(-0.0f))).build();    assertEquals(0, Float.compare(-42.0f, (Float) stats.genericGetMin()));    assertEquals(0, Float.compare(0.0f, (Float) stats.genericGetMax()));    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(intToBytes(floatToIntBits(0.0f))).withMax(intToBytes(floatToIntBits(-0.0f))).build();    assertEquals(0, Float.compare(-0.0f, (Float) stats.genericGetMin()));    assertEquals(0, Float.compare(0.0f, (Float) stats.genericGetMax()));}
public void parquet-mr_f3245_0()
{    PrimitiveType type = Types.required(DOUBLE).named("test_double");    Statistics.Builder builder = Statistics.getBuilderForReading(type);    Statistics<?> stats = builder.withMin(longToBytes(doubleToLongBits(Double.NaN))).withMax(longToBytes(doubleToLongBits(42.0))).withNumNulls(0).build();    assertTrue(stats.isNumNullsSet());    assertEquals(0, stats.getNumNulls());    assertFalse(stats.hasNonNullValue());    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(longToBytes(doubleToLongBits(-42.0))).withMax(longToBytes(doubleToLongBits(Double.NaN))).withNumNulls(11).build();    assertTrue(stats.isNumNullsSet());    assertEquals(11, stats.getNumNulls());    assertFalse(stats.hasNonNullValue());    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(longToBytes(doubleToLongBits(Double.NaN))).withMax(longToBytes(doubleToLongBits(Double.NaN))).withNumNulls(42).build();    assertTrue(stats.isNumNullsSet());    assertEquals(42, stats.getNumNulls());    assertFalse(stats.hasNonNullValue());    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(longToBytes(doubleToLongBits(0.0))).withMax(longToBytes(doubleToLongBits(42.0))).build();    assertEquals(0, Double.compare(-0.0, (Double) stats.genericGetMin()));    assertEquals(0, Double.compare(42.0, (Double) stats.genericGetMax()));    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(longToBytes(doubleToLongBits(-42.0))).withMax(longToBytes(doubleToLongBits(-0.0))).build();    assertEquals(0, Double.compare(-42.0, (Double) stats.genericGetMin()));    assertEquals(0, Double.compare(0.0, (Double) stats.genericGetMax()));    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(longToBytes(doubleToLongBits(0.0))).withMax(longToBytes(doubleToLongBits(-0.0))).build();    assertEquals(0, Double.compare(-0.0, (Double) stats.genericGetMin()));    assertEquals(0, Double.compare(0.0, (Double) stats.genericGetMax()));}
private ColumnDescriptor parquet-mr_f3246_0(String... path)
{    return new ColumnDescriptor(path, PrimitiveType.PrimitiveTypeName.INT32, 0, 0);}
public void parquet-mr_f3247_0() throws Exception
{    assertEquals(column("a").compareTo(column("a")), 0);    assertEquals(column("a", "b").compareTo(column("a", "b")), 0);    assertEquals(column("a").compareTo(column("b")), -1);    assertEquals(column("b").compareTo(column("a")), 1);    assertEquals(column("a", "a").compareTo(column("a", "b")), -1);    assertEquals(column("b", "a").compareTo(column("a", "a")), 1);    assertEquals(column("a").compareTo(column("a", "b")), -1);    assertEquals(column("b").compareTo(column("a", "b")), 1);    assertEquals(column("a", "b").compareTo(column("a")), 1);    assertEquals(column("a", "b").compareTo(column("b")), -1);    assertEquals(column("").compareTo(column("")), 0);    assertEquals(column("").compareTo(column("a")), -1);    assertEquals(column("a").compareTo(column("")), 1);}
public void parquet-mr_f3248_0()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.withV2Pages();    builder.addDictEncoding(Encoding.PLAIN);    builder.addDataEncoding(Encoding.RLE_DICTIONARY, 3);    builder.addDataEncoding(Encoding.DELTA_BYTE_ARRAY);    builder.addDataEncoding(Encoding.DELTA_BYTE_ARRAY);    EncodingStats stats1 = builder.build();    Map<Encoding, Integer> expectedDictStats1 = new HashMap<Encoding, Integer>();    expectedDictStats1.put(Encoding.PLAIN, 1);    Map<Encoding, Integer> expectedDataStats1 = new HashMap<Encoding, Integer>();    expectedDataStats1.put(Encoding.RLE_DICTIONARY, 3);    expectedDataStats1.put(Encoding.DELTA_BYTE_ARRAY, 2);    builder.clear();    builder.addDataEncoding(Encoding.PLAIN);    builder.addDataEncoding(Encoding.PLAIN);    builder.addDataEncoding(Encoding.PLAIN);    builder.addDataEncoding(Encoding.PLAIN);    EncodingStats stats2 = builder.build();    Map<Encoding, Integer> expectedDictStats2 = new HashMap<Encoding, Integer>();    Map<Encoding, Integer> expectedDataStats2 = new HashMap<Encoding, Integer>();    expectedDataStats2.put(Encoding.PLAIN, 4);    assertEquals("Dictionary stats should be correct", expectedDictStats2, stats2.dictStats);    assertEquals("Data stats should be correct", expectedDataStats2, stats2.dataStats);    assertEquals("Dictionary stats should be correct after reuse", expectedDictStats1, stats1.dictStats);    assertEquals("Data stats should be correct after reuse", expectedDataStats1, stats1.dataStats);}
public void parquet-mr_f3249_0()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    EncodingStats stats = builder.build();    assertFalse(stats.usesV2Pages());    assertFalse("Should not have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertFalse("Should not have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertFalse("Should not have dictionary pages", stats.hasDictionaryPages());}
public void parquet-mr_f3250_0()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.addDictEncoding(Encoding.PLAIN_DICTIONARY);    EncodingStats stats = builder.build();    assertFalse(stats.usesV2Pages());    assertFalse("Should not have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertFalse("Should not have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertTrue("Should have dictionary pages", stats.hasDictionaryPages());}
public void parquet-mr_f3251_0()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.addDictEncoding(Encoding.PLAIN_DICTIONARY);    builder.addDataEncoding(Encoding.PLAIN_DICTIONARY);    builder.addDataEncoding(Encoding.PLAIN_DICTIONARY);    EncodingStats stats = builder.build();    assertFalse(stats.usesV2Pages());    assertTrue("Should have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertFalse("Should not have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertTrue("Should have dictionary pages", stats.hasDictionaryPages());}
public void parquet-mr_f3252_0()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.addDataEncoding(Encoding.PLAIN);    EncodingStats stats = builder.build();    assertFalse(stats.usesV2Pages());    assertFalse("Should not have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertTrue("Should have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertFalse("Should not have dictionary pages", stats.hasDictionaryPages());}
public void parquet-mr_f3253_0()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.addDictEncoding(Encoding.PLAIN_DICTIONARY);    builder.addDataEncoding(Encoding.PLAIN_DICTIONARY);    builder.addDataEncoding(Encoding.PLAIN_DICTIONARY);    builder.addDataEncoding(Encoding.PLAIN);    EncodingStats stats = builder.build();    assertFalse(stats.usesV2Pages());    assertTrue("Should have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertTrue("Should have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertTrue("Should have dictionary pages", stats.hasDictionaryPages());}
public void parquet-mr_f3254_0()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.withV2Pages();    builder.addDictEncoding(Encoding.PLAIN);    builder.addDataEncoding(Encoding.RLE_DICTIONARY);    EncodingStats stats = builder.build();    assertTrue(stats.usesV2Pages());    assertTrue("Should have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertFalse("Should not have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertTrue("Should have dictionary pages", stats.hasDictionaryPages());}
public void parquet-mr_f3255_0()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.withV2Pages();    builder.addDataEncoding(Encoding.DELTA_BINARY_PACKED);    builder.addDataEncoding(Encoding.DELTA_BINARY_PACKED);    EncodingStats stats = builder.build();    assertTrue(stats.usesV2Pages());    assertFalse("Should not have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertTrue("Should have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertFalse("Should not have dictionary pages", stats.hasDictionaryPages());}
public void parquet-mr_f3256_0()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.withV2Pages();    builder.addDictEncoding(Encoding.PLAIN);    builder.addDataEncoding(Encoding.RLE_DICTIONARY);    builder.addDataEncoding(Encoding.DELTA_BYTE_ARRAY);    builder.addDataEncoding(Encoding.DELTA_BYTE_ARRAY);    EncodingStats stats = builder.build();    assertTrue(stats.usesV2Pages());    assertTrue("Should have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertTrue("Should have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertTrue("Should have dictionary pages", stats.hasDictionaryPages());}
public void parquet-mr_f3257_0()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.withV2Pages();    builder.addDictEncoding(Encoding.PLAIN);    builder.addDataEncoding(Encoding.RLE_DICTIONARY, 4);    builder.addDataEncoding(Encoding.RLE_DICTIONARY);    builder.addDataEncoding(Encoding.DELTA_BYTE_ARRAY);    builder.addDataEncoding(Encoding.DELTA_BYTE_ARRAY);    EncodingStats stats = builder.build();    assertEquals("Count should match", 1, stats.getNumDictionaryPagesEncodedAs(Encoding.PLAIN));    assertEquals("Count should match", 0, stats.getNumDictionaryPagesEncodedAs(Encoding.PLAIN_DICTIONARY));    assertEquals("Count should match", 0, stats.getNumDictionaryPagesEncodedAs(Encoding.RLE));    assertEquals("Count should match", 0, stats.getNumDictionaryPagesEncodedAs(Encoding.BIT_PACKED));    assertEquals("Count should match", 0, stats.getNumDictionaryPagesEncodedAs(Encoding.DELTA_BYTE_ARRAY));    assertEquals("Count should match", 0, stats.getNumDictionaryPagesEncodedAs(Encoding.DELTA_BINARY_PACKED));    assertEquals("Count should match", 0, stats.getNumDictionaryPagesEncodedAs(Encoding.DELTA_LENGTH_BYTE_ARRAY));    assertEquals("Count should match", 5, stats.getNumDataPagesEncodedAs(Encoding.RLE_DICTIONARY));    assertEquals("Count should match", 2, stats.getNumDataPagesEncodedAs(Encoding.DELTA_BYTE_ARRAY));    assertEquals("Count should match", 0, stats.getNumDataPagesEncodedAs(Encoding.RLE));    assertEquals("Count should match", 0, stats.getNumDataPagesEncodedAs(Encoding.BIT_PACKED));    assertEquals("Count should match", 0, stats.getNumDataPagesEncodedAs(Encoding.PLAIN));    assertEquals("Count should match", 0, stats.getNumDataPagesEncodedAs(Encoding.PLAIN_DICTIONARY));    assertEquals("Count should match", 0, stats.getNumDataPagesEncodedAs(Encoding.DELTA_BINARY_PACKED));    assertEquals("Count should match", 0, stats.getNumDataPagesEncodedAs(Encoding.DELTA_LENGTH_BYTE_ARRAY));}
public static void parquet-mr_f3258_0(String[] args) throws IOException
{    int COUNT = 800000;    ByteArrayOutputStream baos = new ByteArrayOutputStream();    BitPackingWriter w = BitPacking.getBitPackingWriter(1, baos);    long t0 = System.currentTimeMillis();    for (int i = 0; i < COUNT; ++i) {        w.write(i % 2);    }    w.finish();    long t1 = System.currentTimeMillis();    System.out.println("written in " + (t1 - t0) + "ms");    System.out.println();    byte[] bytes = baos.toByteArray();    System.out.println(bytes.length);    int[] result = new int[COUNT];    for (int l = 0; l < 5; l++) {        long s = manual(bytes, result);        long b = generated(bytes, result);        float ratio = (float) b / s;        System.out.println("                                             " + ratio + (ratio < 1 ? " < 1 => GOOD" : " >= 1 => BAD"));    }}
private static void parquet-mr_f3259_0(int[] result)
{    int error = 0;    for (int i = 0; i < result.length; ++i) {        if (result[i] != i % 2) {            error++;        }    }    if (error != 0) {        throw new RuntimeException("errors: " + error + " / " + result.length);    }}
private static long parquet-mr_f3260_0(byte[] bytes, int[] result) throws IOException
{    return readNTimes(bytes, result, new BitPackingValuesReader(1));}
private static long parquet-mr_f3261_0(byte[] bytes, int[] result) throws IOException
{    return readNTimes(bytes, result, new ByteBitPackingValuesReader(1, Packer.BIG_ENDIAN));}
private static long parquet-mr_f3262_0(byte[] bytes, int[] result, ValuesReader r) throws IOException
{    System.out.println();    long t = 0;    int N = 10;    System.gc();    System.out.print("                                             " + r.getClass().getSimpleName());    System.out.print(" no gc <");    for (int k = 0; k < N; k++) {        long t2 = System.nanoTime();        r.initFromPage(result.length, ByteBufferInputStream.wrap(ByteBuffer.wrap(bytes)));        for (int i = 0; i < result.length; i++) {            result[i] = r.readInteger();        }        long t3 = System.nanoTime();        t += t3 - t2;    }    System.out.println("> read in " + t / 1000 + "µs " + (N * result.length / (t / 1000)) + " values per µs");    verify(result);    return t;}
public void parquet-mr_f3263_0() throws IOException
{    int bitLength = 0;    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };    String expected = "";    validateEncodeDecode(bitLength, vals, expected);}
public void parquet-mr_f3264_0() throws IOException
{    int[] vals = { 0 };    String expected = "00000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f3265_0() throws IOException
{    int[] vals = { 1 };    String expected = "10000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f3266_0() throws IOException
{    int[] vals = { 0, 0 };    String expected = "00000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f3267_0() throws IOException
{    int[] vals = { 1, 1 };    String expected = "11000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f3268_0() throws IOException
{    int[] vals = { 1, 1, 1, 1, 1, 1, 1, 1, 1 };    String expected = "11111111 10000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f3269_0() throws IOException
{    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 0, 0 };    String expected = "00000000 00000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f3270_0() throws IOException
{    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 1 };    String expected = "00000001";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f3271_0() throws IOException
{    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 };    String expected = "00000000 01000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f3272_0() throws IOException
{    int[] vals = { 0, 1, 0, 0, 1, 1, 1, 0, 0, 1 };    String expected = "01001110 01000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f3273_0() throws IOException
{    int[] vals = { 0, 1, 2, 3, 3, 3, 2, 1, 1, 0, 0, 0, 1 };    String expected = "00011011 11111001 01000000 01000000";    validateEncodeDecode(2, vals, expected);}
public void parquet-mr_f3274_0() throws IOException
{    int[] vals = { 0, 1, 2, 3, 4, 5, 6, 7, 1 };    String expected = "00000101 00111001 01110111 " + "00100000";    validateEncodeDecode(3, vals, expected);}
public void parquet-mr_f3275_0() throws IOException
{    int[] vals = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1 };    String expected = "00000001 00100011 01000101 01100111 10001001 10101011 11001101 11101111 00010000";    validateEncodeDecode(4, vals, expected);}
public void parquet-mr_f3276_0() throws IOException
{    int[] vals = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 1 };    String expected = "00000000 01000100 00110010 00010100 11000111 " + "01000010 01010100 10110110 00110101 11001111 " + "10000100 01100101 00111010 01010110 11010111 " + "11000110 01110101 10111110 01110111 11011111 " + "00001000";    validateEncodeDecode(5, vals, expected);}
public void parquet-mr_f3277_0() throws IOException
{    int[] vals = { 0, 28, 34, 35, 63, 1 };        String expected = "00000001 11001000 10100011 " + "11111100 00010000";    validateEncodeDecode(6, vals, expected);}
public void parquet-mr_f3278_0() throws IOException
{    int[] vals = { 0, 28, 34, 35, 63, 1, 125, 1, 1 };        String expected = "00000000 01110001 00010010 00110111 11100000 01111110 10000001 " + "00000010";    validateEncodeDecode(7, vals, expected);}
private void parquet-mr_f3279_1(int bitLength, int[] vals, String expected) throws IOException
{    for (PACKING_TYPE type : PACKING_TYPE.values()) {                final int bound = (int) Math.pow(2, bitLength) - 1;        ValuesWriter w = type.getWriter(bound);        for (int i : vals) {            w.writeInteger(i);        }        byte[] bytes = w.getBytes().toByteArray();                        assertEquals(type.toString(), expected, TestBitPacking.toString(bytes));        ValuesReader r = type.getReader(bound);        r.initFromPage(vals.length, ByteBufferInputStream.wrap(ByteBuffer.wrap(bytes)));        int[] result = new int[vals.length];        for (int i = 0; i < result.length; i++) {            result[i] = r.readInteger();        }                assertArrayEquals(type + " result: " + TestBitPacking.toString(result), vals, result);                r.initFromPage(vals.length, ByteBufferInputStream.wrap(ByteBuffer.wrap(bytes)));        for (int i = 0; i < vals.length; i += 2) {            assertEquals(vals[i], r.readInteger());            r.skip();        }                r.initFromPage(vals.length, ByteBufferInputStream.wrap(ByteBuffer.wrap(bytes)));        int skipCount;        for (int i = 0; i < vals.length; i += skipCount + 1) {            skipCount = (vals.length - i) / 2;            assertEquals(vals[i], r.readInteger());            r.skip(skipCount);        }    }}
public ValuesReader parquet-mr_f3280_0(final int bound)
{    return new BitPackingValuesReader(bound);}
public ValuesWriter parquet-mr_f3281_0(final int bound)
{    return new BitPackingValuesWriter(bound, 32 * 1024, 64 * 1024, new DirectByteBufferAllocator());}
public ValuesReader parquet-mr_f3282_0(final int bound)
{    return new ByteBitPackingValuesReader(bound, BIG_ENDIAN);}
public ValuesWriter parquet-mr_f3283_0(final int bound)
{    return new ByteBitPackingValuesWriter(bound, BIG_ENDIAN);}
public void parquet-mr_f3284_0()
{    final Random r = new Random();    testRandomIntegers(new IntFunc() {        @Override        public int getIntValue() {            return r.nextInt();        }    }, 32);}
public int parquet-mr_f3285_0()
{    return r.nextInt();}
public void parquet-mr_f3286_0()
{    final Random r = new Random();    testRandomIntegers(new IntFunc() {        @Override        public int getIntValue() {            return 1000 + r.nextInt(20);        }    }, 10);}
public int parquet-mr_f3287_0()
{    return 1000 + r.nextInt(20);}
public void parquet-mr_f3288_0()
{    final Random r = new Random();    testRandomIntegers(new IntFunc() {        @Override        public int getIntValue() {            return 40 + r.nextInt(20);        }    }, 6);}
public int parquet-mr_f3289_0()
{    return 40 + r.nextInt(20);}
public void parquet-mr_f3290_0()
{    final Random r = new Random();    testRandomIntegers(new IntFunc() {        @Override        public int getIntValue() {            return r.nextInt(20) - 10;        }    }, 4);}
public int parquet-mr_f3291_0()
{    return r.nextInt(20) - 10;}
public void parquet-mr_f3292_0(IntFunc func, int bitWidth)
{    DeltaBinaryPackingValuesWriter delta = new DeltaBinaryPackingValuesWriterForInteger(blockSize, miniBlockNum, 100, 20000, new DirectByteBufferAllocator());    RunLengthBitPackingHybridValuesWriter rle = new RunLengthBitPackingHybridValuesWriter(bitWidth, 100, 20000, new DirectByteBufferAllocator());    for (int i = 0; i < dataSize; i++) {        int v = func.getIntValue();        delta.writeInteger(v);        rle.writeInteger(v);    }    System.out.println("delta size: " + delta.getBytes().size());    System.out.println("estimated size" + estimatedSize());    System.out.println("rle size: " + rle.getBytes().size());}
private double parquet-mr_f3293_0()
{    int miniBlockSize = blockSize / miniBlockNum;    double miniBlockFlushed = Math.ceil(((double) dataSize - 1) / miniBlockSize);    double blockFlushed = Math.ceil(((double) dataSize - 1) / blockSize);    double estimatedSize =     4 * 5 +     4 * miniBlockFlushed * miniBlockSize +     blockFlushed * miniBlockNum +     (5.0 * blockFlushed);    return estimatedSize;}
public static void parquet-mr_f3294_0() throws IOException
{    Random random = new Random();    data = new int[100000 * blockSize];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextInt(100) - 200;    }    ValuesWriter delta = new DeltaBinaryPackingValuesWriterForInteger(blockSize, miniBlockNum, 100, 20000, new DirectByteBufferAllocator());    ValuesWriter rle = new RunLengthBitPackingHybridValuesWriter(32, 100, 20000, new DirectByteBufferAllocator());    for (int i = 0; i < data.length; i++) {        delta.writeInteger(data[i]);        rle.writeInteger(data[i]);    }    deltaBytes = delta.getBytes().toByteArray();    rleBytes = rle.getBytes().toByteArray();}
public void parquet-mr_f3295_0() throws IOException
{    for (int j = 0; j < 10; j++) {        DeltaBinaryPackingValuesReader reader = new DeltaBinaryPackingValuesReader();        readData(reader, deltaBytes);    }}
public void parquet-mr_f3296_0() throws IOException
{    for (int j = 0; j < 10; j++) {        ValuesReader reader = new RunLengthBitPackingHybridValuesReader(32);        readData(reader, rleBytes);    }}
private void parquet-mr_f3297_0(ValuesReader reader, byte[] deltaBytes) throws IOException
{    reader.initFromPage(data.length, ByteBufferInputStream.wrap(ByteBuffer.wrap(deltaBytes)));    for (int i = 0; i < data.length; i++) {        reader.readInteger();    }}
protected void parquet-mr_f3298_0(ValuesWriter writer)
{    int pageCount = 10;    double avg = 0.0;    for (int i = 0; i < pageCount; i++) {        writer.reset();        long startTime = System.nanoTime();        for (int item : data) {            writer.writeInteger(item);        }        long endTime = System.nanoTime();        long duration = endTime - startTime;        avg += (double) duration / pageCount;    }    System.out.println("size is " + writer.getBytes().size());}
public static void parquet-mr_f3299_0()
{    Random random = new Random();    data = new int[10000 * blockSize];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextInt(100) - 200;    }}
public void parquet-mr_f3300_0()
{    DeltaBinaryPackingValuesWriter writer = new DeltaBinaryPackingValuesWriterForInteger(blockSize, miniBlockNum, 100, 20000, new DirectByteBufferAllocator());    runWriteTest(writer);}
public void parquet-mr_f3301_0()
{    ValuesWriter writer = new RunLengthBitPackingHybridValuesWriter(32, 100, 20000, new DirectByteBufferAllocator());    runWriteTest(writer);}
public void parquet-mr_f3302_0()
{    DeltaBinaryPackingValuesWriter writer = new DeltaBinaryPackingValuesWriterForInteger(blockSize, miniBlockNum, 100, 20000, new DirectByteBufferAllocator());    runWriteTest(writer);}
public static void parquet-mr_f3303_0()
{    Random random = new Random();    data = new int[100000 * blockSize];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextInt(2) - 1;    }}
public void parquet-mr_f3304_0()
{    ValuesWriter writer = new RunLengthBitPackingHybridValuesWriter(2, 100, 20000, new DirectByteBufferAllocator());    runWriteTest(writer);}
public void parquet-mr_f3305_0()
{    blockSize = 128;    miniBlockNum = 4;    writer = new DeltaBinaryPackingValuesWriterForInteger(blockSize, miniBlockNum, 100, 200, new DirectByteBufferAllocator());    random = new Random(0);}
public void parquet-mr_f3306_0()
{    new DeltaBinaryPackingValuesWriterForInteger(1281, 4, 100, 100, new DirectByteBufferAllocator());}
public void parquet-mr_f3307_0() throws IOException
{    int[] data = new int[5 * blockSize];    for (int i = 0; i < blockSize * 5; i++) {        data[i] = random.nextInt();    }    shouldWriteAndRead(data);}
public void parquet-mr_f3308_0() throws IOException
{    int[] data = new int[blockSize - 3];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextInt();    }    shouldWriteAndRead(data);}
public void parquet-mr_f3309_0() throws IOException
{    int miniBlockSize = blockSize / miniBlockNum;    int[] data = new int[miniBlockSize - 3];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextInt();    }    shouldWriteAndRead(data);}
public void parquet-mr_f3310_0() throws IOException
{    int[] data = new int[blockSize];    for (int i = 0; i < data.length; i++) {        data[i] = 10 - (i * 32 - random.nextInt(6));    }    shouldWriteAndRead(data);}
public void parquet-mr_f3311_0() throws IOException
{    int[] data = new int[2 * blockSize];    for (int i = 0; i < blockSize; i++) {        data[i] = i * 32;    }    shouldWriteAndRead(data);}
public void parquet-mr_f3312_0() throws IOException
{    int[] data = new int[2 * blockSize];    for (int i = 0; i < blockSize; i++) {        data[i] = 3;    }    shouldWriteAndRead(data);}
public void parquet-mr_f3313_0() throws IOException
{    int[] data = new int[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = (i - 1) / blockSize;    }    shouldWriteAndRead(data);}
public void parquet-mr_f3314_0() throws IOException
{    int[] data = new int[5 * blockSize + 3];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextInt(20) - 10;    }    shouldWriteAndRead(data);}
public void parquet-mr_f3315_0() throws IOException
{    int[] data = new int[10];    for (int i = 0; i < data.length; i++) {        if (i % 2 == 0) {            data[i] = Integer.MIN_VALUE;        } else {            data[i] = Integer.MAX_VALUE;        }    }    shouldWriteAndRead(data);}
public void parquet-mr_f3316_0() throws IOException
{    int[] data = new int[2 * blockSize + 3];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    writeData(data);    reader = new DeltaBinaryPackingValuesReader();    BytesInput bytes = writer.getBytes();    byte[] valueContent = bytes.toByteArray();    byte[] pageContent = new byte[valueContent.length * 10];    int contentOffsetInPage = 33;    System.arraycopy(valueContent, 0, pageContent, contentOffsetInPage, valueContent.length);        ByteBufferInputStream stream = ByteBufferInputStream.wrap(ByteBuffer.wrap(pageContent));    stream.skipFully(contentOffsetInPage);    reader.initFromPage(100, stream);    long offset = stream.position();    assertEquals(valueContent.length + contentOffsetInPage, offset);        for (int i : data) {        assertEquals(i, reader.readInteger());    }        reader = new DeltaBinaryPackingValuesReader();    reader.initFromPage(100, pageContent, contentOffsetInPage);    assertEquals(valueContent.length + contentOffsetInPage, reader.getNextOffset());    for (int i : data) {        assertEquals(i, reader.readInteger());    }}
public void parquet-mr_f3317_0() throws IOException
{    int[] data = new int[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    shouldWriteAndRead(data);    try {        reader.readInteger();    } catch (ParquetDecodingException e) {        assertEquals("no more value to read, total value count is " + data.length, e.getMessage());    }}
public void parquet-mr_f3318_0() throws IOException
{    int[] data = new int[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    writeData(data);    reader = new DeltaBinaryPackingValuesReader();    reader.initFromPage(100, writer.getBytes().toInputStream());    for (int i = 0; i < data.length; i++) {        if (i % 3 == 0) {            reader.skip();        } else {            assertEquals(i * 32, reader.readInteger());        }    }}
public void parquet-mr_f3319_0() throws IOException
{    int[] data = new int[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    writeData(data);    reader = new DeltaBinaryPackingValuesReader();    reader.initFromPage(100, writer.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < data.length; i += skipCount + 1) {        skipCount = (data.length - i) / 2;        assertEquals(i * 32, reader.readInteger());        reader.skip(skipCount);    }}
public void parquet-mr_f3320_0() throws IOException
{    shouldReadWriteWhenDataIsNotAlignedWithBlock();    int[] data = new int[5 * blockSize];    for (int i = 0; i < blockSize * 5; i++) {        data[i] = i * 2;    }    writer.reset();    shouldWriteAndRead(data);}
public void parquet-mr_f3321_0() throws IOException
{    int maxSize = 1000;    int[] data = new int[maxSize];    for (int round = 0; round < 100000; round++) {        int size = random.nextInt(maxSize);        for (int i = 0; i < size; i++) {            data[i] = random.nextInt();        }        shouldReadAndWrite(data, size);        writer.reset();    }}
private void parquet-mr_f3322_0(int[] data) throws IOException
{    shouldReadAndWrite(data, data.length);}
private void parquet-mr_f3323_0(int[] data, int length) throws IOException
{    writeData(data, length);    reader = new DeltaBinaryPackingValuesReader();    byte[] page = writer.getBytes().toByteArray();    int miniBlockSize = blockSize / miniBlockNum;    double miniBlockFlushed = Math.ceil(((double) length - 1) / miniBlockSize);    double blockFlushed = Math.ceil(((double) length - 1) / blockSize);    double estimatedSize =     4 * 5 +     4 * miniBlockFlushed * miniBlockSize +     blockFlushed * miniBlockNum +     (5.0 * blockFlushed);    assertTrue(estimatedSize >= page.length);    reader.initFromPage(100, ByteBufferInputStream.wrap(ByteBuffer.wrap(page)));    for (int i = 0; i < length; i++) {        assertEquals(data[i], reader.readInteger());    }}
private void parquet-mr_f3324_0(int[] data)
{    writeData(data, data.length);}
private void parquet-mr_f3325_0(int[] data, int length)
{    for (int i = 0; i < length; i++) {        writer.writeInteger(data[i]);    }}
public void parquet-mr_f3326_0()
{    blockSize = 128;    miniBlockNum = 4;    writer = new DeltaBinaryPackingValuesWriterForLong(blockSize, miniBlockNum, 100, 200, new DirectByteBufferAllocator());    random = new Random(0);}
public void parquet-mr_f3327_0()
{    new DeltaBinaryPackingValuesWriterForLong(1281, 4, 100, 100, new DirectByteBufferAllocator());}
public void parquet-mr_f3328_0() throws IOException
{    long[] data = new long[5 * blockSize];    for (int i = 0; i < blockSize * 5; i++) {        data[i] = random.nextLong();    }    shouldWriteAndRead(data);}
public void parquet-mr_f3329_0() throws IOException
{    long[] data = new long[blockSize - 3];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextLong();    }    shouldWriteAndRead(data);}
public void parquet-mr_f3330_0() throws IOException
{    int miniBlockSize = blockSize / miniBlockNum;    long[] data = new long[miniBlockSize - 3];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextLong();    }    shouldWriteAndRead(data);}
public void parquet-mr_f3331_0() throws IOException
{    long[] data = new long[blockSize];    for (int i = 0; i < data.length; i++) {        data[i] = 10 - (i * 32 - random.nextInt(6));    }    shouldWriteAndRead(data);}
public void parquet-mr_f3332_0() throws IOException
{    long[] data = new long[2 * blockSize];    for (int i = 0; i < blockSize; i++) {        data[i] = i * 32;    }    shouldWriteAndRead(data);}
public void parquet-mr_f3333_0() throws IOException
{    long[] data = new long[2 * blockSize];    for (int i = 0; i < blockSize; i++) {        data[i] = 3;    }    shouldWriteAndRead(data);}
public void parquet-mr_f3334_0() throws IOException
{    long[] data = new long[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = (i - 1) / blockSize;    }    shouldWriteAndRead(data);}
public void parquet-mr_f3335_0() throws IOException
{    long[] data = new long[5 * blockSize + 3];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextInt(20) - 10;    }    shouldWriteAndRead(data);}
public void parquet-mr_f3336_0() throws IOException
{    long[] data = new long[10];    for (int i = 0; i < data.length; i++) {        if (i % 2 == 0) {            data[i] = Long.MIN_VALUE;        } else {            data[i] = Long.MAX_VALUE;        }    }    shouldWriteAndRead(data);}
public void parquet-mr_f3337_0() throws IOException
{    long[] data = new long[2 * blockSize + 3];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    writeData(data);    reader = new DeltaBinaryPackingValuesReader();    BytesInput bytes = writer.getBytes();    byte[] valueContent = bytes.toByteArray();    byte[] pageContent = new byte[valueContent.length * 10];    int contentOffsetInPage = 33;    System.arraycopy(valueContent, 0, pageContent, contentOffsetInPage, valueContent.length);        ByteBufferInputStream stream = ByteBufferInputStream.wrap(ByteBuffer.wrap(pageContent));    stream.skipFully(contentOffsetInPage);    reader.initFromPage(100, stream);    long offset = stream.position();    assertEquals(valueContent.length + contentOffsetInPage, offset);        for (long i : data) {        assertEquals(i, reader.readLong());    }        reader = new DeltaBinaryPackingValuesReader();    reader.initFromPage(100, pageContent, contentOffsetInPage);    assertEquals(valueContent.length + contentOffsetInPage, reader.getNextOffset());    for (long i : data) {        assertEquals(i, reader.readLong());    }}
public void parquet-mr_f3338_0() throws IOException
{    long[] data = new long[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    shouldWriteAndRead(data);    try {        reader.readLong();    } catch (ParquetDecodingException e) {        assertEquals("no more value to read, total value count is " + data.length, e.getMessage());    }}
public void parquet-mr_f3339_0() throws IOException
{    long[] data = new long[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    writeData(data);    reader = new DeltaBinaryPackingValuesReader();    reader.initFromPage(100, writer.getBytes().toInputStream());    for (int i = 0; i < data.length; i++) {        if (i % 3 == 0) {            reader.skip();        } else {            assertEquals(i * 32, reader.readLong());        }    }}
public void parquet-mr_f3340_0() throws IOException
{    long[] data = new long[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    writeData(data);    reader = new DeltaBinaryPackingValuesReader();    reader.initFromPage(100, writer.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < data.length; i += skipCount + 1) {        skipCount = (data.length - i) / 2;        assertEquals(i * 32, reader.readLong());        reader.skip(skipCount);    }}
public void parquet-mr_f3341_0() throws IOException
{    shouldReadWriteWhenDataIsNotAlignedWithBlock();    long[] data = new long[5 * blockSize];    for (int i = 0; i < blockSize * 5; i++) {        data[i] = i * 2;    }    writer.reset();    shouldWriteAndRead(data);}
public void parquet-mr_f3342_0() throws IOException
{    int maxSize = 1000;    long[] data = new long[maxSize];    for (int round = 0; round < 100000; round++) {        int size = random.nextInt(maxSize);        for (int i = 0; i < size; i++) {            data[i] = random.nextLong();        }        shouldReadAndWrite(data, size);        writer.reset();    }}
private void parquet-mr_f3343_0(long[] data) throws IOException
{    shouldReadAndWrite(data, data.length);}
private void parquet-mr_f3344_0(long[] data, int length) throws IOException
{    writeData(data, length);    reader = new DeltaBinaryPackingValuesReader();    byte[] page = writer.getBytes().toByteArray();    int miniBlockSize = blockSize / miniBlockNum;    double miniBlockFlushed = Math.ceil(((double) length - 1) / miniBlockSize);    double blockFlushed = Math.ceil(((double) length - 1) / blockSize);    double estimatedSize =     3 * 5 + 1 * 10 +     8 * miniBlockFlushed * miniBlockSize +     blockFlushed * miniBlockNum +     (10.0 * blockFlushed);    assertTrue(estimatedSize >= page.length);    reader.initFromPage(100, ByteBufferInputStream.wrap(ByteBuffer.wrap(page)));    for (int i = 0; i < length; i++) {        assertEquals(data[i], reader.readLong());    }}
private void parquet-mr_f3345_0(long[] data)
{    writeData(data, data.length);}
private void parquet-mr_f3346_0(long[] data, int length)
{    for (int i = 0; i < length; i++) {        writer.writeLong(data[i]);    }}
public void parquet-mr_f3347_0() throws IOException
{    PlainValuesWriter writer = new PlainValuesWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    BinaryPlainValuesReader reader = new BinaryPlainValuesReader();    Utils.writeData(writer, values);    ByteBufferInputStream data = writer.getBytes().toInputStream();    Binary[] bin = Utils.readData(reader, data, values.length);    System.out.println("size " + data.position());}
public void parquet-mr_f3348_0() throws IOException
{    DeltaLengthByteArrayValuesWriter writer = new DeltaLengthByteArrayValuesWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    DeltaLengthByteArrayValuesReader reader = new DeltaLengthByteArrayValuesReader();    Utils.writeData(writer, values);    ByteBufferInputStream data = writer.getBytes().toInputStream();    Binary[] bin = Utils.readData(reader, data, values.length);    System.out.println("size " + data.position());}
private DeltaLengthByteArrayValuesWriter parquet-mr_f3349_0()
{    return new DeltaLengthByteArrayValuesWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());}
public void parquet-mr_f3350_0() throws IOException
{    DeltaLengthByteArrayValuesWriter writer = getDeltaLengthByteArrayValuesWriter();    DeltaLengthByteArrayValuesReader reader = new DeltaLengthByteArrayValuesReader();    Utils.writeData(writer, values);    Binary[] bin = Utils.readData(reader, writer.getBytes().toInputStream(), values.length);    for (int i = 0; i < bin.length; i++) {        Assert.assertEquals(Binary.fromString(values[i]), bin[i]);    }}
public void parquet-mr_f3351_0() throws IOException
{    DeltaLengthByteArrayValuesWriter writer = getDeltaLengthByteArrayValuesWriter();    DeltaLengthByteArrayValuesReader reader = new DeltaLengthByteArrayValuesReader();    String[] values = Utils.getRandomStringSamples(1000, 32);    Utils.writeData(writer, values);    Binary[] bin = Utils.readData(reader, writer.getBytes().toInputStream(), values.length);    for (int i = 0; i < bin.length; i++) {        Assert.assertEquals(Binary.fromString(values[i]), bin[i]);    }}
public void parquet-mr_f3352_0() throws IOException
{    DeltaLengthByteArrayValuesWriter writer = getDeltaLengthByteArrayValuesWriter();    DeltaLengthByteArrayValuesReader reader = new DeltaLengthByteArrayValuesReader();    String[] values = Utils.getRandomStringSamples(1000, 32);    Utils.writeData(writer, values);    reader.initFromPage(values.length, writer.getBytes().toInputStream());    for (int i = 0; i < values.length; i += 2) {        Assert.assertEquals(Binary.fromString(values[i]), reader.readBytes());        reader.skip();    }    reader = new DeltaLengthByteArrayValuesReader();    reader.initFromPage(values.length, writer.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < values.length; i += skipCount + 1) {        skipCount = (values.length - i) / 2;        Assert.assertEquals(Binary.fromString(values[i]), reader.readBytes());        reader.skip(skipCount);    }}
public void parquet-mr_f3353_0() throws IOException
{    DeltaLengthByteArrayValuesWriter writer = getDeltaLengthByteArrayValuesWriter();    ValuesReader reader = new DeltaBinaryPackingValuesReader();    Utils.writeData(writer, values);    int[] bin = Utils.readInts(reader, writer.getBytes().toInputStream(), values.length);    for (int i = 0; i < bin.length; i++) {        Assert.assertEquals(values[i].length(), bin[i]);    }}
public void parquet-mr_f3354_0() throws IOException
{    PlainValuesWriter writer = new PlainValuesWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    BinaryPlainValuesReader reader = new BinaryPlainValuesReader();    Utils.writeData(writer, values);    ByteBufferInputStream data = writer.getBytes().toInputStream();    Binary[] bin = Utils.readData(reader, data, values.length);    System.out.println("size " + data.position());}
public void parquet-mr_f3355_0() throws IOException
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    DeltaByteArrayReader reader = new DeltaByteArrayReader();    Utils.writeData(writer, values);    ByteBufferInputStream data = writer.getBytes().toInputStream();    Binary[] bin = Utils.readData(reader, data, values.length);    System.out.println("size " + data.position());}
public void parquet-mr_f3356_0() throws IOException
{    PlainValuesWriter writer = new PlainValuesWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    BinaryPlainValuesReader reader = new BinaryPlainValuesReader();    Utils.writeData(writer, sortedVals);    ByteBufferInputStream data = writer.getBytes().toInputStream();    Binary[] bin = Utils.readData(reader, data, values.length);    System.out.println("size " + data.position());}
public void parquet-mr_f3357_0() throws IOException
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    DeltaByteArrayReader reader = new DeltaByteArrayReader();    Utils.writeData(writer, sortedVals);    ByteBufferInputStream data = writer.getBytes().toInputStream();    Binary[] bin = Utils.readData(reader, data, values.length);    System.out.println("size " + data.position());}
public void parquet-mr_f3358_0() throws Exception
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    DeltaByteArrayReader reader = new DeltaByteArrayReader();    assertReadWrite(writer, reader, values);}
public void parquet-mr_f3359_0() throws Exception
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    DeltaByteArrayReader reader = new DeltaByteArrayReader();    assertReadWrite(writer, reader, randvalues);}
public void parquet-mr_f3360_0() throws Exception
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    DeltaByteArrayReader reader = new DeltaByteArrayReader();    assertReadWriteWithSkip(writer, reader, randvalues);}
public void parquet-mr_f3361_0() throws Exception
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    DeltaByteArrayReader reader = new DeltaByteArrayReader();    assertReadWriteWithSkipN(writer, reader, randvalues);}
public void parquet-mr_f3362_0() throws IOException
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    ValuesReader reader = new DeltaBinaryPackingValuesReader();    Utils.writeData(writer, values);    ByteBufferInputStream data = writer.getBytes().toInputStream();    int[] bin = Utils.readInts(reader, data, values.length);        Assert.assertEquals(0, bin[0]);    Assert.assertEquals(7, bin[1]);    Assert.assertEquals(7, bin[2]);    reader = new DeltaBinaryPackingValuesReader();    bin = Utils.readInts(reader, data, values.length);        Assert.assertEquals(10, bin[0]);    Assert.assertEquals(0, bin[1]);    Assert.assertEquals(7, bin[2]);}
private void parquet-mr_f3363_0(DeltaByteArrayWriter writer, DeltaByteArrayReader reader, String[] vals) throws Exception
{    Utils.writeData(writer, vals);    Binary[] bin = Utils.readData(reader, writer.getBytes().toInputStream(), vals.length);    for (int i = 0; i < bin.length; i++) {        Assert.assertEquals(Binary.fromString(vals[i]), bin[i]);    }}
private void parquet-mr_f3364_0(DeltaByteArrayWriter writer, DeltaByteArrayReader reader, String[] vals) throws Exception
{    Utils.writeData(writer, vals);    reader.initFromPage(vals.length, writer.getBytes().toInputStream());    for (int i = 0; i < vals.length; i += 2) {        Assert.assertEquals(Binary.fromString(vals[i]), reader.readBytes());        reader.skip();    }}
private void parquet-mr_f3365_0(DeltaByteArrayWriter writer, DeltaByteArrayReader reader, String[] vals) throws Exception
{    Utils.writeData(writer, vals);    reader.initFromPage(vals.length, writer.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < vals.length; i += skipCount + 1) {        skipCount = (vals.length - i) / 2;        Assert.assertEquals(Binary.fromString(vals[i]), reader.readBytes());        reader.skip(skipCount);    }}
public void parquet-mr_f3366_0() throws Exception
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    assertReadWrite(writer, new DeltaByteArrayReader(), values);    writer.reset();    assertReadWrite(writer, new DeltaByteArrayReader(), values);}
public void parquet-mr_f3367_0()
{    int testSize = IntList.INITIAL_SLAB_SIZE - 100;    doTestIntList(testSize, IntList.INITIAL_SLAB_SIZE);}
public void parquet-mr_f3368_0()
{    int testSize = IntList.INITIAL_SLAB_SIZE + 100;    doTestIntList(testSize, IntList.INITIAL_SLAB_SIZE * 2);}
public void parquet-mr_f3369_0()
{    int testSize = IntList.MAX_SLAB_SIZE * 4 + 100;    doTestIntList(testSize, IntList.MAX_SLAB_SIZE);}
private void parquet-mr_f3370_0(int testSize, int expectedSlabSize)
{    IntList testList = new IntList();    populateList(testList, testSize);    verifyIteratorResults(testSize, testList);        Assert.assertEquals(expectedSlabSize, testList.getCurrentSlabSize());}
private void parquet-mr_f3371_0(IntList testList, int size)
{    for (int i = 0; i < size; i++) {        testList.add(i);    }}
private void parquet-mr_f3372_0(int testSize, IntList testList)
{    IntList.IntIterator iterator = testList.iterator();    int expected = 0;    while (iterator.hasNext()) {        int val = iterator.next();        Assert.assertEquals(expected, val);        expected++;    }        Assert.assertEquals(testSize, expected);}
private FallbackValuesWriter<I, PlainValuesWriter> parquet-mr_f3373_0(I dvw, int initialSize)
{    return FallbackValuesWriter.of(dvw, new PlainValuesWriter(initialSize, initialSize * 5, new DirectByteBufferAllocator()));}
private FallbackValuesWriter<PlainBinaryDictionaryValuesWriter, PlainValuesWriter> parquet-mr_f3374_0(int maxDictionaryByteSize, int initialSize)
{    return plainFallBack(new PlainBinaryDictionaryValuesWriter(maxDictionaryByteSize, PLAIN_DICTIONARY, PLAIN_DICTIONARY, new DirectByteBufferAllocator()), initialSize);}
private FallbackValuesWriter<PlainLongDictionaryValuesWriter, PlainValuesWriter> parquet-mr_f3375_0(int maxDictionaryByteSize, int initialSize)
{    return plainFallBack(new PlainLongDictionaryValuesWriter(maxDictionaryByteSize, PLAIN_DICTIONARY, PLAIN_DICTIONARY, new DirectByteBufferAllocator()), initialSize);}
private FallbackValuesWriter<PlainIntegerDictionaryValuesWriter, PlainValuesWriter> parquet-mr_f3376_0(int maxDictionaryByteSize, int initialSize)
{    return plainFallBack(new PlainIntegerDictionaryValuesWriter(maxDictionaryByteSize, PLAIN_DICTIONARY, PLAIN_DICTIONARY, new DirectByteBufferAllocator()), initialSize);}
private FallbackValuesWriter<PlainDoubleDictionaryValuesWriter, PlainValuesWriter> parquet-mr_f3377_0(int maxDictionaryByteSize, int initialSize)
{    return plainFallBack(new PlainDoubleDictionaryValuesWriter(maxDictionaryByteSize, PLAIN_DICTIONARY, PLAIN_DICTIONARY, new DirectByteBufferAllocator()), initialSize);}
private FallbackValuesWriter<PlainFloatDictionaryValuesWriter, PlainValuesWriter> parquet-mr_f3378_0(int maxDictionaryByteSize, int initialSize)
{    return plainFallBack(new PlainFloatDictionaryValuesWriter(maxDictionaryByteSize, PLAIN_DICTIONARY, PLAIN_DICTIONARY, new DirectByteBufferAllocator()), initialSize);}
public void parquet-mr_f3379_0() throws IOException
{    int COUNT = 100;    ValuesWriter cw = newPlainBinaryDictionaryValuesWriter(200, 10000);    writeRepeated(COUNT, cw, "a");    BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    writeRepeated(COUNT, cw, "b");    BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);        writeDistinct(COUNT, cw, "c");    BytesInput bytes3 = getBytesAndCheckEncoding(cw, PLAIN);    DictionaryValuesReader cr = initDicReader(cw, BINARY);    checkRepeated(COUNT, bytes1, cr, "a");    checkRepeated(COUNT, bytes2, cr, "b");    BinaryPlainValuesReader cr2 = new BinaryPlainValuesReader();    checkDistinct(COUNT, bytes3, cr2, "c");}
public void parquet-mr_f3380_0() throws Exception
{    ValuesWriter cw = newPlainBinaryDictionaryValuesWriter(1000, 10000);    writeRepeated(100, cw, "a");    writeDistinct(100, cw, "b");    assertEquals(PLAIN_DICTIONARY, cw.getEncoding());        ByteBufferInputStream stream = cw.getBytes().toInputStream();    DictionaryValuesReader cr = initDicReader(cw, BINARY);    cr.initFromPage(200, stream);    for (int i = 0; i < 100; i += 2) {        assertEquals(Binary.fromString("a" + i % 10), cr.readBytes());        cr.skip();    }    int skipCount;    for (int i = 0; i < 100; i += skipCount + 1) {        skipCount = (100 - i) / 2;        assertEquals(Binary.fromString("b" + i), cr.readBytes());        cr.skip(skipCount);    }        writeDistinct(1000, cw, "c");    assertEquals(PLAIN, cw.getEncoding());        ValuesReader plainReader = new BinaryPlainValuesReader();    plainReader.initFromPage(1200, cw.getBytes().toInputStream());    plainReader.skip(200);    for (int i = 0; i < 100; i += 2) {        assertEquals("c" + i, plainReader.readBytes().toStringUsingUTF8());        plainReader.skip();    }    for (int i = 100; i < 1000; i += skipCount + 1) {        skipCount = (1000 - i) / 2;        assertEquals(Binary.fromString("c" + i), plainReader.readBytes());        plainReader.skip(skipCount);    }}
public void parquet-mr_f3381_0() throws IOException
{    int slabSize = 100;    int maxDictionaryByteSize = 50;    final ValuesWriter cw = newPlainBinaryDictionaryValuesWriter(maxDictionaryByteSize, slabSize);    int fallBackThreshold = maxDictionaryByteSize;    int dataSize = 0;    for (long i = 0; i < 100; i++) {        Binary binary = Binary.fromString("str" + i);        cw.writeBytes(binary);        dataSize += (binary.length() + 4);        if (dataSize < fallBackThreshold) {            assertEquals(PLAIN_DICTIONARY, cw.getEncoding());        } else {            assertEquals(PLAIN, cw.getEncoding());        }    }        ValuesReader reader = new BinaryPlainValuesReader();    reader.initFromPage(100, cw.getBytes().toInputStream());    for (long i = 0; i < 100; i++) {        assertEquals(Binary.fromString("str" + i), reader.readBytes());    }        cw.reset();    assertEquals(0, cw.getBufferedSize());}
public void parquet-mr_f3382_0() throws IOException
{    int COUNT = 100;    ValuesWriter cw = newPlainBinaryDictionaryValuesWriter(200, 10000);    writeRepeatedWithReuse(COUNT, cw, "a");    BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    writeRepeatedWithReuse(COUNT, cw, "b");    BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);        writeDistinct(COUNT, cw, "c");    BytesInput bytes3 = getBytesAndCheckEncoding(cw, PLAIN);    DictionaryValuesReader cr = initDicReader(cw, BINARY);    checkRepeated(COUNT, bytes1, cr, "a");    checkRepeated(COUNT, bytes2, cr, "b");    BinaryPlainValuesReader cr2 = new BinaryPlainValuesReader();    checkDistinct(COUNT, bytes3, cr2, "c");}
public void parquet-mr_f3383_0() throws IOException
{    int COUNT = 1000;    ValuesWriter cw = newPlainBinaryDictionaryValuesWriter(10000, 10000);    writeDistinct(COUNT, cw, "a");        BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN);    writeRepeated(COUNT, cw, "b");        BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN);    ValuesReader cr = new BinaryPlainValuesReader();    checkDistinct(COUNT, bytes1, cr, "a");    checkRepeated(COUNT, bytes2, cr, "b");}
public void parquet-mr_f3384_0() throws IOException
{    int COUNT = 1000;    ValuesWriter cw = newPlainBinaryDictionaryValuesWriter(1000, 10000);    writeRepeated(COUNT, cw, "a");    BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    writeDistinct(COUNT, cw, "b");        BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN);    writeRepeated(COUNT, cw, "a");        BytesInput bytes3 = getBytesAndCheckEncoding(cw, PLAIN);    ValuesReader cr = initDicReader(cw, BINARY);    checkRepeated(COUNT, bytes1, cr, "a");    cr = new BinaryPlainValuesReader();    checkDistinct(COUNT, bytes2, cr, "b");    checkRepeated(COUNT, bytes3, cr, "a");}
public void parquet-mr_f3385_0() throws IOException
{    int COUNT = 1000;    int COUNT2 = 2000;    final FallbackValuesWriter<PlainLongDictionaryValuesWriter, PlainValuesWriter> cw = newPlainLongDictionaryValuesWriter(10000, 10000);    for (long i = 0; i < COUNT; i++) {        cw.writeLong(i % 50);    }    BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    for (long i = COUNT2; i > 0; i--) {        cw.writeLong(i % 50);    }    BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    DictionaryValuesReader cr = initDicReader(cw, PrimitiveTypeName.INT64);    cr.initFromPage(COUNT, bytes1.toInputStream());    for (long i = 0; i < COUNT; i++) {        long back = cr.readLong();        assertEquals(i % 50, back);    }    cr.initFromPage(COUNT2, bytes2.toInputStream());    for (long i = COUNT2; i > 0; i--) {        long back = cr.readLong();        assertEquals(i % 50, back);    }}
private void parquet-mr_f3386_0(FallbackValuesWriter<PlainLongDictionaryValuesWriter, PlainValuesWriter> cw, ValuesReader reader, int maxDictionaryByteSize) throws IOException
{    int fallBackThreshold = maxDictionaryByteSize / 8;    for (long i = 0; i < 100; i++) {        cw.writeLong(i);        if (i < fallBackThreshold) {            assertEquals(cw.getEncoding(), PLAIN_DICTIONARY);        } else {            assertEquals(cw.getEncoding(), PLAIN);        }    }    reader.initFromPage(100, cw.getBytes().toInputStream());    for (long i = 0; i < 100; i++) {        assertEquals(i, reader.readLong());    }        reader.initFromPage(100, cw.getBytes().toInputStream());    for (int i = 0; i < 100; i += 2) {        assertEquals(i, reader.readLong());        reader.skip();    }        reader.initFromPage(100, cw.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < 100; i += skipCount + 1) {        skipCount = (100 - i) / 2;        assertEquals(i, reader.readLong());        reader.skip(skipCount);    }}
public void parquet-mr_f3387_0() throws IOException
{    int slabSize = 100;    int maxDictionaryByteSize = 50;    final FallbackValuesWriter<PlainLongDictionaryValuesWriter, PlainValuesWriter> cw = newPlainLongDictionaryValuesWriter(maxDictionaryByteSize, slabSize);        ValuesReader reader = new PlainValuesReader.LongPlainValuesReader();    roundTripLong(cw, reader, maxDictionaryByteSize);        cw.reset();    assertEquals(0, cw.getBufferedSize());    cw.resetDictionary();    roundTripLong(cw, reader, maxDictionaryByteSize);}
public void parquet-mr_f3388_0() throws IOException
{    int COUNT = 1000;    int COUNT2 = 2000;    final FallbackValuesWriter<PlainDoubleDictionaryValuesWriter, PlainValuesWriter> cw = newPlainDoubleDictionaryValuesWriter(10000, 10000);    for (double i = 0; i < COUNT; i++) {        cw.writeDouble(i % 50);    }    BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    for (double i = COUNT2; i > 0; i--) {        cw.writeDouble(i % 50);    }    BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    final DictionaryValuesReader cr = initDicReader(cw, DOUBLE);    cr.initFromPage(COUNT, bytes1.toInputStream());    for (double i = 0; i < COUNT; i++) {        double back = cr.readDouble();        assertEquals(i % 50, back, 0.0);    }    cr.initFromPage(COUNT2, bytes2.toInputStream());    for (double i = COUNT2; i > 0; i--) {        double back = cr.readDouble();        assertEquals(i % 50, back, 0.0);    }}
private void parquet-mr_f3389_0(FallbackValuesWriter<PlainDoubleDictionaryValuesWriter, PlainValuesWriter> cw, ValuesReader reader, int maxDictionaryByteSize) throws IOException
{    int fallBackThreshold = maxDictionaryByteSize / 8;    for (double i = 0; i < 100; i++) {        cw.writeDouble(i);        if (i < fallBackThreshold) {            assertEquals(cw.getEncoding(), PLAIN_DICTIONARY);        } else {            assertEquals(cw.getEncoding(), PLAIN);        }    }    reader.initFromPage(100, cw.getBytes().toInputStream());    for (double i = 0; i < 100; i++) {        assertEquals(i, reader.readDouble(), 0.00001);    }        reader.initFromPage(100, cw.getBytes().toInputStream());    for (int i = 0; i < 100; i += 2) {        assertEquals(i, reader.readDouble(), 0.0);        reader.skip();    }        reader.initFromPage(100, cw.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < 100; i += skipCount + 1) {        skipCount = (100 - i) / 2;        assertEquals(i, reader.readDouble(), 0.0);        reader.skip(skipCount);    }}
public void parquet-mr_f3390_0() throws IOException
{    int slabSize = 100;    int maxDictionaryByteSize = 50;    final FallbackValuesWriter<PlainDoubleDictionaryValuesWriter, PlainValuesWriter> cw = newPlainDoubleDictionaryValuesWriter(maxDictionaryByteSize, slabSize);        ValuesReader reader = new PlainValuesReader.DoublePlainValuesReader();    roundTripDouble(cw, reader, maxDictionaryByteSize);        cw.reset();    assertEquals(0, cw.getBufferedSize());    cw.resetDictionary();    roundTripDouble(cw, reader, maxDictionaryByteSize);}
public void parquet-mr_f3391_0() throws IOException
{    int COUNT = 2000;    int COUNT2 = 4000;    final FallbackValuesWriter<PlainIntegerDictionaryValuesWriter, PlainValuesWriter> cw = newPlainIntegerDictionaryValuesWriter(10000, 10000);    for (int i = 0; i < COUNT; i++) {        cw.writeInteger(i % 50);    }    BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    for (int i = COUNT2; i > 0; i--) {        cw.writeInteger(i % 50);    }    BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    DictionaryValuesReader cr = initDicReader(cw, INT32);    cr.initFromPage(COUNT, bytes1.toInputStream());    for (int i = 0; i < COUNT; i++) {        int back = cr.readInteger();        assertEquals(i % 50, back);    }    cr.initFromPage(COUNT2, bytes2.toInputStream());    for (int i = COUNT2; i > 0; i--) {        int back = cr.readInteger();        assertEquals(i % 50, back);    }}
private void parquet-mr_f3392_0(FallbackValuesWriter<PlainIntegerDictionaryValuesWriter, PlainValuesWriter> cw, ValuesReader reader, int maxDictionaryByteSize) throws IOException
{    int fallBackThreshold = maxDictionaryByteSize / 4;    for (int i = 0; i < 100; i++) {        cw.writeInteger(i);        if (i < fallBackThreshold) {            assertEquals(cw.getEncoding(), PLAIN_DICTIONARY);        } else {            assertEquals(cw.getEncoding(), PLAIN);        }    }    reader.initFromPage(100, cw.getBytes().toInputStream());    for (int i = 0; i < 100; i++) {        assertEquals(i, reader.readInteger());    }        reader.initFromPage(100, cw.getBytes().toInputStream());    for (int i = 0; i < 100; i += 2) {        assertEquals(i, reader.readInteger());        reader.skip();    }        reader.initFromPage(100, cw.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < 100; i += skipCount + 1) {        skipCount = (100 - i) / 2;        assertEquals(i, reader.readInteger());        reader.skip(skipCount);    }}
public void parquet-mr_f3393_0() throws IOException
{    int slabSize = 100;    int maxDictionaryByteSize = 50;    final FallbackValuesWriter<PlainIntegerDictionaryValuesWriter, PlainValuesWriter> cw = newPlainIntegerDictionaryValuesWriter(maxDictionaryByteSize, slabSize);        ValuesReader reader = new PlainValuesReader.IntegerPlainValuesReader();    roundTripInt(cw, reader, maxDictionaryByteSize);        cw.reset();    assertEquals(0, cw.getBufferedSize());    cw.resetDictionary();    roundTripInt(cw, reader, maxDictionaryByteSize);}
public void parquet-mr_f3394_0() throws IOException
{    int COUNT = 2000;    int COUNT2 = 4000;    final FallbackValuesWriter<PlainFloatDictionaryValuesWriter, PlainValuesWriter> cw = newPlainFloatDictionaryValuesWriter(10000, 10000);    for (float i = 0; i < COUNT; i++) {        cw.writeFloat(i % 50);    }    BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    for (float i = COUNT2; i > 0; i--) {        cw.writeFloat(i % 50);    }    BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    DictionaryValuesReader cr = initDicReader(cw, FLOAT);    cr.initFromPage(COUNT, bytes1.toInputStream());    for (float i = 0; i < COUNT; i++) {        float back = cr.readFloat();        assertEquals(i % 50, back, 0.0f);    }    cr.initFromPage(COUNT2, bytes2.toInputStream());    for (float i = COUNT2; i > 0; i--) {        float back = cr.readFloat();        assertEquals(i % 50, back, 0.0f);    }}
private void parquet-mr_f3395_0(FallbackValuesWriter<PlainFloatDictionaryValuesWriter, PlainValuesWriter> cw, ValuesReader reader, int maxDictionaryByteSize) throws IOException
{    int fallBackThreshold = maxDictionaryByteSize / 4;    for (float i = 0; i < 100; i++) {        cw.writeFloat(i);        if (i < fallBackThreshold) {            assertEquals(cw.getEncoding(), PLAIN_DICTIONARY);        } else {            assertEquals(cw.getEncoding(), PLAIN);        }    }    reader.initFromPage(100, cw.getBytes().toInputStream());    for (float i = 0; i < 100; i++) {        assertEquals(i, reader.readFloat(), 0.00001);    }        reader.initFromPage(100, cw.getBytes().toInputStream());    for (int i = 0; i < 100; i += 2) {        assertEquals(i, reader.readFloat(), 0.0f);        reader.skip();    }        reader.initFromPage(100, cw.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < 100; i += skipCount + 1) {        skipCount = (100 - i) / 2;        assertEquals(i, reader.readFloat(), 0.0f);        reader.skip(skipCount);    }}
public void parquet-mr_f3396_0() throws IOException
{    int slabSize = 100;    int maxDictionaryByteSize = 50;    final FallbackValuesWriter<PlainFloatDictionaryValuesWriter, PlainValuesWriter> cw = newPlainFloatDictionaryValuesWriter(maxDictionaryByteSize, slabSize);        ValuesReader reader = new PlainValuesReader.FloatPlainValuesReader();    roundTripFloat(cw, reader, maxDictionaryByteSize);        cw.reset();    assertEquals(0, cw.getBufferedSize());    cw.resetDictionary();    roundTripFloat(cw, reader, maxDictionaryByteSize);}
public void parquet-mr_f3397_0() throws IOException
{    FallbackValuesWriter<PlainIntegerDictionaryValuesWriter, PlainValuesWriter> cw = newPlainIntegerDictionaryValuesWriter(100, 100);    cw.writeInteger(34);    cw.writeInteger(34);    getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    DictionaryValuesReader reader = initDicReader(cw, INT32);            ByteBuffer bytes = ByteBuffer.wrap(new byte[] { 0x00, 0x01, 0x02, 0x03 });    ByteBufferInputStream stream = ByteBufferInputStream.wrap(bytes);    stream.skipFully(stream.available());    reader.initFromPage(100, stream);        reader = initDicReader(cw, INT32);    int offset = bytes.remaining();    reader.initFromPage(100, bytes, offset);}
private DictionaryValuesReader parquet-mr_f3398_0(ValuesWriter cw, PrimitiveTypeName type) throws IOException
{    final DictionaryPage dictionaryPage = cw.toDictPageAndClose().copy();    final ColumnDescriptor descriptor = new ColumnDescriptor(new String[] { "foo" }, type, 0, 0);    final Dictionary dictionary = PLAIN.initDictionary(descriptor, dictionaryPage);    final DictionaryValuesReader cr = new DictionaryValuesReader(dictionary);    return cr;}
private void parquet-mr_f3399_0(int COUNT, BytesInput bytes, ValuesReader cr, String prefix) throws IOException
{    cr.initFromPage(COUNT, bytes.toInputStream());    for (int i = 0; i < COUNT; i++) {        Assert.assertEquals(prefix + i, cr.readBytes().toStringUsingUTF8());    }}
private void parquet-mr_f3400_0(int COUNT, BytesInput bytes, ValuesReader cr, String prefix) throws IOException
{    cr.initFromPage(COUNT, bytes.toInputStream());    for (int i = 0; i < COUNT; i++) {        Assert.assertEquals(prefix + i % 10, cr.readBytes().toStringUsingUTF8());    }}
private void parquet-mr_f3401_0(int COUNT, ValuesWriter cw, String prefix)
{    for (int i = 0; i < COUNT; i++) {        cw.writeBytes(Binary.fromString(prefix + i));    }}
private void parquet-mr_f3402_0(int COUNT, ValuesWriter cw, String prefix)
{    for (int i = 0; i < COUNT; i++) {        cw.writeBytes(Binary.fromString(prefix + i % 10));    }}
private void parquet-mr_f3403_0(int COUNT, ValuesWriter cw, String prefix)
{    Binary reused = Binary.fromReusedByteArray((prefix + "0").getBytes(StandardCharsets.UTF_8));    for (int i = 0; i < COUNT; i++) {        Binary content = Binary.fromString(prefix + i % 10);        System.arraycopy(content.getBytesUnsafe(), 0, reused.getBytesUnsafe(), 0, reused.length());        cw.writeBytes(reused);    }}
private BytesInput parquet-mr_f3404_0(ValuesWriter cw, Encoding encoding) throws IOException
{    BytesInput bytes = BytesInput.copy(cw.getBytes());    assertEquals(encoding, cw.getEncoding());    cw.reset();    return bytes;}
public void parquet-mr_f3405_0()
{    doTestValueWriter(PrimitiveTypeName.BOOLEAN, WriterVersion.PARQUET_1_0, true, BooleanPlainValuesWriter.class);}
public void parquet-mr_f3406_0()
{    doTestValueWriter(PrimitiveTypeName.BOOLEAN, WriterVersion.PARQUET_2_0, true, RunLengthBitPackingHybridValuesWriter.class);}
public void parquet-mr_f3407_0()
{    doTestValueWriter(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, WriterVersion.PARQUET_1_0, true, FixedLenByteArrayPlainValuesWriter.class);}
public void parquet-mr_f3408_0()
{    doTestValueWriter(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, WriterVersion.PARQUET_2_0, true, DictionaryValuesWriter.class, DeltaByteArrayWriter.class);}
public void parquet-mr_f3409_0()
{    doTestValueWriter(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, WriterVersion.PARQUET_2_0, false, DeltaByteArrayWriter.class);}
public void parquet-mr_f3410_0()
{    doTestValueWriter(PrimitiveTypeName.BINARY, WriterVersion.PARQUET_1_0, true, PlainBinaryDictionaryValuesWriter.class, PlainValuesWriter.class);}
public void parquet-mr_f3411_0()
{    doTestValueWriter(PrimitiveTypeName.BINARY, WriterVersion.PARQUET_1_0, false, PlainValuesWriter.class);}
public void parquet-mr_f3412_0()
{    doTestValueWriter(PrimitiveTypeName.BINARY, WriterVersion.PARQUET_2_0, true, PlainBinaryDictionaryValuesWriter.class, DeltaByteArrayWriter.class);}
public void parquet-mr_f3413_0()
{    doTestValueWriter(PrimitiveTypeName.BINARY, WriterVersion.PARQUET_2_0, false, DeltaByteArrayWriter.class);}
public void parquet-mr_f3414_0()
{    doTestValueWriter(PrimitiveTypeName.INT32, WriterVersion.PARQUET_1_0, true, PlainIntegerDictionaryValuesWriter.class, PlainValuesWriter.class);}
public void parquet-mr_f3415_0()
{    doTestValueWriter(PrimitiveTypeName.INT32, WriterVersion.PARQUET_1_0, false, PlainValuesWriter.class);}
public void parquet-mr_f3416_0()
{    doTestValueWriter(PrimitiveTypeName.INT32, WriterVersion.PARQUET_2_0, true, PlainIntegerDictionaryValuesWriter.class, DeltaBinaryPackingValuesWriter.class);}
public void parquet-mr_f3417_0()
{    doTestValueWriter(PrimitiveTypeName.INT32, WriterVersion.PARQUET_2_0, false, DeltaBinaryPackingValuesWriter.class);}
public void parquet-mr_f3418_0()
{    doTestValueWriter(PrimitiveTypeName.INT64, WriterVersion.PARQUET_1_0, true, PlainLongDictionaryValuesWriter.class, PlainValuesWriter.class);}
public void parquet-mr_f3419_0()
{    doTestValueWriter(PrimitiveTypeName.INT64, WriterVersion.PARQUET_1_0, false, PlainValuesWriter.class);}
public void parquet-mr_f3420_0()
{    doTestValueWriter(PrimitiveTypeName.INT64, WriterVersion.PARQUET_2_0, true, PlainLongDictionaryValuesWriter.class, DeltaBinaryPackingValuesWriterForLong.class);}
public void parquet-mr_f3421_0()
{    doTestValueWriter(PrimitiveTypeName.INT64, WriterVersion.PARQUET_2_0, false, DeltaBinaryPackingValuesWriterForLong.class);}
public void parquet-mr_f3422_0()
{    doTestValueWriter(PrimitiveTypeName.INT96, WriterVersion.PARQUET_1_0, true, PlainFixedLenArrayDictionaryValuesWriter.class, FixedLenByteArrayPlainValuesWriter.class);}
public void parquet-mr_f3423_0()
{    doTestValueWriter(PrimitiveTypeName.INT96, WriterVersion.PARQUET_1_0, false, FixedLenByteArrayPlainValuesWriter.class);}
public void parquet-mr_f3424_0()
{    doTestValueWriter(PrimitiveTypeName.INT96, WriterVersion.PARQUET_2_0, true, PlainFixedLenArrayDictionaryValuesWriter.class, FixedLenByteArrayPlainValuesWriter.class);}
public void parquet-mr_f3425_0()
{    doTestValueWriter(PrimitiveTypeName.INT96, WriterVersion.PARQUET_2_0, false, FixedLenByteArrayPlainValuesWriter.class);}
public void parquet-mr_f3426_0()
{    doTestValueWriter(PrimitiveTypeName.DOUBLE, WriterVersion.PARQUET_1_0, true, PlainDoubleDictionaryValuesWriter.class, PlainValuesWriter.class);}
public void parquet-mr_f3427_0()
{    doTestValueWriter(PrimitiveTypeName.DOUBLE, WriterVersion.PARQUET_1_0, false, PlainValuesWriter.class);}
public void parquet-mr_f3428_0()
{    doTestValueWriter(PrimitiveTypeName.DOUBLE, WriterVersion.PARQUET_2_0, true, PlainDoubleDictionaryValuesWriter.class, PlainValuesWriter.class);}
public void parquet-mr_f3429_0()
{    doTestValueWriter(PrimitiveTypeName.DOUBLE, WriterVersion.PARQUET_2_0, false, PlainValuesWriter.class);}
public void parquet-mr_f3430_0()
{    doTestValueWriter(PrimitiveTypeName.FLOAT, WriterVersion.PARQUET_1_0, true, PlainFloatDictionaryValuesWriter.class, PlainValuesWriter.class);}
public void parquet-mr_f3431_0()
{    doTestValueWriter(PrimitiveTypeName.FLOAT, WriterVersion.PARQUET_1_0, false, PlainValuesWriter.class);}
public void parquet-mr_f3432_0()
{    doTestValueWriter(PrimitiveTypeName.FLOAT, WriterVersion.PARQUET_2_0, true, PlainFloatDictionaryValuesWriter.class, PlainValuesWriter.class);}
public void parquet-mr_f3433_0()
{    doTestValueWriter(PrimitiveTypeName.FLOAT, WriterVersion.PARQUET_2_0, false, PlainValuesWriter.class);}
private void parquet-mr_f3434_0(PrimitiveTypeName typeName, WriterVersion version, boolean enableDictionary, Class<? extends ValuesWriter> expectedValueWriterClass)
{    ColumnDescriptor mockPath = getMockColumn(typeName);    ValuesWriterFactory factory = getDefaultFactory(version, enableDictionary);    ValuesWriter writer = factory.newValuesWriter(mockPath);    validateWriterType(writer, expectedValueWriterClass);}
private void parquet-mr_f3435_0(PrimitiveTypeName typeName, WriterVersion version, boolean enableDictionary, Class<? extends ValuesWriter> initialValueWriterClass, Class<? extends ValuesWriter> fallbackValueWriterClass)
{    ColumnDescriptor mockPath = getMockColumn(typeName);    ValuesWriterFactory factory = getDefaultFactory(version, enableDictionary);    ValuesWriter writer = factory.newValuesWriter(mockPath);    validateFallbackWriter(writer, initialValueWriterClass, fallbackValueWriterClass);}
private ColumnDescriptor parquet-mr_f3436_0(PrimitiveTypeName typeName)
{    ColumnDescriptor mockPath = mock(ColumnDescriptor.class);    when(mockPath.getType()).thenReturn(typeName);    return mockPath;}
private ValuesWriterFactory parquet-mr_f3437_0(WriterVersion writerVersion, boolean enableDictionary)
{    ValuesWriterFactory factory = new DefaultValuesWriterFactory();    ParquetProperties.builder().withDictionaryEncoding(enableDictionary).withWriterVersion(writerVersion).withValuesWriterFactory(factory).build();    return factory;}
private void parquet-mr_f3438_0(ValuesWriter writer, Class<? extends ValuesWriter> valuesWriterClass)
{    assertTrue("Not instance of: " + valuesWriterClass.getName(), valuesWriterClass.isInstance(writer));}
private void parquet-mr_f3439_0(ValuesWriter writer, Class<? extends ValuesWriter> initialWriterClass, Class<? extends ValuesWriter> fallbackWriterClass)
{    validateWriterType(writer, FallbackValuesWriter.class);    FallbackValuesWriter wr = (FallbackValuesWriter) writer;    validateWriterType(wr.initialWriter, initialWriterClass);    validateWriterType(wr.fallBackWriter, fallbackWriterClass);}
public String parquet-mr_f3440_0(int len)
{    StringBuffer out = new StringBuffer();    while (out.length() < len) {        int idx = Math.abs((rand.nextInt() % alphanumeric.length));        out.append(alphanumeric[idx]);    }    return out.toString();}
private char[] parquet-mr_f3441_0()
{    StringBuffer buf = new StringBuffer(128);        for (int i = 48; i <= 57; i++) buf.append((char) i);        for (int i = 65; i <= 90; i++) buf.append((char) i);        for (int i = 97; i <= 122; i++) buf.append((char) i);    return buf.toString().toCharArray();}
public void parquet-mr_f3442_0() throws Exception
{    for (int i = 0; i <= 32; i++) {        doIntegrationTest(i);    }}
private void parquet-mr_f3443_0(int bitWidth) throws Exception
{    long modValue = 1L << bitWidth;    RunLengthBitPackingHybridEncoder encoder = new RunLengthBitPackingHybridEncoder(bitWidth, 1000, 64000, new DirectByteBufferAllocator());    int numValues = 0;    for (int i = 0; i < 100; i++) {        encoder.writeInt((int) (i % modValue));    }    numValues += 100;    for (int i = 0; i < 100; i++) {        encoder.writeInt((int) (77 % modValue));    }    numValues += 100;    for (int i = 0; i < 100; i++) {        encoder.writeInt((int) (88 % modValue));    }    numValues += 100;    for (int i = 0; i < 1000; i++) {        encoder.writeInt((int) (i % modValue));        encoder.writeInt((int) (i % modValue));        encoder.writeInt((int) (i % modValue));    }    numValues += 3000;    for (int i = 0; i < 1000; i++) {        encoder.writeInt((int) (17 % modValue));    }    numValues += 1000;    ByteBuffer encodedBytes = encoder.toBytes().toByteBuffer();    ByteBufferInputStream in = ByteBufferInputStream.wrap(encodedBytes);    RunLengthBitPackingHybridDecoder decoder = new RunLengthBitPackingHybridDecoder(bitWidth, in);    for (int i = 0; i < 100; i++) {        assertEquals(i % modValue, decoder.readInt());    }    for (int i = 0; i < 100; i++) {        assertEquals(77 % modValue, decoder.readInt());    }    for (int i = 0; i < 100; i++) {        assertEquals(88 % modValue, decoder.readInt());    }    for (int i = 0; i < 1000; i++) {        assertEquals(i % modValue, decoder.readInt());        assertEquals(i % modValue, decoder.readInt());        assertEquals(i % modValue, decoder.readInt());    }    for (int i = 0; i < 1000; i++) {        assertEquals(17 % modValue, decoder.readInt());    }}
private RunLengthBitPackingHybridEncoder parquet-mr_f3444_0()
{    return getRunLengthBitPackingHybridEncoder(3, 5, 10);}
private RunLengthBitPackingHybridEncoder parquet-mr_f3445_0(int bitWidth, int initialCapacity, int pageSize)
{    return new RunLengthBitPackingHybridEncoder(bitWidth, initialCapacity, pageSize, new DirectByteBufferAllocator());}
public void parquet-mr_f3446_0() throws Exception
{    RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder();    for (int i = 0; i < 100; i++) {        encoder.writeInt(4);    }    for (int i = 0; i < 100; i++) {        encoder.writeInt(5);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());        assertEquals(200, BytesUtils.readUnsignedVarInt(is));        assertEquals(4, BytesUtils.readIntLittleEndianOnOneByte(is));        assertEquals(200, BytesUtils.readUnsignedVarInt(is));        assertEquals(5, BytesUtils.readIntLittleEndianOnOneByte(is));        assertEquals(-1, is.read());}
public void parquet-mr_f3447_0() throws Exception
{                RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder();    for (int i = 0; i < 10; i++) {        encoder.writeInt(0);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());        assertEquals(20, BytesUtils.readUnsignedVarInt(is));        assertEquals(0, BytesUtils.readIntLittleEndianOnOneByte(is));        assertEquals(-1, is.read());}
public void parquet-mr_f3448_0() throws Exception
{    RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder(0, 5, 10);    for (int i = 0; i < 10; i++) {        encoder.writeInt(0);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());        assertEquals(20, BytesUtils.readUnsignedVarInt(is));        assertEquals(-1, is.read());}
public void parquet-mr_f3449_0() throws Exception
{    RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder();    for (int i = 0; i < 100; i++) {        encoder.writeInt(i % 3);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());        assertEquals(27, BytesUtils.readUnsignedVarInt(is));    List<Integer> values = unpack(3, 104, is);    for (int i = 0; i < 100; i++) {        assertEquals(i % 3, (int) values.get(i));    }        assertEquals(-1, is.read());}
public void parquet-mr_f3450_0() throws Exception
{    RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder();    for (int i = 0; i < 1000; i++) {        encoder.writeInt(i % 3);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());                assertEquals(127, BytesUtils.readUnsignedVarInt(is));    List<Integer> values = unpack(3, 504, is);    for (int i = 0; i < 504; i++) {        assertEquals(i % 3, (int) values.get(i));    }            assertEquals(125, BytesUtils.readUnsignedVarInt(is));    values = unpack(3, 496, is);    for (int i = 0; i < 496; i++) {        assertEquals((i + 504) % 3, (int) values.get(i));    }        assertEquals(-1, is.read());}
public void parquet-mr_f3451_0() throws Exception
{    RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder();        encoder.writeInt(0);    encoder.writeInt(1);    encoder.writeInt(0);    encoder.writeInt(1);    encoder.writeInt(0);        encoder.writeInt(2);    encoder.writeInt(2);    encoder.writeInt(2);        for (int i = 0; i < 100; i++) {        encoder.writeInt(2);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());        assertEquals(3, BytesUtils.readUnsignedVarInt(is));    List<Integer> values = unpack(3, 8, is);    assertEquals(Arrays.asList(0, 1, 0, 1, 0, 2, 2, 2), values);        assertEquals(200, BytesUtils.readUnsignedVarInt(is));        assertEquals(2, BytesUtils.readIntLittleEndianOnOneByte(is));        assertEquals(-1, is.read());}
public void parquet-mr_f3452_0() throws Exception
{    RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder(5, 5, 10);    for (int i = 0; i < 9; i++) {        encoder.writeInt(i + 1);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());        assertEquals(5, BytesUtils.readUnsignedVarInt(is));    List<Integer> values = unpack(5, 16, is);    assertEquals(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 0, 0, 0, 0, 0, 0), values);    assertEquals(-1, is.read());}
public void parquet-mr_f3453_0() throws Exception
{    RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder(9, 100, 1000);        for (int i = 0; i < 25; i++) {        encoder.writeInt(17);    }        for (int i = 0; i < 7; i++) {        encoder.writeInt(7);    }    encoder.writeInt(8);    encoder.writeInt(9);    encoder.writeInt(10);        for (int i = 0; i < 25; i++) {        encoder.writeInt(6);    }        for (int i = 0; i < 8; i++) {        encoder.writeInt(5);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());        assertEquals(50, BytesUtils.readUnsignedVarInt(is));        assertEquals(17, BytesUtils.readIntLittleEndianOnTwoBytes(is));        assertEquals(5, BytesUtils.readUnsignedVarInt(is));    List<Integer> values = unpack(9, 16, is);    int v = 0;    for (int i = 0; i < 7; i++) {        assertEquals(7, (int) values.get(v));        v++;    }    assertEquals(8, (int) values.get(v++));    assertEquals(9, (int) values.get(v++));    assertEquals(10, (int) values.get(v++));    for (int i = 0; i < 6; i++) {        assertEquals(6, (int) values.get(v));        v++;    }        assertEquals(38, BytesUtils.readUnsignedVarInt(is));        assertEquals(6, BytesUtils.readIntLittleEndianOnTwoBytes(is));        assertEquals(16, BytesUtils.readUnsignedVarInt(is));        assertEquals(5, BytesUtils.readIntLittleEndianOnTwoBytes(is));        assertEquals(-1, is.read());}
public void parquet-mr_f3454_0() throws Exception
{    byte[] bytes = new byte[2];            bytes[0] = (1 << 1) | 1;    bytes[1] = (1 << 0) | (2 << 2) | (3 << 4);    ByteArrayInputStream stream = new ByteArrayInputStream(bytes);    RunLengthBitPackingHybridDecoder decoder = new RunLengthBitPackingHybridDecoder(2, stream);    assertEquals(decoder.readInt(), 1);    assertEquals(decoder.readInt(), 2);    assertEquals(decoder.readInt(), 3);    assertEquals(stream.available(), 0);}
private static List<Integer> parquet-mr_f3455_0(int bitWidth, int numValues, ByteArrayInputStream is) throws Exception
{    BytePacker packer = Packer.LITTLE_ENDIAN.newBytePacker(bitWidth);    int[] unpacked = new int[8];    byte[] next8Values = new byte[bitWidth];    List<Integer> values = new ArrayList<Integer>(numValues);    while (values.size() < numValues) {        for (int i = 0; i < bitWidth; i++) {            next8Values[i] = (byte) is.read();        }        packer.unpack8Values(next8Values, 0, unpacked, 0);        for (int v = 0; v < 8; v++) {            values.add(unpacked[v]);        }    }    return values;}
public void parquet-mr_f3456_0()
{}
public void parquet-mr_f3457_0(int valueCount, ByteBuffer page, int offset) throws IOException
{    data = new byte[valueCount];    ByteBuffer buffer = page.duplicate();    buffer.position(offset);    buffer.get(data);}
public void parquet-mr_f3458_0(String expected)
{    Assert.assertEquals(expected, new String(data));}
public void parquet-mr_f3459_0()
{}
public Binary parquet-mr_f3460_0()
{    return Binary.fromConstantByteArray(data);}
public void parquet-mr_f3461_0(int valueCount, ByteBufferInputStream in) throws IOException
{    data = new byte[valueCount];    int off = 0;    int len = valueCount;    int read;    while ((read = in.read(data, off, len)) != -1 && len > 0) {        off += read;        len -= read;    }}
public void parquet-mr_f3462_0()
{}
public Binary parquet-mr_f3463_0()
{    return Binary.fromConstantByteArray(data);}
public void parquet-mr_f3464_0() throws IOException
{    ValuesReader reader = new InvalidValuesReaderImpl();    try {        validateWithByteArray(reader);        fail("An UnsupportedOperationException should have been thrown");    } catch (UnsupportedOperationException e) {    }    try {        validateWithByteBuffer(reader);        fail("An UnsupportedOperationException should have been thrown");    } catch (UnsupportedOperationException e) {    }    try {        validateWithByteBufferInputStream(reader);        fail("An UnsupportedOperationException should have been thrown");    } catch (UnsupportedOperationException e) {    }}
public void parquet-mr_f3465_0() throws IOException
{    ValuesReader reader = new ByteBufferValuesReaderImpl();    validateWithByteArray(reader);    validateWithByteBuffer(reader);    validateWithByteBufferInputStream(reader);}
public void parquet-mr_f3466_0() throws IOException
{    ValuesReader reader = new ByteBufferInputStreamValuesReaderImpl();    validateWithByteArray(reader);    validateWithByteBuffer(reader);    validateWithByteBufferInputStream(reader);}
private void parquet-mr_f3467_0(ValuesReader reader) throws IOException
{    reader.initFromPage(25, "==padding==The expected page content".getBytes(), 11);    assertEquals("The expected page content", reader.readBytes().toStringUsingUTF8());}
private void parquet-mr_f3468_0(ValuesReader reader) throws IOException
{    reader.initFromPage(25, ByteBuffer.wrap("==padding==The expected page content".getBytes()), 11);    assertEquals("The expected page content", reader.readBytes().toStringUsingUTF8());}
private void parquet-mr_f3469_0(ValuesReader reader) throws IOException
{    ByteBufferInputStream bbis = ByteBufferInputStream.wrap(ByteBuffer.wrap("==padding==".getBytes()), ByteBuffer.wrap("The expected ".getBytes()), ByteBuffer.wrap("page content".getBytes()));    bbis.skipFully(11);    reader.initFromPage(25, bbis);    assertEquals("The expected page content", reader.readBytes().toStringUsingUTF8());}
public static String[] parquet-mr_f3470_0(int numSamples, int maxLength)
{    String[] samples = new String[numSamples];    for (int i = 0; i < numSamples; i++) {        int len = randomLen.nextInt(maxLength);        samples[i] = randomStr.get(len);    }    return samples;}
public static void parquet-mr_f3471_0(ValuesWriter writer, int[] ints) throws IOException
{    for (int i = 0; i < ints.length; i++) {        writer.writeInteger(ints[i]);    }}
public static void parquet-mr_f3472_0(ValuesWriter writer, String[] strings) throws IOException
{    for (int i = 0; i < strings.length; i++) {        writer.writeBytes(Binary.fromString(strings[i]));    }}
public static Binary[] parquet-mr_f3473_0(ValuesReader reader, ByteBufferInputStream stream, int length) throws IOException
{    Binary[] bins = new Binary[length];    reader.initFromPage(length, stream);    for (int i = 0; i < length; i++) {        bins[i] = reader.readBytes();    }    return bins;}
public static int[] parquet-mr_f3474_0(ValuesReader reader, ByteBufferInputStream stream, int length) throws IOException
{    int[] ints = new int[length];    reader.initFromPage(length, stream);    for (int i = 0; i < length; i++) {        ints[i] = reader.readInteger();    }    return ints;}
public void parquet-mr_f3475_0()
{    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0 (build abcd)", PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0 (build abcd)", PrimitiveTypeName.DOUBLE));}
public void parquet-mr_f3476_0()
{    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.4.2 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.100 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.7.999 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.22rc99 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.22rc99-SNAPSHOT (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.1-SNAPSHOT (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0t-01-abcdefg (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("unparseable string", PrimitiveTypeName.BINARY));        assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version  (build abcd)", PrimitiveTypeName.BINARY));        assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0 (build )", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0 (build)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version (build)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("imapla version 1.6.0 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("imapla version 1.10.0 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.8.0 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.8.1 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.8.1rc3 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.8.1rc3-SNAPSHOT (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.9.0 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 2.0.0 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.9.0t-01-abcdefg (build abcd)", PrimitiveTypeName.BINARY));        assertFalse(CorruptStatistics.shouldIgnoreStatistics("impala version (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("impala version  (build abcd)", PrimitiveTypeName.BINARY));        assertFalse(CorruptStatistics.shouldIgnoreStatistics("impala version 1.6.0 (build )", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("impala version 1.6.0 (build)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("impala version (build)", PrimitiveTypeName.BINARY));}
public void parquet-mr_f3477_0()
{    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.5.0-cdh5.4.999 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.5.0-cdh5.5.0-SNAPSHOT (build 956ed6c14c611b4c4eaaa1d6e5b9a9c6d4dfa336)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.5.0-cdh5.5.0 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.5.0-cdh5.5.1 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.5.0-cdh5.6.0 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.4.10 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.5.0 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.5.1 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.7.0 (build abcd)", PrimitiveTypeName.BINARY));}
public boolean parquet-mr_f3478_0(Integer value)
{    return false;}
public boolean parquet-mr_f3479_0(Statistics<Integer> statistics)
{    return false;}
public boolean parquet-mr_f3480_0(Statistics<Integer> statistics)
{    return false;}
public void parquet-mr_f3481_0()
{    FilterPredicate outerAnd = predicate;    assertTrue(outerAnd instanceof And);    FilterPredicate not = ((And) outerAnd).getLeft();    FilterPredicate gt = ((And) outerAnd).getRight();    assertTrue(not instanceof Not);    FilterPredicate or = ((Not) not).getPredicate();    assertTrue(or instanceof Or);    FilterPredicate leftEq = ((Or) or).getLeft();    FilterPredicate rightNotEq = ((Or) or).getRight();    assertTrue(leftEq instanceof Eq);    assertTrue(rightNotEq instanceof NotEq);    assertEquals(7, ((Eq) leftEq).getValue());    assertEquals(17, ((NotEq) rightNotEq).getValue());    assertEquals(ColumnPath.get("a", "b", "c"), ((Eq) leftEq).getColumn().getColumnPath());    assertEquals(ColumnPath.get("a", "b", "c"), ((NotEq) rightNotEq).getColumn().getColumnPath());    assertTrue(gt instanceof Gt);    assertEquals(100.0, ((Gt) gt).getValue());    assertEquals(ColumnPath.get("x", "y", "z"), ((Gt) gt).getColumn().getColumnPath());}
public void parquet-mr_f3482_0()
{    FilterPredicate pred = or(predicate, notEq(binColumn, Binary.fromString("foobarbaz")));    assertEquals("or(and(not(or(eq(a.b.c, 7), noteq(a.b.c, 17))), gt(x.y.z, 100.0)), " + "noteq(a.string.column, Binary{\"foobarbaz\"}))", pred.toString());}
public void parquet-mr_f3483_0()
{    FilterPredicate predicate = or(eq(doubleColumn, 12.0), userDefined(intColumn, DummyUdp.class));    assertTrue(predicate instanceof Or);    FilterPredicate ud = ((Or) predicate).getRight();    assertTrue(ud instanceof UserDefinedByClass);    assertEquals(DummyUdp.class, ((UserDefinedByClass) ud).getUserDefinedPredicateClass());    assertTrue(((UserDefined) ud).getUserDefinedPredicate() instanceof DummyUdp);}
public void parquet-mr_f3484_0() throws Exception
{    BinaryColumn binary = binaryColumn("foo");    FilterPredicate p = and(or(and(userDefined(intColumn, DummyUdp.class), predicate), eq(binary, Binary.fromString("hi"))), userDefined(longColumn, new IsMultipleOf(7)));    ByteArrayOutputStream baos = new ByteArrayOutputStream();    ObjectOutputStream oos = new ObjectOutputStream(baos);    oos.writeObject(p);    oos.close();    ObjectInputStream is = new ObjectInputStream(new ByteArrayInputStream(baos.toByteArray()));    FilterPredicate read = (FilterPredicate) is.readObject();    assertEquals(p, read);}
public boolean parquet-mr_f3485_0(Long value)
{    if (value == null) {        return false;    }    return value % of == 0;}
public boolean parquet-mr_f3486_0(Statistics<Long> statistics)
{    return false;}
public boolean parquet-mr_f3487_0(Statistics<Long> statistics)
{    return false;}
public boolean parquet-mr_f3488_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    IsMultipleOf that = (IsMultipleOf) o;    return this.of == that.of;}
public int parquet-mr_f3489_0()
{    return new Long(of).hashCode();}
public String parquet-mr_f3490_0()
{    return "IsMultipleOf(" + of + ")";}
private static void parquet-mr_f3491_0(FilterPredicate p)
{    assertEquals(p, rewrite(p));}
public void parquet-mr_f3492_0()
{    UserDefined<Integer, DummyUdp> ud = userDefined(intColumn, DummyUdp.class);    assertNoOp(eq(intColumn, 17));    assertNoOp(notEq(intColumn, 17));    assertNoOp(lt(intColumn, 17));    assertNoOp(ltEq(intColumn, 17));    assertNoOp(gt(intColumn, 17));    assertNoOp(gtEq(intColumn, 17));    assertNoOp(and(eq(intColumn, 17), eq(doubleColumn, 12.0)));    assertNoOp(or(eq(intColumn, 17), eq(doubleColumn, 12.0)));    assertNoOp(ud);    assertEquals(notEq(intColumn, 17), rewrite(not(eq(intColumn, 17))));    assertEquals(eq(intColumn, 17), rewrite(not(notEq(intColumn, 17))));    assertEquals(gtEq(intColumn, 17), rewrite(not(lt(intColumn, 17))));    assertEquals(gt(intColumn, 17), rewrite(not(ltEq(intColumn, 17))));    assertEquals(ltEq(intColumn, 17), rewrite(not(gt(intColumn, 17))));    assertEquals(lt(intColumn, 17), rewrite(not(gtEq(intColumn, 17))));    assertEquals(new LogicalNotUserDefined<Integer, DummyUdp>(ud), rewrite(not(ud)));    FilterPredicate notedAnd = not(and(eq(intColumn, 17), eq(doubleColumn, 12.0)));    FilterPredicate distributedAnd = or(notEq(intColumn, 17), notEq(doubleColumn, 12.0));    assertEquals(distributedAnd, rewrite(notedAnd));    FilterPredicate andWithNots = and(not(gtEq(intColumn, 17)), lt(intColumn, 7));    FilterPredicate andWithoutNots = and(lt(intColumn, 17), lt(intColumn, 7));    assertEquals(andWithoutNots, rewrite(andWithNots));}
public void parquet-mr_f3493_0()
{    assertEquals(complexCollapsed, rewrite(complex));}
public void parquet-mr_f3494_0()
{    assertEquals(notEq(intColumn, 17), invert(eq(intColumn, 17)));    assertEquals(eq(intColumn, 17), invert(notEq(intColumn, 17)));    assertEquals(gtEq(intColumn, 17), invert(lt(intColumn, 17)));    assertEquals(gt(intColumn, 17), invert(ltEq(intColumn, 17)));    assertEquals(ltEq(intColumn, 17), invert(gt(intColumn, 17)));    assertEquals(lt(intColumn, 17), invert(gtEq(intColumn, 17)));    FilterPredicate andPos = and(eq(intColumn, 17), eq(doubleColumn, 12.0));    FilterPredicate andInv = or(notEq(intColumn, 17), notEq(doubleColumn, 12.0));    assertEquals(andInv, invert(andPos));    FilterPredicate orPos = or(eq(intColumn, 17), eq(doubleColumn, 12.0));    FilterPredicate orInv = and(notEq(intColumn, 17), notEq(doubleColumn, 12.0));    assertEquals(orPos, invert(orInv));    assertEquals(eq(intColumn, 17), invert(not(eq(intColumn, 17))));    UserDefined<Integer, DummyUdp> ud = userDefined(intColumn, DummyUdp.class);    assertEquals(new LogicalNotUserDefined<Integer, DummyUdp>(ud), invert(ud));    assertEquals(ud, invert(not(ud)));    assertEquals(ud, invert(new LogicalNotUserDefined<Integer, DummyUdp>(ud)));}
public void parquet-mr_f3495_0()
{    assertEquals(complexInverse, invert(complex));}
public boolean parquet-mr_f3496_0(Long value)
{    return false;}
public boolean parquet-mr_f3497_0(Statistics<Long> statistics)
{    return false;}
public boolean parquet-mr_f3498_0(Statistics<Long> statistics)
{    return false;}
public void parquet-mr_f3499_0()
{    validate(complexValid, schema);}
public void parquet-mr_f3500_0()
{    try {        validate(complexWrongType, schema);        fail("this should throw");    } catch (IllegalArgumentException e) {        assertEquals("FilterPredicate column: x.bar's declared type (java.lang.Long) does not match the schema found in file metadata. " + "Column x.bar is of type: INT32\n" + "Valid types for this column are: [class java.lang.Integer]", e.getMessage());    }}
public void parquet-mr_f3501_0()
{    validate(eq(stringC, Binary.fromString("larry")), schema);    try {        validate(complexMixedType, schema);        fail("this should throw");    } catch (IllegalArgumentException e) {        assertEquals("Column: x.bar was provided with different types in the same predicate. Found both: (class java.lang.Integer, class java.lang.Long)", e.getMessage());    }}
public void parquet-mr_f3502_0()
{    try {        validate(eq(lotsOfLongs, 10l), schema);        fail("this should throw");    } catch (IllegalArgumentException e) {        assertEquals("FilterPredicates do not currently support repeated columns. Column lotsOfLongs is repeated.", e.getMessage());    }}
public int parquet-mr_f3503_0(InvalidColumnType o)
{    return 0;}
public void parquet-mr_f3504_0()
{    assertTypeValid(intColumn, PrimitiveTypeName.INT32);    assertTypeValid(longColumn, PrimitiveTypeName.INT64);    assertTypeValid(floatColumn, PrimitiveTypeName.FLOAT);    assertTypeValid(doubleColumn, PrimitiveTypeName.DOUBLE);    assertTypeValid(booleanColumn, PrimitiveTypeName.BOOLEAN);    assertTypeValid(binaryColumn, PrimitiveTypeName.BINARY);    assertTypeValid(binaryColumn, PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY);    assertTypeValid(binaryColumn, PrimitiveTypeName.INT96);}
public void parquet-mr_f3505_0()
{    try {        assertTypeValid(intColumn, PrimitiveTypeName.DOUBLE);        fail("This should throw!");    } catch (IllegalArgumentException e) {        assertEquals("FilterPredicate column: int.column's declared type (java.lang.Integer) does not match the " + "schema found in file metadata. Column int.column is of type: " + "DOUBLE\n" + "Valid types for this column are: [class java.lang.Double]", e.getMessage());    }}
public void parquet-mr_f3506_0()
{    try {        assertTypeValid(invalidColumn, PrimitiveTypeName.INT32);        fail("This should throw!");    } catch (IllegalArgumentException e) {        assertEquals("Column invalid.column was declared as type: " + "org.apache.parquet.filter2.predicate.TestValidTypeMap$InvalidColumnType which is not supported " + "in FilterPredicates. Supported types for this column are: [class java.lang.Integer]", e.getMessage());    }}
public static ValueInspector parquet-mr_f3507_0()
{    return new ValueInspector() {        @Override        public void updateNull() {            setResult(true);        }        @Override        public void update(int value) {            setResult(false);        }    };}
public void parquet-mr_f3508_0()
{    setResult(true);}
public void parquet-mr_f3509_0(int value)
{    setResult(false);}
public static ValueInspector parquet-mr_f3510_0()
{    return new ValueInspector() {        @Override        public void updateNull() {            setResult(false);        }        @Override        public void update(int value) {            setResult(value % 2 == 0);        }    };}
public void parquet-mr_f3511_0()
{    setResult(false);}
public void parquet-mr_f3512_0(int value)
{    setResult(value % 2 == 0);}
public static ValueInspector parquet-mr_f3513_0()
{    return new ValueInspector() {        @Override        public void updateNull() {            setResult(false);        }        @Override        public void update(double value) {            setResult(value > 10.0);        }    };}
public void parquet-mr_f3514_0()
{    setResult(false);}
public void parquet-mr_f3515_0(double value)
{    setResult(value > 10.0);}
public void parquet-mr_f3516_0()
{        ValueInspector v = intIsEven();    v.update(11);    assertFalse(evaluate(v));    v.reset();        v.update(12);    assertTrue(evaluate(v));    v.reset();        v.updateNull();    assertFalse(evaluate(v));    v.reset();        ValueInspector intIsNull = intIsNull();    intIsNull.update(10);    assertFalse(evaluate(intIsNull));    intIsNull.reset();        intIsNull.updateNull();    assertTrue(evaluate(intIsNull));    intIsNull.reset();        v.reset();    assertFalse(evaluate(v));        intIsNull.reset();    assertTrue(evaluate(intIsNull));}
private void parquet-mr_f3517_0(ValueInspector v1, ValueInspector v2, int v1Value, int v2Value, boolean expected)
{    v1.update(v1Value);    v2.update(v2Value);    IncrementallyUpdatedFilterPredicate or = new Or(v1, v2);    assertEquals(expected, evaluate(or));    v1.reset();    v2.reset();}
private void parquet-mr_f3518_0(ValueInspector v1, ValueInspector v2, int v1Value, int v2Value, boolean expected)
{    v1.update(v1Value);    v2.update(v2Value);    IncrementallyUpdatedFilterPredicate and = new And(v1, v2);    assertEquals(expected, evaluate(and));    v1.reset();    v2.reset();}
public void parquet-mr_f3519_0()
{    ValueInspector v1 = intIsEven();    ValueInspector v2 = intIsEven();    int F = 11;    int T = 12;        doOrTest(v1, v2, F, F, false);        doOrTest(v1, v2, F, T, true);        doOrTest(v1, v2, T, F, true);        doOrTest(v1, v2, T, T, true);}
public void parquet-mr_f3520_0()
{    ValueInspector v1 = intIsEven();    ValueInspector v2 = intIsEven();    int F = 11;    int T = 12;        doAndTest(v1, v2, F, F, false);        doAndTest(v1, v2, F, T, false);        doAndTest(v1, v2, T, F, false);        doAndTest(v1, v2, T, T, true);}
public void parquet-mr_f3521_0()
{    ValueInspector neverCalled = new ValueInspector() {        @Override        public boolean accept(Visitor visitor) {            throw new ShortCircuitException();        }    };    try {        evaluate(neverCalled);        fail("this should throw");    } catch (ShortCircuitException e) {        }        ValueInspector v = intIsEven();    v.update(10);    IncrementallyUpdatedFilterPredicate or = new Or(v, neverCalled);    assertTrue(evaluate(or));    v.reset();        v.update(11);    IncrementallyUpdatedFilterPredicate and = new And(v, neverCalled);    assertFalse(evaluate(and));    v.reset();}
public boolean parquet-mr_f3522_0(Visitor visitor)
{    throw new ShortCircuitException();}
public void parquet-mr_f3523_0()
{    ValueInspector intIsNull = intIsNull();    ValueInspector intIsEven = intIsEven();    ValueInspector doubleMoreThan10 = doubleMoreThan10();    IncrementallyUpdatedFilterPredicate pred = new Or(intIsNull, new And(intIsEven, doubleMoreThan10));    intIsNull.updateNull();    intIsEven.update(11);    doubleMoreThan10.update(20.0D);    assertTrue(intIsNull.isKnown());    assertTrue(intIsEven.isKnown());    assertTrue(doubleMoreThan10.isKnown());    IncrementallyUpdatedFilterPredicateResetter.reset(pred);    assertFalse(intIsNull.isKnown());    assertFalse(intIsEven.isKnown());    assertFalse(doubleMoreThan10.isKnown());    intIsNull.updateNull();    assertTrue(intIsNull.isKnown());    assertFalse(intIsEven.isKnown());    assertFalse(doubleMoreThan10.isKnown());    IncrementallyUpdatedFilterPredicateResetter.reset(pred);    assertFalse(intIsNull.isKnown());    assertFalse(intIsEven.isKnown());    assertFalse(doubleMoreThan10.isKnown());}
public void parquet-mr_f3524_0()
{    ValueInspector v = intIsEven();        assertFalse(v.isKnown());        try {        v.getResult();        fail("this should throw");    } catch (IllegalStateException e) {        assertEquals("getResult() called on a ValueInspector whose result is not yet known!", e.getMessage());    }        v.update(10);        assertTrue(v.isKnown());    assertTrue(v.getResult());        try {        v.update(11);        fail("this should throw");    } catch (IllegalStateException e) {        assertEquals("setResult() called on a ValueInspector whose result is already known!" + " Did you forget to call reset()?", e.getMessage());    }        v.reset();    assertFalse(v.isKnown());        try {        v.getResult();        fail("this should throw");    } catch (IllegalStateException e) {        assertEquals("getResult() called on a ValueInspector whose result is not yet known!", e.getMessage());    }        v.update(11);    assertTrue(v.isKnown());    assertFalse(v.getResult());}
public void parquet-mr_f3525_0()
{    List<Integer> values = Arrays.asList(2, 4, 7, 3, 8, 8, 11, 200);    ValueInspector v = intIsEven();    for (Integer x : values) {        v.update(x);        assertEquals(x % 2 == 0, v.getResult());        v.reset();    }}
public void parquet-mr_f3526_0()
{    BinaryTruncator truncator = BinaryTruncator.getTruncator(Types.required(BINARY).as(DECIMAL).precision(10).scale(2).named("test_binary_decimal"));    assertEquals(binary(0xFF, 0xFE, 0xFD, 0xFC, 0xFB, 0xFA), truncator.truncateMin(binary(0xFF, 0xFE, 0xFD, 0xFC, 0xFB, 0xFA), 2));    assertEquals(binary(0x01, 0x02, 0x03, 0x04, 0x05, 0x06), truncator.truncateMax(binary(0x01, 0x02, 0x03, 0x04, 0x05, 0x06), 2));}
public void parquet-mr_f3527_0()
{    testTruncator(Types.required(FIXED_LEN_BYTE_ARRAY).length(8).as(DECIMAL).precision(18).scale(4).named("test_fixed_decimal"), false);    testTruncator(Types.required(FIXED_LEN_BYTE_ARRAY).length(12).as(INTERVAL).named("test_fixed_interval"), false);    testTruncator(Types.required(BINARY).as(DECIMAL).precision(10).scale(2).named("test_binary_decimal"), false);    testTruncator(Types.required(INT96).named("test_int96"), false);}
public void parquet-mr_f3528_0()
{    BinaryTruncator truncator = BinaryTruncator.getTruncator(Types.required(BINARY).as(UTF8).named("test_utf8"));        assertEquals(Binary.fromString("abc"), truncator.truncateMin(Binary.fromString("abcdef"), 3));    assertEquals(Binary.fromString("abd"), truncator.truncateMax(Binary.fromString("abcdef"), 3));        assertEquals(Binary.fromString("árvízt"), truncator.truncateMin(Binary.fromString("árvíztűrő"), 9));    assertEquals(Binary.fromString("árvízu"), truncator.truncateMax(Binary.fromString("árvíztűrő"), 9));        assertEquals(Binary.fromString(UTF8_1BYTE_MAX_CHAR + UTF8_2BYTES_MAX_CHAR), truncator.truncateMin(Binary.fromString(UTF8_1BYTE_MAX_CHAR + UTF8_2BYTES_MAX_CHAR + UTF8_3BYTES_MAX_CHAR + UTF8_4BYTES_MAX_CHAR), 5));    assertEquals(Binary.fromString(UTF8_1BYTE_MAX_CHAR + UTF8_2BYTES_MAX_CHAR + UTF8_3BYTES_MAX_CHAR + UTF8_4BYTES_MAX_CHAR), truncator.truncateMax(Binary.fromString(UTF8_1BYTE_MAX_CHAR + UTF8_2BYTES_MAX_CHAR + UTF8_3BYTES_MAX_CHAR + UTF8_4BYTES_MAX_CHAR), 5));        assertEquals(Binary.fromString(UTF8_1BYTE_MAX_CHAR + UTF8_2BYTES_MAX_CHAR + "b" + UTF8_3BYTES_MAX_CHAR), truncator.truncateMax(Binary.fromString(UTF8_1BYTE_MAX_CHAR + UTF8_2BYTES_MAX_CHAR + "a" + UTF8_3BYTES_MAX_CHAR + UTF8_4BYTES_MAX_CHAR), 10));        assertEquals(binary(0xFF, 0xFE, 0xFD), truncator.truncateMin(binary(0xFF, 0xFE, 0xFD, 0xFC, 0xFB, 0xFA), 3));    assertEquals(binary(0xFF, 0xFE, 0xFE), truncator.truncateMax(binary(0xFF, 0xFE, 0xFD, 0xFC, 0xFB, 0xFA), 3));    assertEquals(binary(0xFF, 0xFE, 0xFE, 0x00, 0x00), truncator.truncateMax(binary(0xFF, 0xFE, 0xFD, 0xFF, 0xFF, 0xFF), 5));}
public void parquet-mr_f3529_0()
{    testTruncator(Types.required(BINARY).named("test_binary"), true);    testTruncator(Types.required(BINARY).as(UTF8).named("test_utf8"), true);    testTruncator(Types.required(BINARY).as(ENUM).named("test_enum"), true);    testTruncator(Types.required(BINARY).as(JSON).named("test_json"), true);    testTruncator(Types.required(BINARY).as(BSON).named("test_bson"), true);    testTruncator(Types.required(FIXED_LEN_BYTE_ARRAY).length(5).named("test_fixed"), true);}
private void parquet-mr_f3530_0(PrimitiveType type, boolean strict)
{    BinaryTruncator truncator = BinaryTruncator.getTruncator(type);    Comparator<Binary> comparator = type.comparator();    checkContract(truncator, comparator, Binary.fromString("aaaaaaaaaa"), strict, strict);    checkContract(truncator, comparator, Binary.fromString("árvíztűrő tükörfúrógép"), strict, strict);    checkContract(truncator, comparator, Binary.fromString("aaaaaaaaaa" + UTF8_3BYTES_MAX_CHAR), strict, strict);    checkContract(truncator, comparator, Binary.fromString("a" + UTF8_3BYTES_MAX_CHAR + UTF8_1BYTE_MAX_CHAR), strict, strict);    checkContract(truncator, comparator, Binary.fromConstantByteArray(new byte[] { (byte) 0xFE, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, (byte) 0xFF }), strict, strict);        checkContract(truncator, comparator, Binary.fromString(""), false, false);        checkContract(truncator, comparator, Binary.fromString(UTF8_1BYTE_MAX_CHAR + UTF8_4BYTES_MAX_CHAR + UTF8_3BYTES_MAX_CHAR + UTF8_4BYTES_MAX_CHAR + UTF8_2BYTES_MAX_CHAR + UTF8_3BYTES_MAX_CHAR + UTF8_3BYTES_MAX_CHAR + UTF8_1BYTE_MAX_CHAR + UTF8_2BYTES_MAX_CHAR + UTF8_3BYTES_MAX_CHAR + UTF8_4BYTES_MAX_CHAR), strict, false);        checkContract(truncator, comparator, binary(0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF), strict, false);}
private void parquet-mr_f3531_0(BinaryTruncator truncator, Comparator<Binary> comparator, Binary value, boolean strictMin, boolean strictMax)
{    int length = value.length();        assertSame(value, truncator.truncateMin(value, length));    assertSame(value, truncator.truncateMax(value, length));    assertSame(value, truncator.truncateMin(value, random(length + 1, length * 2 + 1)));    assertSame(value, truncator.truncateMax(value, random(length + 1, length * 2 + 1)));    if (length > 1) {        checkMinContract(truncator, comparator, value, length - 1, strictMin);        checkMaxContract(truncator, comparator, value, length - 1, strictMax);        checkMinContract(truncator, comparator, value, random(1, length - 1), strictMin);        checkMaxContract(truncator, comparator, value, random(1, length - 1), strictMax);    }        checkMinContract(truncator, comparator, value, 0, strictMin);        assertSame(value, truncator.truncateMax(value, 0));}
private void parquet-mr_f3532_1(BinaryTruncator truncator, Comparator<Binary> comparator, Binary value, int length, boolean strict)
{    Binary truncated = truncator.truncateMin(value, length);        assertTrue("truncatedMin(value) should be <= than value", comparator.compare(truncated, value) <= 0);    assertFalse("length of truncateMin(value) should not be > than the length of value", truncated.length() > value.length());    if (isValidUtf8(value)) {        checkValidUtf8(truncated);    }    if (strict) {        assertTrue("length of truncateMin(value) ahould be < than the length of value", truncated.length() < value.length());    }}
private void parquet-mr_f3533_1(BinaryTruncator truncator, Comparator<Binary> comparator, Binary value, int length, boolean strict)
{    Binary truncated = truncator.truncateMax(value, length);        assertTrue("truncatedMax(value) should be >= than value", comparator.compare(truncated, value) >= 0);    assertFalse("length of truncateMax(value) should not be > than the length of value", truncated.length() > value.length());    if (isValidUtf8(value)) {        checkValidUtf8(truncated);    }    if (strict) {        assertTrue("length of truncateMax(value) ahould be < than the length of value", truncated.length() < value.length());    }}
private static boolean parquet-mr_f3534_0(Binary binary)
{    try {        UTF8_DECODER.decode(binary.toByteBuffer());        return true;    } catch (CharacterCodingException e) {        return false;    }}
private static void parquet-mr_f3535_0(Binary binary)
{    try {        UTF8_DECODER.decode(binary.toByteBuffer());    } catch (CharacterCodingException e) {        throw new AssertionError("Truncated value should be a valid UTF-8 string", e);    }}
private static int parquet-mr_f3536_0(int min, int max)
{    return RANDOM.nextInt(max - min + 1) + min;}
private static Binary parquet-mr_f3537_0(int... unsignedBytes)
{    byte[] byteArray = new byte[unsignedBytes.length];    for (int i = 0, n = byteArray.length; i < n; ++i) {        int b = unsignedBytes[i];        assert (0xFFFFFF00 & b) == 0;        byteArray[i] = (byte) b;    }    return Binary.fromConstantByteArray(byteArray);}
 int parquet-mr_f3538_0()
{    return compareCount;}
 int parquet-mr_f3539_0()
{    return delegate.arrayLength();}
 int parquet-mr_f3540_0(int arrayIndex)
{    return delegate.translate(arrayIndex);}
 int parquet-mr_f3541_0(int arrayIndex)
{    ++compareCount;    return delegate.compareValueToMin(arrayIndex);}
 int parquet-mr_f3542_0(int arrayIndex)
{    ++compareCount;    return delegate.compareValueToMax(arrayIndex);}
 SpyValueComparator parquet-mr_f3543_0(ColumnIndexBase<?>.ValueComparator comparator)
{    return new SpyValueComparator(comparator);}
 ByteBuffer parquet-mr_f3544_0(int arrayIndex)
{    throw new Error("Shall never be invoked");}
 ByteBuffer parquet-mr_f3545_0(int arrayIndex)
{    throw new Error("Shall never be invoked");}
 String parquet-mr_f3546_0(int arrayIndex)
{    throw new Error("Shall never be invoked");}
 String parquet-mr_f3547_0(int arrayIndex)
{    throw new Error("Shall never be invoked");}
 org.apache.parquet.filter2.predicate.Statistics<T> parquet-mr_f3548_0(int arrayIndex)
{    throw new Error("Shall never be invoked");}
 ColumnIndexBase<Integer>.ValueComparator parquet-mr_f3549_0(Object value)
{    throw new Error("Shall never be invoked");}
 IntList parquet-mr_f3550_0(Function<ColumnIndexBase<?>.ValueComparator, PrimitiveIterator.OfInt> op, ColumnIndexBase<?>.ValueComparator comparator)
{    IntList list = new IntArrayList(comparator.arrayLength());    SpyValueComparatorBuilder.SpyValueComparator spyComparator = SPY_COMPARATOR_BUILDER.build(comparator);    long start = System.nanoTime();    op.apply(spyComparator).forEachRemaining((int value) -> list.add(value));    linearTime = System.nanoTime() - start;    linearCompareCount += spyComparator.getCompareCount();    return list;}
 IntList parquet-mr_f3551_0(Function<ColumnIndexBase<?>.ValueComparator, PrimitiveIterator.OfInt> op, ColumnIndexBase<?>.ValueComparator comparator)
{    IntList list = new IntArrayList(comparator.arrayLength());    SpyValueComparatorBuilder.SpyValueComparator spyComparator = SPY_COMPARATOR_BUILDER.build(comparator);    long start = System.nanoTime();    op.apply(spyComparator).forEachRemaining((int value) -> list.add(value));    binaryTime = System.nanoTime() - start;    binaryCompareCount += spyComparator.getCompareCount();    return list;}
 void parquet-mr_f3552_0(ExecStats stats)
{    linearTime += stats.linearTime;    linearCompareCount += stats.linearCompareCount;    binaryTime += stats.binaryTime;    binaryCompareCount += stats.binaryCompareCount;    ++execCount;}
public String parquet-mr_f3553_0()
{    double linearMs = linearTime / 1_000_000.0;    double binaryMs = binaryTime / 1_000_000.0;    return String.format("Linear search: %.2fms (avg: %.6fms); number of compares: %d (avg: %d) [100.00%%]%n" + "Binary search: %.2fms (avg: %.6fms); number of compares: %d (avg: %d) [%.2f%%]", linearMs, linearMs / execCount, linearCompareCount, linearCompareCount / execCount, binaryMs, binaryMs / execCount, binaryCompareCount, binaryCompareCount / execCount, 100.0 * binaryCompareCount / linearCompareCount);}
private static Statistics<?> parquet-mr_f3554_0(int min, int max)
{    Statistics<?> stats = Statistics.createStats(TYPE);    stats.updateStats(min);    stats.updateStats(max);    return stats;}
private static ExecStats parquet-mr_f3555_0(String msg, Function<ColumnIndexBase<?>.ValueComparator, PrimitiveIterator.OfInt> validatorOp, Function<ColumnIndexBase<?>.ValueComparator, PrimitiveIterator.OfInt> actualOp, ColumnIndexBase<?>.ValueComparator comparator)
{    ExecStats stats = new ExecStats();    IntList expected = stats.measureLinear(validatorOp, comparator);    IntList actual = stats.measureBinary(actualOp, comparator);    Assert.assertEquals(msg, expected, actual);    return stats;}
public void parquet-mr_f3556_1()
{    for (int i = FROM - 1; i <= TO + 1; ++i) {        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::eq, BoundaryOrder.ASCENDING::eq, ASCENDING.createValueComparator(i));        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::eq, BoundaryOrder.DESCENDING::eq, DESCENDING.createValueComparator(i));    }    for (int i = SINGLE_FROM - 1; i <= SINGLE_TO + 1; ++i) {        ColumnIndexBase<?>.ValueComparator singleComparator = SINGLE.createValueComparator(i);        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::eq, BoundaryOrder.ASCENDING::eq, singleComparator);        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::eq, BoundaryOrder.DESCENDING::eq, singleComparator);    }    ExecStats stats = new ExecStats();    for (int i = RAND_FROM - 1; i <= RAND_TO + 1; ++i) {        stats.add(validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::eq, BoundaryOrder.ASCENDING::eq, RAND_ASCENDING.createValueComparator(i)));        stats.add(validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::eq, BoundaryOrder.DESCENDING::eq, RAND_DESCENDING.createValueComparator(i)));    }    }
public void parquet-mr_f3557_1()
{    for (int i = FROM - 1; i <= TO + 1; ++i) {        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::gt, BoundaryOrder.ASCENDING::gt, ASCENDING.createValueComparator(i));        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::gt, BoundaryOrder.DESCENDING::gt, DESCENDING.createValueComparator(i));    }    for (int i = SINGLE_FROM - 1; i <= SINGLE_TO + 1; ++i) {        ColumnIndexBase<?>.ValueComparator singleComparator = SINGLE.createValueComparator(i);        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::gt, BoundaryOrder.ASCENDING::gt, singleComparator);        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::gt, BoundaryOrder.DESCENDING::gt, singleComparator);    }    ExecStats stats = new ExecStats();    for (int i = RAND_FROM - 1; i <= RAND_TO + 1; ++i) {        stats.add(validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::gt, BoundaryOrder.ASCENDING::gt, RAND_ASCENDING.createValueComparator(i)));        stats.add(validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::gt, BoundaryOrder.DESCENDING::gt, RAND_DESCENDING.createValueComparator(i)));    }    }
public void parquet-mr_f3558_1()
{    for (int i = FROM - 1; i <= TO + 1; ++i) {        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::gtEq, BoundaryOrder.ASCENDING::gtEq, ASCENDING.createValueComparator(i));        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::gtEq, BoundaryOrder.DESCENDING::gtEq, DESCENDING.createValueComparator(i));    }    for (int i = SINGLE_FROM - 1; i <= SINGLE_TO + 1; ++i) {        ColumnIndexBase<?>.ValueComparator singleComparator = SINGLE.createValueComparator(i);        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::gtEq, BoundaryOrder.ASCENDING::gtEq, singleComparator);        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::gtEq, BoundaryOrder.DESCENDING::gtEq, singleComparator);    }    ExecStats stats = new ExecStats();    for (int i = RAND_FROM - 1; i <= RAND_TO + 1; ++i) {        stats.add(validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::gtEq, BoundaryOrder.ASCENDING::gtEq, RAND_ASCENDING.createValueComparator(i)));        stats.add(validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::gtEq, BoundaryOrder.DESCENDING::gtEq, RAND_DESCENDING.createValueComparator(i)));    }    }
public void parquet-mr_f3559_1()
{    for (int i = FROM - 1; i <= TO + 1; ++i) {        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::lt, BoundaryOrder.ASCENDING::lt, ASCENDING.createValueComparator(i));        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::lt, BoundaryOrder.DESCENDING::lt, DESCENDING.createValueComparator(i));    }    for (int i = SINGLE_FROM - 1; i <= SINGLE_TO + 1; ++i) {        ColumnIndexBase<?>.ValueComparator singleComparator = SINGLE.createValueComparator(i);        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::lt, BoundaryOrder.ASCENDING::lt, singleComparator);        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::lt, BoundaryOrder.DESCENDING::lt, singleComparator);    }    ExecStats stats = new ExecStats();    for (int i = RAND_FROM - 1; i <= RAND_TO + 1; ++i) {        stats.add(validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::lt, BoundaryOrder.ASCENDING::lt, RAND_ASCENDING.createValueComparator(i)));        stats.add(validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::lt, BoundaryOrder.DESCENDING::lt, RAND_DESCENDING.createValueComparator(i)));    }    }
public void parquet-mr_f3560_1()
{    for (int i = FROM - 1; i <= TO + 1; ++i) {        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::ltEq, BoundaryOrder.ASCENDING::ltEq, ASCENDING.createValueComparator(i));        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::ltEq, BoundaryOrder.DESCENDING::ltEq, DESCENDING.createValueComparator(i));    }    for (int i = SINGLE_FROM - 1; i <= SINGLE_TO + 1; ++i) {        ColumnIndexBase<?>.ValueComparator singleComparator = SINGLE.createValueComparator(i);        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::ltEq, BoundaryOrder.ASCENDING::ltEq, singleComparator);        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::ltEq, BoundaryOrder.DESCENDING::ltEq, singleComparator);    }    ExecStats stats = new ExecStats();    for (int i = RAND_FROM - 1; i <= RAND_TO + 1; ++i) {        stats.add(validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::ltEq, BoundaryOrder.ASCENDING::ltEq, RAND_ASCENDING.createValueComparator(i)));        stats.add(validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::ltEq, BoundaryOrder.DESCENDING::ltEq, RAND_DESCENDING.createValueComparator(i)));    }    }
public void parquet-mr_f3561_1()
{    for (int i = -16; i <= 16; ++i) {        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::notEq, BoundaryOrder.ASCENDING::notEq, ASCENDING.createValueComparator(i));        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::notEq, BoundaryOrder.DESCENDING::notEq, DESCENDING.createValueComparator(i));    }    for (int i = FROM - 1; i <= TO + 1; ++i) {        ColumnIndexBase<?>.ValueComparator singleComparator = SINGLE.createValueComparator(i);        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::notEq, BoundaryOrder.ASCENDING::notEq, singleComparator);        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::notEq, BoundaryOrder.DESCENDING::notEq, singleComparator);    }    ExecStats stats = new ExecStats();    for (int i = RAND_FROM - 1; i <= RAND_TO + 1; ++i) {        stats.add(validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::notEq, BoundaryOrder.ASCENDING::notEq, RAND_ASCENDING.createValueComparator(i)));        stats.add(validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::notEq, BoundaryOrder.DESCENDING::notEq, RAND_DESCENDING.createValueComparator(i)));    }    }
public boolean parquet-mr_f3562_0(Binary value)
{    return value == null || value.equals(ZERO);}
public boolean parquet-mr_f3563_0(org.apache.parquet.filter2.predicate.Statistics<Binary> statistics)
{    Comparator<Binary> cmp = statistics.getComparator();    return cmp.compare(statistics.getMin(), ZERO) > 0 || cmp.compare(statistics.getMax(), ZERO) < 0;}
public boolean parquet-mr_f3564_0(org.apache.parquet.filter2.predicate.Statistics<Binary> statistics)
{    Comparator<Binary> cmp = statistics.getComparator();    return cmp.compare(statistics.getMin(), ZERO) == 0 && cmp.compare(statistics.getMax(), ZERO) == 0;}
public boolean parquet-mr_f3565_0(Binary value)
{    return value != null && value.length() > 0 && value.getBytesUnsafe()[0] == 'B';}
public boolean parquet-mr_f3566_0(org.apache.parquet.filter2.predicate.Statistics<Binary> statistics)
{    Comparator<Binary> cmp = statistics.getComparator();    return cmp.compare(statistics.getMin(), C) >= 0 || cmp.compare(statistics.getMax(), B) < 0;}
public boolean parquet-mr_f3567_0(org.apache.parquet.filter2.predicate.Statistics<Binary> statistics)
{    Comparator<Binary> cmp = statistics.getComparator();    return cmp.compare(statistics.getMin(), B) >= 0 && cmp.compare(statistics.getMax(), C) < 0;}
public boolean parquet-mr_f3568_0(Boolean value)
{    return value == null || value;}
public boolean parquet-mr_f3569_0(org.apache.parquet.filter2.predicate.Statistics<Boolean> statistics)
{    return statistics.getComparator().compare(statistics.getMax(), true) != 0;}
public boolean parquet-mr_f3570_0(org.apache.parquet.filter2.predicate.Statistics<Boolean> statistics)
{    return statistics.getComparator().compare(statistics.getMin(), true) == 0;}
public boolean parquet-mr_f3571_0(Double value)
{    return value != null && Math.floor(value) == value;}
public boolean parquet-mr_f3572_0(org.apache.parquet.filter2.predicate.Statistics<Double> statistics)
{    double min = statistics.getMin();    double max = statistics.getMax();    Comparator<Double> cmp = statistics.getComparator();    return cmp.compare(Math.floor(min), Math.floor(max)) == 0 && cmp.compare(Math.floor(min), min) != 0 && cmp.compare(Math.floor(max), max) != 0;}
public boolean parquet-mr_f3573_0(org.apache.parquet.filter2.predicate.Statistics<Double> statistics)
{    double min = statistics.getMin();    double max = statistics.getMax();    Comparator<Double> cmp = statistics.getComparator();    return cmp.compare(min, max) == 0 && cmp.compare(Math.floor(min), min) == 0;}
private static float parquet-mr_f3574_0(float value)
{    return (float) Math.floor(value);}
public boolean parquet-mr_f3575_0(Float value)
{    return value != null && Math.floor(value) == value;}
public boolean parquet-mr_f3576_0(org.apache.parquet.filter2.predicate.Statistics<Float> statistics)
{    float min = statistics.getMin();    float max = statistics.getMax();    Comparator<Float> cmp = statistics.getComparator();    return cmp.compare(floor(min), floor(max)) == 0 && cmp.compare(floor(min), min) != 0 && cmp.compare(floor(max), max) != 0;}
public boolean parquet-mr_f3577_0(org.apache.parquet.filter2.predicate.Statistics<Float> statistics)
{    float min = statistics.getMin();    float max = statistics.getMax();    Comparator<Float> cmp = statistics.getComparator();    return cmp.compare(min, max) == 0 && cmp.compare(floor(min), min) == 0;}
public boolean parquet-mr_f3578_0(Integer value)
{    return value != null && value % 3 == 0;}
public boolean parquet-mr_f3579_0(org.apache.parquet.filter2.predicate.Statistics<Integer> statistics)
{    int min = statistics.getMin();    int max = statistics.getMax();    return min % 3 != 0 && max % 3 != 0 && max - min < 3;}
public boolean parquet-mr_f3580_0(org.apache.parquet.filter2.predicate.Statistics<Integer> statistics)
{    int min = statistics.getMin();    int max = statistics.getMax();    return min == max && min % 3 == 0;}
public boolean parquet-mr_f3581_0(Long value)
{    return value != null && value % 3 == 0;}
public boolean parquet-mr_f3582_0(org.apache.parquet.filter2.predicate.Statistics<Long> statistics)
{    long min = statistics.getMin();    long max = statistics.getMax();    return min % 3 != 0 && max % 3 != 0 && max - min < 3;}
public boolean parquet-mr_f3583_0(org.apache.parquet.filter2.predicate.Statistics<Long> statistics)
{    long min = statistics.getMin();    long max = statistics.getMax();    return min == max && min % 3 == 0;}
public void parquet-mr_f3584_0()
{    PrimitiveType type = Types.required(BINARY).as(DECIMAL).precision(12).scale(2).named("test_binary_decimal");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(BinaryColumnIndexBuilder.class));    assertNull(builder.build());    BinaryColumn col = binaryColumn("test_col");    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, decimalBinary("-0.17"), decimalBinary("1234567890.12")));    builder.add(sb.stats(type, decimalBinary("-234.23"), null, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, decimalBinary("-9999293.23"), decimalBinary("2348978.45")));    builder.add(sb.stats(type, null, null, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, decimalBinary("87656273")));    assertEquals(8, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 0, 3, 3, 0, 4, 2, 0);    assertCorrectNullPages(columnIndex, true, false, false, true, false, true, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, decimalBinary("1234567890.12"), decimalBinary("-234.23"), null, decimalBinary("2348978.45"), null, null, decimalBinary("87656273"));    assertCorrectValues(columnIndex.getMinValues(), null, decimalBinary("-0.17"), decimalBinary("-234.23"), null, decimalBinary("-9999293.23"), null, null, decimalBinary("87656273"));    assertCorrectFiltering(columnIndex, eq(col, decimalBinary("0.0")), 1, 4);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 5, 6);    assertCorrectFiltering(columnIndex, notEq(col, decimalBinary("87656273")), 0, 1, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 2, 4, 7);    assertCorrectFiltering(columnIndex, gt(col, decimalBinary("2348978.45")), 1);    assertCorrectFiltering(columnIndex, gtEq(col, decimalBinary("2348978.45")), 1, 4);    assertCorrectFiltering(columnIndex, lt(col, decimalBinary("-234.23")), 4);    assertCorrectFiltering(columnIndex, ltEq(col, decimalBinary("-234.23")), 2, 4);    assertCorrectFiltering(columnIndex, userDefined(col, BinaryDecimalIsNullOrZeroUdp.class), 0, 1, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BinaryDecimalIsNullOrZeroUdp.class)), 1, 2, 4, 7);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null, null, null));    builder.add(sb.stats(type, decimalBinary("-9999293.23"), decimalBinary("-234.23")));    builder.add(sb.stats(type, decimalBinary("-0.17"), decimalBinary("87656273")));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, decimalBinary("87656273")));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, decimalBinary("1234567890.12"), null, null, null));    builder.add(sb.stats(type, null, null, null));    assertEquals(8, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 4, 0, 0, 2, 0, 2, 3, 3);    assertCorrectNullPages(columnIndex, true, false, false, true, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, decimalBinary("-234.23"), decimalBinary("87656273"), null, decimalBinary("87656273"), null, decimalBinary("1234567890.12"), null);    assertCorrectValues(columnIndex.getMinValues(), null, decimalBinary("-9999293.23"), decimalBinary("-0.17"), null, decimalBinary("87656273"), null, decimalBinary("1234567890.12"), null);    assertCorrectFiltering(columnIndex, eq(col, decimalBinary("87656273")), 2, 4);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 3, 5, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, decimalBinary("87656273")), 0, 1, 2, 3, 5, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 2, 4, 6);    assertCorrectFiltering(columnIndex, gt(col, decimalBinary("87656273")), 6);    assertCorrectFiltering(columnIndex, gtEq(col, decimalBinary("87656273")), 2, 4, 6);    assertCorrectFiltering(columnIndex, lt(col, decimalBinary("-0.17")), 1);    assertCorrectFiltering(columnIndex, ltEq(col, decimalBinary("-0.17")), 1, 2);    assertCorrectFiltering(columnIndex, userDefined(col, BinaryDecimalIsNullOrZeroUdp.class), 0, 2, 3, 5, 6, 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BinaryDecimalIsNullOrZeroUdp.class)), 1, 2, 4, 6);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, decimalBinary("1234567890.12"), null, null, null));    builder.add(sb.stats(type, null, null, null, null));    builder.add(sb.stats(type, decimalBinary("1234567890.12"), decimalBinary("87656273")));    builder.add(sb.stats(type, decimalBinary("987656273"), decimalBinary("-0.17")));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, decimalBinary("-234.23"), decimalBinary("-9999293.23")));    assertEquals(8, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 3, 2, 3, 4, 0, 0, 2, 0);    assertCorrectNullPages(columnIndex, true, true, false, true, false, false, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, null, decimalBinary("1234567890.12"), null, decimalBinary("1234567890.12"), decimalBinary("987656273"), null, decimalBinary("-234.23"));    assertCorrectValues(columnIndex.getMinValues(), null, null, decimalBinary("1234567890.12"), null, decimalBinary("87656273"), decimalBinary("-0.17"), null, decimalBinary("-9999293.23"));    assertCorrectFiltering(columnIndex, eq(col, decimalBinary("1234567890.12")), 2, 4);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 6);    assertCorrectFiltering(columnIndex, notEq(col, decimalBinary("0.0")), 0, 1, 2, 3, 4, 5, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, null), 2, 4, 5, 7);    assertCorrectFiltering(columnIndex, gt(col, decimalBinary("1234567890.12")));    assertCorrectFiltering(columnIndex, gtEq(col, decimalBinary("1234567890.12")), 2, 4);    assertCorrectFiltering(columnIndex, lt(col, decimalBinary("-0.17")), 7);    assertCorrectFiltering(columnIndex, ltEq(col, decimalBinary("-0.17")), 5, 7);    assertCorrectFiltering(columnIndex, userDefined(col, BinaryDecimalIsNullOrZeroUdp.class), 0, 1, 2, 3, 5, 6);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BinaryDecimalIsNullOrZeroUdp.class)), 2, 4, 5, 7);}
public void parquet-mr_f3585_0()
{    PrimitiveType type = Types.required(BINARY).as(UTF8).named("test_binary_utf8");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(BinaryColumnIndexBuilder.class));    assertNull(builder.build());    BinaryColumn col = binaryColumn("test_col");    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, stringBinary("Jeltz"), stringBinary("Slartibartfast"), null, null));    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, stringBinary("Beeblebrox"), stringBinary("Prefect")));    builder.add(sb.stats(type, stringBinary("Dent"), stringBinary("Trilian"), null));    builder.add(sb.stats(type, stringBinary("Beeblebrox")));    builder.add(sb.stats(type, null, null));    assertEquals(8, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 2, 5, 2, 0, 1, 0, 2);    assertCorrectNullPages(columnIndex, true, false, true, true, false, false, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, stringBinary("Slartibartfast"), null, null, stringBinary("Prefect"), stringBinary("Trilian"), stringBinary("Beeblebrox"), null);    assertCorrectValues(columnIndex.getMinValues(), null, stringBinary("Jeltz"), null, null, stringBinary("Beeblebrox"), stringBinary("Dent"), stringBinary("Beeblebrox"), null);    assertCorrectFiltering(columnIndex, eq(col, stringBinary("Marvin")), 1, 4, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 5, 7);    assertCorrectFiltering(columnIndex, notEq(col, stringBinary("Beeblebrox")), 0, 1, 2, 3, 4, 5, 7);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 4, 5, 6);    assertCorrectFiltering(columnIndex, gt(col, stringBinary("Prefect")), 1, 5);    assertCorrectFiltering(columnIndex, gtEq(col, stringBinary("Prefect")), 1, 4, 5);    assertCorrectFiltering(columnIndex, lt(col, stringBinary("Dent")), 4, 6);    assertCorrectFiltering(columnIndex, ltEq(col, stringBinary("Dent")), 4, 5, 6);    assertCorrectFiltering(columnIndex, userDefined(col, BinaryUtf8StartsWithB.class), 4, 6);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BinaryUtf8StartsWithB.class)), 0, 1, 2, 3, 4, 5, 7);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, stringBinary("Beeblebrox"), stringBinary("Dent"), null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, stringBinary("Dent"), stringBinary("Jeltz")));    builder.add(sb.stats(type, stringBinary("Dent"), stringBinary("Prefect"), null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, stringBinary("Slartibartfast")));    builder.add(sb.stats(type, null, null));    assertEquals(8, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 2, 5, 0, 1, 2, 0, 2);    assertCorrectNullPages(columnIndex, false, true, true, false, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), stringBinary("Dent"), null, null, stringBinary("Jeltz"), stringBinary("Prefect"), null, stringBinary("Slartibartfast"), null);    assertCorrectValues(columnIndex.getMinValues(), stringBinary("Beeblebrox"), null, null, stringBinary("Dent"), stringBinary("Dent"), null, stringBinary("Slartibartfast"), null);    assertCorrectFiltering(columnIndex, eq(col, stringBinary("Jeltz")), 3, 4);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 4, 5, 7);    assertCorrectFiltering(columnIndex, notEq(col, stringBinary("Slartibartfast")), 0, 1, 2, 3, 4, 5, 7);    assertCorrectFiltering(columnIndex, notEq(col, null), 0, 3, 4, 6);    assertCorrectFiltering(columnIndex, gt(col, stringBinary("Marvin")), 4, 6);    assertCorrectFiltering(columnIndex, gtEq(col, stringBinary("Marvin")), 4, 6);    assertCorrectFiltering(columnIndex, lt(col, stringBinary("Dent")), 0);    assertCorrectFiltering(columnIndex, ltEq(col, stringBinary("Dent")), 0, 3, 4);    assertCorrectFiltering(columnIndex, userDefined(col, BinaryUtf8StartsWithB.class), 0);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BinaryUtf8StartsWithB.class)), 0, 1, 2, 3, 4, 5, 6, 7);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, stringBinary("Slartibartfast")));    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, stringBinary("Prefect"), stringBinary("Jeltz"), null));    builder.add(sb.stats(type, stringBinary("Dent"), stringBinary("Dent")));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, stringBinary("Dent"), stringBinary("Beeblebrox"), null, null));    assertEquals(8, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 0, 5, 1, 0, 2, 2, 2);    assertCorrectNullPages(columnIndex, true, false, true, false, false, true, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, stringBinary("Slartibartfast"), null, stringBinary("Prefect"), stringBinary("Dent"), null, null, stringBinary("Dent"));    assertCorrectValues(columnIndex.getMinValues(), null, stringBinary("Slartibartfast"), null, stringBinary("Jeltz"), stringBinary("Dent"), null, null, stringBinary("Beeblebrox"));    assertCorrectFiltering(columnIndex, eq(col, stringBinary("Marvin")), 3);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 5, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, stringBinary("Dent")), 0, 1, 2, 3, 5, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 3, 4, 7);    assertCorrectFiltering(columnIndex, gt(col, stringBinary("Prefect")), 1);    assertCorrectFiltering(columnIndex, gtEq(col, stringBinary("Prefect")), 1, 3);    assertCorrectFiltering(columnIndex, lt(col, stringBinary("Marvin")), 3, 4, 7);    assertCorrectFiltering(columnIndex, ltEq(col, stringBinary("Marvin")), 3, 4, 7);    assertCorrectFiltering(columnIndex, userDefined(col, BinaryUtf8StartsWithB.class), 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BinaryUtf8StartsWithB.class)), 0, 1, 2, 3, 4, 5, 6, 7);}
public void parquet-mr_f3586_0()
{    ColumnIndex columnIndex = ColumnIndexBuilder.build(Types.required(BINARY).as(UTF8).named("test_binary_utf8"), BoundaryOrder.ASCENDING, asList(true, true, false, false, true, false, true, false), asList(1l, 2l, 3l, 4l, 5l, 6l, 7l, 8l), toBBList(null, null, stringBinary("Beeblebrox"), stringBinary("Dent"), null, stringBinary("Jeltz"), null, stringBinary("Slartibartfast")), toBBList(null, null, stringBinary("Dent"), stringBinary("Dent"), null, stringBinary("Prefect"), null, stringBinary("Slartibartfast")));    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectNullPages(columnIndex, true, true, false, false, true, false, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, null, stringBinary("Dent"), stringBinary("Dent"), null, stringBinary("Prefect"), null, stringBinary("Slartibartfast"));    assertCorrectValues(columnIndex.getMinValues(), null, null, stringBinary("Beeblebrox"), stringBinary("Dent"), null, stringBinary("Jeltz"), null, stringBinary("Slartibartfast"));}
public void parquet-mr_f3587_0()
{    ColumnIndex columnIndex = ColumnIndexBuilder.build(Types.required(BINARY).as(UTF8).named("test_binary_utf8"), BoundaryOrder.ASCENDING, asList(true, true, false, false, true, false, true, false), null, toBBList(null, null, stringBinary("Beeblebrox"), stringBinary("Dent"), null, stringBinary("Jeltz"), null, stringBinary("Slartibartfast")), toBBList(null, null, stringBinary("Dent"), stringBinary("Dent"), null, stringBinary("Prefect"), null, stringBinary("Slartibartfast")));    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertNull(columnIndex.getNullCounts());    assertCorrectNullPages(columnIndex, true, true, false, false, true, false, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, null, stringBinary("Dent"), stringBinary("Dent"), null, stringBinary("Prefect"), null, stringBinary("Slartibartfast"));    assertCorrectValues(columnIndex.getMinValues(), null, null, stringBinary("Beeblebrox"), stringBinary("Dent"), null, stringBinary("Jeltz"), null, stringBinary("Slartibartfast"));    BinaryColumn col = binaryColumn("test_col");    assertCorrectFiltering(columnIndex, eq(col, stringBinary("Dent")), 2, 3);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 4, 5, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, stringBinary("Dent")), 0, 1, 2, 3, 4, 5, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, null), 2, 3, 5, 7);    assertCorrectFiltering(columnIndex, userDefined(col, BinaryDecimalIsNullOrZeroUdp.class), 0, 1, 2, 3, 4, 5, 6, 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BinaryDecimalIsNullOrZeroUdp.class)), 2, 3, 5, 7);}
public void parquet-mr_f3588_0()
{    PrimitiveType type = Types.required(BOOLEAN).named("test_boolean");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(BooleanColumnIndexBuilder.class));    assertNull(builder.build());    BooleanColumn col = booleanColumn("test_col");    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, false, true));    builder.add(sb.stats(type, true, false, null));    builder.add(sb.stats(type, true, true, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, false, false));    assertEquals(5, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0, 1, 2, 3, 0);    assertCorrectNullPages(columnIndex, false, false, false, true, false);    assertCorrectValues(columnIndex.getMaxValues(), true, true, true, null, false);    assertCorrectValues(columnIndex.getMinValues(), false, false, true, null, false);    assertCorrectFiltering(columnIndex, eq(col, true), 0, 1, 2);    assertCorrectFiltering(columnIndex, eq(col, null), 1, 2, 3);    assertCorrectFiltering(columnIndex, notEq(col, true), 0, 1, 2, 3, 4);    assertCorrectFiltering(columnIndex, notEq(col, null), 0, 1, 2, 4);    assertCorrectFiltering(columnIndex, userDefined(col, BooleanIsTrueOrNull.class), 0, 1, 2, 3);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BooleanIsTrueOrNull.class)), 0, 1, 4);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, false, false));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, null, null, null, null));    builder.add(sb.stats(type, false, true, null));    builder.add(sb.stats(type, false, true, null, null));    builder.add(sb.stats(type, null, null, null));    assertEquals(7, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 0, 3, 4, 1, 2, 3);    assertCorrectNullPages(columnIndex, true, false, true, true, false, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, false, null, null, true, true, null);    assertCorrectValues(columnIndex.getMinValues(), null, false, null, null, false, false, null);    assertCorrectFiltering(columnIndex, eq(col, true), 4, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, notEq(col, true), 0, 1, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 4, 5);    assertCorrectFiltering(columnIndex, userDefined(col, BooleanIsTrueOrNull.class), 0, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BooleanIsTrueOrNull.class)), 1, 4, 5);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, true, true));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, null, null, null, null));    builder.add(sb.stats(type, true, false, null));    builder.add(sb.stats(type, false, false, null, null));    builder.add(sb.stats(type, null, null, null));    assertEquals(7, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 0, 3, 4, 1, 2, 3);    assertCorrectNullPages(columnIndex, true, false, true, true, false, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, true, null, null, true, false, null);    assertCorrectValues(columnIndex.getMinValues(), null, true, null, null, false, false, null);    assertCorrectFiltering(columnIndex, eq(col, true), 1, 4);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, notEq(col, true), 0, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 4, 5);    assertCorrectFiltering(columnIndex, userDefined(col, BooleanIsTrueOrNull.class), 0, 1, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BooleanIsTrueOrNull.class)), 4, 5);}
public void parquet-mr_f3589_0()
{    ColumnIndex columnIndex = ColumnIndexBuilder.build(Types.required(BOOLEAN).named("test_boolean"), BoundaryOrder.DESCENDING, asList(false, true, false, true, false, true), asList(9l, 8l, 7l, 6l, 5l, 0l), toBBList(false, null, false, null, true, null), toBBList(true, null, false, null, true, null));    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 9, 8, 7, 6, 5, 0);    assertCorrectNullPages(columnIndex, false, true, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), true, null, false, null, true, null);    assertCorrectValues(columnIndex.getMinValues(), false, null, false, null, true, null);}
public void parquet-mr_f3590_0()
{    PrimitiveType type = Types.required(DOUBLE).named("test_double");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(DoubleColumnIndexBuilder.class));    assertNull(builder.build());    DoubleColumn col = doubleColumn("test_col");    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, -4.2, -4.1));    builder.add(sb.stats(type, -11.7, 7.0, null));    builder.add(sb.stats(type, 2.2, 2.2, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 1.9, 2.32));    builder.add(sb.stats(type, -21.0, 8.1));    assertEquals(6, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0, 1, 2, 3, 0, 0);    assertCorrectNullPages(columnIndex, false, false, false, true, false, false);    assertCorrectValues(columnIndex.getMaxValues(), -4.1, 7.0, 2.2, null, 2.32, 8.1);    assertCorrectValues(columnIndex.getMinValues(), -4.2, -11.7, 2.2, null, 1.9, -21.0);    assertCorrectFiltering(columnIndex, eq(col, 0.0), 1, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 1, 2, 3);    assertCorrectFiltering(columnIndex, notEq(col, 2.2), 0, 1, 2, 3, 4, 5);    assertCorrectFiltering(columnIndex, notEq(col, null), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, gt(col, 2.2), 1, 4, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2.2), 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, lt(col, -4.2), 1, 5);    assertCorrectFiltering(columnIndex, ltEq(col, -4.2), 0, 1, 5);    assertCorrectFiltering(columnIndex, userDefined(col, DoubleIsInteger.class), 1, 4, 5);    assertCorrectFiltering(columnIndex, invert(userDefined(col, DoubleIsInteger.class)), 0, 1, 2, 3, 4, 5);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -532.3, -345.2, null, null));    builder.add(sb.stats(type, -234.7, -234.6, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, -234.6, 2.99999));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 3.0, 42.83));    builder.add(sb.stats(type, null, null));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 2, 1, 2, 3, 0, 2, 0, 2);    assertCorrectNullPages(columnIndex, true, false, false, true, true, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, -345.2, -234.6, null, null, 2.99999, null, 42.83, null);    assertCorrectValues(columnIndex.getMinValues(), null, -532.3, -234.7, null, null, -234.6, null, 3.0, null);    assertCorrectFiltering(columnIndex, eq(col, 0.0), 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 4, 6, 8);    assertCorrectFiltering(columnIndex, notEq(col, 0.0), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, gt(col, 2.99999), 7);    assertCorrectFiltering(columnIndex, gtEq(col, 2.99999), 5, 7);    assertCorrectFiltering(columnIndex, lt(col, -234.6), 1, 2);    assertCorrectFiltering(columnIndex, ltEq(col, -234.6), 1, 2, 5);    assertCorrectFiltering(columnIndex, userDefined(col, DoubleIsInteger.class), 1, 5, 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, DoubleIsInteger.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, 532.3, 345.2));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 234.7, 234.6, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 234.69, -2.99999));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -3.0, -42.83));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 5, 0, 3, 1, 2, 0, 2, 2, 0);    assertCorrectNullPages(columnIndex, true, false, true, false, true, false, true, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, 532.3, null, 234.7, null, 234.69, null, null, -3.0);    assertCorrectValues(columnIndex.getMinValues(), null, 345.2, null, 234.6, null, -2.99999, null, null, -42.83);    assertCorrectFiltering(columnIndex, eq(col, 234.6), 3, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 4, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, 2.2), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, gt(col, 2.2), 1, 3, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 234.69), 1, 3, 5);    assertCorrectFiltering(columnIndex, lt(col, -2.99999), 8);    assertCorrectFiltering(columnIndex, ltEq(col, -2.99999), 5, 8);    assertCorrectFiltering(columnIndex, userDefined(col, DoubleIsInteger.class), 1, 5, 8);    assertCorrectFiltering(columnIndex, invert(userDefined(col, DoubleIsInteger.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);}
public void parquet-mr_f3591_0()
{    PrimitiveType type = Types.required(DOUBLE).named("test_double");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, -1.0, -0.0));    builder.add(sb.stats(type, 0.0, 1.0));    builder.add(sb.stats(type, 1.0, 100.0));    ColumnIndex columnIndex = builder.build();    assertCorrectValues(columnIndex.getMinValues(), -1.0, -0.0, 1.0);    assertCorrectValues(columnIndex.getMaxValues(), 0.0, 1.0, 100.0);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    builder.add(sb.stats(type, -1.0, -0.0));    builder.add(sb.stats(type, 0.0, Double.NaN));    builder.add(sb.stats(type, 1.0, 100.0));    assertNull(builder.build());}
public void parquet-mr_f3592_0()
{    ColumnIndex columnIndex = ColumnIndexBuilder.build(Types.required(DOUBLE).named("test_double"), BoundaryOrder.UNORDERED, asList(false, false, false, false, false, false), asList(0l, 1l, 2l, 3l, 4l, 5l), toBBList(-1.0, -2.0, -3.0, -4.0, -5.0, -6.0), toBBList(1.0, 2.0, 3.0, 4.0, 5.0, 6.0));    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0, 1, 2, 3, 4, 5);    assertCorrectNullPages(columnIndex, false, false, false, false, false, false);    assertCorrectValues(columnIndex.getMaxValues(), 1.0, 2.0, 3.0, 4.0, 5.0, 6.0);    assertCorrectValues(columnIndex.getMinValues(), -1.0, -2.0, -3.0, -4.0, -5.0, -6.0);}
public void parquet-mr_f3593_0()
{    PrimitiveType type = Types.required(FLOAT).named("test_float");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(FloatColumnIndexBuilder.class));    assertNull(builder.build());    FloatColumn col = floatColumn("test_col");    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, -4.2f, -4.1f));    builder.add(sb.stats(type, -11.7f, 7.0f, null));    builder.add(sb.stats(type, 2.2f, 2.2f, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 1.9f, 2.32f));    builder.add(sb.stats(type, -21.0f, 8.1f));    assertEquals(6, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0, 1, 2, 3, 0, 0);    assertCorrectNullPages(columnIndex, false, false, false, true, false, false);    assertCorrectValues(columnIndex.getMaxValues(), -4.1f, 7.0f, 2.2f, null, 2.32f, 8.1f);    assertCorrectValues(columnIndex.getMinValues(), -4.2f, -11.7f, 2.2f, null, 1.9f, -21.0f);    assertCorrectFiltering(columnIndex, eq(col, 0.0f), 1, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 1, 2, 3);    assertCorrectFiltering(columnIndex, notEq(col, 2.2f), 0, 1, 2, 3, 4, 5);    assertCorrectFiltering(columnIndex, notEq(col, null), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, gt(col, 2.2f), 1, 4, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2.2f), 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, lt(col, 0.0f), 0, 1, 5);    assertCorrectFiltering(columnIndex, ltEq(col, 1.9f), 0, 1, 4, 5);    assertCorrectFiltering(columnIndex, userDefined(col, FloatIsInteger.class), 1, 4, 5);    assertCorrectFiltering(columnIndex, invert(userDefined(col, FloatIsInteger.class)), 0, 1, 2, 3, 4, 5);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -532.3f, -345.2f, null, null));    builder.add(sb.stats(type, -300.6f, -234.7f, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, -234.6f, 2.99999f));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 3.0f, 42.83f));    builder.add(sb.stats(type, null, null));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 2, 1, 2, 3, 0, 2, 0, 2);    assertCorrectNullPages(columnIndex, true, false, false, true, true, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, -345.2f, -234.7f, null, null, 2.99999f, null, 42.83f, null);    assertCorrectValues(columnIndex.getMinValues(), null, -532.3f, -300.6f, null, null, -234.6f, null, 3.0f, null);    assertCorrectFiltering(columnIndex, eq(col, 0.0f), 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 4, 6, 8);    assertCorrectFiltering(columnIndex, notEq(col, 2.2f), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, gt(col, 2.2f), 5, 7);    assertCorrectFiltering(columnIndex, gtEq(col, -234.7f), 2, 5, 7);    assertCorrectFiltering(columnIndex, lt(col, -234.6f), 1, 2);    assertCorrectFiltering(columnIndex, ltEq(col, -234.6f), 1, 2, 5);    assertCorrectFiltering(columnIndex, userDefined(col, FloatIsInteger.class), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, FloatIsInteger.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, 532.3f, 345.2f));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 234.7f, 234.6f, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 234.6f, -2.99999f));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -3.0f, -42.83f));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 5, 0, 3, 1, 2, 0, 2, 2, 0);    assertCorrectNullPages(columnIndex, true, false, true, false, true, false, true, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, 532.3f, null, 234.7f, null, 234.6f, null, null, -3.0f);    assertCorrectValues(columnIndex.getMinValues(), null, 345.2f, null, 234.6f, null, -2.99999f, null, null, -42.83f);    assertCorrectFiltering(columnIndex, eq(col, 234.65f), 3);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 4, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, 2.2f), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, gt(col, 2.2f), 1, 3, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2.2f), 1, 3, 5);    assertCorrectFiltering(columnIndex, lt(col, 0.0f), 5, 8);    assertCorrectFiltering(columnIndex, ltEq(col, 0.0f), 5, 8);    assertCorrectFiltering(columnIndex, userDefined(col, FloatIsInteger.class), 1, 5, 8);    assertCorrectFiltering(columnIndex, invert(userDefined(col, FloatIsInteger.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);}
public void parquet-mr_f3594_0()
{    PrimitiveType type = Types.required(FLOAT).named("test_float");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, -1.0f, -0.0f));    builder.add(sb.stats(type, 0.0f, 1.0f));    builder.add(sb.stats(type, 1.0f, 100.0f));    ColumnIndex columnIndex = builder.build();    assertCorrectValues(columnIndex.getMinValues(), -1.0f, -0.0f, 1.0f);    assertCorrectValues(columnIndex.getMaxValues(), 0.0f, 1.0f, 100.0f);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    builder.add(sb.stats(type, -1.0f, -0.0f));    builder.add(sb.stats(type, 0.0f, Float.NaN));    builder.add(sb.stats(type, 1.0f, 100.0f));    assertNull(builder.build());}
public void parquet-mr_f3595_0()
{    ColumnIndex columnIndex = ColumnIndexBuilder.build(Types.required(FLOAT).named("test_float"), BoundaryOrder.ASCENDING, asList(true, true, true, false, false, false), asList(9l, 8l, 7l, 6l, 0l, 0l), toBBList(null, null, null, -3.0f, -2.0f, 0.1f), toBBList(null, null, null, -2.0f, 0.0f, 6.0f));    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 9, 8, 7, 6, 0, 0);    assertCorrectNullPages(columnIndex, true, true, true, false, false, false);    assertCorrectValues(columnIndex.getMaxValues(), null, null, null, -2.0f, 0.0f, 6.0f);    assertCorrectValues(columnIndex.getMinValues(), null, null, null, -3.0f, -2.0f, 0.1f);}
public void parquet-mr_f3596_0()
{    PrimitiveType type = Types.required(INT32).named("test_int32");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(IntColumnIndexBuilder.class));    assertNull(builder.build());    IntColumn col = intColumn("test_col");    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, -4, 10));    builder.add(sb.stats(type, -11, 7, null));    builder.add(sb.stats(type, 2, 2, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 1, 2));    builder.add(sb.stats(type, -21, 8));    assertEquals(6, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0, 1, 2, 3, 0, 0);    assertCorrectNullPages(columnIndex, false, false, false, true, false, false);    assertCorrectValues(columnIndex.getMaxValues(), 10, 7, 2, null, 2, 8);    assertCorrectValues(columnIndex.getMinValues(), -4, -11, 2, null, 1, -21);    assertCorrectFiltering(columnIndex, eq(col, 2), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 1, 2, 3);    assertCorrectFiltering(columnIndex, notEq(col, 2), 0, 1, 2, 3, 4, 5);    assertCorrectFiltering(columnIndex, notEq(col, null), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, gt(col, 2), 0, 1, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, lt(col, 2), 0, 1, 4, 5);    assertCorrectFiltering(columnIndex, ltEq(col, 2), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, userDefined(col, IntegerIsDivisableWith3.class), 0, 1, 5);    assertCorrectFiltering(columnIndex, invert(userDefined(col, IntegerIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -532, -345, null, null));    builder.add(sb.stats(type, -500, -42, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, -42, 2));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 3, 42));    builder.add(sb.stats(type, null, null));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 2, 1, 2, 3, 0, 2, 0, 2);    assertCorrectNullPages(columnIndex, true, false, false, true, true, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, -345, -42, null, null, 2, null, 42, null);    assertCorrectValues(columnIndex.getMinValues(), null, -532, -500, null, null, -42, null, 3, null);    assertCorrectFiltering(columnIndex, eq(col, 2), 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 4, 6, 8);    assertCorrectFiltering(columnIndex, notEq(col, 2), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, gt(col, 2), 7);    assertCorrectFiltering(columnIndex, gtEq(col, 2), 5, 7);    assertCorrectFiltering(columnIndex, lt(col, 2), 1, 2, 5);    assertCorrectFiltering(columnIndex, ltEq(col, 2), 1, 2, 5);    assertCorrectFiltering(columnIndex, userDefined(col, IntegerIsDivisableWith3.class), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, IntegerIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, 532, 345));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 234, 42, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 42, -2));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -3, -42));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 5, 0, 3, 1, 2, 0, 2, 2, 0);    assertCorrectNullPages(columnIndex, true, false, true, false, true, false, true, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, 532, null, 234, null, 42, null, null, -3);    assertCorrectValues(columnIndex.getMinValues(), null, 345, null, 42, null, -2, null, null, -42);    assertCorrectFiltering(columnIndex, eq(col, 2), 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 4, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, 2), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, gt(col, 2), 1, 3, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2), 1, 3, 5);    assertCorrectFiltering(columnIndex, lt(col, 2), 5, 8);    assertCorrectFiltering(columnIndex, ltEq(col, 2), 5, 8);    assertCorrectFiltering(columnIndex, userDefined(col, IntegerIsDivisableWith3.class), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, invert(userDefined(col, IntegerIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);}
public void parquet-mr_f3597_0()
{    ColumnIndex columnIndex = ColumnIndexBuilder.build(Types.required(INT32).named("test_int32"), BoundaryOrder.DESCENDING, asList(false, false, false, true, true, true), asList(0l, 10l, 0l, 3l, 5l, 7l), toBBList(10, 8, 6, null, null, null), toBBList(9, 7, 5, null, null, null));    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0, 10, 0, 3, 5, 7);    assertCorrectNullPages(columnIndex, false, false, false, true, true, true);    assertCorrectValues(columnIndex.getMaxValues(), 9, 7, 5, null, null, null);    assertCorrectValues(columnIndex.getMinValues(), 10, 8, 6, null, null, null);}
public void parquet-mr_f3598_0()
{    PrimitiveType type = Types.required(INT32).as(UINT_8).named("test_uint8");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(IntColumnIndexBuilder.class));    assertNull(builder.build());    IntColumn col = intColumn("test_col");    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, 4, 10));    builder.add(sb.stats(type, 11, 17, null));    builder.add(sb.stats(type, 2, 2, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 1, 0xFF));    builder.add(sb.stats(type, 0xEF, 0xFA));    assertEquals(6, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0, 1, 2, 3, 0, 0);    assertCorrectNullPages(columnIndex, false, false, false, true, false, false);    assertCorrectValues(columnIndex.getMaxValues(), 10, 17, 2, null, 0xFF, 0xFA);    assertCorrectValues(columnIndex.getMinValues(), 4, 11, 2, null, 1, 0xEF);    assertCorrectFiltering(columnIndex, eq(col, 2), 2, 4);    assertCorrectFiltering(columnIndex, eq(col, null), 1, 2, 3);    assertCorrectFiltering(columnIndex, notEq(col, 2), 0, 1, 2, 3, 4, 5);    assertCorrectFiltering(columnIndex, notEq(col, null), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, gt(col, 2), 0, 1, 4, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, lt(col, 0xEF), 0, 1, 2, 4);    assertCorrectFiltering(columnIndex, ltEq(col, 0xEF), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, userDefined(col, IntegerIsDivisableWith3.class), 0, 1, 4, 5);    assertCorrectFiltering(columnIndex, invert(userDefined(col, IntegerIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 0, 0, null, null));    builder.add(sb.stats(type, 0, 42, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 42, 0xEE));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 0xEF, 0xFF));    builder.add(sb.stats(type, null, null));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 2, 1, 2, 3, 0, 2, 0, 2);    assertCorrectNullPages(columnIndex, true, false, false, true, true, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, 0, 42, null, null, 0xEE, null, 0xFF, null);    assertCorrectValues(columnIndex.getMinValues(), null, 0, 0, null, null, 42, null, 0xEF, null);    assertCorrectFiltering(columnIndex, eq(col, 2), 2);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 4, 6, 8);    assertCorrectFiltering(columnIndex, notEq(col, 2), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, gt(col, 0xEE), 7);    assertCorrectFiltering(columnIndex, gtEq(col, 0xEE), 5, 7);    assertCorrectFiltering(columnIndex, lt(col, 42), 1, 2);    assertCorrectFiltering(columnIndex, ltEq(col, 42), 1, 2, 5);    assertCorrectFiltering(columnIndex, userDefined(col, IntegerIsDivisableWith3.class), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, IntegerIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, 0xFF, 0xFF));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 0xEF, 0xEA, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 0xEE, 42));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 41, 0));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 5, 0, 3, 1, 2, 0, 2, 2, 0);    assertCorrectNullPages(columnIndex, true, false, true, false, true, false, true, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, 0xFF, null, 0xEF, null, 0xEE, null, null, 41);    assertCorrectValues(columnIndex.getMinValues(), null, 0xFF, null, 0xEA, null, 42, null, null, 0);    assertCorrectFiltering(columnIndex, eq(col, 0xAB), 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 4, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, 0xFF), 0, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, gt(col, 0xFF));    assertCorrectFiltering(columnIndex, gtEq(col, 0xFF), 1);    assertCorrectFiltering(columnIndex, lt(col, 42), 8);    assertCorrectFiltering(columnIndex, ltEq(col, 42), 5, 8);    assertCorrectFiltering(columnIndex, userDefined(col, IntegerIsDivisableWith3.class), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, invert(userDefined(col, IntegerIsDivisableWith3.class)), 0, 2, 3, 4, 5, 6, 7, 8);}
public void parquet-mr_f3599_0()
{    PrimitiveType type = Types.required(INT64).named("test_int64");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(LongColumnIndexBuilder.class));    assertNull(builder.build());    LongColumn col = longColumn("test_col");    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, -4l, 10l));    builder.add(sb.stats(type, -11l, 7l, null));    builder.add(sb.stats(type, 2l, 2l, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 1l, 2l));    builder.add(sb.stats(type, -21l, 8l));    assertEquals(6, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0l, 1l, 2l, 3l, 0l, 0l);    assertCorrectNullPages(columnIndex, false, false, false, true, false, false);    assertCorrectValues(columnIndex.getMaxValues(), 10l, 7l, 2l, null, 2l, 8l);    assertCorrectValues(columnIndex.getMinValues(), -4l, -11l, 2l, null, 1l, -21l);    assertCorrectFiltering(columnIndex, eq(col, 0l), 0, 1, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 1, 2, 3);    assertCorrectFiltering(columnIndex, notEq(col, 0l), 0, 1, 2, 3, 4, 5);    assertCorrectFiltering(columnIndex, notEq(col, null), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, gt(col, 2l), 0, 1, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2l), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, lt(col, -21l));    assertCorrectFiltering(columnIndex, ltEq(col, -21l), 5);    assertCorrectFiltering(columnIndex, userDefined(col, LongIsDivisableWith3.class), 0, 1, 5);    assertCorrectFiltering(columnIndex, invert(userDefined(col, LongIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -532l, -345l, null, null));    builder.add(sb.stats(type, -234l, -42l, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, -42l, 2l));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -3l, 42l));    builder.add(sb.stats(type, null, null));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 2, 1, 2, 3, 0, 2, 0, 2);    assertCorrectNullPages(columnIndex, true, false, false, true, true, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, -345l, -42l, null, null, 2l, null, 42l, null);    assertCorrectValues(columnIndex.getMinValues(), null, -532l, -234l, null, null, -42l, null, -3l, null);    assertCorrectFiltering(columnIndex, eq(col, -42l), 2, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 4, 6, 8);    assertCorrectFiltering(columnIndex, notEq(col, -42l), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, gt(col, 2l), 7);    assertCorrectFiltering(columnIndex, gtEq(col, 2l), 5, 7);    assertCorrectFiltering(columnIndex, lt(col, -42l), 1, 2);    assertCorrectFiltering(columnIndex, ltEq(col, -42l), 1, 2, 5);    assertCorrectFiltering(columnIndex, userDefined(col, LongIsDivisableWith3.class), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, LongIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, 532l, 345l));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 234l, 42l, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 42l, -2l));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -3l, -42l));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 5, 0, 3, 1, 2, 0, 2, 2, 0);    assertCorrectNullPages(columnIndex, true, false, true, false, true, false, true, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, 532l, null, 234l, null, 42l, null, null, -3l);    assertCorrectValues(columnIndex.getMinValues(), null, 345l, null, 42l, null, -2l, null, null, -42l);    assertCorrectFiltering(columnIndex, eq(col, 0l), 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 4, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, 0l), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, gt(col, 2l), 1, 3, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2l), 1, 3, 5);    assertCorrectFiltering(columnIndex, lt(col, -42l));    assertCorrectFiltering(columnIndex, ltEq(col, -42l), 8);    assertCorrectFiltering(columnIndex, userDefined(col, LongIsDivisableWith3.class), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, invert(userDefined(col, LongIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);}
public void parquet-mr_f3600_0()
{    ColumnIndex columnIndex = ColumnIndexBuilder.build(Types.required(INT64).named("test_int64"), BoundaryOrder.UNORDERED, asList(true, false, true, false, true, false), asList(1l, 2l, 3l, 4l, 5l, 6l), toBBList(null, 2l, null, 4l, null, 9l), toBBList(null, 3l, null, 15l, null, 10l));    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 1, 2, 3, 4, 5, 6);    assertCorrectNullPages(columnIndex, true, false, true, false, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, 3l, null, 15l, null, 10l);    assertCorrectValues(columnIndex.getMinValues(), null, 2l, null, 4l, null, 9l);}
public void parquet-mr_f3601_0()
{    ColumnIndexBuilder builder = ColumnIndexBuilder.getNoOpBuilder();    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(Types.required(BINARY).as(UTF8).named("test_binary_utf8"), stringBinary("Jeltz"), stringBinary("Slartibartfast"), null, null));    builder.add(sb.stats(Types.required(BOOLEAN).named("test_boolean"), true, true, null, null));    builder.add(sb.stats(Types.required(DOUBLE).named("test_double"), null, null, null));    builder.add(sb.stats(Types.required(INT32).named("test_int32"), null, null));    builder.add(sb.stats(Types.required(INT64).named("test_int64"), -234l, -42l, null));    assertEquals(0, builder.getPageCount());    assertEquals(0, builder.getMinMaxSize());    assertNull(builder.build());}
private static List<ByteBuffer> parquet-mr_f3602_0(Binary... values)
{    List<ByteBuffer> buffers = new ArrayList<>(values.length);    for (Binary value : values) {        if (value == null) {            buffers.add(ByteBuffer.allocate(0));        } else {            buffers.add(value.toByteBuffer());        }    }    return buffers;}
private static List<ByteBuffer> parquet-mr_f3603_0(Boolean... values)
{    List<ByteBuffer> buffers = new ArrayList<>(values.length);    for (Boolean value : values) {        if (value == null) {            buffers.add(ByteBuffer.allocate(0));        } else {            buffers.add(ByteBuffer.wrap(BytesUtils.booleanToBytes(value)));        }    }    return buffers;}
private static List<ByteBuffer> parquet-mr_f3604_0(Double... values)
{    List<ByteBuffer> buffers = new ArrayList<>(values.length);    for (Double value : values) {        if (value == null) {            buffers.add(ByteBuffer.allocate(0));        } else {            buffers.add(ByteBuffer.wrap(BytesUtils.longToBytes(Double.doubleToLongBits(value))));        }    }    return buffers;}
private static List<ByteBuffer> parquet-mr_f3605_0(Float... values)
{    List<ByteBuffer> buffers = new ArrayList<>(values.length);    for (Float value : values) {        if (value == null) {            buffers.add(ByteBuffer.allocate(0));        } else {            buffers.add(ByteBuffer.wrap(BytesUtils.intToBytes(Float.floatToIntBits(value))));        }    }    return buffers;}
private static List<ByteBuffer> parquet-mr_f3606_0(Integer... values)
{    List<ByteBuffer> buffers = new ArrayList<>(values.length);    for (Integer value : values) {        if (value == null) {            buffers.add(ByteBuffer.allocate(0));        } else {            buffers.add(ByteBuffer.wrap(BytesUtils.intToBytes(value)));        }    }    return buffers;}
private static List<ByteBuffer> parquet-mr_f3607_0(Long... values)
{    List<ByteBuffer> buffers = new ArrayList<>(values.length);    for (Long value : values) {        if (value == null) {            buffers.add(ByteBuffer.allocate(0));        } else {            buffers.add(ByteBuffer.wrap(BytesUtils.longToBytes(value)));        }    }    return buffers;}
private static Binary parquet-mr_f3608_0(String num)
{    return Binary.fromConstantByteArray(new BigDecimal(num).unscaledValue().toByteArray());}
private static Binary parquet-mr_f3609_0(String str)
{    return Binary.fromString(str);}
private static void parquet-mr_f3610_0(List<ByteBuffer> values, Binary... expectedValues)
{    assertEquals(expectedValues.length, values.size());    for (int i = 0; i < expectedValues.length; ++i) {        Binary expectedValue = expectedValues[i];        ByteBuffer value = values.get(i);        if (expectedValue == null) {            assertFalse("The byte buffer should be empty for null pages", value.hasRemaining());        } else {            assertArrayEquals("Invalid value for page " + i, expectedValue.getBytesUnsafe(), value.array());        }    }}
private static void parquet-mr_f3611_0(List<ByteBuffer> values, Boolean... expectedValues)
{    assertEquals(expectedValues.length, values.size());    for (int i = 0; i < expectedValues.length; ++i) {        Boolean expectedValue = expectedValues[i];        ByteBuffer value = values.get(i);        if (expectedValue == null) {            assertFalse("The byte buffer should be empty for null pages", value.hasRemaining());        } else {            assertEquals("The byte buffer should be 1 byte long for boolean", 1, value.remaining());            assertEquals("Invalid value for page " + i, expectedValue.booleanValue(), value.get(0) != 0);        }    }}
private static void parquet-mr_f3612_0(List<ByteBuffer> values, Double... expectedValues)
{    assertEquals(expectedValues.length, values.size());    for (int i = 0; i < expectedValues.length; ++i) {        Double expectedValue = expectedValues[i];        ByteBuffer value = values.get(i);        if (expectedValue == null) {            assertFalse("The byte buffer should be empty for null pages", value.hasRemaining());        } else {            assertEquals("The byte buffer should be 8 bytes long for double", 8, value.remaining());            assertTrue("Invalid value for page " + i, Double.compare(expectedValue.doubleValue(), value.getDouble(0)) == 0);        }    }}
private static void parquet-mr_f3613_0(List<ByteBuffer> values, Float... expectedValues)
{    assertEquals(expectedValues.length, values.size());    for (int i = 0; i < expectedValues.length; ++i) {        Float expectedValue = expectedValues[i];        ByteBuffer value = values.get(i);        if (expectedValue == null) {            assertFalse("The byte buffer should be empty for null pages", value.hasRemaining());        } else {            assertEquals("The byte buffer should be 4 bytes long for double", 4, value.remaining());            assertTrue("Invalid value for page " + i, Float.compare(expectedValue.floatValue(), value.getFloat(0)) == 0);        }    }}
private static void parquet-mr_f3614_0(List<ByteBuffer> values, Integer... expectedValues)
{    assertEquals(expectedValues.length, values.size());    for (int i = 0; i < expectedValues.length; ++i) {        Integer expectedValue = expectedValues[i];        ByteBuffer value = values.get(i);        if (expectedValue == null) {            assertFalse("The byte buffer should be empty for null pages", value.hasRemaining());        } else {            assertEquals("The byte buffer should be 4 bytes long for int32", 4, value.remaining());            assertEquals("Invalid value for page " + i, expectedValue.intValue(), value.getInt(0));        }    }}
private static void parquet-mr_f3615_0(List<ByteBuffer> values, Long... expectedValues)
{    assertEquals(expectedValues.length, values.size());    for (int i = 0; i < expectedValues.length; ++i) {        Long expectedValue = expectedValues[i];        ByteBuffer value = values.get(i);        if (expectedValue == null) {            assertFalse("The byte buffer should be empty for null pages", value.hasRemaining());        } else {            assertEquals("The byte buffer should be 8 bytes long for int64", 8, value.remaining());            assertEquals("Invalid value for page " + i, expectedValue.intValue(), value.getLong(0));        }    }}
private static void parquet-mr_f3616_0(ColumnIndex columnIndex, long... expectedNullCounts)
{    List<Long> nullCounts = columnIndex.getNullCounts();    assertEquals(expectedNullCounts.length, nullCounts.size());    for (int i = 0; i < expectedNullCounts.length; ++i) {        assertEquals("Invalid null count at page " + i, expectedNullCounts[i], nullCounts.get(i).longValue());    }}
private static void parquet-mr_f3617_0(ColumnIndex columnIndex, boolean... expectedNullPages)
{    List<Boolean> nullPages = columnIndex.getNullPages();    assertEquals(expectedNullPages.length, nullPages.size());    for (int i = 0; i < expectedNullPages.length; ++i) {        assertEquals("Invalid null pages at page " + i, expectedNullPages[i], nullPages.get(i).booleanValue());    }}
 Statistics<?> parquet-mr_f3618_0(PrimitiveType type, Object... values)
{    Statistics<?> stats = Statistics.createStats(type);    for (Object value : values) {        if (value == null) {            stats.incrementNumNulls();            continue;        }        switch(type.getPrimitiveTypeName()) {            case BINARY:            case FIXED_LEN_BYTE_ARRAY:            case INT96:                stats.updateStats((Binary) value);                break;            case BOOLEAN:                stats.updateStats((boolean) value);                break;            case DOUBLE:                stats.updateStats((double) value);                break;            case FLOAT:                stats.updateStats((float) value);                break;            case INT32:                stats.updateStats((int) value);                break;            case INT64:                stats.updateStats((long) value);                break;            default:                fail("Unsupported value type for stats: " + value.getClass());        }    }    if (stats.hasNonNullValue()) {        minMaxSize += stats.getMinBytes().length;        minMaxSize += stats.getMaxBytes().length;    }    return stats;}
 long parquet-mr_f3619_0()
{    return minMaxSize;}
private static void parquet-mr_f3620_0(ColumnIndex ci, FilterPredicate predicate, int... expectedIndexes)
{    TestIndexIterator.assertEquals(predicate.accept(ci), expectedIndexes);}
public void parquet-mr_f3621_0()
{    assertEquals(IndexIterator.all(10), 0, 1, 2, 3, 4, 5, 6, 7, 8, 9);}
public void parquet-mr_f3622_0()
{    assertEquals(IndexIterator.filter(30, value -> value % 3 == 0), 0, 3, 6, 9, 12, 15, 18, 21, 24, 27);}
public void parquet-mr_f3623_0()
{    assertEquals(IndexIterator.filterTranslate(20, value -> value < 5, Math::negateExact), 0, -1, -2, -3, -4);}
public void parquet-mr_f3624_0()
{    assertEquals(IndexIterator.rangeTranslate(11, 18, i -> i - 10), 1, 2, 3, 4, 5, 6, 7, 8);}
 static void parquet-mr_f3625_0(PrimitiveIterator.OfInt actualIt, int... expectedValues)
{    IntList actualList = new IntArrayList();    actualIt.forEachRemaining((int value) -> actualList.add(value));    int[] actualValues = actualList.toIntArray();    assertArrayEquals("ExpectedValues: " + Arrays.toString(expectedValues) + " ActualValues: " + Arrays.toString(actualValues), expectedValues, actualValues);}
public void parquet-mr_f3626_0()
{    OffsetIndexBuilder builder = OffsetIndexBuilder.getBuilder();    assertNull(builder.build());    assertNull(builder.build(1234));    builder.add(1000, 10);    builder.add(2000, 19);    builder.add(3000, 27);    builder.add(1200, 9);    assertCorrectValues(builder.build(), 0, 1000, 0, 1000, 2000, 10, 3000, 3000, 29, 6000, 1200, 56);    assertCorrectValues(builder.build(10000), 10000, 1000, 0, 11000, 2000, 10, 13000, 3000, 29, 16000, 1200, 56);}
public void parquet-mr_f3627_0()
{    OffsetIndexBuilder builder = OffsetIndexBuilder.getNoOpBuilder();    builder.add(1, 2);    builder.add(3, 4);    builder.add(5, 6);    builder.add(7, 8);    assertNull(builder.build());    assertNull(builder.build(1000));}
public void parquet-mr_f3628_0()
{    OffsetIndexBuilder builder = OffsetIndexBuilder.getBuilder();    assertNull(builder.build());    assertNull(builder.build(1234));    builder.add(1000, 10000, 0);    builder.add(22000, 12000, 100);    builder.add(48000, 22000, 211);    builder.add(90000, 30000, 361);    assertCorrectValues(builder.build(), 1000, 10000, 0, 22000, 12000, 100, 48000, 22000, 211, 90000, 30000, 361);    assertCorrectValues(builder.build(100000), 101000, 10000, 0, 122000, 12000, 100, 148000, 22000, 211, 190000, 30000, 361);}
public void parquet-mr_f3629_0()
{    OffsetIndexBuilder builder = OffsetIndexBuilder.getNoOpBuilder();    builder.add(1, 2, 3);    builder.add(4, 5, 6);    builder.add(7, 8, 9);    builder.add(10, 11, 12);    assertNull(builder.build());    assertNull(builder.build(1000));}
private void parquet-mr_f3630_0(OffsetIndex offsetIndex, long... offset_size_rowIndex_triplets)
{    assertEquals(offset_size_rowIndex_triplets.length % 3, 0);    int pageCount = offset_size_rowIndex_triplets.length / 3;    assertEquals("Invalid pageCount", pageCount, offsetIndex.getPageCount());    for (int i = 0; i < pageCount; ++i) {        assertEquals("Invalid offsetIndex at page " + i, offset_size_rowIndex_triplets[3 * i], offsetIndex.getOffset(i));        assertEquals("Invalid compressedPageSize at page " + i, offset_size_rowIndex_triplets[3 * i + 1], offsetIndex.getCompressedPageSize(i));        assertEquals("Invalid firstRowIndex at page " + i, offset_size_rowIndex_triplets[3 * i + 2], offsetIndex.getFirstRowIndex(i));        long expectedLastPageIndex = (i < pageCount - 1) ? (offset_size_rowIndex_triplets[3 * i + 5] - 1) : 999;        assertEquals("Invalid lastRowIndex at page " + i, expectedLastPageIndex, offsetIndex.getLastRowIndex(i, 1000));    }}
 CIBuilder parquet-mr_f3631_0(long nullCount)
{    nullPages.add(true);    nullCounts.add(nullCount);    minValues.add(EMPTY);    maxValues.add(EMPTY);    return this;}
 CIBuilder parquet-mr_f3632_0(long nullCount, int min, int max)
{    nullPages.add(false);    nullCounts.add(nullCount);    minValues.add(ByteBuffer.wrap(BytesUtils.intToBytes(min)));    maxValues.add(ByteBuffer.wrap(BytesUtils.intToBytes(max)));    return this;}
 CIBuilder parquet-mr_f3633_0(long nullCount, String min, String max)
{    nullPages.add(false);    nullCounts.add(nullCount);    minValues.add(ByteBuffer.wrap(min.getBytes(UTF_8)));    maxValues.add(ByteBuffer.wrap(max.getBytes(UTF_8)));    return this;}
 CIBuilder parquet-mr_f3634_0(long nullCount, double min, double max)
{    nullPages.add(false);    nullCounts.add(nullCount);    minValues.add(ByteBuffer.wrap(BytesUtils.longToBytes(Double.doubleToLongBits(min))));    maxValues.add(ByteBuffer.wrap(BytesUtils.longToBytes(Double.doubleToLongBits(max))));    return this;}
 ColumnIndex parquet-mr_f3635_0()
{    return ColumnIndexBuilder.build(type, order, nullPages, nullCounts, minValues, maxValues);}
 OIBuilder parquet-mr_f3636_0(long rowCount)
{    builder.add(1234, rowCount);    return this;}
 OffsetIndex parquet-mr_f3637_0()
{    return builder.build();}
public boolean parquet-mr_f3638_0(Integer value)
{    return true;}
public boolean parquet-mr_f3639_0(Statistics<Integer> statistics)
{    return false;}
public boolean parquet-mr_f3640_0(Statistics<Integer> statistics)
{    return true;}
public ColumnIndex parquet-mr_f3641_0(ColumnPath column)
{    switch(column.toDotString()) {        case "column1":            return COLUMN1_CI;        case "column2":            return COLUMN2_CI;        case "column3":            return COLUMN3_CI;        case "column4":            return COLUMN4_CI;        default:            return null;    }}
public OffsetIndex parquet-mr_f3642_0(ColumnPath column)
{    switch(column.toDotString()) {        case "column1":            return COLUMN1_OI;        case "column2":            return COLUMN2_OI;        case "column3":            return COLUMN3_OI;        case "column4":            return COLUMN4_OI;        default:            throw new MissingOffsetIndexException(column);    }}
private static Set<ColumnPath> parquet-mr_f3643_0(String... columns)
{    Set<ColumnPath> paths = new HashSet<>();    for (String column : columns) {        paths.add(ColumnPath.fromDotString(column));    }    return paths;}
private static void parquet-mr_f3644_0(RowRanges ranges, long rowCount)
{    LongList actualList = new LongArrayList();    ranges.iterator().forEachRemaining((long value) -> actualList.add(value));    LongList expectedList = new LongArrayList();    LongStream.range(0, rowCount).forEach(expectedList::add);    assertArrayEquals(expectedList + " != " + actualList, expectedList.toLongArray(), actualList.toLongArray());}
private static void parquet-mr_f3645_0(RowRanges ranges, long... expectedRows)
{    LongList actualList = new LongArrayList();    ranges.iterator().forEachRemaining((long value) -> actualList.add(value));    assertArrayEquals(Arrays.toString(expectedRows) + " != " + actualList, expectedRows, actualList.toLongArray());}
public void parquet-mr_f3646_0()
{    Set<ColumnPath> paths = paths("column1", "column2", "column3", "column4");    assertAllRows(calculateRowRanges(FilterCompat.get(userDefined(intColumn("column1"), AnyInt.class)), STORE, paths, TOTAL_ROW_COUNT), TOTAL_ROW_COUNT);    assertRows(calculateRowRanges(FilterCompat.get(and(and(eq(intColumn("column1"), null), eq(binaryColumn("column2"), null)), and(eq(doubleColumn("column3"), null), eq(booleanColumn("column4"), null)))), STORE, paths, TOTAL_ROW_COUNT), 6, 9);    assertRows(calculateRowRanges(FilterCompat.get(and(and(notEq(intColumn("column1"), null), notEq(binaryColumn("column2"), null)), and(notEq(doubleColumn("column3"), null), notEq(booleanColumn("column4"), null)))), STORE, paths, TOTAL_ROW_COUNT), 0, 1, 2, 3, 4, 5, 6, 7, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25);    assertRows(calculateRowRanges(FilterCompat.get(or(and(lt(intColumn("column1"), 20), gtEq(binaryColumn("column2"), fromString("Quebec"))), and(gt(doubleColumn("column3"), 5.32), ltEq(binaryColumn("column4"), fromString("XYZ"))))), STORE, paths, TOTAL_ROW_COUNT), 0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 23, 24, 25);    assertRows(calculateRowRanges(FilterCompat.get(and(and(gtEq(intColumn("column1"), 7), gt(binaryColumn("column2"), fromString("India"))), and(eq(doubleColumn("column3"), null), notEq(binaryColumn("column4"), null)))), STORE, paths, TOTAL_ROW_COUNT), 7, 16, 17, 18, 19, 20);    assertRows(calculateRowRanges(FilterCompat.get(and(or(invert(userDefined(intColumn("column1"), AnyInt.class)), eq(binaryColumn("column2"), fromString("Echo"))), eq(doubleColumn("column3"), 6.0))), STORE, paths, TOTAL_ROW_COUNT), 23, 24, 25);    assertRows(calculateRowRanges(FilterCompat.get(and(userDefined(intColumn("column1"), IntegerIsDivisableWith3.class), and(userDefined(binaryColumn("column2"), BinaryUtf8StartsWithB.class), userDefined(doubleColumn("column3"), DoubleIsInteger.class)))), STORE, paths, TOTAL_ROW_COUNT), 21, 22, 23, 24, 25);    assertRows(calculateRowRanges(FilterCompat.get(and(and(gtEq(intColumn("column1"), 7), lt(intColumn("column1"), 11)), and(gt(binaryColumn("column2"), fromString("Romeo")), ltEq(binaryColumn("column2"), fromString("Tango"))))), STORE, paths, TOTAL_ROW_COUNT), 7, 11, 12, 13);}
public void parquet-mr_f3647_0()
{    Set<ColumnPath> paths = paths("column1", "column2", "column3", "column4");        assertAllRows(calculateRowRanges(FilterCompat.get(notEq(intColumn("missing_column"), 0)), STORE, paths, TOTAL_ROW_COUNT), TOTAL_ROW_COUNT);    assertRows(calculateRowRanges(FilterCompat.get(and(and(gtEq(intColumn("column1"), 7), lt(intColumn("column1"), 11)), eq(binaryColumn("missing_column"), null))), STORE, paths, TOTAL_ROW_COUNT), 7, 8, 9, 10, 11, 12, 13);        assertRows(calculateRowRanges(FilterCompat.get(or(and(gtEq(intColumn("column1"), 7), lt(intColumn("column1"), 11)), notEq(binaryColumn("missing_column"), null))), STORE, paths, TOTAL_ROW_COUNT), 7, 8, 9, 10, 11, 12, 13);    assertRows(calculateRowRanges(FilterCompat.get(gt(intColumn("missing_column"), 0)), STORE, paths, TOTAL_ROW_COUNT));}
public void parquet-mr_f3648_0()
{    Set<ColumnPath> paths = paths("column1", "column2", "column3", "column4", "column_wo_oi");    assertAllRows(calculateRowRanges(FilterCompat.get(and(and(gtEq(intColumn("column1"), 7), lt(intColumn("column1"), 11)), and(gt(binaryColumn("column2"), fromString("Romeo")), ltEq(binaryColumn("column_wo_oi"), fromString("Tango"))))), STORE, paths, TOTAL_ROW_COUNT), TOTAL_ROW_COUNT);}
private static RowRanges parquet-mr_f3649_0(long... rowIndexes)
{    if (rowIndexes.length == 0) {        return RowRanges.EMPTY;    }    OffsetIndexBuilder builder = OffsetIndexBuilder.getBuilder();    for (int i = 0, n = rowIndexes.length; i < n; i += 2) {        long from = rowIndexes[i];        long to = rowIndexes[i + 1];        builder.add(0, 0, from);        builder.add(0, 0, to + 1);    }    PrimitiveIterator.OfInt pageIndexes = new PrimitiveIterator.OfInt() {        private int index = 0;        @Override        public boolean hasNext() {            return index < rowIndexes.length;        }        @Override        public int nextInt() {            int ret = index;            index += 2;            return ret;        }    };    return RowRanges.create(rowIndexes[rowIndexes.length - 1], pageIndexes, builder.build());}
public boolean parquet-mr_f3650_0()
{    return index < rowIndexes.length;}
public int parquet-mr_f3651_0()
{    int ret = index;    index += 2;    return ret;}
private static void parquet-mr_f3652_0(PrimitiveIterator.OfLong actualIt, long... expectedValues)
{    LongList actualList = new LongArrayList();    actualIt.forEachRemaining((long value) -> actualList.add(value));    assertArrayEquals(Arrays.toString(expectedValues) + "!= " + actualList, expectedValues, actualList.toLongArray());}
public void parquet-mr_f3653_0()
{    RowRanges ranges = buildRanges(1, 2, 3, 4, 6, 7, 7, 10, 15, 17);    assertAllRowsEqual(ranges.iterator(), 1, 2, 3, 4, 6, 7, 8, 9, 10, 15, 16, 17);    assertEquals(12, ranges.rowCount());    assertTrue(ranges.isOverlapping(4, 5));    assertFalse(ranges.isOverlapping(5, 5));    assertTrue(ranges.isOverlapping(10, 14));    assertFalse(ranges.isOverlapping(11, 14));    assertFalse(ranges.isOverlapping(18, Long.MAX_VALUE));    ranges = RowRanges.createSingle(5);    assertAllRowsEqual(ranges.iterator(), 0, 1, 2, 3, 4);    assertEquals(5, ranges.rowCount());    assertTrue(ranges.isOverlapping(0, 100));    assertFalse(ranges.isOverlapping(5, Long.MAX_VALUE));    ranges = RowRanges.EMPTY;    assertAllRowsEqual(ranges.iterator());    assertEquals(0, ranges.rowCount());    assertFalse(ranges.isOverlapping(0, Long.MAX_VALUE));}
public void parquet-mr_f3654_0()
{    RowRanges ranges1 = buildRanges(2, 5, 7, 9, 14, 14, 20, 24);    RowRanges ranges2 = buildRanges(1, 2, 4, 5, 11, 12, 14, 15, 21, 22);    RowRanges empty = buildRanges();    assertAllRowsEqual(union(ranges1, ranges2).iterator(), 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 14, 15, 20, 21, 22, 23, 24);    assertAllRowsEqual(union(ranges2, ranges1).iterator(), 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 14, 15, 20, 21, 22, 23, 24);    assertAllRowsEqual(union(ranges1, ranges1).iterator(), 2, 3, 4, 5, 7, 8, 9, 14, 20, 21, 22, 23, 24);    assertAllRowsEqual(union(ranges1, empty).iterator(), 2, 3, 4, 5, 7, 8, 9, 14, 20, 21, 22, 23, 24);    assertAllRowsEqual(union(empty, ranges1).iterator(), 2, 3, 4, 5, 7, 8, 9, 14, 20, 21, 22, 23, 24);    assertAllRowsEqual(union(ranges2, ranges2).iterator(), 1, 2, 4, 5, 11, 12, 14, 15, 21, 22);    assertAllRowsEqual(union(ranges2, empty).iterator(), 1, 2, 4, 5, 11, 12, 14, 15, 21, 22);    assertAllRowsEqual(union(empty, ranges2).iterator(), 1, 2, 4, 5, 11, 12, 14, 15, 21, 22);    assertAllRowsEqual(union(empty, empty).iterator());}
public void parquet-mr_f3655_0()
{    RowRanges ranges1 = buildRanges(2, 5, 7, 9, 14, 14, 20, 24);    RowRanges ranges2 = buildRanges(1, 2, 6, 7, 9, 9, 11, 12, 14, 15, 21, 22);    RowRanges empty = buildRanges();    assertAllRowsEqual(intersection(ranges1, ranges2).iterator(), 2, 7, 9, 14, 21, 22);    assertAllRowsEqual(intersection(ranges2, ranges1).iterator(), 2, 7, 9, 14, 21, 22);    assertAllRowsEqual(intersection(ranges1, ranges1).iterator(), 2, 3, 4, 5, 7, 8, 9, 14, 20, 21, 22, 23, 24);    assertAllRowsEqual(intersection(ranges1, empty).iterator());    assertAllRowsEqual(intersection(empty, ranges1).iterator());    assertAllRowsEqual(intersection(ranges2, ranges2).iterator(), 1, 2, 6, 7, 9, 11, 12, 14, 15, 21, 22);    assertAllRowsEqual(intersection(ranges2, empty).iterator());    assertAllRowsEqual(intersection(empty, ranges2).iterator());    assertAllRowsEqual(intersection(empty, empty).iterator());}
private static void parquet-mr_f3656_0(byte[] bytes)
{    for (int i = 0; i < bytes.length; i++) {        bytes[i] = (byte) (bytes[i] + 1);    }}
public BinaryAndOriginal parquet-mr_f3657_0(byte[] bytes, boolean reused) throws Exception
{    byte[] orig = Arrays.copyOf(bytes, bytes.length);    if (reused) {        return new BinaryAndOriginal(Binary.fromReusedByteArray(orig), orig);    } else {        return new BinaryAndOriginal(Binary.fromConstantByteArray(orig), orig);    }}
public BinaryAndOriginal parquet-mr_f3658_0(byte[] bytes, boolean reused) throws Exception
{    byte[] orig = padded(bytes);    Binary b;    if (reused) {        b = Binary.fromReusedByteArray(orig, 5, bytes.length);    } else {        b = Binary.fromConstantByteArray(orig, 5, bytes.length);    }    assertArrayEquals(bytes, b.getBytes());    return new BinaryAndOriginal(b, orig);}
public BinaryAndOriginal parquet-mr_f3659_0(byte[] bytes, boolean reused) throws Exception
{    byte[] orig = padded(bytes);    ByteBuffer buff = ByteBuffer.wrap(orig, 5, bytes.length);    Binary b;    if (reused) {        b = Binary.fromReusedByteBuffer(buff);    } else {        b = Binary.fromConstantByteBuffer(buff);    }    buff.mark();    assertArrayEquals(bytes, b.getBytes());    buff.reset();    return new BinaryAndOriginal(b, orig);}
public BinaryAndOriginal parquet-mr_f3660_0(byte[] bytes, boolean reused) throws Exception
{    Binary b = Binary.fromString(new String(bytes, UTF8));        return new BinaryAndOriginal(b, b.getBytesUnsafe());}
private static byte[] parquet-mr_f3661_0(byte[] bytes)
{    byte[] padded = new byte[bytes.length + 10];    for (int i = 0; i < 5; i++) {        padded[i] = (byte) i;    }    System.arraycopy(bytes, 0, padded, 5, bytes.length);    for (int i = 0; i < 5; i++) {        padded[i + 5 + bytes.length] = (byte) i;    }    return padded;}
public void parquet-mr_f3662_0() throws Exception
{    testBinary(BYTE_ARRAY_BACKED_BF, true);    testBinary(BYTE_ARRAY_BACKED_BF, false);}
public void parquet-mr_f3663_0() throws Exception
{    testBinary(BYTE_ARRAY_SLICE_BACKED_BF, true);    testBinary(BYTE_ARRAY_SLICE_BACKED_BF, false);}
public void parquet-mr_f3664_0() throws Exception
{    testBinary(BUFFER_BF, true);    testBinary(BUFFER_BF, false);}
public void parquet-mr_f3665_0() throws Exception
{    Binary bin1 = Binary.fromConstantByteArray("alice".getBytes(), 1, 3);    Binary bin2 = Binary.fromConstantByteBuffer(ByteBuffer.wrap("alice".getBytes(), 1, 3));    assertEquals(bin1, bin2);}
public void parquet-mr_f3666_0() throws Exception
{    byte[] orig = { 10, 9, 8, 7, 6, 5, 4, 3, 2, 1 };    testWriteAllToHelper(Binary.fromConstantByteBuffer(ByteBuffer.wrap(orig)), orig);    ByteBuffer buf = ByteBuffer.allocateDirect(orig.length);    buf.put(orig);    buf.flip();    testWriteAllToHelper(Binary.fromConstantByteBuffer(buf), orig);}
private void parquet-mr_f3667_0(Binary binary, byte[] orig) throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream(orig.length);    binary.writeTo(out);    assertArrayEquals(orig, out.toByteArray());}
public void parquet-mr_f3668_0() throws Exception
{    testBinary(STRING_BF, false);}
private void parquet-mr_f3669_0(BinaryFactory bf, boolean reused) throws Exception
{    BinaryAndOriginal bao = bf.get(testString.getBytes(UTF8), reused);    assertArrayEquals(testString.getBytes(UTF8), bao.binary.slice(0, testString.length()).getBytesUnsafe());    assertArrayEquals("123".getBytes(UTF8), bao.binary.slice(5, 3).getBytesUnsafe());}
private void parquet-mr_f3670_0(BinaryFactory bf) throws Exception
{    BinaryAndOriginal bao = bf.get(testString.getBytes(UTF8), false);    assertEquals(false, bao.binary.isBackingBytesReused());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.getBytes());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.getBytesUnsafe());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.copy().getBytesUnsafe());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.copy().getBytes());    bao = bf.get(testString.getBytes(UTF8), false);    assertEquals(false, bao.binary.isBackingBytesReused());    Binary copy = bao.binary.copy();    assertSame(copy, bao.binary);}
private void parquet-mr_f3671_0(BinaryFactory bf) throws Exception
{    BinaryAndOriginal bao = bf.get(testString.getBytes(UTF8), true);    assertEquals(true, bao.binary.isBackingBytesReused());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.getBytes());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.getBytesUnsafe());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.copy().getBytesUnsafe());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.copy().getBytes());    bao = bf.get(testString.getBytes(UTF8), true);    assertEquals(true, bao.binary.isBackingBytesReused());    Binary copy = bao.binary.copy();    mutate(bao.original);    assertArrayEquals(testString.getBytes(UTF8), copy.getBytes());    assertArrayEquals(testString.getBytes(UTF8), copy.getBytesUnsafe());    assertArrayEquals(testString.getBytes(UTF8), copy.copy().getBytesUnsafe());    assertArrayEquals(testString.getBytes(UTF8), copy.copy().getBytes());}
private void parquet-mr_f3672_0(BinaryFactory bf, boolean reused) throws Exception
{    BinaryAndOriginal bao = bf.get("polygon".getBytes(UTF8), reused);    ByteArrayOutputStream baos = new ByteArrayOutputStream();    ObjectOutputStream out = new ObjectOutputStream(baos);    out.writeObject(bao.binary);    out.close();    baos.close();    ObjectInputStream in = new ObjectInputStream(new ByteArrayInputStream(baos.toByteArray()));    Object object = in.readObject();    assertTrue(object instanceof Binary);    assertEquals(bao.binary, object);}
private void parquet-mr_f3673_0(BinaryFactory bf, boolean reused) throws Exception
{    testSlice(bf, reused);    if (reused) {        testReusedCopy(bf);    } else {        testConstantCopy(bf);    }    testSerializable(bf, reused);}
public void parquet-mr_f3674_0()
{    Binary b1 = Binary.fromCharSequence("aaaaaaaa");    Binary b2 = Binary.fromString("aaaaaaab");    Binary b3 = Binary.fromReusedByteArray("aaaaaaaaaaa".getBytes(), 1, 8);    Binary b4 = Binary.fromConstantByteBuffer(ByteBuffer.wrap("aaaaaaac".getBytes()));    assertTrue(b1.compareTo(b2) < 0);    assertTrue(b2.compareTo(b1) > 0);    assertTrue(b3.compareTo(b4) < 0);    assertTrue(b4.compareTo(b3) > 0);    assertTrue(b1.compareTo(b4) < 0);    assertTrue(b4.compareTo(b1) > 0);    assertTrue(b2.compareTo(b4) < 0);    assertTrue(b4.compareTo(b2) > 0);    assertTrue(b1.compareTo(b3) == 0);    assertTrue(b3.compareTo(b1) == 0);}
public void parquet-mr_f3675_0()
{    root.start();    this.currentType = schema;    this.current = root;}
public void parquet-mr_f3676_0()
{    root.end();}
public void parquet-mr_f3677_0(String field, int index)
{    path.push(current);    typePath.push(currentType);    currentType = currentType.asGroupType().getType(index);    if (currentType.isPrimitive()) {        currentPrimitive = current.getConverter(index).asPrimitiveConverter();    } else {        current = current.getConverter(index).asGroupConverter();    }}
public void parquet-mr_f3678_0(String field, int index)
{    currentType = typePath.pop();    current = path.pop();}
public void parquet-mr_f3679_0()
{    current.start();}
public void parquet-mr_f3680_0()
{    current.end();}
public void parquet-mr_f3681_0(int value)
{    currentPrimitive.addInt(value);}
public void parquet-mr_f3682_0(long value)
{    currentPrimitive.addLong(value);}
public void parquet-mr_f3683_0(boolean value)
{    currentPrimitive.addBoolean(value);}
public void parquet-mr_f3684_0(Binary value)
{    currentPrimitive.addBinary(value);}
public void parquet-mr_f3685_0(float value)
{    currentPrimitive.addFloat(value);}
public void parquet-mr_f3686_0(double value)
{    currentPrimitive.addDouble(value);}
public void parquet-mr_f3687_0()
{}
public void parquet-mr_f3688_0(String got)
{    assertEquals("event #" + count, expectations.pop(), got);    ++count;}
public Converter parquet-mr_f3689_0(final List<GroupType> path, final PrimitiveType primitiveType)
{    return new PrimitiveConverter() {        private void validate(String message) {            ExpectationValidatingConverter.this.validate(path(path, primitiveType) + message);        }        @Override        public void addBinary(Binary value) {            validate("addBinary(" + value.toStringUsingUTF8() + ")");        }        @Override        public void addBoolean(boolean value) {            validate("addBoolean(" + value + ")");        }        @Override        public void addDouble(double value) {            validate("addDouble(" + value + ")");        }        @Override        public void addFloat(float value) {            validate("addFloat(" + value + ")");        }        @Override        public void addInt(int value) {            validate("addInt(" + value + ")");        }        @Override        public void addLong(long value) {            validate("addLong(" + value + ")");        }    };}
private void parquet-mr_f3690_0(String message)
{    ExpectationValidatingConverter.this.validate(path(path, primitiveType) + message);}
public void parquet-mr_f3691_0(Binary value)
{    validate("addBinary(" + value.toStringUsingUTF8() + ")");}
public void parquet-mr_f3692_0(boolean value)
{    validate("addBoolean(" + value + ")");}
public void parquet-mr_f3693_0(double value)
{    validate("addDouble(" + value + ")");}
public void parquet-mr_f3694_0(float value)
{    validate("addFloat(" + value + ")");}
public void parquet-mr_f3695_0(int value)
{    validate("addInt(" + value + ")");}
public void parquet-mr_f3696_0(long value)
{    validate("addLong(" + value + ")");}
public Converter parquet-mr_f3697_0(final List<GroupType> path, final GroupType groupType, final List<Converter> children)
{    return new GroupConverter() {        private void validate(String message) {            ExpectationValidatingConverter.this.validate(path(path, groupType) + message);        }        @Override        public void start() {            validate("start()");        }        @Override        public void end() {            validate("end()");        }        @Override        public Converter getConverter(int fieldIndex) {            return children.get(fieldIndex);        }    };}
private void parquet-mr_f3698_0(String message)
{    ExpectationValidatingConverter.this.validate(path(path, groupType) + message);}
public void parquet-mr_f3699_0()
{    validate("start()");}
public void parquet-mr_f3700_0()
{    validate("end()");}
public Converter parquet-mr_f3701_0(int fieldIndex)
{    return children.get(fieldIndex);}
public Converter parquet-mr_f3702_0(MessageType messageType, final List<Converter> children)
{    return new GroupConverter() {        @Override        public Converter getConverter(int fieldIndex) {            return children.get(fieldIndex);        }        @Override        public void start() {            validate("startMessage()");        }        @Override        public void end() {            validate("endMessage()");        }    };}
public Converter parquet-mr_f3703_0(int fieldIndex)
{    return children.get(fieldIndex);}
public void parquet-mr_f3704_0()
{    validate("startMessage()");}
public void parquet-mr_f3705_0()
{    validate("endMessage()");}
public Void parquet-mr_f3706_0()
{    return null;}
private String parquet-mr_f3707_0(List<GroupType> path, Type type)
{    String pathString = "";    if (path.size() > 0) {        for (int i = 1; i < path.size(); i++) {            pathString += path.get(i).getName() + ".";        }    }    pathString += type.getName() + ".";    return pathString;}
public GroupConverter parquet-mr_f3708_0()
{    return root;}
private void parquet-mr_f3709_0(String got)
{        assertEquals("event #" + count, expectations.pop(), got);    ++count;}
public void parquet-mr_f3710_0()
{    validate("startMessage()");}
public void parquet-mr_f3711_0()
{    validate("startGroup()");}
public void parquet-mr_f3712_0(String field, int index)
{    validate("startField(" + field + ", " + index + ")");}
public void parquet-mr_f3713_0()
{    validate("endMessage()");}
public void parquet-mr_f3714_0()
{    validate("endGroup()");}
public void parquet-mr_f3715_0(String field, int index)
{    validate("endField(" + field + ", " + index + ")");}
public void parquet-mr_f3716_0(int value)
{    validate("addInt(" + value + ")");}
public void parquet-mr_f3717_0(long value)
{    validate("addLong(" + value + ")");}
public void parquet-mr_f3718_0(boolean value)
{    validate("addBoolean(" + value + ")");}
public void parquet-mr_f3719_0(Binary value)
{    validate("addBinary(" + value.toStringUsingUTF8() + ")");}
public void parquet-mr_f3720_0(float value)
{    validate("addFloat(" + value + ")");}
public void parquet-mr_f3721_0(double value)
{    validate("addDouble(" + value + ")");}
public void parquet-mr_f3722_0()
{    validate("flush()");}
public static void parquet-mr_f3723_0(String[] args)
{    MemPageStore memPageStore = new MemPageStore(0);    write(memPageStore);    read(memPageStore);}
private static void parquet-mr_f3724_0(MemPageStore memPageStore)
{    read(memPageStore, schema, "read all");    read(memPageStore, schema, "read all");    read(memPageStore, schema2, "read projected");    read(memPageStore, schema3, "read projected no Strings");}
private static void parquet-mr_f3725_0(MemPageStore memPageStore, MessageType myschema, String message)
{    MessageColumnIO columnIO = newColumnFactory(myschema);    System.out.println(message);    RecordMaterializer<Object> recordConsumer = new DummyRecordConverter(myschema);    RecordReader<Object> recordReader = columnIO.getRecordReader(memPageStore, recordConsumer);    read(recordReader, 2, myschema);    read(recordReader, 10000, myschema);    read(recordReader, 10000, myschema);    read(recordReader, 10000, myschema);    read(recordReader, 10000, myschema);    read(recordReader, 10000, myschema);    read(recordReader, 100000, myschema);    read(recordReader, 1000000, myschema);    System.out.println();}
private static void parquet-mr_f3726_0(MemPageStore memPageStore)
{    ColumnWriteStoreV1 columns = new ColumnWriteStoreV1(memPageStore, ParquetProperties.builder().withPageSize(50 * 1024 * 1024).withDictionaryEncoding(false).build());    MessageColumnIO columnIO = newColumnFactory(schema);    GroupWriter groupWriter = new GroupWriter(columnIO.getRecordWriter(columns), schema);    groupWriter.write(r1);    groupWriter.write(r2);    write(memPageStore, groupWriter, 10000);    write(memPageStore, groupWriter, 10000);    write(memPageStore, groupWriter, 10000);    write(memPageStore, groupWriter, 10000);    write(memPageStore, groupWriter, 10000);    write(memPageStore, groupWriter, 100000);    write(memPageStore, groupWriter, 1000000);    columns.flush();    System.out.println();    System.out.println(columns.getBufferedSize() + " bytes used total");    System.out.println("max col size: " + columns.maxColMemSize() + " bytes");}
private static MessageColumnIO parquet-mr_f3727_0(MessageType schema)
{    return new ColumnIOFactory().getColumnIO(schema);}
private static void parquet-mr_f3728_0(RecordReader<Object> recordReader, int count, MessageType schema)
{    Object[] records = new Object[count];    System.gc();    System.out.print("no gc <");    long t0 = System.currentTimeMillis();    for (int i = 0; i < records.length; i++) {        records[i] = recordReader.read();    }    long t1 = System.currentTimeMillis();    System.out.print("> ");    long t = t1 - t0;        float err = (float) 100 * 2 / t;    System.out.printf("                                            read %,9d recs in %,5d ms at %,9d rec/s err: %3.2f%%\n", count, t, t == 0 ? 0 : count * 1000 / t, err);    if (!records[0].equals("end()")) {        throw new RuntimeException("" + records[0]);    }}
private static void parquet-mr_f3729_0(MemPageStore memPageStore, GroupWriter groupWriter, int count)
{    long t0 = System.currentTimeMillis();    for (int i = 0; i < count; i++) {        groupWriter.write(r1);    }    long t1 = System.currentTimeMillis();    long t = t1 - t0;    memPageStore.addRowCount(count);    System.out.printf("written %,9d recs in %,5d ms at %,9d rec/s\n", count, t, t == 0 ? 0 : count * 1000 / t);}
public static Collection<Object[]> parquet-mr_f3730_0() throws IOException
{    Object[][] data = { { true }, { false } };    return Arrays.asList(data);}
public void parquet-mr_f3731_0()
{    assertEquals(schemaString, schema.toString());}
public void parquet-mr_f3732_0()
{    MessageType orginalSchema = new MessageType("schema", new PrimitiveType(REQUIRED, INT32, "a"), new PrimitiveType(OPTIONAL, INT32, "b"));    MessageType schemaWithExtraField = new MessageType("schema", new PrimitiveType(OPTIONAL, INT32, "b"), new PrimitiveType(OPTIONAL, INT32, "a"), new PrimitiveType(OPTIONAL, INT32, "c"));    MemPageStore memPageStoreForOriginalSchema = new MemPageStore(1);    MemPageStore memPageStoreForSchemaWithExtraField = new MemPageStore(1);    SimpleGroupFactory groupFactory = new SimpleGroupFactory(orginalSchema);    writeGroups(orginalSchema, memPageStoreForOriginalSchema, groupFactory.newGroup().append("a", 1).append("b", 2));    SimpleGroupFactory groupFactory2 = new SimpleGroupFactory(schemaWithExtraField);    writeGroups(schemaWithExtraField, memPageStoreForSchemaWithExtraField, groupFactory2.newGroup().append("a", 1).append("b", 2).append("c", 3));    {        List<Group> groups = new ArrayList<Group>();        groups.addAll(readGroups(memPageStoreForOriginalSchema, orginalSchema, schemaWithExtraField, 1));        groups.addAll(readGroups(memPageStoreForSchemaWithExtraField, schemaWithExtraField, schemaWithExtraField, 1));                        Object[][] expected = { { 2, 1, null }, { 2, 1, 3 }         };        validateGroups(groups, expected);    }}
public void parquet-mr_f3733_0()
{    MessageType originalSchema = new MessageType("schema", new PrimitiveType(OPTIONAL, INT32, "e"));    MemPageStore store = new MemPageStore(1);    SimpleGroupFactory groupFactory = new SimpleGroupFactory(originalSchema);    writeGroups(originalSchema, store, groupFactory.newGroup().append("e", 4));    try {        MessageType schemaWithIncompatibleField = new MessageType("schema",         new PrimitiveType(OPTIONAL, BINARY, "e"));        readGroups(store, originalSchema, schemaWithIncompatibleField, 1);        fail("should have thrown an incompatible schema exception");    } catch (ParquetDecodingException e) {        assertEquals("The requested schema is not compatible with the file schema. incompatible types: optional binary e != optional int32 e", e.getMessage());    }}
public void parquet-mr_f3734_0()
{    MessageType originalSchema = new MessageType("schema", new PrimitiveType(OPTIONAL, INT32, "e"));    MemPageStore store = new MemPageStore(1);    SimpleGroupFactory groupFactory = new SimpleGroupFactory(originalSchema);    writeGroups(originalSchema, store, groupFactory.newGroup().append("e", 4));    try {        MessageType schemaWithRequiredFieldThatWasOptional = new MessageType("schema",         new PrimitiveType(REQUIRED, INT32, "e"));        readGroups(store, originalSchema, schemaWithRequiredFieldThatWasOptional, 1);        fail("should have thrown an incompatible schema exception");    } catch (ParquetDecodingException e) {        assertEquals("The requested schema is not compatible with the file schema. incompatible types: required int32 e != optional int32 e", e.getMessage());    }}
public void parquet-mr_f3735_0()
{    MessageType orginalSchema = new MessageType("schema", new PrimitiveType(REQUIRED, INT32, "a"), new PrimitiveType(REQUIRED, INT32, "b"));    MessageType projectedSchema = new MessageType("schema", new PrimitiveType(OPTIONAL, INT32, "b"));    MemPageStore store = new MemPageStore(1);    SimpleGroupFactory groupFactory = new SimpleGroupFactory(orginalSchema);    writeGroups(orginalSchema, store, groupFactory.newGroup().append("a", 1).append("b", 2));    {        List<Group> groups = new ArrayList<Group>();        groups.addAll(readGroups(store, orginalSchema, projectedSchema, 1));        Object[][] expected = { { 2 } };        validateGroups(groups, expected);    }}
private void parquet-mr_f3736_0(List<Group> groups1, Object[][] e1)
{    Iterator<Group> i1 = groups1.iterator();    for (int i = 0; i < e1.length; i++) {        Object[] objects = e1[i];        Group next = i1.next();        for (int j = 0; j < objects.length; j++) {            Object object = objects[j];            if (object == null) {                assertEquals(0, next.getFieldRepetitionCount(j));            } else {                assertEquals("looking for r[" + i + "][" + j + "][0]=" + object, 1, next.getFieldRepetitionCount(j));                assertEquals(object, next.getInteger(j, 0));            }        }    }}
private List<Group> parquet-mr_f3737_0(MemPageStore memPageStore, MessageType fileSchema, MessageType requestedSchema, int n)
{    ColumnIOFactory columnIOFactory = new ColumnIOFactory(true);    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);    RecordReaderImplementation<Group> recordReader = getRecordReader(columnIO, requestedSchema, memPageStore);    List<Group> groups = new ArrayList<Group>();    for (int i = 0; i < n; i++) {        groups.add(recordReader.read());    }    return groups;}
private void parquet-mr_f3738_0(MessageType writtenSchema, MemPageStore memPageStore, Group... groups)
{    ColumnIOFactory columnIOFactory = new ColumnIOFactory(true);    ColumnWriteStoreV1 columns = newColumnWriteStore(memPageStore);    MessageColumnIO columnIO = columnIOFactory.getColumnIO(writtenSchema);    RecordConsumer recordWriter = columnIO.getRecordWriter(columns);    GroupWriter groupWriter = new GroupWriter(recordWriter, writtenSchema);    for (Group group : groups) {        groupWriter.write(group);    }    recordWriter.flush();    columns.flush();}
public void parquet-mr_f3739_0()
{    log(schema);    log("r1");    log(r1);    log("r2");    log(r2);    MemPageStore memPageStore = new MemPageStore(2);    ColumnWriteStoreV1 columns = newColumnWriteStore(memPageStore);    ColumnIOFactory columnIOFactory = new ColumnIOFactory(true);    {        MessageColumnIO columnIO = columnIOFactory.getColumnIO(schema);        log(columnIO);        RecordConsumer recordWriter = columnIO.getRecordWriter(columns);        GroupWriter groupWriter = new GroupWriter(recordWriter, schema);        groupWriter.write(r1);        groupWriter.write(r2);        recordWriter.flush();        columns.flush();        log(columns);        log("=========");        RecordReaderImplementation<Group> recordReader = getRecordReader(columnIO, schema, memPageStore);        validateFSA(expectedFSA, columnIO, recordReader);        List<Group> records = new ArrayList<Group>();        records.add(recordReader.read());        records.add(recordReader.read());        int i = 0;        for (Group record : records) {            log("r" + (++i));            log(record);        }        assertEquals("deserialization does not display the same result", r1.toString(), records.get(0).toString());        assertEquals("deserialization does not display the same result", r2.toString(), records.get(1).toString());    }    {        MessageColumnIO columnIO2 = columnIOFactory.getColumnIO(schema2);        List<Group> records = new ArrayList<Group>();        RecordReaderImplementation<Group> recordReader = getRecordReader(columnIO2, schema2, memPageStore);        validateFSA(expectedFSA2, columnIO2, recordReader);        records.add(recordReader.read());        records.add(recordReader.read());        int i = 0;        for (Group record : records) {            log("r" + (++i));            log(record);        }        assertEquals("deserialization does not display the expected result", pr1.toString(), records.get(0).toString());        assertEquals("deserialization does not display the expected result", pr2.toString(), records.get(1).toString());    }}
public void parquet-mr_f3740_0()
{    MessageType oneOfEachSchema = MessageTypeParser.parseMessageType(oneOfEach);    GroupFactory gf = new SimpleGroupFactory(oneOfEachSchema);    Group g1 = gf.newGroup().append("a", 1l).append("b", 2).append("c", 3.0f).append("d", 4.0d).append("e", true).append("f", Binary.fromString("6")).append("g", new NanoTime(1234, System.currentTimeMillis() * 1000)).append("h", Binary.fromString("abc"));    testSchema(oneOfEachSchema, Arrays.asList(g1));}
public void parquet-mr_f3741_0()
{    MessageType reqreqSchema = MessageTypeParser.parseMessageType("message Document {\n" + "  required group foo {\n" + "    required int64 bar;\n" + "  }\n" + "}\n");    GroupFactory gf = new SimpleGroupFactory(reqreqSchema);    Group g1 = gf.newGroup();    g1.addGroup("foo").append("bar", 2l);    testSchema(reqreqSchema, Arrays.asList(g1));}
public void parquet-mr_f3742_0()
{    for (int i = 0; i < 6; i++) {        Type current = new PrimitiveType(Repetition.REQUIRED, PrimitiveTypeName.BINARY, "primitive");        for (int j = 0; j < i; j++) {            current = new GroupType(Repetition.REQUIRED, "req" + j, current);        }        MessageType groupSchema = new MessageType("schema" + i, current);        GroupFactory gf = new SimpleGroupFactory(groupSchema);        List<Group> groups = new ArrayList<Group>();        Group root = gf.newGroup();        Group currentGroup = root;        for (int j = 0; j < i; j++) {            currentGroup = currentGroup.addGroup(0);        }        currentGroup.add(0, Binary.fromString("foo"));        groups.add(root);        testSchema(groupSchema, groups);    }    for (int i = 0; i < 6; i++) {        Type current = new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, "primitive");        for (int j = 0; j < i; j++) {            current = new GroupType(Repetition.REQUIRED, "req" + j, current);        }        MessageType groupSchema = new MessageType("schema" + (i + 6), current);        GroupFactory gf = new SimpleGroupFactory(groupSchema);        List<Group> groups = new ArrayList<Group>();        Group rootDefined = gf.newGroup();        Group rootUndefined = gf.newGroup();        Group currentDefinedGroup = rootDefined;        Group currentUndefinedGroup = rootUndefined;        for (int j = 0; j < i; j++) {            currentDefinedGroup = currentDefinedGroup.addGroup(0);            currentUndefinedGroup = currentUndefinedGroup.addGroup(0);        }        currentDefinedGroup.add(0, Binary.fromString("foo"));        groups.add(rootDefined);        groups.add(rootUndefined);        testSchema(groupSchema, groups);    }    for (int i = 0; i < 6; i++) {        Type current = new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, "primitive");        for (int j = 0; j < 6; j++) {            current = new GroupType(i == j ? Repetition.OPTIONAL : Repetition.REQUIRED, "req" + j, current);        }        MessageType groupSchema = new MessageType("schema" + (i + 12), current);        GroupFactory gf = new SimpleGroupFactory(groupSchema);        List<Group> groups = new ArrayList<Group>();        Group rootDefined = gf.newGroup();        Group rootUndefined = gf.newGroup();        Group currentDefinedGroup = rootDefined;        Group currentUndefinedGroup = rootUndefined;        for (int j = 0; j < 6; j++) {            currentDefinedGroup = currentDefinedGroup.addGroup(0);            if (i < j) {                currentUndefinedGroup = currentUndefinedGroup.addGroup(0);            }        }        currentDefinedGroup.add(0, Binary.fromString("foo"));        groups.add(rootDefined);        groups.add(rootUndefined);        testSchema(groupSchema, groups);    }}
private void parquet-mr_f3743_0(MessageType messageSchema, List<Group> groups)
{    MemPageStore memPageStore = new MemPageStore(groups.size());    ColumnWriteStoreV1 columns = newColumnWriteStore(memPageStore);    ColumnIOFactory columnIOFactory = new ColumnIOFactory(true);    MessageColumnIO columnIO = columnIOFactory.getColumnIO(messageSchema);    log(columnIO);        RecordConsumer recordWriter = columnIO.getRecordWriter(columns);    GroupWriter groupWriter = new GroupWriter(recordWriter, messageSchema);    for (Group group : groups) {        groupWriter.write(group);    }    recordWriter.flush();    columns.flush();        RecordReaderImplementation<Group> recordReader = getRecordReader(columnIO, messageSchema, memPageStore);    for (Group group : groups) {        final Group got = recordReader.read();        assertEquals("deserialization does not display the same result", group.toString(), got.toString());    }}
private RecordReaderImplementation<Group> parquet-mr_f3744_0(MessageColumnIO columnIO, MessageType schema, PageReadStore pageReadStore)
{    RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    return (RecordReaderImplementation<Group>) columnIO.getRecordReader(pageReadStore, recordConverter);}
private void parquet-mr_f3745_1(Object o)
{    }
private void parquet-mr_f3746_0(int[][] expectedFSA, MessageColumnIO columnIO, RecordReaderImplementation<?> recordReader)
{    log("FSA: ----");    List<PrimitiveColumnIO> leaves = columnIO.getLeaves();    for (int i = 0; i < leaves.size(); ++i) {        PrimitiveColumnIO primitiveColumnIO = leaves.get(i);        log(Arrays.toString(primitiveColumnIO.getFieldPath()));        for (int r = 0; r < expectedFSA[i].length; r++) {            int next = expectedFSA[i][r];            log(" " + r + " -> " + (next == leaves.size() ? "end" : Arrays.toString(leaves.get(next).getFieldPath())) + ": " + recordReader.getNextLevel(i, r));            assertEquals(Arrays.toString(primitiveColumnIO.getFieldPath()) + ": " + r + " -> ", next, recordReader.getNextReader(i, r));        }    }    log("----");}
public void parquet-mr_f3747_0()
{    MemPageStore memPageStore = new MemPageStore(1);    ColumnWriteStoreV1 columns = newColumnWriteStore(memPageStore);    MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);    RecordConsumer recordWriter = columnIO.getRecordWriter(columns);    new GroupWriter(recordWriter, schema).write(r1);    recordWriter.flush();    columns.flush();    RecordReader<Void> recordReader = columnIO.getRecordReader(memPageStore, new ExpectationValidatingConverter(expectedEventsForR1, schema));    recordReader.read();}
private ColumnWriteStoreV1 parquet-mr_f3748_0(MemPageStore memPageStore)
{    return new ColumnWriteStoreV1(memPageStore, ParquetProperties.builder().withPageSize(800).withDictionaryPageSize(800).withDictionaryEncoding(useDictionary).build());}
public void parquet-mr_f3749_0()
{    MemPageStore memPageStore = new MemPageStore(1);    ColumnWriteStoreV1 columns = newColumnWriteStore(memPageStore);    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    final RecordConsumer recordWriter = columnIO.getRecordWriter(columns);    recordWriter.startMessage();    recordWriter.startField("DocId", 0);    recordWriter.addLong(0);    recordWriter.endField("DocId", 0);    recordWriter.startField("Links", 1);    try {        recordWriter.endField("Links", 1);        Assert.fail("expected exception because of empty field");    } catch (ParquetEncodingException e) {        Assert.assertEquals("empty fields are illegal, the field should be ommited completely instead", e.getMessage());    }}
public void parquet-mr_f3750_0()
{    List<Group> result = new ArrayList<Group>();    final GroupRecordConverter groupRecordConverter = new GroupRecordConverter(schema);    RecordConsumer groupConsumer = new ConverterConsumer(groupRecordConverter.getRootConverter(), schema);    GroupWriter groupWriter = new GroupWriter(new RecordConsumerLoggingWrapper(groupConsumer), schema);    groupWriter.write(r1);    result.add(groupRecordConverter.getCurrentRecord());    groupWriter.write(r2);    result.add(groupRecordConverter.getCurrentRecord());    assertEquals("deserialization does not display the expected result", result.get(0).toString(), r1.toString());    assertEquals("deserialization does not display the expected result", result.get(1).toString(), r2.toString());}
public void parquet-mr_f3751_0()
{    final String[] expected = { "[DocId]: 10, r:0, d:0", "[Links, Forward]: 20, r:0, d:2", "[Links, Forward]: 40, r:1, d:2", "[Links, Forward]: 60, r:1, d:2", "[Links, Backward]: null, r:0, d:1", "[Name, Language, Code]: en-us, r:0, d:2", "[Name, Language, Country]: us, r:0, d:3", "[Name, Language, Code]: en, r:2, d:2", "[Name, Language, Country]: null, r:2, d:2", "[Name, Url]: http://A, r:0, d:2", "[Name, Url]: http://B, r:1, d:2", "[Name, Language, Code]: null, r:1, d:1", "[Name, Language, Country]: null, r:1, d:1", "[Name, Language, Code]: en-gb, r:1, d:2", "[Name, Language, Country]: gb, r:1, d:3", "[Name, Url]: null, r:1, d:1", "[DocId]: 20, r:0, d:0", "[Links, Backward]: 10, r:0, d:2", "[Links, Backward]: 30, r:1, d:2", "[Links, Forward]: 80, r:0, d:2", "[Name, Url]: http://C, r:0, d:2", "[Name, Language, Code]: null, r:0, d:1", "[Name, Language, Country]: null, r:0, d:1" };    ValidatingColumnWriteStore columns = new ValidatingColumnWriteStore(expected);    MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);    RecordConsumer recordWriter = columnIO.getRecordWriter(columns);    GroupWriter groupWriter = new GroupWriter(recordWriter, schema);    groupWriter.write(r1);    groupWriter.write(r2);    recordWriter.flush();    columns.validate();    columns.flush();    columns.close();}
public void parquet-mr_f3752_0()
{}
public ColumnWriter parquet-mr_f3753_0(final ColumnDescriptor path)
{    return new ColumnWriter() {        private void validate(Object value, int repetitionLevel, int definitionLevel) {            String actual = Arrays.toString(path.getPath()) + ": " + value + ", r:" + repetitionLevel + ", d:" + definitionLevel;            assertEquals("event #" + counter, expected[counter], actual);            ++counter;        }        @Override        public void writeNull(int repetitionLevel, int definitionLevel) {            validate(null, repetitionLevel, definitionLevel);        }        @Override        public void write(Binary value, int repetitionLevel, int definitionLevel) {            validate(value.toStringUsingUTF8(), repetitionLevel, definitionLevel);        }        @Override        public void write(float value, int repetitionLevel, int definitionLevel) {            validate(value, repetitionLevel, definitionLevel);        }        @Override        public void write(boolean value, int repetitionLevel, int definitionLevel) {            validate(value, repetitionLevel, definitionLevel);        }        @Override        public void write(int value, int repetitionLevel, int definitionLevel) {            validate(value, repetitionLevel, definitionLevel);        }        @Override        public void write(long value, int repetitionLevel, int definitionLevel) {            validate(value, repetitionLevel, definitionLevel);        }        @Override        public void close() {        }        @Override        public long getBufferedSizeInMemory() {            throw new UnsupportedOperationException();        }        @Override        public void write(double value, int repetitionLevel, int definitionLevel) {            validate(value, repetitionLevel, definitionLevel);        }    };}
private void parquet-mr_f3754_0(Object value, int repetitionLevel, int definitionLevel)
{    String actual = Arrays.toString(path.getPath()) + ": " + value + ", r:" + repetitionLevel + ", d:" + definitionLevel;    assertEquals("event #" + counter, expected[counter], actual);    ++counter;}
public void parquet-mr_f3755_0(int repetitionLevel, int definitionLevel)
{    validate(null, repetitionLevel, definitionLevel);}
public void parquet-mr_f3756_0(Binary value, int repetitionLevel, int definitionLevel)
{    validate(value.toStringUsingUTF8(), repetitionLevel, definitionLevel);}
public void parquet-mr_f3757_0(float value, int repetitionLevel, int definitionLevel)
{    validate(value, repetitionLevel, definitionLevel);}
public void parquet-mr_f3758_0(boolean value, int repetitionLevel, int definitionLevel)
{    validate(value, repetitionLevel, definitionLevel);}
public void parquet-mr_f3759_0(int value, int repetitionLevel, int definitionLevel)
{    validate(value, repetitionLevel, definitionLevel);}
public void parquet-mr_f3760_0(long value, int repetitionLevel, int definitionLevel)
{    validate(value, repetitionLevel, definitionLevel);}
public void parquet-mr_f3761_0()
{}
public long parquet-mr_f3762_0()
{    throw new UnsupportedOperationException();}
public void parquet-mr_f3763_0(double value, int repetitionLevel, int definitionLevel)
{    validate(value, repetitionLevel, definitionLevel);}
public void parquet-mr_f3764_0()
{    assertEquals("read all events", expected.length, counter);}
public void parquet-mr_f3765_0()
{}
public void parquet-mr_f3766_0()
{}
public long parquet-mr_f3767_0()
{    return 0;}
public long parquet-mr_f3768_0()
{    return 0;}
public String parquet-mr_f3769_0()
{    return null;}
public boolean parquet-mr_f3770_0(long input)
{    return input > 15;}
public boolean parquet-mr_f3771_0(String input)
{    return input.endsWith("A");}
private List<Group> parquet-mr_f3772_0(RecordReader<Group> reader)
{    List<Group> result = new ArrayList<Group>();    Group g;    while ((g = reader.read()) != null) {        result.add(g);    }    return result;}
private void parquet-mr_f3773_0(RecordReader<Group> reader, String message, Group expected)
{    List<Group> result = readAll(reader);    assertEquals(message + ": " + result, 1, result.size());    assertEquals("filtering did not return the correct record", expected.toString(), result.get(0).toString());}
public void parquet-mr_f3774_0()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 1);        RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("DocId", equalTo(10l))));    readOne(recordReader, "r2 filtered out", r1);        recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("DocId", equalTo(20l))));    readOne(recordReader, "r1 filtered out", r2);}
public void parquet-mr_f3775_0()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 1);        RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("DocId", equalTo(10l))));    readOne(recordReader, "r2 filtered out", r1);        recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("DocId", applyFunctionToLong(new LongGreaterThan15Predicate()))));    readOne(recordReader, "r1 filtered out", r2);}
public void parquet-mr_f3776_0()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 1);        RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("Name.Url", equalTo("http://A"))));    readOne(recordReader, "r2 filtered out", r1);            recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("Name.Url", equalTo("http://B"))));    List<Group> all = readAll(recordReader);    assertEquals("There should be no matching records: " + all, 0, all.size());        recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("Name.Url", equalTo("http://C"))));    readOne(recordReader, "r1 filtered out", r2);}
public void parquet-mr_f3777_0()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 1);        RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("Name.Url", applyFunctionToString(new StringEndsWithAPredicate()))));    readOne(recordReader, "r2 filtered out", r1);            recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("Name.Url", equalTo("http://B"))));    List<Group> all = readAll(recordReader);    assertEquals("There should be no matching records: " + all, 0, all.size());        recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("Name.Url", equalTo("http://C"))));    readOne(recordReader, "r1 filtered out", r2);}
public void parquet-mr_f3778_0()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 6);    RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(page(4, 4)));    List<Group> all = readAll(recordReader);    assertEquals("expecting records " + all, 4, all.size());    for (int i = 0; i < all.size(); i++) {        assertEquals("expecting record", (i % 2 == 0 ? r2 : r1).toString(), all.get(i).toString());    }}
public void parquet-mr_f3779_0()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 8);    RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(and(column("DocId", equalTo(10l)), page(2, 4))));    List<Group> all = readAll(recordReader);    assertEquals("expecting 4 records " + all, 4, all.size());    for (int i = 0; i < all.size(); i++) {        assertEquals("expecting record1", r1.toString(), all.get(i).toString());    }}
public void parquet-mr_f3780_0()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 8);    RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(or(column("DocId", equalTo(10l)), column("DocId", equalTo(20l)))));    List<Group> all = readAll(recordReader);    assertEquals("expecting 8 records " + all, 16, all.size());    for (int i = 0; i < all.size() / 2; i++) {        assertEquals("expecting record1", r1.toString(), all.get(2 * i).toString());        assertEquals("expecting record2", r2.toString(), all.get(2 * i + 1).toString());    }}
public void parquet-mr_f3781_0()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 8);    RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(not(column("DocId", equalTo(10l)))));    List<Group> all = readAll(recordReader);    assertEquals("expecting 8 records " + all, 8, all.size());    for (int i = 0; i < all.size(); i++) {        assertEquals("expecting record2", r2.toString(), all.get(i).toString());    }}
private MemPageStore parquet-mr_f3782_0(MessageColumnIO columnIO, int number)
{    MemPageStore memPageStore = new MemPageStore(number * 2);    ColumnWriteStoreV1 columns = new ColumnWriteStoreV1(memPageStore, ParquetProperties.builder().withPageSize(800).withDictionaryEncoding(false).build());    RecordConsumer recordWriter = columnIO.getRecordWriter(columns);    GroupWriter groupWriter = new GroupWriter(recordWriter, schema);    for (int i = 0; i < number; i++) {        groupWriter.write(r1);        groupWriter.write(r2);    }    recordWriter.flush();    columns.flush();    return memPageStore;}
public void parquet-mr_f3783_0()
{    String example = "message Document {\n" + "  required int64 DocId;\n" + "  optional group Links {\n" + "    repeated int64 Backward;\n" + "    repeated int64 Forward; }\n" + "  repeated group Name {\n" + "    repeated group Language {\n" + "      required binary Code;\n" + "      required binary Country; }\n" + "    optional binary Url; }}";    MessageType parsed = parseMessageType(example);    MessageType manuallyMade = new MessageType("Document", new PrimitiveType(REQUIRED, INT64, "DocId"), new GroupType(OPTIONAL, "Links", new PrimitiveType(REPEATED, INT64, "Backward"), new PrimitiveType(REPEATED, INT64, "Forward")), new GroupType(REPEATED, "Name", new GroupType(REPEATED, "Language", new PrimitiveType(REQUIRED, BINARY, "Code"), new PrimitiveType(REQUIRED, BINARY, "Country")), new PrimitiveType(OPTIONAL, BINARY, "Url")));    assertEquals(manuallyMade, parsed);    MessageType parsedThenReparsed = parseMessageType(parsed.toString());    assertEquals(manuallyMade, parsedThenReparsed);    parsed = parseMessageType("message m { required group a {required binary b;} required group c { required int64 d; }}");    manuallyMade = new MessageType("m", new GroupType(REQUIRED, "a", new PrimitiveType(REQUIRED, BINARY, "b")), new GroupType(REQUIRED, "c", new PrimitiveType(REQUIRED, INT64, "d")));    assertEquals(manuallyMade, parsed);    parsedThenReparsed = parseMessageType(parsed.toString());    assertEquals(manuallyMade, parsedThenReparsed);}
public void parquet-mr_f3784_0()
{    MessageTypeBuilder builder = buildMessage();    StringBuilder schema = new StringBuilder();    schema.append("message EachType {\n");    for (PrimitiveTypeName type : PrimitiveTypeName.values()) {                if (type == FIXED_LEN_BYTE_ARRAY) {            schema.append("  required fixed_len_byte_array(3) fixed_;");            builder.required(FIXED_LEN_BYTE_ARRAY).length(3).named("fixed_");        } else {            schema.append("  required ").append(type).append(" ").append(type).append("_;\n");            builder.required(type).named(type.toString() + "_");        }    }    schema.append("}\n");    MessageType expected = builder.named("EachType");    MessageType parsed = parseMessageType(schema.toString());    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
public void parquet-mr_f3785_0()
{    String message = "message StringMessage {\n" + "  required binary string (STRING);\n" + "}\n";    MessageType parsed = parseMessageType(message);    MessageType expected = buildMessage().required(BINARY).as(stringType()).named("string").named("StringMessage");    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
public void parquet-mr_f3786_0()
{    String message = "message StringMessage {\n" + "  required binary string (UTF8);\n" + "}\n";    MessageType parsed = parseMessageType(message);    MessageType expected = buildMessage().required(BINARY).as(UTF8).named("string").named("StringMessage");    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
public void parquet-mr_f3787_0()
{    String message = "message Message {\n" + "  required binary string (UTF8) = 6;\n" + "  required int32 i=1;\n" + "  required binary s2= 3;\n" + "  required binary s3 =4;\n" + "}\n";    MessageType parsed = parseMessageType(message);    MessageType expected = buildMessage().required(BINARY).as(OriginalType.UTF8).id(6).named("string").required(INT32).id(1).named("i").required(BINARY).id(3).named("s2").required(BINARY).id(4).named("s3").named("Message");    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
public void parquet-mr_f3788_0()
{        String message = "message Message {\n" + "  optional group aMap (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      required int32 value;\n" + "    }\n" + "  }\n" + "}\n";    MessageType parsed = parseMessageType(message);    MessageType expected = buildMessage().optionalGroup().as(MAP).repeatedGroup().as(MAP_KEY_VALUE).required(BINARY).as(UTF8).named("key").required(INT32).named("value").named("map").named("aMap").named("Message");    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
public void parquet-mr_f3789_0()
{        String message = "message Message {\n" + "  required group aList (LIST) {\n" + "    repeated binary string (UTF8);\n" + "  }\n" + "}\n";    MessageType parsed = parseMessageType(message);    MessageType expected = buildMessage().requiredGroup().as(LIST).repeated(BINARY).as(UTF8).named("string").named("aList").named("Message");    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
public void parquet-mr_f3790_0()
{    String message = "message DecimalMessage {\n" + "  required FIXED_LEN_BYTE_ARRAY(4) aDecimal (DECIMAL(9,2));\n" + "}\n";    MessageType parsed = parseMessageType(message);    MessageType expected = buildMessage().required(FIXED_LEN_BYTE_ARRAY).length(4).as(DECIMAL).precision(9).scale(2).named("aDecimal").named("DecimalMessage");    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
public void parquet-mr_f3791_0()
{    String message = "message DecimalMessage {\n" + "  required binary aDecimal (DECIMAL(9,2));\n" + "}\n";    MessageType parsed = parseMessageType(message);    MessageType expected = buildMessage().required(BINARY).as(DECIMAL).precision(9).scale(2).named("aDecimal").named("DecimalMessage");    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
public void parquet-mr_f3792_0()
{    String message = "message TimeMessage {" + "  required int32 date (DATE);" + "  required int32 time (TIME_MILLIS);" + "  required int64 timestamp (TIMESTAMP_MILLIS);" + "  required FIXED_LEN_BYTE_ARRAY(12) interval (INTERVAL);" + "  required int32 newTime (TIME(MILLIS,true));" + "  required int64 nanoTime (TIME(NANOS,true));" + "  required int64 newTimestamp (TIMESTAMP(MILLIS,false));" + "  required int64 nanoTimestamp (TIMESTAMP(NANOS,false));" + "}\n";    MessageType parsed = MessageTypeParser.parseMessageType(message);    MessageType expected = Types.buildMessage().required(INT32).as(DATE).named("date").required(INT32).as(TIME_MILLIS).named("time").required(INT64).as(TIMESTAMP_MILLIS).named("timestamp").required(FIXED_LEN_BYTE_ARRAY).length(12).as(INTERVAL).named("interval").required(INT32).as(timeType(true, MILLIS)).named("newTime").required(INT64).as(timeType(true, NANOS)).named("nanoTime").required(INT64).as(timestampType(false, MILLIS)).named("newTimestamp").required(INT64).as(timestampType(false, NANOS)).named("nanoTimestamp").named("TimeMessage");    assertEquals(expected, parsed);    MessageType reparsed = MessageTypeParser.parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
public void parquet-mr_f3793_0()
{    String message = "message IntMessage {" + "  required int32 i8 (INT_8);" + "  required int32 i16 (INT_16);" + "  required int32 i32 (INT_32);" + "  required int64 i64 (INT_64);" + "  required int32 u8 (UINT_8);" + "  required int32 u16 (UINT_16);" + "  required int32 u32 (UINT_32);" + "  required int64 u64 (UINT_64);" + "}\n";    MessageType parsed = MessageTypeParser.parseMessageType(message);    MessageType expected = Types.buildMessage().required(INT32).as(INT_8).named("i8").required(INT32).as(INT_16).named("i16").required(INT32).as(INT_32).named("i32").required(INT64).as(INT_64).named("i64").required(INT32).as(UINT_8).named("u8").required(INT32).as(UINT_16).named("u16").required(INT32).as(UINT_32).named("u32").required(INT64).as(UINT_64).named("u64").named("IntMessage");    assertEquals(expected, parsed);    MessageType reparsed = MessageTypeParser.parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
public void parquet-mr_f3794_0()
{    String message = "message IntMessage {" + "  required int32 i8 (INTEGER(8,true));" + "  required int32 i16 (INTEGER(16,true));" + "  required int32 i32 (INTEGER(32,true));" + "  required int64 i64 (INTEGER(64,true));" + "  required int32 u8 (INTEGER(8,false));" + "  required int32 u16 (INTEGER(16,false));" + "  required int32 u32 (INTEGER(32,false));" + "  required int64 u64 (INTEGER(64,false));" + "}\n";    MessageType parsed = MessageTypeParser.parseMessageType(message);    MessageType expected = Types.buildMessage().required(INT32).as(intType(8, true)).named("i8").required(INT32).as(intType(16, true)).named("i16").required(INT32).as(intType(32, true)).named("i32").required(INT64).as(intType(64, true)).named("i64").required(INT32).as(intType(8, false)).named("u8").required(INT32).as(intType(16, false)).named("u16").required(INT32).as(intType(32, false)).named("u32").required(INT64).as(intType(64, false)).named("u64").named("IntMessage");    assertEquals(expected, parsed);    MessageType reparsed = MessageTypeParser.parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
public void parquet-mr_f3795_0()
{    String message = "message EmbeddedMessage {" + "  required binary json (JSON);" + "  required binary bson (BSON);" + "}\n";    MessageType parsed = MessageTypeParser.parseMessageType(message);    MessageType expected = Types.buildMessage().required(BINARY).as(JSON).named("json").required(BINARY).as(BSON).named("bson").named("EmbeddedMessage");    assertEquals(expected, parsed);    MessageType reparsed = MessageTypeParser.parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
public void parquet-mr_f3796_0() throws Exception
{    MessageType schema = MessageTypeParser.parseMessageType(Paper.schema.toString());    assertEquals(Paper.schema, schema);    assertEquals(schema.toString(), Paper.schema.toString());}
public void parquet-mr_f3797_0()
{    MessageType schema = MessageTypeParser.parseMessageType(Paper.schema.toString());    Type type = schema.getType("Links", "Backward");    assertEquals(PrimitiveTypeName.INT64, type.asPrimitiveType().getPrimitiveTypeName());    assertEquals(0, schema.getMaxRepetitionLevel("DocId"));    assertEquals(1, schema.getMaxRepetitionLevel("Name"));    assertEquals(2, schema.getMaxRepetitionLevel("Name", "Language"));    assertEquals(0, schema.getMaxDefinitionLevel("DocId"));    assertEquals(1, schema.getMaxDefinitionLevel("Links"));    assertEquals(2, schema.getMaxDefinitionLevel("Links", "Backward"));}
public void parquet-mr_f3798_0()
{    MessageType t1 = new MessageType("root1", new PrimitiveType(REPEATED, BINARY, "a"), new PrimitiveType(OPTIONAL, BINARY, "b"));    MessageType t2 = new MessageType("root2", new PrimitiveType(REQUIRED, BINARY, "c"));    assertEquals(t1.union(t2), new MessageType("root1", new PrimitiveType(REPEATED, BINARY, "a"), new PrimitiveType(OPTIONAL, BINARY, "b"), new PrimitiveType(REQUIRED, BINARY, "c")));    assertEquals(t2.union(t1), new MessageType("root2", new PrimitiveType(REQUIRED, BINARY, "c"), new PrimitiveType(REPEATED, BINARY, "a"), new PrimitiveType(OPTIONAL, BINARY, "b")));    MessageType t3 = new MessageType("root1", new PrimitiveType(OPTIONAL, BINARY, "a"));    MessageType t4 = new MessageType("root2", new PrimitiveType(REQUIRED, BINARY, "a"));    assertEquals(t3.union(t4), new MessageType("root1", new PrimitiveType(OPTIONAL, BINARY, "a")));    assertEquals(t4.union(t3), new MessageType("root2", new PrimitiveType(OPTIONAL, BINARY, "a")));    MessageType t5 = new MessageType("root1", new GroupType(REQUIRED, "g1", new PrimitiveType(OPTIONAL, BINARY, "a")), new GroupType(REQUIRED, "g2", new PrimitiveType(OPTIONAL, BINARY, "b")));    MessageType t6 = new MessageType("root1", new GroupType(REQUIRED, "g1", new PrimitiveType(OPTIONAL, BINARY, "a")), new GroupType(REQUIRED, "g2", new GroupType(REQUIRED, "g3", new PrimitiveType(OPTIONAL, BINARY, "c")), new PrimitiveType(OPTIONAL, BINARY, "b")));    assertEquals(t5.union(t6), new MessageType("root1", new GroupType(REQUIRED, "g1", new PrimitiveType(OPTIONAL, BINARY, "a")), new GroupType(REQUIRED, "g2", new PrimitiveType(OPTIONAL, BINARY, "b"), new GroupType(REQUIRED, "g3", new PrimitiveType(OPTIONAL, BINARY, "c")))));    MessageType t7 = new MessageType("root1", new PrimitiveType(OPTIONAL, BINARY, "a"));    MessageType t8 = new MessageType("root2", new PrimitiveType(OPTIONAL, INT32, "a"));    try {        t7.union(t8);        fail("moving from BINARY to INT32");    } catch (IncompatibleSchemaModificationException e) {        assertEquals("can not merge type optional int32 a into optional binary a", e.getMessage());    }    MessageType t9 = Types.buildMessage().addField(Types.optional(BINARY).as(OriginalType.UTF8).named("a")).named("root1");    MessageType t10 = Types.buildMessage().addField(Types.optional(BINARY).named("a")).named("root1");    assertEquals(t9.union(t9), t9);    try {        t9.union(t10);        fail("moving from BINARY (UTF8) to BINARY");    } catch (IncompatibleSchemaModificationException e) {        assertEquals("cannot merge logical type null into STRING", e.getMessage());    }    MessageType t11 = Types.buildMessage().addField(Types.optional(FIXED_LEN_BYTE_ARRAY).length(10).named("a")).named("root1");    MessageType t12 = Types.buildMessage().addField(Types.optional(FIXED_LEN_BYTE_ARRAY).length(20).named("a")).named("root2");    try {        t11.union(t12);        fail("moving from FIXED_LEN_BYTE_ARRAY(10) to FIXED_LEN_BYTE_ARRAY(20)");    } catch (IncompatibleSchemaModificationException e) {        assertEquals("can not merge type optional fixed_len_byte_array(20) a into optional fixed_len_byte_array(10) a", e.getMessage());    }}
public void parquet-mr_f3799_0() throws Exception
{    MessageType t5 = new MessageType("root1", new GroupType(REQUIRED, "g1", LIST, new PrimitiveType(OPTIONAL, BINARY, "a")), new GroupType(REQUIRED, "g2", new PrimitiveType(OPTIONAL, BINARY, "b")));    MessageType t6 = new MessageType("root1", new GroupType(REQUIRED, "g1", LIST, new PrimitiveType(OPTIONAL, BINARY, "a")), new GroupType(REQUIRED, "g2", LIST, new GroupType(REQUIRED, "g3", new PrimitiveType(OPTIONAL, BINARY, "c")), new PrimitiveType(OPTIONAL, BINARY, "b")));    assertEquals(new MessageType("root1", new GroupType(REQUIRED, "g1", LIST, new PrimitiveType(OPTIONAL, BINARY, "a")), new GroupType(REQUIRED, "g2", LIST, new PrimitiveType(OPTIONAL, BINARY, "b"), new GroupType(REQUIRED, "g3", new PrimitiveType(OPTIONAL, BINARY, "c")))), t5.union(t6));}
public void parquet-mr_f3800_0()
{    MessageType m1 = Types.buildMessage().addFields(Types.requiredList().element(Types.optional(BINARY).columnOrder(ColumnOrder.undefined()).named("a")).named("g"), Types.optional(INT96).named("b")).named("root");    MessageType m2 = Types.buildMessage().addFields(Types.requiredList().element(Types.optional(BINARY).columnOrder(ColumnOrder.undefined()).named("a")).named("g"), Types.optional(BINARY).named("c")).named("root");    MessageType m3 = Types.buildMessage().addFields(Types.requiredList().element(Types.optional(BINARY).named("a")).named("g")).named("root");    assertEquals(Types.buildMessage().addFields(Types.requiredList().element(Types.optional(BINARY).named("a")).named("g"), Types.optional(INT96).named("b"), Types.optional(BINARY).named("c")).named("root"), m1.union(m2));    try {        m1.union(m3);        fail("An IncompatibleSchemaModificationException should have been thrown");    } catch (Exception e) {        assertTrue("The thrown exception should have been IncompatibleSchemaModificationException but was " + e.getClass(), e instanceof IncompatibleSchemaModificationException);        assertEquals("can not merge type optional binary a with column order TYPE_DEFINED_ORDER into optional binary a with column order UNDEFINED", e.getMessage());    }}
public void parquet-mr_f3801_0() throws Exception
{    MessageType schema = new MessageType("test", new PrimitiveType(REQUIRED, BINARY, "foo").withId(4), new GroupType(REQUIRED, "bar", new PrimitiveType(REQUIRED, BINARY, "baz").withId(3)).withId(8));    MessageType schema2 = MessageTypeParser.parseMessageType(schema.toString());    assertEquals(schema, schema2);    assertEquals(schema.toString(), schema2.toString());}
public void parquet-mr_f3802_0()
{    Boolean[] valuesInAscendingOrder = { null, false, true };    for (int i = 0; i < valuesInAscendingOrder.length; ++i) {        for (int j = 0; j < valuesInAscendingOrder.length; ++j) {            Boolean vi = valuesInAscendingOrder[i];            Boolean vj = valuesInAscendingOrder[j];            int exp = i - j;            assertSignumEquals(vi, vj, exp, BOOLEAN_COMPARATOR.compare(vi, vj));            if (vi != null && vj != null) {                assertSignumEquals(vi, vj, exp, BOOLEAN_COMPARATOR.compare(vi.booleanValue(), vj.booleanValue()));            }        }    }    checkThrowingUnsupportedException(BOOLEAN_COMPARATOR, Boolean.TYPE);}
public void parquet-mr_f3803_0()
{    testInt32Comparator(SIGNED_INT32_COMPARATOR, null, Integer.MIN_VALUE, -12345, -1, 0, 1, 12345, Integer.MAX_VALUE);}
public void parquet-mr_f3804_0()
{    testInt32Comparator(UNSIGNED_INT32_COMPARATOR, null,     0,     1,     12345,     Integer.MAX_VALUE,     Integer.MIN_VALUE,     -12345,     -1);}
private void parquet-mr_f3805_0(PrimitiveComparator<Integer> comparator, Integer... valuesInAscendingOrder)
{    for (int i = 0; i < valuesInAscendingOrder.length; ++i) {        for (int j = 0; j < valuesInAscendingOrder.length; ++j) {            Integer vi = valuesInAscendingOrder[i];            Integer vj = valuesInAscendingOrder[j];            int exp = i - j;            assertSignumEquals(vi, vj, exp, comparator.compare(vi, vj));            if (vi != null && vj != null) {                assertSignumEquals(vi, vj, exp, comparator.compare(vi.intValue(), vj.intValue()));            }        }    }    checkThrowingUnsupportedException(comparator, Integer.TYPE);}
public void parquet-mr_f3806_0()
{    testInt64Comparator(SIGNED_INT64_COMPARATOR, null, Long.MIN_VALUE, -12345678901L, -1L, 0L, 1L, 12345678901L, Long.MAX_VALUE);}
public void parquet-mr_f3807_0()
{    testInt64Comparator(UNSIGNED_INT64_COMPARATOR, null,     0L,     1L,     12345678901L,     Long.MAX_VALUE,     Long.MIN_VALUE,     -12345678901L,     -1L);}
private void parquet-mr_f3808_0(PrimitiveComparator<Long> comparator, Long... valuesInAscendingOrder)
{    for (int i = 0; i < valuesInAscendingOrder.length; ++i) {        for (int j = 0; j < valuesInAscendingOrder.length; ++j) {            Long vi = valuesInAscendingOrder[i];            Long vj = valuesInAscendingOrder[j];            int exp = i - j;            assertSignumEquals(vi, vj, exp, comparator.compare(vi, vj));            if (vi != null && vj != null) {                assertSignumEquals(vi, vj, exp, comparator.compare(vi.longValue(), vj.longValue()));            }        }    }    checkThrowingUnsupportedException(comparator, Long.TYPE);}
public void parquet-mr_f3809_0()
{    Float[] valuesInAscendingOrder = { null, Float.NEGATIVE_INFINITY, -Float.MAX_VALUE, -1234.5678F, -Float.MIN_VALUE, 0.0F, Float.MIN_VALUE, 1234.5678F, Float.MAX_VALUE, Float.POSITIVE_INFINITY };    for (int i = 0; i < valuesInAscendingOrder.length; ++i) {        for (int j = 0; j < valuesInAscendingOrder.length; ++j) {            Float vi = valuesInAscendingOrder[i];            Float vj = valuesInAscendingOrder[j];            int exp = i - j;            assertSignumEquals(vi, vj, exp, FLOAT_COMPARATOR.compare(vi, vj));            if (vi != null && vj != null) {                assertSignumEquals(vi, vj, exp, FLOAT_COMPARATOR.compare(vi.floatValue(), vj.floatValue()));            }        }    }    checkThrowingUnsupportedException(FLOAT_COMPARATOR, Float.TYPE);}
public void parquet-mr_f3810_0()
{    Double[] valuesInAscendingOrder = { null, Double.NEGATIVE_INFINITY, -Double.MAX_VALUE, -123456.7890123456789, -Double.MIN_VALUE, 0.0, Double.MIN_VALUE, 123456.7890123456789, Double.MAX_VALUE, Double.POSITIVE_INFINITY };    for (int i = 0; i < valuesInAscendingOrder.length; ++i) {        for (int j = 0; j < valuesInAscendingOrder.length; ++j) {            Double vi = valuesInAscendingOrder[i];            Double vj = valuesInAscendingOrder[j];            int exp = i - j;            assertSignumEquals(vi, vj, exp, DOUBLE_COMPARATOR.compare(vi, vj));            if (vi != null && vj != null) {                assertSignumEquals(vi, vj, exp, DOUBLE_COMPARATOR.compare(vi.doubleValue(), vj.doubleValue()));            }        }    }    checkThrowingUnsupportedException(DOUBLE_COMPARATOR, Double.TYPE);}
public void parquet-mr_f3811_0()
{    testObjectComparator(UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR, null,     Binary.fromConstantByteArray(new byte[0]),     Binary.fromConstantByteArray(new byte[] { 127, 127, 0, 127 }, 2, 1),     Binary.fromCharSequence("aaa"),     Binary.fromString("aaaa"),     Binary.fromReusedByteArray("aaab".getBytes()),     Binary.fromReusedByteArray("azzza".getBytes(), 1, 3),     Binary.fromReusedByteBuffer(ByteBuffer.wrap("zzzzzz".getBytes())),     Binary.fromReusedByteBuffer(ByteBuffer.wrap("aazzzzzzaa".getBytes(), 2, 7)),     Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { -128, -128, -128 })),     Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { -128, -128, -1 }, 1, 2)));}
public void parquet-mr_f3812_0()
{    testObjectComparator(BINARY_AS_SIGNED_INTEGER_COMPARATOR, null, Binary.fromConstantByteArray(new BigInteger("-9999999999999999999999999999999999999999").toByteArray()), Binary.fromReusedByteArray(new BigInteger("-9999999999999999999999999999999999999998").toByteArray()), Binary.fromConstantByteArray(BigInteger.valueOf(Long.MIN_VALUE).subtract(BigInteger.ONE).toByteArray()), Binary.fromConstantByteArray(BigInteger.valueOf(Long.MIN_VALUE).toByteArray()), Binary.fromConstantByteArray(BigInteger.valueOf(Long.MIN_VALUE).add(BigInteger.ONE).toByteArray()), Binary.fromReusedByteArray(new byte[] { (byte) 0xFF, (byte) 0xFF, (byte) 0xFF, -2 }, 1, 3), Binary.fromReusedByteArray(new BigInteger("-1").toByteArray()), Binary.fromConstantByteBuffer(ByteBuffer.wrap(new BigInteger("0").toByteArray())), Binary.fromReusedByteBuffer(ByteBuffer.wrap(new byte[] { 0, 0, 0, 1 })), Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { 0, 0, 0, 2 }), 2, 2), Binary.fromConstantByteBuffer(ByteBuffer.wrap(BigInteger.valueOf(Long.MAX_VALUE).subtract(BigInteger.ONE).toByteArray())), Binary.fromConstantByteBuffer(ByteBuffer.wrap(BigInteger.valueOf(Long.MAX_VALUE).toByteArray())), Binary.fromConstantByteBuffer(ByteBuffer.wrap(BigInteger.valueOf(Long.MAX_VALUE).add(BigInteger.ONE).toByteArray())), Binary.fromConstantByteBuffer(ByteBuffer.wrap(new BigInteger("999999999999999999999999999999999999999").toByteArray())), Binary.fromReusedByteBuffer(ByteBuffer.wrap(new BigInteger("9999999999999999999999999999999999999998").toByteArray())), Binary.fromConstantByteBuffer(ByteBuffer.wrap(new BigInteger("9999999999999999999999999999999999999999").toByteArray())));}
public void parquet-mr_f3813_0()
{    List<Binary> valuesToCompare = new ArrayList<>();    valuesToCompare.add(Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { 0, 0, -108 })));    valuesToCompare.add(Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { 0, 0, 0, 0, 0, -108 })));    valuesToCompare.add(Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { 0, 0, 0, -108 })));    valuesToCompare.add(Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { 0, 0, 0, 0, -108 })));    valuesToCompare.add(Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { 0, -108 })));    for (Binary v1 : valuesToCompare) {        for (Binary v2 : valuesToCompare) {            assertEquals(String.format("Wrong result of comparison %s and %s", v1, v2), 0, BINARY_AS_SIGNED_INTEGER_COMPARATOR.compare(v1, v2));        }    }}
private void parquet-mr_f3814_0(PrimitiveComparator<T> comparator, T... valuesInAscendingOrder)
{    for (int i = 0; i < valuesInAscendingOrder.length; ++i) {        for (int j = 0; j < valuesInAscendingOrder.length; ++j) {            T vi = valuesInAscendingOrder[i];            T vj = valuesInAscendingOrder[j];            int exp = i - j;            assertSignumEquals(vi, vj, exp, comparator.compare(vi, vj));        }    }    checkThrowingUnsupportedException(comparator, null);}
private void parquet-mr_f3815_0(T v1, T v2, int expected, int actual)
{    String sign = expected < 0 ? " < " : expected > 0 ? " > " : " = ";    assertEquals("expected: " + v1 + sign + v2, signum(expected), signum(actual));}
private int parquet-mr_f3816_0(int i)
{    return i < 0 ? -1 : i > 0 ? 1 : 0;}
private void parquet-mr_f3817_0(PrimitiveComparator<?> comparator, Class<?> exclude)
{    if (Integer.TYPE != exclude) {        try {            comparator.compare(0, 0);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (Long.TYPE != exclude) {        try {            comparator.compare(0L, 0L);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (Float.TYPE != exclude) {        try {            comparator.compare(0.0F, 0.0F);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (Double.TYPE != exclude) {        try {            comparator.compare(0.0, 0.0);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (Boolean.TYPE != exclude) {        try {            comparator.compare(false, false);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }}
public void parquet-mr_f3818_0()
{    PrimitiveStringifier stringifier = DEFAULT_STRINGIFIER;    assertEquals("true", stringifier.stringify(true));    assertEquals("false", stringifier.stringify(false));    assertEquals("0.0", stringifier.stringify(0.0));    assertEquals("123456.7891234567", stringifier.stringify(123456.7891234567));    assertEquals("-98765.43219876543", stringifier.stringify(-98765.43219876543));    assertEquals("0.0", stringifier.stringify(0.0f));    assertEquals("987.6543", stringifier.stringify(987.6543f));    assertEquals("-123.4567", stringifier.stringify(-123.4567f));    assertEquals("0", stringifier.stringify(0));    assertEquals("1234567890", stringifier.stringify(1234567890));    assertEquals("-987654321", stringifier.stringify(-987654321));    assertEquals("0", stringifier.stringify(0l));    assertEquals("1234567890123456789", stringifier.stringify(1234567890123456789l));    assertEquals("-987654321987654321", stringifier.stringify(-987654321987654321l));    assertEquals("null", stringifier.stringify(null));    assertEquals("0x", stringifier.stringify(Binary.EMPTY));    assertEquals("0x0123456789ABCDEF", stringifier.stringify(Binary.fromConstantByteArray(new byte[] { 0x01, 0x23, 0x45, 0x67, (byte) 0x89, (byte) 0xAB, (byte) 0xCD, (byte) 0xEF })));}
public void parquet-mr_f3819_0()
{    PrimitiveStringifier stringifier = UNSIGNED_STRINGIFIER;    assertEquals("0", stringifier.stringify(0));    assertEquals("2147483647", stringifier.stringify(2147483647));    assertEquals("4294967295", stringifier.stringify(0xFFFFFFFF));    assertEquals("0", stringifier.stringify(0l));    assertEquals("9223372036854775807", stringifier.stringify(9223372036854775807l));    assertEquals("18446744073709551615", stringifier.stringify(0xFFFFFFFFFFFFFFFFl));    checkThrowingUnsupportedException(stringifier, Integer.TYPE, Long.TYPE);}
public void parquet-mr_f3820_0()
{    PrimitiveStringifier stringifier = UTF8_STRINGIFIER;    assertEquals("null", stringifier.stringify(null));    assertEquals("", stringifier.stringify(Binary.EMPTY));    assertEquals("This is a UTF-8 test", stringifier.stringify(Binary.fromString("This is a UTF-8 test")));    assertEquals("これはUTF-8のテストです", stringifier.stringify(Binary.fromConstantByteArray("これはUTF-8のテストです".getBytes(UTF_8))));    checkThrowingUnsupportedException(stringifier, Binary.class);}
public void parquet-mr_f3821_0()
{    PrimitiveStringifier stringifier = INTERVAL_STRINGIFIER;    assertEquals("null", stringifier.stringify(null));    assertEquals("<INVALID>", stringifier.stringify(Binary.EMPTY));    assertEquals("<INVALID>", stringifier.stringify(Binary.fromConstantByteArray(new byte[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 })));    assertEquals("<INVALID>", stringifier.stringify(Binary.fromReusedByteArray(new byte[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13 })));    ByteBuffer buffer = ByteBuffer.allocate(12);    assertEquals("interval(0 months, 0 days, 0 millis)", stringifier.stringify(Binary.fromConstantByteBuffer(buffer)));    buffer.putInt(0x03000000);    buffer.putInt(0x06000000);    buffer.putInt(0x09000000);    buffer.flip();    assertEquals("interval(3 months, 6 days, 9 millis)", stringifier.stringify(Binary.fromConstantByteBuffer(buffer)));    buffer.clear();    buffer.putInt(0xFFFFFFFF);    buffer.putInt(0xFEFFFFFF);    buffer.putInt(0xFDFFFFFF);    buffer.flip();    assertEquals("interval(4294967295 months, 4294967294 days, 4294967293 millis)", stringifier.stringify(Binary.fromReusedByteBuffer(buffer)));    checkThrowingUnsupportedException(stringifier, Binary.class);}
public void parquet-mr_f3822_0()
{    PrimitiveStringifier stringifier = DATE_STRINGIFIER;    assertEquals("1970-01-01", stringifier.stringify(0));    Calendar cal = Calendar.getInstance(UTC);    cal.clear();    cal.set(2017, Calendar.DECEMBER, 14);    assertEquals("2017-12-14", stringifier.stringify((int) MILLISECONDS.toDays(cal.getTimeInMillis())));    cal.clear();    cal.set(1583, Calendar.AUGUST, 3);    assertEquals("1583-08-03", stringifier.stringify((int) MILLISECONDS.toDays(cal.getTimeInMillis())));    checkThrowingUnsupportedException(stringifier, Integer.TYPE);}
public void parquet-mr_f3823_0()
{    for (PrimitiveStringifier stringifier : asList(TIMESTAMP_MILLIS_STRINGIFIER, TIMESTAMP_MILLIS_UTC_STRINGIFIER)) {        String timezoneAmendment = (stringifier == TIMESTAMP_MILLIS_STRINGIFIER ? "" : "+0000");        assertEquals(withZoneString("1970-01-01T00:00:00.000", timezoneAmendment), stringifier.stringify(0l));        Calendar cal = Calendar.getInstance(UTC);        cal.clear();        cal.set(2017, Calendar.DECEMBER, 15, 10, 9, 54);        cal.set(Calendar.MILLISECOND, 120);        assertEquals(withZoneString("2017-12-15T10:09:54.120", timezoneAmendment), stringifier.stringify(cal.getTimeInMillis()));        cal.clear();        cal.set(1948, Calendar.NOVEMBER, 23, 20, 19, 1);        cal.set(Calendar.MILLISECOND, 9);        assertEquals(withZoneString("1948-11-23T20:19:01.009", timezoneAmendment), stringifier.stringify(cal.getTimeInMillis()));        checkThrowingUnsupportedException(stringifier, Long.TYPE);    }}
public void parquet-mr_f3824_0()
{    for (PrimitiveStringifier stringifier : asList(TIMESTAMP_MICROS_STRINGIFIER, TIMESTAMP_MICROS_UTC_STRINGIFIER)) {        String timezoneAmendment = (stringifier == TIMESTAMP_MICROS_STRINGIFIER ? "" : "+0000");        assertEquals(withZoneString("1970-01-01T00:00:00.000000", timezoneAmendment), stringifier.stringify(0l));        Calendar cal = Calendar.getInstance(UTC);        cal.clear();        cal.set(2053, Calendar.JULY, 10, 22, 13, 24);        cal.set(Calendar.MILLISECOND, 84);        long micros = cal.getTimeInMillis() * 1000 + 900;        assertEquals(withZoneString("2053-07-10T22:13:24.084900", timezoneAmendment), stringifier.stringify(micros));        cal.clear();        cal.set(1848, Calendar.MARCH, 15, 9, 23, 59);        cal.set(Calendar.MILLISECOND, 765);        micros = cal.getTimeInMillis() * 1000 - 1;        assertEquals(withZoneString("1848-03-15T09:23:59.764999", timezoneAmendment), stringifier.stringify(micros));        checkThrowingUnsupportedException(stringifier, Long.TYPE);    }}
public void parquet-mr_f3825_0()
{    for (PrimitiveStringifier stringifier : asList(TIMESTAMP_NANOS_STRINGIFIER, TIMESTAMP_NANOS_UTC_STRINGIFIER)) {        String timezoneAmendment = (stringifier == TIMESTAMP_NANOS_STRINGIFIER ? "" : "+0000");        assertEquals(withZoneString("1970-01-01T00:00:00.000000000", timezoneAmendment), stringifier.stringify(0l));        Calendar cal = Calendar.getInstance(UTC);        cal.clear();        cal.set(2053, Calendar.JULY, 10, 22, 13, 24);        cal.set(Calendar.MILLISECOND, 84);        long nanos = cal.getTimeInMillis() * 1_000_000 + 536;        assertEquals(withZoneString("2053-07-10T22:13:24.084000536", timezoneAmendment), stringifier.stringify(nanos));        cal.clear();        cal.set(1848, Calendar.MARCH, 15, 9, 23, 59);        cal.set(Calendar.MILLISECOND, 765);        nanos = cal.getTimeInMillis() * 1_000_000 - 1;        assertEquals(withZoneString("1848-03-15T09:23:59.764999999", timezoneAmendment), stringifier.stringify(nanos));        checkThrowingUnsupportedException(stringifier, Long.TYPE);    }}
public void parquet-mr_f3826_0()
{    for (PrimitiveStringifier stringifier : asList(TIME_STRINGIFIER, TIME_UTC_STRINGIFIER)) {        String timezoneAmendment = (stringifier == TIME_STRINGIFIER ? "" : "+0000");        assertEquals(withZoneString("00:00:00.000", timezoneAmendment), stringifier.stringify(0));        assertEquals(withZoneString("00:00:00.000000", timezoneAmendment), stringifier.stringify(0l));        assertEquals(withZoneString("12:34:56.789", timezoneAmendment), stringifier.stringify((int) convert(MILLISECONDS, 12, 34, 56, 789)));        assertEquals(withZoneString("12:34:56.789012", timezoneAmendment), stringifier.stringify(convert(MICROSECONDS, 12, 34, 56, 789012)));        assertEquals(withZoneString("-12:34:56.789", timezoneAmendment), stringifier.stringify((int) convert(MILLISECONDS, -12, -34, -56, -789)));        assertEquals(withZoneString("-12:34:56.789012", timezoneAmendment), stringifier.stringify(convert(MICROSECONDS, -12, -34, -56, -789012)));        assertEquals(withZoneString("123:12:34.567", timezoneAmendment), stringifier.stringify((int) convert(MILLISECONDS, 123, 12, 34, 567)));        assertEquals(withZoneString("12345:12:34.056789", timezoneAmendment), stringifier.stringify(convert(MICROSECONDS, 12345, 12, 34, 56789)));        assertEquals(withZoneString("-123:12:34.567", timezoneAmendment), stringifier.stringify((int) convert(MILLISECONDS, -123, -12, -34, -567)));        assertEquals(withZoneString("-12345:12:34.056789", timezoneAmendment), stringifier.stringify(convert(MICROSECONDS, -12345, -12, -34, -56789)));        checkThrowingUnsupportedException(stringifier, Integer.TYPE, Long.TYPE);    }}
public void parquet-mr_f3827_0()
{    for (PrimitiveStringifier stringifier : asList(TIME_NANOS_STRINGIFIER, TIME_NANOS_UTC_STRINGIFIER)) {        String timezoneAmendment = (stringifier == TIME_NANOS_STRINGIFIER ? "" : "+0000");        assertEquals(withZoneString("00:00:00.000000000", timezoneAmendment), stringifier.stringify(0l));        assertEquals(withZoneString("12:34:56.789012987", timezoneAmendment), stringifier.stringify(convert(NANOSECONDS, 12, 34, 56, 789012987)));        assertEquals(withZoneString("-12:34:56.000789012", timezoneAmendment), stringifier.stringify(convert(NANOSECONDS, -12, -34, -56, -789012)));        assertEquals(withZoneString("12345:12:34.000056789", timezoneAmendment), stringifier.stringify(convert(NANOSECONDS, 12345, 12, 34, 56789)));        assertEquals(withZoneString("-12345:12:34.000056789", timezoneAmendment), stringifier.stringify(convert(NANOSECONDS, -12345, -12, -34, -56789)));        checkThrowingUnsupportedException(stringifier, Integer.TYPE, Long.TYPE);    }}
private String parquet-mr_f3828_0(String expected, String zoneString)
{    return expected + zoneString;}
private long parquet-mr_f3829_0(TimeUnit unit, long hours, long minutes, long seconds, long rest)
{    return unit.convert(hours, HOURS) + unit.convert(minutes, MINUTES) + unit.convert(seconds, SECONDS) + rest;}
public void parquet-mr_f3830_0()
{    PrimitiveStringifier stringifier = PrimitiveStringifier.createDecimalStringifier(4);    assertEquals("0.0000", stringifier.stringify(0));    assertEquals("123456.7890", stringifier.stringify(1234567890));    assertEquals("-98765.4321", stringifier.stringify(-987654321));    assertEquals("0.0000", stringifier.stringify(0l));    assertEquals("123456789012345.6789", stringifier.stringify(1234567890123456789l));    assertEquals("-98765432109876.5432", stringifier.stringify(-987654321098765432l));    assertEquals("null", stringifier.stringify(null));    assertEquals("<INVALID>", stringifier.stringify(Binary.EMPTY));    assertEquals("0.0000", stringifier.stringify(Binary.fromReusedByteArray(new byte[] { 0 })));    assertEquals("9876543210987654321098765432109876543210987654.3210", stringifier.stringify(Binary.fromConstantByteArray(new BigInteger("98765432109876543210987654321098765432109876543210").toByteArray())));    assertEquals("-1234567890123456789012345678901234567890123456.7890", stringifier.stringify(Binary.fromConstantByteArray(new BigInteger("-12345678901234567890123456789012345678901234567890").toByteArray())));    checkThrowingUnsupportedException(stringifier, Integer.TYPE, Long.TYPE, Binary.class);}
private void parquet-mr_f3831_0(PrimitiveStringifier stringifier, Class<?>... excludes)
{    Set<Class<?>> set = new HashSet<>(asList(excludes));    if (!set.contains(Integer.TYPE)) {        try {            stringifier.stringify(0);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (!set.contains(Long.TYPE)) {        try {            stringifier.stringify(0l);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (!set.contains(Float.TYPE)) {        try {            stringifier.stringify(0.0f);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (!set.contains(Double.TYPE)) {        try {            stringifier.stringify(0.0);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (!set.contains(Boolean.TYPE)) {        try {            stringifier.stringify(false);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (!set.contains(Binary.class)) {        try {            stringifier.stringify(Binary.EMPTY);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }}
public void parquet-mr_f3832_0()
{    Type.Repetition REQUIRED = Type.Repetition.REQUIRED;    Type.Repetition OPTIONAL = Type.Repetition.OPTIONAL;    Type.Repetition REPEATED = Type.Repetition.REPEATED;    assertEquals(REPEATED, Type.Repetition.leastRestrictive(REQUIRED, OPTIONAL, REPEATED, REQUIRED, OPTIONAL, REPEATED));    assertEquals(OPTIONAL, Type.Repetition.leastRestrictive(REQUIRED, OPTIONAL, REQUIRED, OPTIONAL));    assertEquals(REQUIRED, Type.Repetition.leastRestrictive(REQUIRED, REQUIRED));}
public void parquet-mr_f3833_0()
{    MessageType expected = new MessageType("Document", new PrimitiveType(REQUIRED, INT64, "DocId"), new GroupType(OPTIONAL, "Links", new PrimitiveType(REPEATED, INT64, "Backward"), new PrimitiveType(REPEATED, INT64, "Forward")), new GroupType(REPEATED, "Name", new GroupType(REPEATED, "Language", new PrimitiveType(REQUIRED, BINARY, "Code"), new PrimitiveType(REQUIRED, BINARY, "Country")), new PrimitiveType(OPTIONAL, BINARY, "Url")));    MessageType builderType = Types.buildMessage().required(INT64).named("DocId").optionalGroup().repeated(INT64).named("Backward").repeated(INT64).named("Forward").named("Links").repeatedGroup().repeatedGroup().required(BINARY).named("Code").required(BINARY).named("Country").named("Language").optional(BINARY).named("Url").named("Name").named("Document");    Assert.assertEquals(expected, builderType);}
public void parquet-mr_f3834_0()
{    PrimitiveType f1 = Types.required(BINARY).as(UTF8).named("f1");    PrimitiveType f2 = Types.required(INT32).named("f2");    PrimitiveType f3 = Types.optional(INT32).named("f3");    String name = "group";    for (Type.Repetition repetition : Type.Repetition.values()) {        GroupType expected = new GroupType(repetition, name, f1, new GroupType(repetition, "g1", f2, f3));        GroupType built = Types.buildGroup(repetition).addField(f1).group(repetition).addFields(f2, f3).named("g1").named(name);        Assert.assertEquals(expected, built);        switch(repetition) {            case REQUIRED:                built = Types.requiredGroup().addField(f1).requiredGroup().addFields(f2, f3).named("g1").named(name);                break;            case OPTIONAL:                built = Types.optionalGroup().addField(f1).optionalGroup().addFields(f2, f3).named("g1").named(name);                break;            case REPEATED:                built = Types.repeatedGroup().addField(f1).repeatedGroup().addFields(f2, f3).named("g1").named(name);                break;        }        Assert.assertEquals(expected, built);    }}
public void parquet-mr_f3835_0()
{    PrimitiveTypeName[] types = new PrimitiveTypeName[] { BOOLEAN, INT32, INT64, INT96, FLOAT, DOUBLE, BINARY };    for (PrimitiveTypeName type : types) {        String name = type.toString() + "_";        for (Type.Repetition repetition : Type.Repetition.values()) {            PrimitiveType expected = new PrimitiveType(repetition, type, name);            PrimitiveType built = Types.primitive(type, repetition).named(name);            Assert.assertEquals(expected, built);            switch(repetition) {                case REQUIRED:                    built = Types.required(type).named(name);                    break;                case OPTIONAL:                    built = Types.optional(type).named(name);                    break;                case REPEATED:                    built = Types.repeated(type).named(name);                    break;            }            Assert.assertEquals(expected, built);        }    }}
public void parquet-mr_f3836_0()
{    String name = "fixed_";    int len = 5;    for (Type.Repetition repetition : Type.Repetition.values()) {        PrimitiveType expected = new PrimitiveType(repetition, FIXED_LEN_BYTE_ARRAY, len, name);        PrimitiveType built = Types.primitive(FIXED_LEN_BYTE_ARRAY, repetition).length(len).named(name);        Assert.assertEquals(expected, built);        switch(repetition) {            case REQUIRED:                built = Types.required(FIXED_LEN_BYTE_ARRAY).length(len).named(name);                break;            case OPTIONAL:                built = Types.optional(FIXED_LEN_BYTE_ARRAY).length(len).named(name);                break;            case REPEATED:                built = Types.repeated(FIXED_LEN_BYTE_ARRAY).length(len).named(name);                break;        }        Assert.assertEquals(expected, built);    }}
public void parquet-mr_f3837_0()
{        Assert.assertEquals("Should not complain about an empty required group", Types.requiredGroup().named("g"), new GroupType(REQUIRED, "g"));    Assert.assertEquals("Should not complain about an empty required group", Types.optionalGroup().named("g"), new GroupType(OPTIONAL, "g"));    Assert.assertEquals("Should not complain about an empty required group", Types.repeatedGroup().named("g"), new GroupType(REPEATED, "g"));}
public void parquet-mr_f3838_0()
{        Assert.assertEquals("Should not complain about an empty required group", Types.buildMessage().named("m"), new MessageType("m"));}
public void parquet-mr_f3839_0()
{    Types.required(FIXED_LEN_BYTE_ARRAY).named("fixed");}
public void parquet-mr_f3840_0()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, FIXED_LEN_BYTE_ARRAY, 7, "fixed");    PrimitiveType fixed = Types.required(FIXED_LEN_BYTE_ARRAY).length(7).named("fixed");    Assert.assertEquals(expected, fixed);}
public void parquet-mr_f3841_0()
{    Type f4 = Types.required(FIXED_LEN_BYTE_ARRAY).length(4).named("f4");    Type f8 = Types.required(FIXED_LEN_BYTE_ARRAY).length(8).named("f8");    Assert.assertFalse("Types with different lengths should not be equal", f4.equals(f8));}
public void parquet-mr_f3842_0()
{        MessageType expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, INT32, 0, "aDecimal", DECIMAL, new DecimalMetadata(9, 2), null));    MessageType builderType = Types.buildMessage().required(INT32).as(DECIMAL).precision(9).scale(2).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);        expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, INT64, 0, "aDecimal", DECIMAL, new DecimalMetadata(18, 2), null));    builderType = Types.buildMessage().required(INT64).as(DECIMAL).precision(18).scale(2).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);        expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, BINARY, 0, "aDecimal", DECIMAL, new DecimalMetadata(9, 2), null));    builderType = Types.buildMessage().required(BINARY).as(DECIMAL).precision(9).scale(2).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);        expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, FIXED_LEN_BYTE_ARRAY, 4, "aDecimal", DECIMAL, new DecimalMetadata(9, 2), null));    builderType = Types.buildMessage().required(FIXED_LEN_BYTE_ARRAY).length(4).as(DECIMAL).precision(9).scale(2).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);}
public void parquet-mr_f3843_0()
{    MessageType expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, INT32, 0, "aDecimal", DECIMAL, new DecimalMetadata(9, 0), null));    MessageType builderType = Types.buildMessage().required(INT32).as(DECIMAL).precision(9).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);    expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, INT64, 0, "aDecimal", DECIMAL, new DecimalMetadata(9, 0), null));    builderType = Types.buildMessage().required(INT64).as(DECIMAL).precision(9).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);    expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, BINARY, 0, "aDecimal", DECIMAL, new DecimalMetadata(9, 0), null));    builderType = Types.buildMessage().required(BINARY).as(DECIMAL).precision(9).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);    expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, FIXED_LEN_BYTE_ARRAY, 7, "aDecimal", DECIMAL, new DecimalMetadata(9, 0), null));    builderType = Types.buildMessage().required(FIXED_LEN_BYTE_ARRAY).length(7).as(DECIMAL).precision(9).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);}
public void parquet-mr_f3844_0()
{    assertThrows("Should reject decimal annotation without precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(INT32).as(DECIMAL).scale(2).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject decimal annotation without precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(INT64).as(DECIMAL).scale(2).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject decimal annotation without precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(BINARY).as(DECIMAL).scale(2).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject decimal annotation without precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(FIXED_LEN_BYTE_ARRAY).length(7).as(DECIMAL).scale(2).named("aDecimal").named("DecimalMessage"));}
public void parquet-mr_f3845_0()
{    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(INT32).as(DECIMAL).precision(3).scale(4).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(INT64).as(DECIMAL).precision(3).scale(4).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(BINARY).as(DECIMAL).precision(3).scale(4).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(FIXED_LEN_BYTE_ARRAY).length(7).as(DECIMAL).precision(3).scale(4).named("aDecimal").named("DecimalMessage"));}
public void parquet-mr_f3846_0()
{        assertThrows("should reject precision 10 with length 4", IllegalStateException.class, (Callable<Type>) () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(4).as(DECIMAL).precision(10).scale(2).named("aDecimal"));    assertThrows("should reject precision 10 with length 4", IllegalStateException.class, (Callable<Type>) () -> Types.required(INT32).as(DECIMAL).precision(10).scale(2).named("aDecimal"));        assertThrows("should reject precision 19 with length 8", IllegalStateException.class, (Callable<Type>) () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(8).as(DECIMAL).precision(19).scale(4).named("aDecimal"));    assertThrows("should reject precision 19 with length 8", IllegalStateException.class, (Callable<Type>) () -> Types.required(INT64).length(8).as(DECIMAL).precision(19).scale(4).named("aDecimal"));}
public void parquet-mr_f3847_0()
{    PrimitiveTypeName[] unsupported = new PrimitiveTypeName[] { BOOLEAN, INT96, DOUBLE, FLOAT };    for (final PrimitiveTypeName type : unsupported) {        assertThrows("Should reject non-binary type: " + type, IllegalStateException.class, (Callable<Type>) () -> Types.required(type).as(DECIMAL).precision(9).scale(2).named("d"));    }}
public void parquet-mr_f3848_0()
{    OriginalType[] types = new OriginalType[] { UTF8, JSON, BSON };    for (final OriginalType logicalType : types) {        PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "col", logicalType);        PrimitiveType string = Types.required(BINARY).as(logicalType).named("col");        Assert.assertEquals(expected, string);    }}
public void parquet-mr_f3849_0()
{    OriginalType[] types = new OriginalType[] { UTF8, JSON, BSON };    for (final OriginalType logicalType : types) {        PrimitiveTypeName[] nonBinary = new PrimitiveTypeName[] { BOOLEAN, INT32, INT64, INT96, DOUBLE, FLOAT };        for (final PrimitiveTypeName type : nonBinary) {            assertThrows("Should reject non-binary type: " + type, IllegalStateException.class, (Callable<Type>) () -> Types.required(type).as(logicalType).named("col"));        }        assertThrows("Should reject non-binary type: FIXED_LEN_BYTE_ARRAY", IllegalStateException.class, (Callable<Type>) () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(1).as(logicalType).named("col"));    }}
public void parquet-mr_f3850_0()
{    OriginalType[] types = new OriginalType[] { DATE, TIME_MILLIS, UINT_8, UINT_16, UINT_32, INT_8, INT_16, INT_32 };    for (OriginalType logicalType : types) {        PrimitiveType expected = new PrimitiveType(REQUIRED, INT32, "col", logicalType);        PrimitiveType date = Types.required(INT32).as(logicalType).named("col");        Assert.assertEquals(expected, date);    }}
public void parquet-mr_f3851_0()
{    OriginalType[] types = new OriginalType[] { DATE, TIME_MILLIS, UINT_8, UINT_16, UINT_32, INT_8, INT_16, INT_32 };    for (final OriginalType logicalType : types) {        PrimitiveTypeName[] nonInt32 = new PrimitiveTypeName[] { BOOLEAN, INT64, INT96, DOUBLE, FLOAT, BINARY };        for (final PrimitiveTypeName type : nonInt32) {            assertThrows("Should reject non-int32 type: " + type, IllegalStateException.class, (Callable<Type>) () -> Types.required(type).as(logicalType).named("col"));        }        assertThrows("Should reject non-int32 type: FIXED_LEN_BYTE_ARRAY", IllegalStateException.class, (Callable<Type>) () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(1).as(logicalType).named("col"));    }}
public void parquet-mr_f3852_0()
{    OriginalType[] types = new OriginalType[] { TIME_MICROS, TIMESTAMP_MILLIS, TIMESTAMP_MICROS, UINT_64, INT_64 };    for (OriginalType logicalType : types) {        PrimitiveType expected = new PrimitiveType(REQUIRED, INT64, "col", logicalType);        PrimitiveType date = Types.required(INT64).as(logicalType).named("col");        Assert.assertEquals(expected, date);    }}
public void parquet-mr_f3853_0()
{    OriginalType[] types = new OriginalType[] { TIME_MICROS, TIMESTAMP_MILLIS, TIMESTAMP_MICROS, UINT_64, INT_64 };    for (final OriginalType logicalType : types) {        PrimitiveTypeName[] nonInt64 = new PrimitiveTypeName[] { BOOLEAN, INT32, INT96, DOUBLE, FLOAT, BINARY };        for (final PrimitiveTypeName type : nonInt64) {            assertThrows("Should reject non-int64 type: " + type, IllegalStateException.class, (Callable<Type>) () -> Types.required(type).as(logicalType).named("col"));        }        assertThrows("Should reject non-int64 type: FIXED_LEN_BYTE_ARRAY", IllegalStateException.class, (Callable<Type>) () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(1).as(logicalType).named("col"));    }}
public void parquet-mr_f3854_0()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, FIXED_LEN_BYTE_ARRAY, 12, "interval", INTERVAL);    PrimitiveType string = Types.required(FIXED_LEN_BYTE_ARRAY).length(12).as(INTERVAL).named("interval");    Assert.assertEquals(expected, string);}
public void parquet-mr_f3855_0()
{    PrimitiveTypeName[] nonFixed = new PrimitiveTypeName[] { BOOLEAN, INT32, INT64, INT96, DOUBLE, FLOAT, BINARY };    for (final PrimitiveTypeName type : nonFixed) {        assertThrows("Should reject non-fixed type: " + type, IllegalStateException.class, (Callable<Type>) () -> Types.required(type).as(INTERVAL).named("interval"));    }}
public void parquet-mr_f3856_0()
{    assertThrows("Should reject fixed with length != 12: " + 11, IllegalStateException.class, (Callable<Type>) () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(11).as(INTERVAL).named("interval"));}
public void parquet-mr_f3857_0()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new PrimitiveType(REQUIRED, INT64, "value"));    GroupType expected = new GroupType(REQUIRED, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    GroupType actual = Types.requiredMap().key(INT64).requiredValue(INT64).named("myMap");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3858_0()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new PrimitiveType(REQUIRED, INT64, "value"));    GroupType expected = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    GroupType actual = Types.optionalMap().key(INT64).requiredValue(INT64).named("myMap");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3859_0()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new PrimitiveType(REQUIRED, INT64, "value"));    GroupType map = new GroupType(REQUIRED, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().requiredMap().key(INT64).requiredValue(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3860_0()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new PrimitiveType(OPTIONAL, INT64, "value"));    GroupType map = new GroupType(REQUIRED, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().requiredMap().key(INT64).optionalValue(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3861_0()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> keyFields = new ArrayList<Type>();    keyFields.add(new PrimitiveType(OPTIONAL, INT64, "first"));    keyFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "second"));    typeList.add(new GroupType(REQUIRED, "key", keyFields));    List<Type> valueFields = new ArrayList<Type>();    valueFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "one"));    valueFields.add(new PrimitiveType(OPTIONAL, INT32, "two"));    typeList.add(new GroupType(OPTIONAL, "value", valueFields));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    GroupType actual = Types.optionalMap().groupKey().optional(INT64).named("first").optional(DOUBLE).named("second").optionalGroupValue().optional(DOUBLE).named("one").optional(INT32).named("two").named("myMap");    Assert.assertEquals(map, actual);}
public void parquet-mr_f3862_0()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> keyFields = new ArrayList<Type>();    keyFields.add(new PrimitiveType(OPTIONAL, INT64, "first"));    keyFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "second"));    typeList.add(new GroupType(REQUIRED, "key", keyFields));    List<Type> valueFields = new ArrayList<Type>();    valueFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "one"));    valueFields.add(new PrimitiveType(OPTIONAL, INT32, "two"));    typeList.add(new GroupType(REQUIRED, "value", valueFields));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().groupKey().optional(INT64).named("first").optional(DOUBLE).named("second").requiredGroupValue().optional(DOUBLE).named("one").optional(INT32).named("two").named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3863_0()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> keyFields = new ArrayList<Type>();    keyFields.add(new PrimitiveType(OPTIONAL, INT64, "first"));    keyFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "second"));    typeList.add(new GroupType(REQUIRED, "key", keyFields));    typeList.add(new PrimitiveType(OPTIONAL, DOUBLE, "value"));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().groupKey().optional(INT64).named("first").optional(DOUBLE).named("second").optionalValue(DOUBLE).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3864_0()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> keyFields = new ArrayList<Type>();    keyFields.add(new PrimitiveType(OPTIONAL, INT64, "first"));    keyFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "second"));    typeList.add(new GroupType(REQUIRED, "key", keyFields));    typeList.add(new PrimitiveType(REQUIRED, DOUBLE, "value"));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().groupKey().optional(INT64).named("first").optional(DOUBLE).named("second").requiredValue(DOUBLE).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3865_0()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> keyFields = new ArrayList<Type>();    keyFields.add(new PrimitiveType(OPTIONAL, INT64, "first"));    keyFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "second"));    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    List<Type> valueFields = new ArrayList<Type>();    valueFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "one"));    valueFields.add(new PrimitiveType(OPTIONAL, INT32, "two"));    typeList.add(new GroupType(OPTIONAL, "value", valueFields));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().key(INT64).optionalGroupValue().optional(DOUBLE).named("one").optional(INT32).named("two").named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3866_0()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    List<Type> valueFields = new ArrayList<Type>();    valueFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "one"));    valueFields.add(new PrimitiveType(OPTIONAL, INT32, "two"));    typeList.add(new GroupType(REQUIRED, "value", valueFields));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().key(INT64).requiredGroupValue().optional(DOUBLE).named("one").optional(INT32).named("two").named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3867_0()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> innerFields = new ArrayList<Type>();    innerFields.add(new PrimitiveType(REQUIRED, FLOAT, "inner_key_1"));    innerFields.add(new PrimitiveType(OPTIONAL, INT32, "inner_key_2"));    List<Type> keyFields = new ArrayList<Type>();    keyFields.add(new PrimitiveType(OPTIONAL, INT64, "first"));    keyFields.add(new GroupType(REQUIRED, "second", innerFields));    typeList.add(new GroupType(REQUIRED, "key", keyFields));    List<Type> valueFields = new ArrayList<Type>();    valueFields.add(new GroupType(OPTIONAL, "one", innerFields));    valueFields.add(new PrimitiveType(OPTIONAL, INT32, "two"));    typeList.add(new GroupType(OPTIONAL, "value", valueFields));    GroupType map = new GroupType(REQUIRED, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().requiredMap().groupKey().optional(INT64).named("first").requiredGroup().required(FLOAT).named("inner_key_1").optional(INT32).named("inner_key_2").named("second").optionalGroupValue().optionalGroup().required(FLOAT).named("inner_key_1").optional(INT32).named("inner_key_2").named("one").optional(INT32).named("two").named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3868_0()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new GroupType(REQUIRED, "value", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element"))));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().key(INT64).requiredListValue().optionalElement(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3869_0()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new GroupType(OPTIONAL, "value", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element"))));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().key(INT64).optionalListValue().optionalElement(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3870_0()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> innerMapTypeList = new ArrayList<Type>();    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "value"));    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new GroupType(REQUIRED, "value", OriginalType.MAP, new GroupType(REPEATED, "map", innerMapTypeList)));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().key(INT64).requiredMapValue().key(INT64).requiredValue(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3871_0()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> innerMapTypeList = new ArrayList<Type>();    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "value"));    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new GroupType(OPTIONAL, "value", OriginalType.MAP, new GroupType(REPEATED, "map", innerMapTypeList)));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().key(INT64).optionalMapValue().key(INT64).requiredValue(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3872_0()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new GroupType(REQUIRED, "key", new PrimitiveType(REQUIRED, INT64, "first")));    typeList.add(new GroupType(REQUIRED, "value", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element"))));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().groupKey().required(INT64).named("first").requiredListValue().optionalElement(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3873_0()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new GroupType(REQUIRED, "key", new PrimitiveType(REQUIRED, INT64, "first")));    typeList.add(new GroupType(OPTIONAL, "value", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element"))));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().groupKey().required(INT64).named("first").optionalListValue().optionalElement(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3874_0()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> innerMapTypeList = new ArrayList<Type>();    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "value"));    typeList.add(new GroupType(REQUIRED, "key", new PrimitiveType(REQUIRED, INT64, "first")));    typeList.add(new GroupType(REQUIRED, "value", OriginalType.MAP, new GroupType(REPEATED, "map", innerMapTypeList)));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().groupKey().required(INT64).named("first").requiredMapValue().key(INT64).requiredValue(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3875_0()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> innerMapTypeList = new ArrayList<Type>();    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "value"));    typeList.add(new GroupType(REQUIRED, "key", new PrimitiveType(REQUIRED, INT64, "first")));    typeList.add(new GroupType(OPTIONAL, "value", OriginalType.MAP, new GroupType(REPEATED, "map", innerMapTypeList)));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().groupKey().required(INT64).named("first").optionalMapValue().key(INT64).requiredValue(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3876_0()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().key(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3877_0()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, BINARY, "key", OriginalType.UTF8));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3878_0()
{    Type keyType = Types.required(INT64).named("key");    Type valueType = Types.required(BOOLEAN).named("value");    GroupType map = new GroupType(REQUIRED, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", new Type[] { keyType, valueType }));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().requiredMap().key(keyType).value(valueType).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3879_0()
{    GroupType expected = new GroupType(REQUIRED, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(REQUIRED, INT64, "element")));    Type element = Types.primitive(INT64, REQUIRED).named("element");    Type actual = Types.requiredList().element(element).named("myList");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3880_0()
{    GroupType expected = new GroupType(REQUIRED, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element")));    Type actual = Types.requiredList().optionalElement(INT64).named("myList");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3881_0()
{    GroupType expected = new GroupType(OPTIONAL, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element")));    Type actual = Types.optionalList().optionalElement(INT64).named("myList");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3882_0()
{    List<Type> fields = new ArrayList<Type>();    fields.add(new PrimitiveType(OPTIONAL, BOOLEAN, "field"));    GroupType expected = new GroupType(REQUIRED, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", new GroupType(REQUIRED, "element", fields)));    Type actual = Types.requiredList().requiredGroupElement().optional(BOOLEAN).named("field").named("myList");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3883_0()
{    List<Type> fields = new ArrayList<Type>();    fields.add(new PrimitiveType(OPTIONAL, BOOLEAN, "field"));    GroupType expected = new GroupType(REQUIRED, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", new GroupType(OPTIONAL, "element", fields)));    Type actual = Types.requiredList().optionalGroupElement().optional(BOOLEAN).named("field").named("myList");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3884_0()
{    List<Type> fields = new ArrayList<Type>();    fields.add(new GroupType(REQUIRED, "element", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, DOUBLE, "element"))));    GroupType expected = new GroupType(OPTIONAL, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", fields));    Type actual = Types.optionalList().requiredListElement().optionalElement(DOUBLE).named("myList");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3885_0()
{    List<Type> fields = new ArrayList<Type>();    fields.add(new GroupType(OPTIONAL, "element", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, DOUBLE, "element"))));    GroupType expected = new GroupType(OPTIONAL, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", fields));    Type actual = Types.optionalList().optionalListElement().optionalElement(DOUBLE).named("myList");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3886_0()
{    List<Type> fields = new ArrayList<Type>();    fields.add(new GroupType(REQUIRED, "element", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element"))));    GroupType expected = new GroupType(REQUIRED, "topGroup", fields);    Type actual = Types.requiredGroup().requiredList().optionalElement(INT64).named("element").named("topGroup");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3887_0()
{    List<Type> fields = new ArrayList<Type>();    fields.add(new GroupType(OPTIONAL, "element", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element"))));    GroupType expected = new GroupType(REQUIRED, "topGroup", fields);    Type actual = Types.requiredGroup().optionalList().optionalElement(INT64).named("element").named("topGroup");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3888_0()
{    List<Type> fields = new ArrayList<Type>();    fields.add(new GroupType(OPTIONAL, "element", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(REQUIRED, INT64, "element"))));    GroupType expected = new GroupType(REQUIRED, "topGroup", fields);    Type actual = Types.requiredGroup().optionalList().requiredElement(INT64).named("element").named("topGroup");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3889_0()
{    List<Type> innerFields = new ArrayList<Type>();    innerFields.add(new PrimitiveType(REQUIRED, DOUBLE, "key"));    innerFields.add(new PrimitiveType(REQUIRED, INT32, "value"));    List<Type> fields = new ArrayList<Type>();    fields.add(new GroupType(REQUIRED, "element", OriginalType.MAP, new GroupType(REPEATED, "map", innerFields)));    GroupType expected = new GroupType(OPTIONAL, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", fields));    Type actual = Types.optionalList().requiredMapElement().key(DOUBLE).requiredValue(INT32).named("myList");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3890_0()
{    List<Type> innerFields = new ArrayList<Type>();    innerFields.add(new PrimitiveType(REQUIRED, DOUBLE, "key"));    innerFields.add(new PrimitiveType(REQUIRED, INT32, "value"));    List<Type> fields = new ArrayList<Type>();    fields.add(new GroupType(OPTIONAL, "element", OriginalType.MAP, new GroupType(REPEATED, "map", innerFields)));    GroupType expected = new GroupType(OPTIONAL, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", fields));    Type actual = Types.optionalList().optionalMapElement().key(DOUBLE).requiredValue(INT32).named("myList");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3891_0()
{    PrimitiveTypeName[] types = new PrimitiveTypeName[] { BOOLEAN, INT32, INT64, INT96, FLOAT, DOUBLE, BINARY, FIXED_LEN_BYTE_ARRAY };    for (PrimitiveTypeName type : types) {        String name = type.toString() + "_";        int len = type == FIXED_LEN_BYTE_ARRAY ? 42 : 0;        PrimitiveType expected = new PrimitiveType(Repetition.OPTIONAL, type, len, name, null, null, null, ColumnOrder.undefined());        PrimitiveType built = Types.optional(type).length(len).columnOrder(ColumnOrder.undefined()).named(name);        Assert.assertEquals(expected, built);    }}
public void parquet-mr_f3892_0()
{    PrimitiveTypeName[] types = new PrimitiveTypeName[] { BOOLEAN, INT32, INT64, FLOAT, DOUBLE, BINARY, FIXED_LEN_BYTE_ARRAY };    for (PrimitiveTypeName type : types) {        String name = type.toString() + "_";        int len = type == FIXED_LEN_BYTE_ARRAY ? 42 : 0;        PrimitiveType expected = new PrimitiveType(Repetition.OPTIONAL, type, len, name, null, null, null, ColumnOrder.typeDefined());        PrimitiveType built = Types.optional(type).length(len).columnOrder(ColumnOrder.typeDefined()).named(name);        Assert.assertEquals(expected, built);    }}
public void parquet-mr_f3893_0()
{    assertThrows(null, IllegalArgumentException.class, (Callable<PrimitiveType>) () -> Types.optional(INT96).columnOrder(ColumnOrder.typeDefined()).named("int96_unsupported"));    assertThrows(null, IllegalArgumentException.class, (Callable<PrimitiveType>) () -> Types.optional(FIXED_LEN_BYTE_ARRAY).length(12).as(INTERVAL).columnOrder(ColumnOrder.typeDefined()).named("interval_unsupported"));}
public void parquet-mr_f3894_0()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "aDecimal", LogicalTypeAnnotation.decimalType(3, 4));    PrimitiveType actual = Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).named("aDecimal");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3895_0()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "aDecimal", LogicalTypeAnnotation.decimalType(3, 4));    PrimitiveType actual = Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).scale(3).named("aDecimal");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3896_0()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "aDecimal", LogicalTypeAnnotation.decimalType(3, 4));    PrimitiveType actual = Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).precision(4).named("aDecimal");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3897_0()
{    PrimitiveType utcMillisExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(true, MILLIS));    PrimitiveType nonUtcMillisExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(false, MILLIS));    PrimitiveType utcMicrosExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(true, MICROS));    PrimitiveType nonUtcMicrosExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(false, MICROS));    PrimitiveType utcMillisActual = Types.required(INT64).as(timestampType(true, MILLIS)).named("aTimestamp");    PrimitiveType nonUtcMillisActual = Types.required(INT64).as(timestampType(false, MILLIS)).named("aTimestamp");    PrimitiveType utcMicrosActual = Types.required(INT64).as(timestampType(true, MICROS)).named("aTimestamp");    PrimitiveType nonUtcMicrosActual = Types.required(INT64).as(timestampType(false, MICROS)).named("aTimestamp");    Assert.assertEquals(utcMillisExpected, utcMillisActual);    Assert.assertEquals(nonUtcMillisExpected, nonUtcMillisActual);    Assert.assertEquals(utcMicrosExpected, utcMicrosActual);    Assert.assertEquals(nonUtcMicrosExpected, nonUtcMicrosActual);}
public void parquet-mr_f3898_0()
{    Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).scale(4).named("aDecimal");}
public void parquet-mr_f3899_0()
{    Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).precision(5).named("aDecimal");}
public static void parquet-mr_f3900_0(String message, Class<? extends Exception> expected, Callable callable)
{    try {        callable.call();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        Assert.assertEquals(message, expected, actual.getClass());    }}
public void parquet-mr_f3901_0()
{    PrimitiveType f1 = Types.required(BINARY).as(stringType()).named("f1");    PrimitiveType f2 = Types.required(INT32).named("f2");    PrimitiveType f3 = Types.optional(INT32).named("f3");    String name = "group";    for (Repetition repetition : Repetition.values()) {        GroupType expected = new GroupType(repetition, name, f1, new GroupType(repetition, "g1", f2, f3));        GroupType built = Types.buildGroup(repetition).addField(f1).group(repetition).addFields(f2, f3).named("g1").named(name);        Assert.assertEquals(expected, built);        switch(repetition) {            case REQUIRED:                built = Types.requiredGroup().addField(f1).requiredGroup().addFields(f2, f3).named("g1").named(name);                break;            case OPTIONAL:                built = Types.optionalGroup().addField(f1).optionalGroup().addFields(f2, f3).named("g1").named(name);                break;            case REPEATED:                built = Types.repeatedGroup().addField(f1).repeatedGroup().addFields(f2, f3).named("g1").named(name);                break;        }        Assert.assertEquals(expected, built);    }}
public void parquet-mr_f3902_0()
{        MessageType expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, INT32, 0, "aDecimal", decimalType(2, 9), null));    MessageType builderType = Types.buildMessage().required(INT32).as(decimalType(2, 9)).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);        expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, INT64, 0, "aDecimal", decimalType(2, 18), null));    builderType = Types.buildMessage().required(INT64).as(decimalType(2, 18)).precision(18).scale(2).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);        expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, BINARY, 0, "aDecimal", decimalType(2, 9), null));    builderType = Types.buildMessage().required(BINARY).as(decimalType(2, 9)).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);        expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, FIXED_LEN_BYTE_ARRAY, 4, "aDecimal", decimalType(2, 9), null));    builderType = Types.buildMessage().required(FIXED_LEN_BYTE_ARRAY).length(4).as(decimalType(2, 9)).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);}
public void parquet-mr_f3903_0()
{    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, () -> Types.buildMessage().required(INT32).as(decimalType(4, 3)).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, () -> Types.buildMessage().required(INT64).as(decimalType(4, 3)).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, () -> Types.buildMessage().required(BINARY).as(decimalType(4, 3)).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, () -> Types.buildMessage().required(FIXED_LEN_BYTE_ARRAY).length(7).as(decimalType(4, 3)).named("aDecimal").named("DecimalMessage"));}
public void parquet-mr_f3904_0()
{        assertThrows("should reject precision 10 with length 4", IllegalStateException.class, () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(4).as(decimalType(2, 10)).named("aDecimal"));    assertThrows("should reject precision 10 with length 4", IllegalStateException.class, () -> Types.required(INT32).as(decimalType(2, 10)).named("aDecimal"));        assertThrows("should reject precision 19 with length 8", IllegalStateException.class, () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(8).as(decimalType(4, 19)).named("aDecimal"));    assertThrows("should reject precision 19 with length 8", IllegalStateException.class, () -> Types.required(INT64).length(8).as(decimalType(4, 19)).named("aDecimal"));}
public void parquet-mr_f3905_0()
{    PrimitiveTypeName[] unsupported = new PrimitiveTypeName[] { BOOLEAN, INT96, DOUBLE, FLOAT };    for (final PrimitiveTypeName type : unsupported) {        assertThrows("Should reject non-binary type: " + type, IllegalStateException.class, () -> Types.required(type).as(decimalType(2, 9)).named("d"));    }}
public void parquet-mr_f3906_0()
{    LogicalTypeAnnotation[] types = new LogicalTypeAnnotation[] { stringType(), jsonType(), bsonType() };    for (final LogicalTypeAnnotation logicalType : types) {        PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "col", logicalType);        PrimitiveType string = Types.required(BINARY).as(logicalType).named("col");        Assert.assertEquals(expected, string);    }}
public void parquet-mr_f3907_0()
{    LogicalTypeAnnotation[] types = new LogicalTypeAnnotation[] { stringType(), jsonType(), bsonType() };    for (final LogicalTypeAnnotation logicalType : types) {        PrimitiveTypeName[] nonBinary = new PrimitiveTypeName[] { BOOLEAN, INT32, INT64, INT96, DOUBLE, FLOAT };        for (final PrimitiveTypeName type : nonBinary) {            assertThrows("Should reject non-binary type: " + type, IllegalStateException.class, () -> Types.required(type).as(logicalType).named("col"));        }        assertThrows("Should reject non-binary type: FIXED_LEN_BYTE_ARRAY", IllegalStateException.class, () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(1).as(logicalType).named("col"));    }}
public void parquet-mr_f3908_0()
{    LogicalTypeAnnotation[] types = new LogicalTypeAnnotation[] { dateType(), timeType(true, MILLIS), timeType(false, MILLIS), intType(8, false), intType(16, false), intType(32, false), intType(8, true), intType(16, true), intType(32, true) };    for (LogicalTypeAnnotation logicalType : types) {        PrimitiveType expected = new PrimitiveType(REQUIRED, INT32, "col", logicalType);        PrimitiveType date = Types.required(INT32).as(logicalType).named("col");        Assert.assertEquals(expected, date);    }}
public void parquet-mr_f3909_0()
{    LogicalTypeAnnotation[] types = new LogicalTypeAnnotation[] { dateType(), timeType(true, MILLIS), timeType(false, MILLIS), intType(8, false), intType(16, false), intType(32, false), intType(8, true), intType(16, true), intType(32, true) };    for (final LogicalTypeAnnotation logicalType : types) {        PrimitiveTypeName[] nonInt32 = new PrimitiveTypeName[] { BOOLEAN, INT64, INT96, DOUBLE, FLOAT, BINARY };        for (final PrimitiveTypeName type : nonInt32) {            assertThrows("Should reject non-int32 type: " + type, IllegalStateException.class, () -> Types.required(type).as(logicalType).named("col"));        }        assertThrows("Should reject non-int32 type: FIXED_LEN_BYTE_ARRAY", IllegalStateException.class, () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(1).as(logicalType).named("col"));    }}
public void parquet-mr_f3910_0()
{    LogicalTypeAnnotation[] types = new LogicalTypeAnnotation[] { timeType(true, MICROS), timeType(false, MICROS), timeType(true, NANOS), timeType(false, NANOS), timestampType(true, MILLIS), timestampType(false, MILLIS), timestampType(true, MICROS), timestampType(false, MICROS), timestampType(true, NANOS), timestampType(false, NANOS), intType(64, true), intType(64, false) };    for (LogicalTypeAnnotation logicalType : types) {        PrimitiveType expected = new PrimitiveType(REQUIRED, INT64, "col", logicalType);        PrimitiveType date = Types.required(INT64).as(logicalType).named("col");        Assert.assertEquals(expected, date);    }}
public void parquet-mr_f3911_0()
{    LogicalTypeAnnotation[] types = new LogicalTypeAnnotation[] { timeType(true, MICROS), timeType(false, MICROS), timeType(true, NANOS), timeType(false, NANOS), timestampType(true, MILLIS), timestampType(false, MILLIS), timestampType(true, MICROS), timestampType(false, MICROS), timestampType(true, NANOS), timestampType(false, NANOS), intType(64, true), intType(64, false) };    for (final LogicalTypeAnnotation logicalType : types) {        PrimitiveTypeName[] nonInt64 = new PrimitiveTypeName[] { BOOLEAN, INT32, INT96, DOUBLE, FLOAT, BINARY };        for (final PrimitiveTypeName type : nonInt64) {            assertThrows("Should reject non-int64 type: " + type, IllegalStateException.class, (Callable<Type>) () -> Types.required(type).as(logicalType).named("col"));        }        assertThrows("Should reject non-int64 type: FIXED_LEN_BYTE_ARRAY", IllegalStateException.class, (Callable<Type>) () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(1).as(logicalType).named("col"));    }}
public void parquet-mr_f3912_0()
{    PrimitiveTypeName[] nonFixed = new PrimitiveTypeName[] { BOOLEAN, INT32, INT64, INT96, DOUBLE, FLOAT, BINARY };    for (final PrimitiveTypeName type : nonFixed) {        assertThrows("Should reject non-fixed type: " + type, IllegalStateException.class, () -> Types.required(type).as(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation.getInstance()).named("interval"));    }}
public void parquet-mr_f3913_0()
{    assertThrows("Should reject fixed with length != 12: " + 11, IllegalStateException.class, () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(11).as(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation.getInstance()).named("interval"));}
public void parquet-mr_f3914_0()
{    assertThrows(null, IllegalArgumentException.class, () -> Types.optional(INT96).columnOrder(ColumnOrder.typeDefined()).named("int96_unsupported"));    assertThrows(null, IllegalArgumentException.class, () -> Types.optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY).length(12).as(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation.getInstance()).columnOrder(ColumnOrder.typeDefined()).named("interval_unsupported"));}
public void parquet-mr_f3915_0()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "aDecimal", LogicalTypeAnnotation.decimalType(3, 4));    PrimitiveType actual = Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).named("aDecimal");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3916_0()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "aDecimal", LogicalTypeAnnotation.decimalType(3, 4));    PrimitiveType actual = Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).scale(3).named("aDecimal");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3917_0()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "aDecimal", LogicalTypeAnnotation.decimalType(3, 4));    PrimitiveType actual = Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).precision(4).named("aDecimal");    Assert.assertEquals(expected, actual);}
public void parquet-mr_f3918_0()
{    PrimitiveType utcMillisExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(true, MILLIS));    PrimitiveType nonUtcMillisExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(false, MILLIS));    PrimitiveType utcMicrosExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(true, MICROS));    PrimitiveType nonUtcMicrosExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(false, MICROS));    PrimitiveType utcMillisActual = Types.required(INT64).as(timestampType(true, MILLIS)).named("aTimestamp");    PrimitiveType nonUtcMillisActual = Types.required(INT64).as(timestampType(false, MILLIS)).named("aTimestamp");    PrimitiveType utcMicrosActual = Types.required(INT64).as(timestampType(true, MICROS)).named("aTimestamp");    PrimitiveType nonUtcMicrosActual = Types.required(INT64).as(timestampType(false, MICROS)).named("aTimestamp");    Assert.assertEquals(utcMillisExpected, utcMillisActual);    Assert.assertEquals(nonUtcMillisExpected, nonUtcMillisActual);    Assert.assertEquals(utcMicrosExpected, utcMicrosActual);    Assert.assertEquals(nonUtcMicrosExpected, nonUtcMicrosActual);}
public void parquet-mr_f3919_0()
{    Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).scale(4).named("aDecimal");}
public void parquet-mr_f3920_0()
{    Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).precision(5).named("aDecimal");}
public static void parquet-mr_f3921_0(String message, Class<? extends Exception> expected, Callable callable)
{    try {        callable.call();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        Assert.assertEquals(message, expected, actual.getClass());    }}
public void parquet-mr_f3922_0()
{    TypeUtil.checkValidWriteSchema(Types.buildMessage().required(INT32).named("a").optional(BINARY).as(UTF8).named("b").named("valid_schema"));    TestTypeBuilders.assertThrows("Should complain about empty MessageType", InvalidSchemaException.class, (Callable<Void>) () -> {        TypeUtil.checkValidWriteSchema(new MessageType("invalid_schema"));        return null;    });}
public void parquet-mr_f3923_0()
{    TypeUtil.checkValidWriteSchema(Types.repeatedGroup().required(INT32).named("a").optional(BINARY).as(UTF8).named("b").named("valid_group"));    TestTypeBuilders.assertThrows("Should complain about empty GroupType", InvalidSchemaException.class, (Callable<Void>) () -> {        TypeUtil.checkValidWriteSchema(new GroupType(REPEATED, "invalid_group"));        return null;    });}
public void parquet-mr_f3924_0()
{    TypeUtil.checkValidWriteSchema(Types.buildMessage().repeatedGroup().required(INT32).named("a").optional(BINARY).as(UTF8).named("b").named("valid_group").named("valid_message"));    TestTypeBuilders.assertThrows("Should complain about empty GroupType", InvalidSchemaException.class, (Callable<Void>) () -> {        TypeUtil.checkValidWriteSchema(Types.buildMessage().addField(new GroupType(REPEATED, "invalid_group")).named("invalid_message"));        return null;    });}
public static ByteBufferInputStream parquet-mr_f3925_0(ByteBuffer... buffers)
{    if (buffers.length == 1) {        return new SingleBufferInputStream(buffers[0]);    } else {        return new MultiBufferInputStream(Arrays.asList(buffers));    }}
public static ByteBufferInputStream parquet-mr_f3926_0(List<ByteBuffer> buffers)
{    if (buffers.size() == 1) {        return new SingleBufferInputStream(buffers.get(0));    } else {        return new MultiBufferInputStream(buffers);    }}
public ByteBuffer parquet-mr_f3927_0()
{    try {        return slice(available());    } catch (EOFException e) {        throw new ShouldNeverHappenException(e);    }}
public long parquet-mr_f3928_0()
{    return delegate.position();}
public void parquet-mr_f3929_0(long n) throws IOException
{    long skipped = skip(n);    if (skipped < n) {        throw new EOFException("Not enough bytes to skip: " + skipped + " < " + n);    }}
public int parquet-mr_f3930_0(ByteBuffer out)
{    return delegate.read(out);}
public ByteBuffer parquet-mr_f3931_0(int length) throws EOFException
{    return delegate.slice(length);}
public List<ByteBuffer> parquet-mr_f3932_0(long length) throws EOFException
{    return delegate.sliceBuffers(length);}
public ByteBufferInputStream parquet-mr_f3933_0(long length) throws EOFException
{    return ByteBufferInputStream.wrap(sliceBuffers(length));}
public List<ByteBuffer> parquet-mr_f3934_0()
{    return delegate.remainingBuffers();}
public ByteBufferInputStream parquet-mr_f3935_0()
{    return ByteBufferInputStream.wrap(remainingBuffers());}
public int parquet-mr_f3936_0() throws IOException
{    return delegate.read();}
public int parquet-mr_f3937_0(byte[] b, int off, int len) throws IOException
{    return delegate.read(b, off, len);}
public long parquet-mr_f3938_0(long n)
{    return delegate.skip(n);}
public int parquet-mr_f3939_0()
{    return delegate.available();}
public void parquet-mr_f3940_0(int readlimit)
{    delegate.mark(readlimit);}
public void parquet-mr_f3941_0() throws IOException
{    delegate.reset();}
public boolean parquet-mr_f3942_0()
{    return delegate.markSupported();}
public static BytesInput parquet-mr_f3943_0(BytesInput... inputs)
{    return new SequenceBytesIn(Arrays.asList(inputs));}
public static BytesInput parquet-mr_f3944_0(List<BytesInput> inputs)
{    return new SequenceBytesIn(inputs);}
public static BytesInput parquet-mr_f3945_0(InputStream in, int bytes)
{    return new StreamBytesInput(in, bytes);}
public static BytesInput parquet-mr_f3946_0(ByteBuffer buffer, int offset, int length)
{    ByteBuffer tmp = buffer.duplicate();    tmp.position(offset);    ByteBuffer slice = tmp.slice();    slice.limit(length);    return new ByteBufferBytesInput(slice);}
public static BytesInput parquet-mr_f3947_0(ByteBuffer... buffers)
{    if (buffers.length == 1) {        return new ByteBufferBytesInput(buffers[0]);    }    return new BufferListBytesInput(Arrays.asList(buffers));}
public static BytesInput parquet-mr_f3948_0(List<ByteBuffer> buffers)
{    if (buffers.size() == 1) {        return new ByteBufferBytesInput(buffers.get(0));    }    return new BufferListBytesInput(buffers);}
public static BytesInput parquet-mr_f3949_1(byte[] in)
{        return new ByteArrayBytesInput(in, 0, in.length);}
public static BytesInput parquet-mr_f3950_1(byte[] in, int offset, int length)
{        return new ByteArrayBytesInput(in, offset, length);}
public static BytesInput parquet-mr_f3951_0(int intValue)
{    return new IntBytesInput(intValue);}
public static BytesInput parquet-mr_f3952_0(int intValue)
{    return new UnsignedVarIntBytesInput(intValue);}
public static BytesInput parquet-mr_f3953_0(int intValue)
{    int zigZag = (intValue << 1) ^ (intValue >> 31);    return new UnsignedVarIntBytesInput(zigZag);}
public static BytesInput parquet-mr_f3954_0(long longValue)
{    return new UnsignedVarLongBytesInput(longValue);}
public static BytesInput parquet-mr_f3955_0(long longValue)
{    long zigZag = (longValue << 1) ^ (longValue >> 63);    return new UnsignedVarLongBytesInput(zigZag);}
public static BytesInput parquet-mr_f3956_0(CapacityByteArrayOutputStream arrayOut)
{    return new CapacityBAOSBytesInput(arrayOut);}
public static BytesInput parquet-mr_f3957_0(ByteArrayOutputStream baos)
{    return new BAOSBytesInput(baos);}
public static BytesInput parquet-mr_f3958_0()
{    return EMPTY_BYTES_INPUT;}
public static BytesInput parquet-mr_f3959_0(BytesInput bytesInput) throws IOException
{    return from(bytesInput.toByteArray());}
public byte[] parquet-mr_f3960_1() throws IOException
{    BAOS baos = new BAOS((int) size());    this.writeAllTo(baos);        return baos.getBuf();}
public ByteBuffer parquet-mr_f3961_0() throws IOException
{    return ByteBuffer.wrap(toByteArray());}
public ByteBufferInputStream parquet-mr_f3962_0() throws IOException
{    return ByteBufferInputStream.wrap(toByteBuffer());}
public byte[] parquet-mr_f3963_0()
{    return this.buf;}
public void parquet-mr_f3964_1(OutputStream out) throws IOException
{            out.write(this.toByteArray());}
public byte[] parquet-mr_f3965_1() throws IOException
{        byte[] buf = new byte[byteCount];    new DataInputStream(in).readFully(buf);    return buf;}
public long parquet-mr_f3966_0()
{    return byteCount;}
public long parquet-mr_f3968_0()
{    return size;}
public void parquet-mr_f3969_0(OutputStream out) throws IOException
{    BytesUtils.writeIntLittleEndian(out, intValue);}
public ByteBuffer parquet-mr_f3970_0() throws IOException
{    return ByteBuffer.allocate(4).putInt(0, intValue);}
public long parquet-mr_f3971_0()
{    return 4;}
public void parquet-mr_f3972_0(OutputStream out) throws IOException
{    BytesUtils.writeUnsignedVarInt(intValue, out);}
public ByteBuffer parquet-mr_f3973_0() throws IOException
{    ByteBuffer ret = ByteBuffer.allocate((int) size());    BytesUtils.writeUnsignedVarInt(intValue, ret);    return ret;}
public long parquet-mr_f3974_0()
{    int s = (38 - Integer.numberOfLeadingZeros(intValue)) / 7;    return s == 0 ? 1 : s;}
public void parquet-mr_f3975_0(OutputStream out) throws IOException
{    BytesUtils.writeUnsignedVarLong(longValue, out);}
public long parquet-mr_f3976_0()
{    int s = (70 - Long.numberOfLeadingZeros(longValue)) / 7;    return s == 0 ? 1 : s;}
public void parquet-mr_f3977_0(OutputStream out) throws IOException
{}
public long parquet-mr_f3978_0()
{    return 0;}
public ByteBuffer parquet-mr_f3979_0() throws IOException
{    return ByteBuffer.allocate(0);}
public void parquet-mr_f3980_0(OutputStream out) throws IOException
{    arrayOut.writeTo(out);}
public long parquet-mr_f3981_0()
{    return arrayOut.size();}
public void parquet-mr_f3982_0(OutputStream out) throws IOException
{    arrayOut.writeTo(out);}
public long parquet-mr_f3983_0()
{    return arrayOut.size();}
public void parquet-mr_f3984_0(OutputStream out) throws IOException
{    out.write(in, offset, length);}
public ByteBuffer parquet-mr_f3985_0() throws IOException
{    return java.nio.ByteBuffer.wrap(in, offset, length);}
public long parquet-mr_f3986_0()
{    return length;}
public void parquet-mr_f3987_0(OutputStream out) throws IOException
{    WritableByteChannel channel = Channels.newChannel(out);    for (ByteBuffer buffer : buffers) {        channel.write(buffer.duplicate());    }}
public ByteBufferInputStream parquet-mr_f3988_0()
{    return ByteBufferInputStream.wrap(buffers);}
public long parquet-mr_f3989_0()
{    return length;}
public void parquet-mr_f3990_0(OutputStream out) throws IOException
{    Channels.newChannel(out).write(buffer.duplicate());}
public ByteBufferInputStream parquet-mr_f3991_0()
{    return ByteBufferInputStream.wrap(buffer);}
public long parquet-mr_f3992_0()
{    return buffer.remaining();}
public static int parquet-mr_f3993_0(int bound)
{    return 32 - Integer.numberOfLeadingZeros(bound);}
public static int parquet-mr_f3994_0(ByteBuffer in, int offset) throws IOException
{    int ch4 = in.get(offset) & 0xff;    int ch3 = in.get(offset + 1) & 0xff;    int ch2 = in.get(offset + 2) & 0xff;    int ch1 = in.get(offset + 3) & 0xff;    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0));}
public static int parquet-mr_f3995_0(byte[] in, int offset) throws IOException
{    int ch4 = in[offset] & 0xff;    int ch3 = in[offset + 1] & 0xff;    int ch2 = in[offset + 2] & 0xff;    int ch1 = in[offset + 3] & 0xff;    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0));}
public static int parquet-mr_f3996_0(InputStream in) throws IOException
{        int ch1 = in.read();    int ch2 = in.read();    int ch3 = in.read();    int ch4 = in.read();    if ((ch1 | ch2 | ch3 | ch4) < 0) {        throw new EOFException();    }    return ((ch4 << 24) + (ch3 << 16) + (ch2 << 8) + (ch1 << 0));}
public static int parquet-mr_f3997_0(InputStream in) throws IOException
{    int ch1 = in.read();    if (ch1 < 0) {        throw new EOFException();    }    return ch1;}
public static int parquet-mr_f3998_0(InputStream in) throws IOException
{    int ch1 = in.read();    int ch2 = in.read();    if ((ch1 | ch2) < 0) {        throw new EOFException();    }    return ((ch2 << 8) + (ch1 << 0));}
public static int parquet-mr_f3999_0(InputStream in) throws IOException
{    int ch1 = in.read();    int ch2 = in.read();    int ch3 = in.read();    if ((ch1 | ch2 | ch3) < 0) {        throw new EOFException();    }    return ((ch3 << 16) + (ch2 << 8) + (ch1 << 0));}
public static int parquet-mr_f4000_0(InputStream in, int bitWidth) throws IOException
{    int bytesWidth = paddedByteCountFromBits(bitWidth);    switch(bytesWidth) {        case 0:            return 0;        case 1:            return BytesUtils.readIntLittleEndianOnOneByte(in);        case 2:            return BytesUtils.readIntLittleEndianOnTwoBytes(in);        case 3:            return BytesUtils.readIntLittleEndianOnThreeBytes(in);        case 4:            return BytesUtils.readIntLittleEndian(in);        default:            throw new IOException(String.format("Encountered bitWidth (%d) that requires more than 4 bytes", bitWidth));    }}
public static void parquet-mr_f4001_0(OutputStream out, int v) throws IOException
{    out.write((v >>> 0) & 0xFF);}
public static void parquet-mr_f4002_0(OutputStream out, int v) throws IOException
{    out.write((v >>> 0) & 0xFF);    out.write((v >>> 8) & 0xFF);}
public static void parquet-mr_f4003_0(OutputStream out, int v) throws IOException
{    out.write((v >>> 0) & 0xFF);    out.write((v >>> 8) & 0xFF);    out.write((v >>> 16) & 0xFF);}
public static void parquet-mr_f4005_0(OutputStream out, int v, int bitWidth) throws IOException
{    int bytesWidth = paddedByteCountFromBits(bitWidth);    switch(bytesWidth) {        case 0:            break;        case 1:            writeIntLittleEndianOnOneByte(out, v);            break;        case 2:            writeIntLittleEndianOnTwoBytes(out, v);            break;        case 3:            writeIntLittleEndianOnThreeBytes(out, v);            break;        case 4:            writeIntLittleEndian(out, v);            break;        default:            throw new IOException(String.format("Encountered value (%d) that requires more than 4 bytes", v));    }}
public static int parquet-mr_f4006_0(InputStream in) throws IOException
{    int value = 0;    int i = 0;    int b;    while (((b = in.read()) & 0x80) != 0) {        value |= (b & 0x7F) << i;        i += 7;    }    return value | (b << i);}
public static int parquet-mr_f4007_0(InputStream in) throws IOException
{    int raw = readUnsignedVarInt(in);    int temp = (((raw << 31) >> 31) ^ raw) >> 1;    return temp ^ (raw & (1 << 31));}
public static void parquet-mr_f4008_0(int value, OutputStream out) throws IOException
{    while ((value & 0xFFFFFF80) != 0L) {        out.write((value & 0x7F) | 0x80);        value >>>= 7;    }    out.write(value & 0x7F);}
public static void parquet-mr_f4009_0(int value, ByteBuffer dest) throws IOException
{    while ((value & 0xFFFFFF80) != 0L) {        dest.putInt((value & 0x7F) | 0x80);        value >>>= 7;    }    dest.putInt(value & 0x7F);}
public static void parquet-mr_f4010_0(int intValue, OutputStream out) throws IOException
{    writeUnsignedVarInt((intValue << 1) ^ (intValue >> 31), out);}
public static long parquet-mr_f4011_0(InputStream in) throws IOException
{    long raw = readUnsignedVarLong(in);    long temp = (((raw << 63) >> 63) ^ raw) >> 1;    return temp ^ (raw & (1L << 63));}
public static long parquet-mr_f4012_0(InputStream in) throws IOException
{    long value = 0;    int i = 0;    long b;    while (((b = in.read()) & 0x80) != 0) {        value |= (b & 0x7F) << i;        i += 7;    }    return value | (b << i);}
public static void parquet-mr_f4013_0(long value, OutputStream out) throws IOException
{    while ((value & 0xFFFFFFFFFFFFFF80L) != 0L) {        out.write((int) ((value & 0x7F) | 0x80));        value >>>= 7;    }    out.write((int) (value & 0x7F));}
public static void parquet-mr_f4014_0(long longValue, OutputStream out) throws IOException
{    writeUnsignedVarLong((longValue << 1) ^ (longValue >> 63), out);}
public static int parquet-mr_f4015_0(int bitLength)
{    return (bitLength + 7) / 8;}
public static byte[] parquet-mr_f4016_0(int value)
{    byte[] outBuffer = new byte[4];    outBuffer[3] = (byte) (value >>> 24);    outBuffer[2] = (byte) (value >>> 16);    outBuffer[1] = (byte) (value >>> 8);    outBuffer[0] = (byte) (value >>> 0);    return outBuffer;}
public static int parquet-mr_f4017_0(byte[] bytes)
{    return ((int) (bytes[3] & 255) << 24) + ((int) (bytes[2] & 255) << 16) + ((int) (bytes[1] & 255) << 8) + ((int) (bytes[0] & 255) << 0);}
public static byte[] parquet-mr_f4018_0(long value)
{    byte[] outBuffer = new byte[8];    outBuffer[7] = (byte) (value >>> 56);    outBuffer[6] = (byte) (value >>> 48);    outBuffer[5] = (byte) (value >>> 40);    outBuffer[4] = (byte) (value >>> 32);    outBuffer[3] = (byte) (value >>> 24);    outBuffer[2] = (byte) (value >>> 16);    outBuffer[1] = (byte) (value >>> 8);    outBuffer[0] = (byte) (value >>> 0);    return outBuffer;}
public static long parquet-mr_f4019_0(byte[] bytes)
{    return (((long) bytes[7] << 56) + ((long) (bytes[6] & 255) << 48) + ((long) (bytes[5] & 255) << 40) + ((long) (bytes[4] & 255) << 32) + ((long) (bytes[3] & 255) << 24) + ((long) (bytes[2] & 255) << 16) + ((long) (bytes[1] & 255) << 8) + ((long) (bytes[0] & 255) << 0));}
public static byte[] parquet-mr_f4020_0(boolean value)
{    byte[] outBuffer = new byte[1];    outBuffer[0] = (byte) (value ? 1 : 0);    return outBuffer;}
public static boolean parquet-mr_f4021_0(byte[] bytes)
{    return ((int) (bytes[0] & 255) != 0);}
public static int parquet-mr_f4022_0(int minSlabSize, int targetCapacity, int targetNumSlabs)
{        return max(minSlabSize, ((int) (targetCapacity / pow(2, targetNumSlabs))));}
public static CapacityByteArrayOutputStream parquet-mr_f4023_0(int minSlabSize, int maxCapacityHint, int targetNumSlabs)
{    return withTargetNumSlabs(minSlabSize, maxCapacityHint, targetNumSlabs, new HeapByteBufferAllocator());}
public static CapacityByteArrayOutputStream parquet-mr_f4024_0(int minSlabSize, int maxCapacityHint, int targetNumSlabs, ByteBufferAllocator allocator)
{    return new CapacityByteArrayOutputStream(initialSlabSizeHeuristic(minSlabSize, maxCapacityHint, targetNumSlabs), maxCapacityHint, allocator);}
private void parquet-mr_f4025_1(int minimumSize)
{    int nextSlabSize;    if (bytesUsed == 0) {        nextSlabSize = initialSlabSize;    } else if (bytesUsed > maxCapacityHint / 5) {                nextSlabSize = maxCapacityHint / 5;    } else {                nextSlabSize = bytesUsed;    }    if (nextSlabSize < minimumSize) {                nextSlabSize = minimumSize;    }        this.currentSlab = allocator.allocate(nextSlabSize);    this.slabs.add(currentSlab);    this.bytesAllocated += nextSlabSize;    this.currentSlabIndex = 0;}
public void parquet-mr_f4026_0(int b)
{    if (!currentSlab.hasRemaining()) {        addSlab(1);    }    currentSlab.put(currentSlabIndex, (byte) b);    currentSlabIndex += 1;    currentSlab.position(currentSlabIndex);    bytesUsed += 1;}
public void parquet-mr_f4027_0(byte[] b, int off, int len)
{    if ((off < 0) || (off > b.length) || (len < 0) || ((off + len) - b.length > 0)) {        throw new IndexOutOfBoundsException(String.format("Given byte array of size %d, with requested length(%d) and offset(%d)", b.length, len, off));    }    if (len >= currentSlab.remaining()) {        final int length1 = currentSlab.remaining();        currentSlab.put(b, off, length1);        bytesUsed += length1;        currentSlabIndex += length1;        final int length2 = len - length1;        addSlab(length2);        currentSlab.put(b, off + length1, length2);        currentSlabIndex = length2;        bytesUsed += length2;    } else {        currentSlab.put(b, off, len);        currentSlabIndex += len;        bytesUsed += len;    }}
private void parquet-mr_f4028_0(OutputStream out, ByteBuffer buf, int len) throws IOException
{    if (buf.hasArray()) {        out.write(buf.array(), buf.arrayOffset(), len);    } else {                        byte[] copy = new byte[len];        buf.flip();        buf.get(copy);        out.write(copy);    }}
public void parquet-mr_f4029_0(OutputStream out) throws IOException
{    for (int i = 0; i < slabs.size() - 1; i++) {        writeToOutput(out, slabs.get(i), slabs.get(i).position());    }    writeToOutput(out, currentSlab, currentSlabIndex);}
public long parquet-mr_f4030_0()
{    return bytesUsed;}
public int parquet-mr_f4031_0()
{    return bytesAllocated;}
public void parquet-mr_f4032_1()
{            this.initialSlabSize = max(bytesUsed / 7, initialSlabSize);        for (ByteBuffer slab : slabs) {        allocator.release(slab);    }    this.slabs.clear();    this.bytesAllocated = 0;    this.bytesUsed = 0;    this.currentSlab = EMPTY_SLAB;    this.currentSlabIndex = 0;}
public long parquet-mr_f4033_0()
{    checkArgument(bytesUsed > 0, "This is an empty stream");    return bytesUsed - 1;}
public void parquet-mr_f4034_0(long index, byte value)
{    checkArgument(index < bytesUsed, "Index: " + index + " is >= the current size of: " + bytesUsed);    long seen = 0;    for (int i = 0; i < slabs.size(); i++) {        ByteBuffer slab = slabs.get(i);        if (index < seen + slab.limit()) {                        slab.put((int) (index - seen), value);            break;        }        seen += slab.limit();    }}
public String parquet-mr_f4035_0(String prefix)
{    return format("%s %s %d slabs, %,d bytes", prefix, getClass().getSimpleName(), slabs.size(), getCapacity());}
 int parquet-mr_f4036_0()
{    return slabs.size();}
public void parquet-mr_f4037_0()
{    for (ByteBuffer slab : slabs) {        allocator.release(slab);    }    try {        super.close();    } catch (IOException e) {        throw new OutputStreamCloseException(e);    }}
public void parquet-mr_f4038_0(BytesInput bytesInput) throws IOException
{    byte[] bytes = bytesInput.toByteArray();    slabs.add(bytes);    size += bytes.length;}
public void parquet-mr_f4039_0()
{    size = 0;    slabs.clear();}
public void parquet-mr_f4040_0(OutputStream out) throws IOException
{    for (byte[] slab : slabs) {        out.write(slab);    }}
public long parquet-mr_f4041_0()
{    return size;}
public String parquet-mr_f4042_0(String prefix)
{    return format("%s %s %d slabs, %,d bytes", prefix, getClass().getSimpleName(), slabs.size(), size);}
public static final DirectByteBufferAllocator parquet-mr_f4043_0()
{    return new DirectByteBufferAllocator();}
public ByteBuffer parquet-mr_f4044_0(final int size)
{    return ByteBuffer.allocateDirect(size);}
public void parquet-mr_f4045_0(ByteBuffer b)
{        return;}
public boolean parquet-mr_f4046_0()
{    return true;}
public static final HeapByteBufferAllocator parquet-mr_f4047_0()
{    return new HeapByteBufferAllocator();}
public ByteBuffer parquet-mr_f4048_0(final int size)
{    return ByteBuffer.allocate(size);}
public void parquet-mr_f4049_0(ByteBuffer b)
{    return;}
public boolean parquet-mr_f4050_0()
{    return false;}
public final void parquet-mr_f4051_0(byte[] b) throws IOException
{    readFully(b, 0, b.length);}
public final void parquet-mr_f4052_0(byte[] b, int off, int len) throws IOException
{    if (len < 0)        throw new IndexOutOfBoundsException();    int n = 0;    while (n < len) {        int count = in.read(b, off + n, len - n);        if (count < 0)            throw new EOFException();        n += count;    }}
public final int parquet-mr_f4053_0(int n) throws IOException
{    int total = 0;    int cur = 0;    while ((total < n) && ((cur = (int) in.skip(n - total)) > 0)) {        total += cur;    }    return total;}
public int parquet-mr_f4054_0() throws IOException
{    return in.read();}
public int parquet-mr_f4055_0()
{    return in.hashCode();}
public int parquet-mr_f4056_0(byte[] b) throws IOException
{    return in.read(b);}
public boolean parquet-mr_f4057_0(Object obj)
{    return in.equals(obj);}
public int parquet-mr_f4058_0(byte[] b, int off, int len) throws IOException
{    return in.read(b, off, len);}
public long parquet-mr_f4059_0(long n) throws IOException
{    return in.skip(n);}
public int parquet-mr_f4060_0() throws IOException
{    return in.available();}
public void parquet-mr_f4061_0() throws IOException
{    in.close();}
public void parquet-mr_f4062_0(int readlimit)
{    in.mark(readlimit);}
public void parquet-mr_f4063_0() throws IOException
{    in.reset();}
public boolean parquet-mr_f4064_0()
{    return in.markSupported();}
public final boolean parquet-mr_f4065_0() throws IOException
{    int ch = in.read();    if (ch < 0)        throw new EOFException();    return (ch != 0);}
public final byte parquet-mr_f4066_0() throws IOException
{    int ch = in.read();    if (ch < 0)        throw new EOFException();    return (byte) (ch);}
public final int parquet-mr_f4067_0() throws IOException
{    int ch = in.read();    if (ch < 0)        throw new EOFException();    return ch;}
public final short parquet-mr_f4068_0() throws IOException
{    int ch2 = in.read();    int ch1 = in.read();    if ((ch1 | ch2) < 0)        throw new EOFException();    return (short) ((ch1 << 8) + (ch2 << 0));}
public final int parquet-mr_f4069_0() throws IOException
{    int ch2 = in.read();    int ch1 = in.read();    if ((ch1 | ch2) < 0)        throw new EOFException();    return (ch1 << 8) + (ch2 << 0);}
public final int parquet-mr_f4070_0() throws IOException
{                        int ch4 = in.read();    int ch3 = in.read();    int ch2 = in.read();    int ch1 = in.read();    if ((ch1 | ch2 | ch3 | ch4) < 0)        throw new EOFException();    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0));}
public final long parquet-mr_f4071_0() throws IOException
{        readFully(readBuffer, 0, 8);    return (((long) readBuffer[7] << 56) + ((long) (readBuffer[6] & 255) << 48) + ((long) (readBuffer[5] & 255) << 40) + ((long) (readBuffer[4] & 255) << 32) + ((long) (readBuffer[3] & 255) << 24) + ((readBuffer[2] & 255) << 16) + ((readBuffer[1] & 255) << 8) + ((readBuffer[0] & 255) << 0));}
public final float parquet-mr_f4072_0() throws IOException
{    return Float.intBitsToFloat(readInt());}
public final double parquet-mr_f4073_0() throws IOException
{    return Double.longBitsToDouble(readLong());}
public void parquet-mr_f4074_0(int b) throws IOException
{    out.write(b);}
public void parquet-mr_f4075_0(byte[] b, int off, int len) throws IOException
{    out.write(b, off, len);}
public void parquet-mr_f4076_0() throws IOException
{    out.flush();}
public final void parquet-mr_f4077_0(boolean v) throws IOException
{    out.write(v ? 1 : 0);}
public final void parquet-mr_f4078_0(int v) throws IOException
{    out.write(v);}
public final void parquet-mr_f4079_0(int v) throws IOException
{    out.write((v >>> 0) & 0xFF);    out.write((v >>> 8) & 0xFF);}
public final void parquet-mr_f4080_0(int v) throws IOException
{                out.write((v >>> 0) & 0xFF);    out.write((v >>> 8) & 0xFF);    out.write((v >>> 16) & 0xFF);    out.write((v >>> 24) & 0xFF);}
public final void parquet-mr_f4081_0(long v) throws IOException
{    writeBuffer[7] = (byte) (v >>> 56);    writeBuffer[6] = (byte) (v >>> 48);    writeBuffer[5] = (byte) (v >>> 40);    writeBuffer[4] = (byte) (v >>> 32);    writeBuffer[3] = (byte) (v >>> 24);    writeBuffer[2] = (byte) (v >>> 16);    writeBuffer[1] = (byte) (v >>> 8);    writeBuffer[0] = (byte) (v >>> 0);    out.write(writeBuffer, 0, 8);}
public final void parquet-mr_f4082_0(float v) throws IOException
{    writeInt(Float.floatToIntBits(v));}
public final void parquet-mr_f4083_0(double v) throws IOException
{    writeLong(Double.doubleToLongBits(v));}
public void parquet-mr_f4084_0()
{    IOExceptionUtils.closeQuietly(out);}
public long parquet-mr_f4085_0()
{    return position;}
public long parquet-mr_f4086_0(long n)
{    if (n <= 0) {        return 0;    }    if (current == null) {        return -1;    }    long bytesSkipped = 0;    while (bytesSkipped < n) {        if (current.remaining() > 0) {            long bytesToSkip = Math.min(n - bytesSkipped, current.remaining());            current.position(current.position() + (int) bytesToSkip);            bytesSkipped += bytesToSkip;            this.position += bytesToSkip;        } else if (!nextBuffer()) {                        return bytesSkipped > 0 ? bytesSkipped : -1;        }    }    return bytesSkipped;}
public int parquet-mr_f4087_0(ByteBuffer out)
{    int len = out.remaining();    if (len <= 0) {        return 0;    }    if (current == null) {        return -1;    }    int bytesCopied = 0;    while (bytesCopied < len) {        if (current.remaining() > 0) {            int bytesToCopy;            ByteBuffer copyBuffer;            if (current.remaining() <= out.remaining()) {                                bytesToCopy = current.remaining();                copyBuffer = current;            } else {                                bytesToCopy = out.remaining();                copyBuffer = current.duplicate();                copyBuffer.limit(copyBuffer.position() + bytesToCopy);                current.position(copyBuffer.position() + bytesToCopy);            }            out.put(copyBuffer);            bytesCopied += bytesToCopy;            this.position += bytesToCopy;        } else if (!nextBuffer()) {                        return bytesCopied > 0 ? bytesCopied : -1;        }    }    return bytesCopied;}
public ByteBuffer parquet-mr_f4088_0(int length) throws EOFException
{    if (length <= 0) {        return EMPTY;    }    if (current == null) {        throw new EOFException();    }    ByteBuffer slice;    if (length > current.remaining()) {                        slice = ByteBuffer.allocate(length);        int bytesCopied = read(slice);        slice.flip();        if (bytesCopied < length) {            throw new EOFException();        }    } else {        slice = current.duplicate();        slice.limit(slice.position() + length);        current.position(slice.position() + length);        this.position += length;    }    return slice;}
public List<ByteBuffer> parquet-mr_f4089_0(long len) throws EOFException
{    if (len <= 0) {        return Collections.emptyList();    }    if (current == null) {        throw new EOFException();    }    List<ByteBuffer> buffers = new ArrayList<>();    long bytesAccumulated = 0;    while (bytesAccumulated < len) {        if (current.remaining() > 0) {                                    int bufLen = (int) Math.min(len - bytesAccumulated, current.remaining());            ByteBuffer slice = current.duplicate();            slice.limit(slice.position() + bufLen);            buffers.add(slice);            bytesAccumulated += bufLen;                        current.position(current.position() + bufLen);            this.position += bufLen;        } else if (!nextBuffer()) {                        throw new EOFException();        }    }    return buffers;}
public List<ByteBuffer> parquet-mr_f4090_0()
{    if (position >= length) {        return Collections.emptyList();    }    try {        return sliceBuffers(length - position);    } catch (EOFException e) {        throw new RuntimeException("[Parquet bug] Stream is bad: incorrect bytes remaining " + (length - position));    }}
public int parquet-mr_f4091_0(byte[] bytes, int off, int len)
{    if (len <= 0) {        if (len < 0) {            throw new IndexOutOfBoundsException("Read length must be greater than 0: " + len);        }        return 0;    }    if (current == null) {        return -1;    }    int bytesRead = 0;    while (bytesRead < len) {        if (current.remaining() > 0) {            int bytesToRead = Math.min(len - bytesRead, current.remaining());            current.get(bytes, off + bytesRead, bytesToRead);            bytesRead += bytesToRead;            this.position += bytesToRead;        } else if (!nextBuffer()) {                        return bytesRead > 0 ? bytesRead : -1;        }    }    return bytesRead;}
public int parquet-mr_f4092_0(byte[] bytes)
{    return read(bytes, 0, bytes.length);}
public int parquet-mr_f4093_0() throws IOException
{    if (current == null) {        throw new EOFException();    }    while (true) {        if (current.remaining() > 0) {            this.position += 1;                        return current.get() & 0xFF;        } else if (!nextBuffer()) {                        throw new EOFException();        }    }}
public int parquet-mr_f4094_0()
{    long remaining = length - position;    if (remaining > Integer.MAX_VALUE) {        return Integer.MAX_VALUE;    } else {        return (int) remaining;    }}
public void parquet-mr_f4095_0(int readlimit)
{    if (mark >= 0) {        discardMark();    }    this.mark = position;    this.markLimit = mark + readlimit + 1;    if (current != null) {        markBuffers.add(current.duplicate());    }}
public void parquet-mr_f4096_0() throws IOException
{    if (mark >= 0 && position < markLimit) {        this.position = mark;                        this.iterator = concat(markBuffers.iterator(), iterator);        discardMark();                nextBuffer();    } else {        throw new IOException("No mark defined or has read past the previous mark limit");    }}
private void parquet-mr_f4097_0()
{    this.mark = -1;    this.markLimit = 0;    markBuffers = new ArrayList<>();}
public boolean parquet-mr_f4098_0()
{    return true;}
private boolean parquet-mr_f4099_0()
{    if (!iterator.hasNext()) {        this.current = null;        return false;    }    this.current = iterator.next().duplicate();    if (mark >= 0) {        if (position < markLimit) {                        markBuffers.add(current.duplicate());        } else {                        discardMark();        }    }    return true;}
private static Iterator<E> parquet-mr_f4100_0(Iterator<E> first, Iterator<E> second)
{    return new ConcatIterator<>(first, second);}
public boolean parquet-mr_f4101_0()
{    if (useFirst) {        if (first.hasNext()) {            return true;        } else {            useFirst = false;            return second.hasNext();        }    }    return second.hasNext();}
public E parquet-mr_f4102_0()
{    if (useFirst && !first.hasNext()) {        useFirst = false;    }    if (!useFirst && !second.hasNext()) {        throw new NoSuchElementException();    }    if (useFirst) {        return first.next();    }    return second.next();}
public void parquet-mr_f4103_0()
{    if (useFirst) {        first.remove();    }    second.remove();}
public long parquet-mr_f4104_0()
{        return buffer.position() - startPosition;}
public int parquet-mr_f4105_0() throws IOException
{    if (!buffer.hasRemaining()) {        throw new EOFException();    }        return buffer.get() & 0xFF;}
public int parquet-mr_f4106_0(byte[] bytes, int offset, int length) throws IOException
{    if (length == 0) {        return 0;    }    int remaining = buffer.remaining();    if (remaining <= 0) {        return -1;    }    int bytesToRead = Math.min(buffer.remaining(), length);    buffer.get(bytes, offset, bytesToRead);    return bytesToRead;}
public long parquet-mr_f4107_0(long n)
{    if (n == 0) {        return 0;    }    if (buffer.remaining() <= 0) {        return -1;    }        int bytesToSkip = (int) Math.min(buffer.remaining(), n);    buffer.position(buffer.position() + bytesToSkip);    return bytesToSkip;}
public int parquet-mr_f4108_0(ByteBuffer out)
{    int bytesToCopy;    ByteBuffer copyBuffer;    if (buffer.remaining() <= out.remaining()) {                bytesToCopy = buffer.remaining();        copyBuffer = buffer;    } else {                bytesToCopy = out.remaining();        copyBuffer = buffer.duplicate();        copyBuffer.limit(buffer.position() + bytesToCopy);        buffer.position(buffer.position() + bytesToCopy);    }    out.put(copyBuffer);    out.flip();    return bytesToCopy;}
public ByteBuffer parquet-mr_f4109_0(int length) throws EOFException
{    if (buffer.remaining() < length) {        throw new EOFException();    }        ByteBuffer copy = buffer.duplicate();    copy.limit(copy.position() + length);    buffer.position(buffer.position() + length);    return copy;}
public List<ByteBuffer> parquet-mr_f4110_0(long length) throws EOFException
{    if (length == 0) {        return Collections.emptyList();    }    if (length > buffer.remaining()) {        throw new EOFException();    }        return Collections.singletonList(slice((int) length));}
public List<ByteBuffer> parquet-mr_f4111_0()
{    if (buffer.remaining() <= 0) {        return Collections.emptyList();    }    ByteBuffer remaining = buffer.duplicate();    buffer.position(buffer.limit());    return Collections.singletonList(remaining);}
public void parquet-mr_f4112_0(int readlimit)
{    this.mark = buffer.position();}
public void parquet-mr_f4113_0() throws IOException
{    if (mark >= 0) {        buffer.position(mark);        this.mark = -1;    } else {        throw new IOException("No mark defined");    }}
public boolean parquet-mr_f4114_0()
{    return true;}
public int parquet-mr_f4115_0()
{    return buffer.remaining();}
public static void parquet-mr_f4116_0(Closeable c) throws IOException
{    if (c == null) {        return;    }    c.close();}
public static void parquet-mr_f4117_1(Closeable c)
{    if (c == null) {        return;    }    try {        c.close();    } catch (IOException e) {            }}
public static void parquet-mr_f4118_0(Throwable t, Class<E> excClass) throws E
{    if (excClass.isAssignableFrom(t.getClass())) {                throw excClass.cast(t);    }}
public static List<String> parquet-mr_f4119_0(File file, Charset charset) throws IOException
{    BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(file), charset));    try {        List<String> result = new ArrayList<String>();        for (; ; ) {            String line = reader.readLine();            if (line == null)                break;            result.add(line);        }        return result;    } finally {        reader.close();    }}
public static List<String> parquet-mr_f4120_0(String globPattern)
{    return GlobExpanderImpl.expand(GlobParser.parse(globPattern));}
public static List<String> parquet-mr_f4121_0(GlobNode node)
{    return node.accept(INSTANCE);}
public List<String> parquet-mr_f4122_0(Atom atom)
{        return Arrays.asList(atom.get());}
public List<String> parquet-mr_f4123_0(OneOf oneOf)
{                List<String> results = new ArrayList<String>();    for (GlobNode n : oneOf.getChildren()) {        results.addAll(n.accept(this));    }    return results;}
public List<String> parquet-mr_f4124_0(GlobNodeSequence seq)
{                    List<String> results = new ArrayList<String>();    for (GlobNode n : seq.getChildren()) {        results = crossOrTakeNonEmpty(results, n.accept(this));    }    return results;}
public static List<String> parquet-mr_f4125_0(List<String> list1, List<String> list2)
{    if (list1.isEmpty()) {        ArrayList<String> result = new ArrayList<String>(list2.size());        result.addAll(list2);        return result;    }    if (list2.isEmpty()) {        ArrayList<String> result = new ArrayList<String>(list1.size());        result.addAll(list1);        return result;    }    List<String> result = new ArrayList<String>(list1.size() * list2.size());    for (String s1 : list1) {        for (String s2 : list2) {            result.add(s1 + s2);        }    }    return result;}
public String parquet-mr_f4126_0()
{    return s;}
public boolean parquet-mr_f4127_0(Object o)
{    if (this == o)        return true;    return o != null && getClass() == o.getClass() && s.equals(((Atom) o).s);}
public int parquet-mr_f4128_0()
{    return s.hashCode();}
public String parquet-mr_f4129_0()
{    return "Atom(" + s + ")";}
public R parquet-mr_f4130_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public List<GlobNode> parquet-mr_f4131_0()
{    return children;}
public boolean parquet-mr_f4132_0(Object o)
{    if (this == o)        return true;    return o != null && getClass() == o.getClass() && children.equals(((OneOf) o).children);}
public int parquet-mr_f4133_0()
{    return children.hashCode();}
public String parquet-mr_f4134_0()
{    return "OneOf" + children;}
public R parquet-mr_f4135_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public List<GlobNode> parquet-mr_f4136_0()
{    return children;}
public boolean parquet-mr_f4137_0(Object o)
{    if (this == o)        return true;    return o != null && getClass() == o.getClass() && children.equals(((OneOf) o).children);}
public int parquet-mr_f4138_0()
{    return children.hashCode();}
public String parquet-mr_f4139_0()
{    return "GlobNodeSequence" + children;}
public R parquet-mr_f4140_0(Visitor<R> visitor)
{    return visitor.visit(this);}
public static GlobNodeSequence parquet-mr_f4141_0(String pattern)
{    if (pattern.isEmpty() || pattern.equals("{}")) {        return new GlobNodeSequence(Arrays.<GlobNode>asList(new Atom("")));    }            List<GlobNode> children = new ArrayList<GlobNode>();        int unmatchedBraces = 0;        int firstBrace = 0;        int anchor = 0;    for (int i = 0; i < pattern.length(); i++) {        char c = pattern.charAt(i);        switch(c) {            case ',':                if (unmatchedBraces == 0) {                                        throw new GlobParseException("Unexpected comma outside of a {} group:\n" + annotateMessage(pattern, i));                }                break;            case '{':                if (unmatchedBraces == 0) {                                        firstBrace = i;                }                unmatchedBraces++;                break;            case '}':                unmatchedBraces--;                if (unmatchedBraces < 0) {                    throw new GlobParseException("Unexpected closing }:\n" + annotateMessage(pattern, i));                }                if (unmatchedBraces == 0) {                                        if (anchor != firstBrace) {                                                                        children.add(new Atom(pattern.substring(anchor, firstBrace)));                    }                                                            children.add(parseOneOf(pattern.substring(firstBrace + 1, i)));                                        anchor = i + 1;                }                break;        }    }    if (unmatchedBraces > 0) {        throw new GlobParseException("Not enough close braces in: " + pattern);    }    if (anchor != pattern.length()) {                                children.add(new Atom(pattern.substring(anchor, pattern.length())));    }    return new GlobNodeSequence(children);}
private static OneOf parquet-mr_f4142_0(String pattern)
{    /*     * This method is only called when parsing the inside of a {} expression.     * So in the example above, of calling parse("apache{one,pre{x,y}post,two}parquet{a,b}")     * this method will get called on first "one,pre{x,y}post,two", then on "x,y" and then on "a,b"     *     * The inside of a {} expression essentially means "one of these comma separated expressions".     * So this gets parsed slightly differently than the top level string passed to parse().     *     * The algorithm works as follows:     * 1) Split the string on ',' -- but only commas that are not inside of {} expressions     * 2) Each of the splits can be parsed via the parse() method above     * 3) Add all parsed splits to a single parent OneOf.     */            List<GlobNode> children = new ArrayList<GlobNode>();        int unmatchedBraces = 0;        int anchor = 0;    for (int i = 0; i < pattern.length(); i++) {        char c = pattern.charAt(i);        switch(c) {            case ',':                                if (unmatchedBraces == 0) {                                                                                children.add(parse(pattern.substring(anchor, i)));                                        anchor = i + 1;                }                break;            case '{':                unmatchedBraces++;                break;            case '}':                unmatchedBraces--;                if (unmatchedBraces < 0) {                    throw new GlobParseException("Unexpected closing }:\n" + annotateMessage(pattern, i));                }                break;        }    }    if (unmatchedBraces > 0) {        throw new GlobParseException("Not enough close braces in: " + pattern);    }    if (anchor != pattern.length()) {                                children.add(parse(pattern.substring(anchor, pattern.length())));    }    if (pattern.length() > 0 && pattern.charAt(pattern.length() - 1) == ',') {                children.add(parse(""));    }    return new OneOf(children);}
private static String parquet-mr_f4143_0(String message, int pos)
{    StringBuilder sb = new StringBuilder(message);    sb.append('\n');    for (int i = 0; i < pos; i++) {        sb.append('-');    }    sb.append('^');    return sb.toString();}
public static String parquet-mr_f4144_0(String wildcardPath, char delim)
{    if (wildcardPath.isEmpty()) {        return wildcardPath;    }    String delimStr = Pattern.quote(Character.toString(delim));        String[] splits = wildcardPath.split("\\*", -1);    StringBuilder regex = new StringBuilder();    for (int i = 0; i < splits.length; i++) {        if ((i == 0 || i == splits.length - 1) && splits[i].isEmpty()) {                        regex.append(STAR_REGEX);            continue;        }        if (splits[i].isEmpty()) {                        continue;        }                        regex.append(Pattern.quote(splits[i]));        if (i < splits.length - 1) {                        regex.append(STAR_REGEX);        }    }        regex.append(String.format(MORE_NESTED_FIELDS_TEMPLATE, delimStr));    return regex.toString();}
public boolean parquet-mr_f4145_0(String path)
{    return pattern.matcher(path).matches();}
public String parquet-mr_f4146_0()
{    return parentGlobPath;}
public String parquet-mr_f4147_0()
{    return originalPattern;}
public String parquet-mr_f4148_0()
{    return String.format("WildcardPath(parentGlobPath: '%s', pattern: '%s')", parentGlobPath, originalPattern);}
public boolean parquet-mr_f4149_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    WildcardPath wildcardPath = (WildcardPath) o;    return originalPattern.equals(wildcardPath.originalPattern);}
public int parquet-mr_f4150_0()
{    return originalPattern.hashCode();}
public Class parquet-mr_f4151_0()
{    return codecClass;}
public final T parquet-mr_f4152_0(T value)
{    T canonical = canonicals.get(value);    if (canonical == null) {        value = toCanonical(value);        T existing = canonicals.putIfAbsent(value, value);                if (existing == null) {            canonical = value;        } else {            canonical = existing;        }    }    return canonical;}
protected T parquet-mr_f4153_0(T value)
{    return value;}
protected ColumnPath parquet-mr_f4154_0(ColumnPath value)
{    String[] path = new String[value.p.length];    for (int i = 0; i < value.p.length; i++) {        path[i] = value.p[i].intern();    }    return new ColumnPath(path);}
public static ColumnPath parquet-mr_f4155_0(String path)
{    checkNotNull(path, "path");    return get(path.split("\\."));}
public static ColumnPath parquet-mr_f4156_0(String... path)
{    return paths.canonicalize(new ColumnPath(path));}
public boolean parquet-mr_f4157_0(Object obj)
{    if (obj instanceof ColumnPath) {        return Arrays.equals(p, ((ColumnPath) obj).p);    }    return false;}
public int parquet-mr_f4158_0()
{    return Arrays.hashCode(p);}
public String parquet-mr_f4159_0()
{    return Strings.join(p, ".");}
public String parquet-mr_f4160_0()
{    return Arrays.toString(p);}
public Iterator<String> parquet-mr_f4161_0()
{    return Arrays.asList(p).iterator();}
public int parquet-mr_f4162_0()
{    return p.length;}
public String[] parquet-mr_f4163_0()
{    return p;}
public static CompressionCodecName parquet-mr_f4164_0(String name)
{    if (name == null) {        return UNCOMPRESSED;    }    return valueOf(name.toUpperCase(Locale.ENGLISH));}
public static CompressionCodecName parquet-mr_f4165_0(Class<?> clazz)
{    if (clazz == null) {        return UNCOMPRESSED;    }    String name = clazz.getName();    for (CompressionCodecName codec : CompressionCodecName.values()) {        if (name.equals(codec.getHadoopCompressionCodecClassName())) {            return codec;        }    }    throw new CompressionCodecNotSupportedException(clazz);}
public static CompressionCodecName parquet-mr_f4166_0(CompressionCodec codec)
{    for (CompressionCodecName codecName : CompressionCodecName.values()) {        if (codec.equals(codecName.parquetCompressionCodec)) {            return codecName;        }    }    throw new IllegalArgumentException("Unknown compression codec " + codec);}
public String parquet-mr_f4167_0()
{    return hadoopCompressionCodecClass;}
public Class parquet-mr_f4168_0()
{    String codecClassName = getHadoopCompressionCodecClassName();    if (codecClassName == null) {        return null;    }    try {        return Class.forName(codecClassName);    } catch (ClassNotFoundException e) {        return null;    }}
public CompressionCodec parquet-mr_f4169_0()
{    return parquetCompressionCodec;}
public String parquet-mr_f4170_0()
{    return extension;}
public static int parquet-mr_f4171_0(long value)
{    int valueI = (int) value;    if (valueI != value) {        throw new IllegalArgumentException(String.format("Overflow casting %d to an int", value));    }    return valueI;}
public OutputStream parquet-mr_f4172_0()
{    return stream;}
public void parquet-mr_f4173_0() throws IOException
{    stream.close();}
public void parquet-mr_f4174_0() throws IOException
{    stream.flush();}
public void parquet-mr_f4175_0(int b) throws IOException
{    stream.write(b);}
public void parquet-mr_f4176_0(byte[] b) throws IOException
{    stream.write(b);}
public void parquet-mr_f4177_0(byte[] b, int off, int len) throws IOException
{    stream.write(b, off, len);}
public InputStream parquet-mr_f4178_0()
{    return stream;}
public void parquet-mr_f4179_0() throws IOException
{    stream.close();}
public int parquet-mr_f4180_0() throws IOException
{    return stream.read();}
public int parquet-mr_f4181_0(byte[] b, int off, int len) throws IOException
{    return stream.read(b, off, len);}
public void parquet-mr_f4182_0(byte[] bytes) throws IOException
{    readFully(stream, bytes, 0, bytes.length);}
public void parquet-mr_f4183_0(byte[] bytes, int start, int len) throws IOException
{    readFully(stream, bytes, start, len);}
public int parquet-mr_f4184_0(ByteBuffer buf) throws IOException
{    if (buf.hasArray()) {        return readHeapBuffer(stream, buf);    } else {        return readDirectBuffer(stream, buf, temp);    }}
public void parquet-mr_f4185_0(ByteBuffer buf) throws IOException
{    if (buf.hasArray()) {        readFullyHeapBuffer(stream, buf);    } else {        readFullyDirectBuffer(stream, buf, temp);    }}
 static void parquet-mr_f4186_0(InputStream f, byte[] bytes, int start, int len) throws IOException
{    int offset = start;    int remaining = len;    while (remaining > 0) {        int bytesRead = f.read(bytes, offset, remaining);        if (bytesRead < 0) {            throw new EOFException("Reached the end of stream with " + remaining + " bytes left to read");        }        remaining -= bytesRead;        offset += bytesRead;    }}
 static int parquet-mr_f4187_0(InputStream f, ByteBuffer buf) throws IOException
{    int bytesRead = f.read(buf.array(), buf.arrayOffset() + buf.position(), buf.remaining());    if (bytesRead < 0) {                return bytesRead;    } else {        buf.position(buf.position() + bytesRead);        return bytesRead;    }}
 static void parquet-mr_f4188_0(InputStream f, ByteBuffer buf) throws IOException
{    readFully(f, buf.array(), buf.arrayOffset() + buf.position(), buf.remaining());    buf.position(buf.limit());}
 static int parquet-mr_f4189_0(InputStream f, ByteBuffer buf, byte[] temp) throws IOException
{            int nextReadLength = Math.min(buf.remaining(), temp.length);    int totalBytesRead = 0;    int bytesRead;    while ((bytesRead = f.read(temp, 0, nextReadLength)) == temp.length) {        buf.put(temp);        totalBytesRead += bytesRead;        nextReadLength = Math.min(buf.remaining(), temp.length);    }    if (bytesRead < 0) {                return totalBytesRead == 0 ? -1 : totalBytesRead;    } else {                buf.put(temp, 0, bytesRead);        totalBytesRead += bytesRead;        return totalBytesRead;    }}
 static void parquet-mr_f4190_0(InputStream f, ByteBuffer buf, byte[] temp) throws IOException
{    int nextReadLength = Math.min(buf.remaining(), temp.length);    int bytesRead = 0;    while (nextReadLength > 0 && (bytesRead = f.read(temp, 0, nextReadLength)) >= 0) {        buf.put(temp, 0, bytesRead);        nextReadLength = Math.min(buf.remaining(), temp.length);    }    if (bytesRead < 0 && buf.remaining() > 0) {        throw new EOFException("Reached the end of stream with " + buf.remaining() + " bytes left to read");    }}
public static void parquet-mr_f4191_0(Closeable closeable)
{    try {        closeable.close();    } catch (IOException e) {        throw new ParquetRuntimeException("Error closing I/O related resources.", e) {        };    }}
public static Log parquet-mr_f4192_0(Class<?> c)
{    return new Log(c);}
public void parquet-mr_f4193_1(Object m)
{    if (m instanceof Throwable) {            } else {            }}
public void parquet-mr_f4194_1(Object m, Throwable t)
{    }
public void parquet-mr_f4195_1(Object m)
{    if (m instanceof Throwable) {            } else {            }}
public void parquet-mr_f4196_1(Object m, Throwable t)
{    }
public void parquet-mr_f4197_1(Object m)
{    if (m instanceof Throwable) {            } else {            }}
public void parquet-mr_f4198_1(Object m, Throwable t)
{    }
public void parquet-mr_f4199_1(Object m)
{    if (m instanceof Throwable) {            } else {            }}
public void parquet-mr_f4200_1(Object m, Throwable t)
{    }
public static T parquet-mr_f4201_0(T o, String name) throws NullPointerException
{    if (o == null) {        throw new NullPointerException(name + " should not be null");    }    return o;}
public static void parquet-mr_f4202_0(boolean isValid, String message) throws IllegalArgumentException
{    if (!isValid) {        throw new IllegalArgumentException(message);    }}
public static void parquet-mr_f4203_0(boolean isValid, String message, Object... args) throws IllegalArgumentException
{    if (!isValid) {        throw new IllegalArgumentException(String.format(String.valueOf(message), strings(args)));    }}
public static void parquet-mr_f4204_0(boolean isValid, String message) throws IllegalStateException
{    if (!isValid) {        throw new IllegalStateException(message);    }}
public static void parquet-mr_f4205_0(boolean isValid, String message, Object... args) throws IllegalStateException
{    if (!isValid) {        throw new IllegalStateException(String.format(String.valueOf(message), strings(args)));    }}
private static String[] parquet-mr_f4206_0(Object[] objects)
{    String[] strings = new String[objects.length];    for (int i = 0; i < objects.length; i += 1) {        strings[i] = String.valueOf(objects[i]);    }    return strings;}
public static SemanticVersion parquet-mr_f4207_0(String version) throws SemanticVersionParseException
{    Matcher matcher = PATTERN.matcher(version);    if (!matcher.matches()) {        throw new SemanticVersionParseException("" + version + " does not match format " + FORMAT);    }    final int major;    final int minor;    final int patch;    try {        major = Integer.valueOf(matcher.group(1));        minor = Integer.valueOf(matcher.group(2));        patch = Integer.valueOf(matcher.group(3));    } catch (NumberFormatException e) {        throw new SemanticVersionParseException(e);    }    final String unknown = matcher.group(4);    final String prerelease = matcher.group(5);    final String buildInfo = matcher.group(6);    if (major < 0 || minor < 0 || patch < 0) {        throw new SemanticVersionParseException(String.format("major(%d), minor(%d), and patch(%d) must all be >= 0", major, minor, patch));    }    return new SemanticVersion(major, minor, patch, unknown, prerelease, buildInfo);}
public int parquet-mr_f4208_0(SemanticVersion o)
{    int cmp;    cmp = compareIntegers(major, o.major);    if (cmp != 0) {        return cmp;    }    cmp = compareIntegers(minor, o.minor);    if (cmp != 0) {        return cmp;    }    cmp = compareIntegers(patch, o.patch);    if (cmp != 0) {        return cmp;    }    cmp = compareBooleans(o.prerelease, prerelease);    if (cmp != 0) {        return cmp;    }    if (pre != null) {        if (o.pre != null) {            return pre.compareTo(o.pre);        } else {            return -1;        }    } else if (o.pre != null) {        return 1;    }    return 0;}
private static int parquet-mr_f4209_0(int x, int y)
{    return (x < y) ? -1 : ((x == y) ? 0 : 1);}
private static int parquet-mr_f4210_0(boolean x, boolean y)
{    return (x == y) ? 0 : (x ? 1 : -1);}
public boolean parquet-mr_f4211_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    SemanticVersion that = (SemanticVersion) o;    return compareTo(that) == 0;}
public int parquet-mr_f4212_0()
{    int result = major;    result = 31 * result + minor;    result = 31 * result + patch;    return result;}
public String parquet-mr_f4213_0()
{    StringBuilder sb = new StringBuilder();    sb.append(major).append(".").append(minor).append(".").append(patch);    if (prerelease) {        sb.append(unknown);    }    if (pre != null) {        sb.append(pre.original);    }    if (buildInfo != null) {        sb.append(buildInfo);    }    return sb.toString();}
public int parquet-mr_f4214_0(NumberOrString that)
{        int cmp = compareBooleans(that.isNumeric, this.isNumeric);    if (cmp != 0) {        return cmp;    }    if (isNumeric) {                return compareIntegers(this.number, that.number);    }        return this.original.compareTo(that.original);}
public String parquet-mr_f4215_0()
{    return original;}
public int parquet-mr_f4216_0(Prerelease that)
{            int size = Math.min(this.identifiers.size(), that.identifiers.size());    for (int i = 0; i < size; i += 1) {        int cmp = identifiers.get(i).compareTo(that.identifiers.get(i));        if (cmp != 0) {            return cmp;        }    }    return compareIntegers(this.identifiers.size(), that.identifiers.size());}
public String parquet-mr_f4217_0()
{    return original;}
public static String parquet-mr_f4218_0(Iterable<String> s, String on)
{    return join(s.iterator(), on);}
public static String parquet-mr_f4219_0(Iterator<String> iter, String on)
{    StringBuilder sb = new StringBuilder();    while (iter.hasNext()) {        sb.append(iter.next());        if (iter.hasNext()) {            sb.append(on);        }    }    return sb.toString();}
public static String parquet-mr_f4220_0(String[] s, String on)
{    return join(Arrays.asList(s), on);}
public static boolean parquet-mr_f4221_0(String s)
{    return s == null || s.isEmpty();}
public static List<String> parquet-mr_f4222_0(String globPattern)
{    return GlobExpander.expand(globPattern);}
public static List<WildcardPath> parquet-mr_f4223_0(String globPattern, char delim)
{    List<WildcardPath> ret = new ArrayList<WildcardPath>();    for (String expandedGlob : Strings.expandGlob(globPattern)) {        ret.add(new WildcardPath(globPattern, expandedGlob, delim));    }    return ret;}
public Class<? extends C> parquet-mr_f4224_0()
{    return constructed;}
public C parquet-mr_f4225_0(Object... args) throws Exception
{    try {        return ctor.newInstance(args);    } catch (InstantiationException e) {        throw e;    } catch (IllegalAccessException e) {        throw e;    } catch (InvocationTargetException e) {        throwIfInstance(e.getCause(), Exception.class);        throwIfInstance(e.getCause(), RuntimeException.class);        throw new RuntimeException(e.getCause());    }}
public C parquet-mr_f4226_0(Object... args)
{    try {        return newInstanceChecked(args);    } catch (Exception e) {        throwIfInstance(e, RuntimeException.class);        throw new RuntimeException(e);    }}
public R parquet-mr_f4227_0(Object target, Object... args)
{    Preconditions.checkArgument(target == null, "Invalid call to constructor: target must be null");    return (R) newInstance(args);}
public R parquet-mr_f4228_0(Object target, Object... args) throws Exception
{    Preconditions.checkArgument(target == null, "Invalid call to constructor: target must be null");    return (R) newInstanceChecked(args);}
public DynMethods.BoundMethod parquet-mr_f4229_0(Object receiver)
{    throw new IllegalStateException("Cannot bind constructors");}
public boolean parquet-mr_f4230_0()
{    return true;}
public String parquet-mr_f4231_0()
{    return getClass().getSimpleName() + "(constructor=" + ctor + ", class=" + constructed + ")";}
public Builder parquet-mr_f4232_0(ClassLoader loader)
{    this.loader = loader;    return this;}
public Builder parquet-mr_f4233_0(String className, Class<?>... types)
{        if (ctor != null) {        return this;    }    try {        Class<?> targetClass = Class.forName(className, true, loader);        impl(targetClass, types);    } catch (NoClassDefFoundError e) {                problems.put(className, e);    } catch (ClassNotFoundException e) {                problems.put(className, e);    }    return this;}
public Builder parquet-mr_f4234_0(Class<T> targetClass, Class<?>... types)
{        if (ctor != null) {        return this;    }    try {        ctor = new Ctor<T>(targetClass.getConstructor(types), targetClass);    } catch (NoSuchMethodException e) {                problems.put(methodName(targetClass, types), e);    }    return this;}
public Builder parquet-mr_f4235_0(Class<?>... types)
{    hiddenImpl(baseClass, types);    return this;}
public Builder parquet-mr_f4236_0(String className, Class<?>... types)
{        if (ctor != null) {        return this;    }    try {        Class targetClass = Class.forName(className, true, loader);        hiddenImpl(targetClass, types);    } catch (NoClassDefFoundError e) {                problems.put(className, e);    } catch (ClassNotFoundException e) {                problems.put(className, e);    }    return this;}
public Builder parquet-mr_f4237_0(Class<T> targetClass, Class<?>... types)
{        if (ctor != null) {        return this;    }    try {        Constructor<T> hidden = targetClass.getDeclaredConstructor(types);        AccessController.doPrivileged(new MakeAccessible(hidden));        ctor = new Ctor<T>(hidden, targetClass);    } catch (SecurityException e) {                problems.put(methodName(targetClass, types), e);    } catch (NoSuchMethodException e) {                problems.put(methodName(targetClass, types), e);    }    return this;}
public Ctor<C> parquet-mr_f4238_0() throws NoSuchMethodException
{    if (ctor != null) {        return ctor;    }    throw new NoSuchMethodException("Cannot find constructor for " + baseClass + "\n" + formatProblems(problems));}
public Ctor<C> parquet-mr_f4239_0()
{    if (ctor != null) {        return ctor;    }    throw new RuntimeException("Cannot find constructor for " + baseClass + "\n" + formatProblems(problems));}
public Void parquet-mr_f4240_0()
{    hidden.setAccessible(true);    return null;}
private static String parquet-mr_f4241_0(Map<String, Throwable> problems)
{    StringBuilder sb = new StringBuilder();    boolean first = true;    for (Map.Entry<String, Throwable> problem : problems.entrySet()) {        if (first) {            first = false;        } else {            sb.append("\n");        }        sb.append("\tMissing ").append(problem.getKey()).append(" [").append(problem.getValue().getClass().getName()).append(": ").append(problem.getValue().getMessage()).append("]");    }    return sb.toString();}
private static String parquet-mr_f4242_0(Class<?> targetClass, Class<?>... types)
{    StringBuilder sb = new StringBuilder();    sb.append(targetClass.getName()).append("(");    boolean first = true;    for (Class<?> type : types) {        if (first) {            first = false;        } else {            sb.append(",");        }        sb.append(type.getName());    }    sb.append(")");    return sb.toString();}
public R parquet-mr_f4243_0(Object target, Object... args) throws Exception
{    try {        if (argLength < 0) {            return (R) method.invoke(target, args);        } else {            return (R) method.invoke(target, Arrays.copyOfRange(args, 0, argLength));        }    } catch (InvocationTargetException e) {        throwIfInstance(e.getCause(), Exception.class);        throwIfInstance(e.getCause(), RuntimeException.class);        throw new RuntimeException(e.getCause());    }}
public R parquet-mr_f4244_0(Object target, Object... args)
{    try {        return this.<R>invokeChecked(target, args);    } catch (Exception e) {        throwIfInstance(e, RuntimeException.class);        throw new RuntimeException(e);    }}
public BoundMethod parquet-mr_f4245_0(Object receiver)
{    Preconditions.checkState(!isStatic(), "Cannot bind static method " + method.toGenericString());    Preconditions.checkArgument(method.getDeclaringClass().isAssignableFrom(receiver.getClass()), "Cannot bind " + method.toGenericString() + " to instance of " + receiver.getClass());    return new BoundMethod(this, receiver);}
public boolean parquet-mr_f4246_0()
{    return Modifier.isStatic(method.getModifiers());}
public boolean parquet-mr_f4247_0()
{    return this == NOOP;}
public StaticMethod parquet-mr_f4248_0()
{    Preconditions.checkState(isStatic(), "Method is not static");    return new StaticMethod(this);}
public String parquet-mr_f4249_0()
{    return "DynMethods.UnboundMethod(name=" + name + " method=" + method.toGenericString() + ")";}
public R parquet-mr_f4250_0(Object target, Object... args) throws Exception
{    return null;}
public BoundMethod parquet-mr_f4251_0(Object receiver)
{    return new BoundMethod(this, receiver);}
public StaticMethod parquet-mr_f4252_0()
{    return new StaticMethod(this);}
public boolean parquet-mr_f4253_0()
{    return true;}
public String parquet-mr_f4254_0()
{    return "DynMethods.UnboundMethod(NOOP)";}
public R parquet-mr_f4255_0(Object... args) throws Exception
{    return method.invokeChecked(receiver, args);}
public R parquet-mr_f4256_0(Object... args)
{    return method.invoke(receiver, args);}
public R parquet-mr_f4257_0(Object... args) throws Exception
{    return method.invokeChecked(null, args);}
public R parquet-mr_f4258_0(Object... args)
{    return method.invoke(null, args);}
public Builder parquet-mr_f4259_0(ClassLoader loader)
{    this.loader = loader;    return this;}
public Builder parquet-mr_f4260_0()
{    if (method == null) {        this.method = UnboundMethod.NOOP;    }    return this;}
public Builder parquet-mr_f4261_0(String className, String methodName, Class<?>... argClasses)
{        if (method != null) {        return this;    }    try {        Class<?> targetClass = Class.forName(className, true, loader);        impl(targetClass, methodName, argClasses);    } catch (ClassNotFoundException e) {        }    return this;}
public Builder parquet-mr_f4262_0(String className, Class<?>... argClasses)
{    impl(className, name, argClasses);    return this;}
public Builder parquet-mr_f4263_0(Class<?> targetClass, String methodName, Class<?>... argClasses)
{        if (method != null) {        return this;    }    try {        this.method = new UnboundMethod(targetClass.getMethod(methodName, argClasses), name);    } catch (NoSuchMethodException e) {        }    return this;}
public Builder parquet-mr_f4264_0(Class<?> targetClass, Class<?>... argClasses)
{    impl(targetClass, name, argClasses);    return this;}
public Builder parquet-mr_f4265_0(Class<?> targetClass, Class<?>... argClasses)
{        if (method != null) {        return this;    }    try {        this.method = new DynConstructors.Builder().impl(targetClass, argClasses).buildChecked();    } catch (NoSuchMethodException e) {        }    return this;}
public Builder parquet-mr_f4266_0(String className, Class<?>... argClasses)
{        if (method != null) {        return this;    }    try {        this.method = new DynConstructors.Builder().impl(className, argClasses).buildChecked();    } catch (NoSuchMethodException e) {        }    return this;}
public Builder parquet-mr_f4267_0(String className, String methodName, Class<?>... argClasses)
{        if (method != null) {        return this;    }    try {        Class<?> targetClass = Class.forName(className, true, loader);        hiddenImpl(targetClass, methodName, argClasses);    } catch (ClassNotFoundException e) {        }    return this;}
public Builder parquet-mr_f4268_0(String className, Class<?>... argClasses)
{    hiddenImpl(className, name, argClasses);    return this;}
public Builder parquet-mr_f4269_0(Class<?> targetClass, String methodName, Class<?>... argClasses)
{        if (method != null) {        return this;    }    try {        Method hidden = targetClass.getDeclaredMethod(methodName, argClasses);        AccessController.doPrivileged(new MakeAccessible(hidden));        this.method = new UnboundMethod(hidden, name);    } catch (SecurityException e) {        } catch (NoSuchMethodException e) {        }    return this;}
public Builder parquet-mr_f4270_0(Class<?> targetClass, Class<?>... argClasses)
{    hiddenImpl(targetClass, name, argClasses);    return this;}
public UnboundMethod parquet-mr_f4271_0() throws NoSuchMethodException
{    if (method != null) {        return method;    } else {        throw new NoSuchMethodException("Cannot find method: " + name);    }}
public UnboundMethod parquet-mr_f4272_0()
{    if (method != null) {        return method;    } else {        throw new RuntimeException("Cannot find method: " + name);    }}
public BoundMethod parquet-mr_f4273_0(Object receiver) throws NoSuchMethodException
{    return buildChecked().bind(receiver);}
public BoundMethod parquet-mr_f4274_0(Object receiver)
{    return build().bind(receiver);}
public StaticMethod parquet-mr_f4275_0() throws NoSuchMethodException
{    return buildChecked().asStatic();}
public StaticMethod parquet-mr_f4276_0()
{    return build().asStatic();}
public Void parquet-mr_f4277_0()
{    hidden.setAccessible(true);    return null;}
public boolean parquet-mr_f4278_0()
{    return hasSemver;}
public SemanticVersion parquet-mr_f4279_0()
{    return semver;}
public boolean parquet-mr_f4280_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ParsedVersion version = (ParsedVersion) o;    if (appBuildHash != null ? !appBuildHash.equals(version.appBuildHash) : version.appBuildHash != null)        return false;    if (application != null ? !application.equals(version.application) : version.application != null)        return false;    if (this.version != null ? !this.version.equals(version.version) : version.version != null)        return false;    return true;}
public int parquet-mr_f4281_0()
{    int result = application != null ? application.hashCode() : 0;    result = 31 * result + (version != null ? version.hashCode() : 0);    result = 31 * result + (appBuildHash != null ? appBuildHash.hashCode() : 0);    return result;}
public String parquet-mr_f4282_0()
{    return "ParsedVersion(" + "application=" + application + ", semver=" + version + ", appBuildHash=" + appBuildHash + ')';}
public static ParsedVersion parquet-mr_f4283_0(String createdBy) throws VersionParseException
{    Matcher matcher = PATTERN.matcher(createdBy);    if (!matcher.matches()) {        throw new VersionParseException("Could not parse created_by: " + createdBy + " using format: " + FORMAT);    }    String application = matcher.group(1);    String semver = matcher.group(2);    String appBuildHash = matcher.group(3);    if (Strings.isNullOrEmpty(application)) {        throw new VersionParseException("application cannot be null or empty");    }    return new ParsedVersion(application, semver, appBuildHash);}
public void parquet-mr_f4284_0() throws Exception
{    byte[] bytes = new byte[0];    ByteBufferInputStream stream = newStream();    Assert.assertEquals("Should read 0 bytes", 0, stream.read(bytes));    int bytesRead = stream.read(new byte[100]);    Assert.assertTrue("Should read to end of stream", bytesRead < 100);    Assert.assertEquals("Should read 0 bytes at end of stream", 0, stream.read(bytes));}
public void parquet-mr_f4285_0() throws Exception
{    byte[] bytes = new byte[35];    ByteBufferInputStream stream = newStream();    int bytesRead = stream.read(bytes);    Assert.assertEquals("Should read the entire buffer", bytes.length, bytesRead);    for (int i = 0; i < bytes.length; i += 1) {        Assert.assertEquals("Byte i should be i", i, bytes[i]);        Assert.assertEquals("Should advance position", 35, stream.position());    }    Assert.assertEquals("Should have no more remaining content", 0, stream.available());    Assert.assertEquals("Should return -1 at end of stream", -1, stream.read(bytes));    Assert.assertEquals("Should have no more remaining content", 0, stream.available());    checkOriginalData();}
public void parquet-mr_f4286_0() throws Exception
{    for (int size = 1; size < 36; size += 1) {        byte[] bytes = new byte[size];        ByteBufferInputStream stream = newStream();        long length = stream.available();        int lastBytesRead = bytes.length;        for (int offset = 0; offset < length; offset += bytes.length) {            Assert.assertEquals("Should read requested len", bytes.length, lastBytesRead);            lastBytesRead = stream.read(bytes, 0, bytes.length);            Assert.assertEquals("Should advance position", offset + lastBytesRead, stream.position());                        for (int i = 0; i < lastBytesRead; i += 1) {                Assert.assertEquals("Byte i should be i", offset + i, bytes[i]);            }        }        Assert.assertEquals("Should read fewer bytes at end of buffer", length % bytes.length, lastBytesRead % bytes.length);        Assert.assertEquals("Should have no more remaining content", 0, stream.available());        Assert.assertEquals("Should return -1 at end of stream", -1, stream.read(bytes));        Assert.assertEquals("Should have no more remaining content", 0, stream.available());    }    checkOriginalData();}
public void parquet-mr_f4287_0() throws Exception
{    for (int size = 1; size < 35; size += 1) {        byte[] bytes = new byte[33];        ByteBufferInputStream stream = newStream();        int lastBytesRead = size;        for (int offset = 0; offset < bytes.length; offset += size) {            Assert.assertEquals("Should read requested len", size, lastBytesRead);            lastBytesRead = stream.read(bytes, offset, Math.min(size, bytes.length - offset));            Assert.assertEquals("Should advance position", lastBytesRead > 0 ? offset + lastBytesRead : offset, stream.position());        }        Assert.assertEquals("Should read fewer bytes at end of buffer", bytes.length % size, lastBytesRead % size);        for (int i = 0; i < bytes.length; i += 1) {            Assert.assertEquals("Byte i should be i", i, bytes[i]);        }        Assert.assertEquals("Should have no more remaining content", 2, stream.available());        Assert.assertEquals("Should return 2 more bytes", 2, stream.read(bytes));        Assert.assertEquals("Should have no more remaining content", 0, stream.available());        Assert.assertEquals("Should return -1 at end of stream", -1, stream.read(bytes));        Assert.assertEquals("Should have no more remaining content", 0, stream.available());    }    checkOriginalData();}
public void parquet-mr_f4288_0() throws Exception
{    final ByteBufferInputStream stream = newStream();    int length = stream.available();    for (int i = 0; i < length; i += 1) {        Assert.assertEquals("Position should increment", i, stream.position());        Assert.assertEquals(i, stream.read());    }    assertThrows("Should throw EOFException at end of stream", EOFException.class, (Callable<Integer>) stream::read);    checkOriginalData();}
public void parquet-mr_f4289_0() throws Exception
{    ByteBufferInputStream stream = newStream();    int length = stream.available();    ByteBuffer empty = stream.slice(0);    Assert.assertNotNull("slice(0) should produce a non-null buffer", empty);    Assert.assertEquals("slice(0) should produce an empty buffer", 0, empty.remaining());    Assert.assertEquals("Position should be at start", 0, stream.position());    int i = 0;    while (stream.available() > 0) {        int bytesToSlice = Math.min(stream.available(), 10);        ByteBuffer buffer = stream.slice(bytesToSlice);        for (int j = 0; j < bytesToSlice; j += 1) {            Assert.assertEquals("Data should be correct", i + j, buffer.get());        }        i += bytesToSlice;    }    Assert.assertEquals("Position should be at end", length, stream.position());    checkOriginalData();}
public void parquet-mr_f4290_0() throws Exception
{    ByteBufferInputStream stream = newStream();    Assert.assertEquals("Should return an empty list", Collections.emptyList(), stream.sliceBuffers(0));}
public void parquet-mr_f4291_0() throws Exception
{    final ByteBufferInputStream stream = newStream();    final int length = stream.available();    List<ByteBuffer> buffers = stream.sliceBuffers(stream.available());    Assert.assertEquals("Should consume all buffers", length, stream.position());    assertThrows("Should throw EOFException when empty", EOFException.class, (Callable<List<ByteBuffer>>) () -> stream.sliceBuffers(length));    ByteBufferInputStream copy = ByteBufferInputStream.wrap(buffers);    for (int i = 0; i < length; i += 1) {        Assert.assertEquals("Slice should have identical data", i, copy.read());    }    checkOriginalData();}
public void parquet-mr_f4292_0() throws Exception
{    for (int size = 1; size < 36; size += 1) {        ByteBufferInputStream stream = newStream();        int length = stream.available();        List<ByteBuffer> buffers = new ArrayList<>();        while (stream.available() > 0) {            buffers.addAll(stream.sliceBuffers(Math.min(size, stream.available())));        }        Assert.assertEquals("Should consume all content", length, stream.position());        ByteBufferInputStream newStream = new MultiBufferInputStream(buffers);        for (int i = 0; i < length; i += 1) {            Assert.assertEquals("Data should be correct", i, newStream.read());        }    }    checkOriginalData();}
public void parquet-mr_f4293_0() throws Exception
{    ByteBufferInputStream stream = newStream();    int length = stream.available();    int sliceLength = 5;    List<ByteBuffer> buffers = stream.sliceBuffers(sliceLength);    Assert.assertEquals("Should advance the original stream", length - sliceLength, stream.available());    Assert.assertEquals("Should advance the original stream position", sliceLength, stream.position());    Assert.assertEquals("Should return a slice of the first buffer", 1, buffers.size());    ByteBuffer buffer = buffers.get(0);    Assert.assertEquals("Should have requested bytes", sliceLength, buffer.remaining());            buffer.limit(sliceLength + 1);    for (int i = 0; i < sliceLength + 1; i += 1) {        Assert.assertEquals("Should have correct data", i, buffer.get());    }    Assert.assertEquals("Reading a slice shouldn't advance the original stream", sliceLength, stream.position());    Assert.assertEquals("Reading a slice shouldn't change the underlying data", sliceLength, stream.read());        buffer.limit(sliceLength + 2);    int originalValue = buffer.duplicate().get();    ByteBuffer undoBuffer = buffer.duplicate();    try {        buffer.put((byte) 255);        Assert.assertEquals("Writing to a slice shouldn't advance the original stream", sliceLength + 1, stream.position());        Assert.assertEquals("Writing to a slice should change the underlying data", 255, stream.read());    } finally {        undoBuffer.put((byte) originalValue);    }}
public void parquet-mr_f4294_0() throws Exception
{    ByteBufferInputStream stream = newStream();    while (stream.available() > 0) {        int bytesToSkip = Math.min(stream.available(), 10);        Assert.assertEquals("Should skip all, regardless of backing buffers", bytesToSkip, stream.skip(bytesToSkip));    }    stream = newStream();    Assert.assertEquals(0, stream.skip(0));    int length = stream.available();    Assert.assertEquals("Should stop at end when out of bytes", length, stream.skip(length + 10));    Assert.assertEquals("Should return -1 when at end", -1, stream.skip(10));}
public void parquet-mr_f4295_0() throws Exception
{    ByteBufferInputStream stream = newStream();    long lastPosition = 0;    while (stream.available() > 0) {        int bytesToSkip = Math.min(stream.available(), 10);        stream.skipFully(bytesToSkip);        Assert.assertEquals("Should skip all, regardless of backing buffers", bytesToSkip, stream.position() - lastPosition);        lastPosition = stream.position();    }    final ByteBufferInputStream stream2 = newStream();    stream2.skipFully(0);    Assert.assertEquals(0, stream2.position());    final int length = stream2.available();    assertThrows("Should throw when out of bytes", EOFException.class, () -> {        stream2.skipFully(length + 10);        return null;    });}
public void parquet-mr_f4296_0() throws Exception
{    ByteBufferInputStream stream = newStream();    stream.read(new byte[7]);    stream.mark(100);    long mark = stream.position();    byte[] expected = new byte[100];    int expectedBytesRead = stream.read(expected);    long end = stream.position();    stream.reset();    Assert.assertEquals("Position should return to the mark", mark, stream.position());    byte[] afterReset = new byte[100];    int bytesReadAfterReset = stream.read(afterReset);    Assert.assertEquals("Should read the same number of bytes", expectedBytesRead, bytesReadAfterReset);    Assert.assertEquals("Read should end at the same position", end, stream.position());    Assert.assertArrayEquals("Content should be equal", expected, afterReset);}
public void parquet-mr_f4297_0() throws Exception
{    ByteBufferInputStream stream = newStream();    stream.read(new byte[7]);    stream.mark(1);    stream.mark(100);    long mark = stream.position();    byte[] expected = new byte[100];    int expectedBytesRead = stream.read(expected);    long end = stream.position();    stream.reset();    Assert.assertEquals("Position should return to the mark", mark, stream.position());    byte[] afterReset = new byte[100];    int bytesReadAfterReset = stream.read(afterReset);    Assert.assertEquals("Should read the same number of bytes", expectedBytesRead, bytesReadAfterReset);    Assert.assertEquals("Read should end at the same position", end, stream.position());    Assert.assertArrayEquals("Content should be equal", expected, afterReset);}
public void parquet-mr_f4298_0() throws Exception
{    ByteBufferInputStream stream = newStream();    stream.mark(100);    long mark = stream.position();    byte[] expected = new byte[10];    Assert.assertEquals("Should read 10 bytes", 10, stream.read(expected));    long end = stream.position();    stream.reset();    Assert.assertEquals("Position should return to the mark", mark, stream.position());    byte[] afterReset = new byte[10];    Assert.assertEquals("Should read 10 bytes", 10, stream.read(afterReset));    Assert.assertEquals("Read should end at the same position", end, stream.position());    Assert.assertArrayEquals("Content should be equal", expected, afterReset);}
public void parquet-mr_f4299_0() throws Exception
{    ByteBufferInputStream stream = newStream();    int bytesRead = stream.read(new byte[100]);    Assert.assertTrue("Should read to end of stream", bytesRead < 100);    stream.mark(100);    long mark = stream.position();    byte[] expected = new byte[10];    Assert.assertEquals("Should read 0 bytes", -1, stream.read(expected));    long end = stream.position();    stream.reset();    Assert.assertEquals("Position should return to the mark", mark, stream.position());    byte[] afterReset = new byte[10];    Assert.assertEquals("Should read 0 bytes", -1, stream.read(afterReset));    Assert.assertEquals("Read should end at the same position", end, stream.position());    Assert.assertArrayEquals("Content should be equal", expected, afterReset);}
public void parquet-mr_f4300_0()
{    final ByteBufferInputStream stream = newStream();    assertThrows("Should throw an error for reset() without mark()", IOException.class, () -> {        stream.reset();        return null;    });}
public void parquet-mr_f4301_0() throws Exception
{    final ByteBufferInputStream stream = newStream();    byte[] expected = new byte[6];    stream.mark(10);    Assert.assertEquals("Should read expected bytes", expected.length, stream.read(expected));    stream.reset();    stream.mark(10);    byte[] firstRead = new byte[6];    Assert.assertEquals("Should read firstRead bytes", firstRead.length, stream.read(firstRead));    stream.reset();    byte[] secondRead = new byte[6];    Assert.assertEquals("Should read secondRead bytes", secondRead.length, stream.read(secondRead));    Assert.assertArrayEquals("First read should be correct", expected, firstRead);    Assert.assertArrayEquals("Second read should be correct", expected, secondRead);}
public void parquet-mr_f4302_0() throws Exception
{    final ByteBufferInputStream stream = newStream();    stream.mark(5);    Assert.assertEquals("Should read 5 bytes", 5, stream.read(new byte[5]));    stream.reset();    Assert.assertEquals("Should read 6 bytes", 6, stream.read(new byte[6]));    assertThrows("Should throw an error for reset() after limit", IOException.class, () -> {        stream.reset();        return null;    });}
public void parquet-mr_f4303_0() throws Exception
{    final ByteBufferInputStream stream = newStream();    stream.mark(5);    Assert.assertEquals("Should read 5 bytes", 5, stream.read(new byte[5]));    stream.reset();    assertThrows("Should throw an error for double reset()", IOException.class, () -> {        stream.reset();        return null;    });}
public void parquet-mr_f4304_0()
{    final ByteBufferInputStream stream = newStream();    ByteBuffer buffer = stream.toByteBuffer();    for (int i = 0; i < DATA_LENGTH; ++i) {        assertEquals(i, buffer.get());    }}
public static void parquet-mr_f4305_0(String message, Class<? extends Exception> expected, Callable callable)
{    try {        callable.call();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        try {            Assert.assertEquals(message, expected, actual.getClass());        } catch (AssertionError e) {            e.addSuppressed(actual);            throw e;        }    }}
public void parquet-mr_f4306_0()
{    assertEquals(0, getWidthFromMaxInt(0));    assertEquals(1, getWidthFromMaxInt(1));    assertEquals(2, getWidthFromMaxInt(2));    assertEquals(2, getWidthFromMaxInt(3));    assertEquals(3, getWidthFromMaxInt(4));    assertEquals(3, getWidthFromMaxInt(5));    assertEquals(3, getWidthFromMaxInt(6));    assertEquals(3, getWidthFromMaxInt(7));    assertEquals(4, getWidthFromMaxInt(8));    assertEquals(4, getWidthFromMaxInt(15));    assertEquals(5, getWidthFromMaxInt(16));    assertEquals(5, getWidthFromMaxInt(31));    assertEquals(6, getWidthFromMaxInt(32));    assertEquals(6, getWidthFromMaxInt(63));    assertEquals(7, getWidthFromMaxInt(64));    assertEquals(7, getWidthFromMaxInt(127));    assertEquals(8, getWidthFromMaxInt(128));    assertEquals(8, getWidthFromMaxInt(255));}
public static List<Object[]> parquet-mr_f4307_0()
{    return Arrays.asList(new Object[] { TestSingleBufferInputStream.DATA, null }, new Object[] { TestSingleBufferInputStream.DATA, 0 }, new Object[] { ByteBuffer.wrap(new byte[] { -1, -1, -1, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34 }), 4 }, new Object[] { ByteBuffer.wrap(new byte[] { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, -1, -1, -1 }), 0 }, new Object[] { ByteBuffer.wrap(new byte[] { -1, -1, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, -1, -1 }), 3 });}
protected ByteBufferInputStream parquet-mr_f4308_0()
{    if (offset == null) {        return new ByteBufferInputStream(data);    } else {        return new ByteBufferInputStream(data, offset, DATA_LENGTH);    }}
protected void parquet-mr_f4309_0()
{    Assert.assertEquals("Position should not change", 0, data.position());    Assert.assertEquals("Limit should not change", data.array().length, data.limit());}
public void parquet-mr_f4310_0() throws Exception
{    ByteBufferInputStream stream = newStream();    int length = stream.available();    List<ByteBuffer> buffers = new ArrayList<>();        while (stream.available() > 0) {        int bytesToSlice = Math.min(stream.available(), 8);        buffers.add(stream.slice(bytesToSlice));    }    Assert.assertEquals("Position should be at end", length, stream.position());    Assert.assertEquals("Should produce 5 buffers", 5, buffers.size());    int i = 0;    ByteBuffer one = buffers.get(0);    Assert.assertSame("Should use the same backing array", one.array(), data.array());    Assert.assertEquals(8, one.remaining());    Assert.assertEquals(0, one.position());    Assert.assertEquals(8, one.limit());    for (; i < 8; i += 1) {        Assert.assertEquals("Should produce correct values", i, one.get());    }    ByteBuffer two = buffers.get(1);    Assert.assertSame("Should use the same backing array", two.array(), data.array());    Assert.assertEquals(8, two.remaining());    Assert.assertEquals(8, two.position());    Assert.assertEquals(16, two.limit());    for (; i < 16; i += 1) {        Assert.assertEquals("Should produce correct values", i, two.get());    }        ByteBuffer three = buffers.get(2);    Assert.assertSame("Should use the same backing array", three.array(), data.array());    Assert.assertEquals(8, three.remaining());    Assert.assertEquals(16, three.position());    Assert.assertEquals(24, three.limit());    for (; i < 24; i += 1) {        Assert.assertEquals("Should produce correct values", i, three.get());    }        ByteBuffer four = buffers.get(3);    Assert.assertSame("Should use the same backing array", four.array(), data.array());    Assert.assertEquals(8, four.remaining());    Assert.assertEquals(24, four.position());    Assert.assertEquals(32, four.limit());    for (; i < 32; i += 1) {        Assert.assertEquals("Should produce correct values", i, four.get());    }        ByteBuffer five = buffers.get(4);    Assert.assertSame("Should use the same backing array", five.array(), data.array());    Assert.assertEquals(3, five.remaining());    Assert.assertEquals(32, five.position());    Assert.assertEquals(35, five.limit());    for (; i < 35; i += 1) {        Assert.assertEquals("Should produce correct values", i, five.get());    }}
public void parquet-mr_f4311_0() throws Exception
{    ByteBufferInputStream stream = newStream();    List<ByteBuffer> buffers = stream.sliceBuffers(stream.available());    Assert.assertEquals("Should return duplicates of all non-empty buffers", Collections.singletonList(TestSingleBufferInputStream.DATA), buffers);}
protected ByteBufferInputStream parquet-mr_f4312_0()
{    return new MultiBufferInputStream(DATA);}
protected void parquet-mr_f4313_0()
{    for (ByteBuffer buffer : DATA) {        Assert.assertEquals("Position should not change", 0, buffer.position());        Assert.assertEquals("Limit should not change", buffer.array().length, buffer.limit());    }}
public void parquet-mr_f4314_0() throws Exception
{    ByteBufferInputStream stream = newStream();    int length = stream.available();    List<ByteBuffer> buffers = new ArrayList<>();        while (stream.available() > 0) {        int bytesToSlice = Math.min(stream.available(), 8);        buffers.add(stream.slice(bytesToSlice));    }    Assert.assertEquals("Position should be at end", length, stream.position());    Assert.assertEquals("Should produce 5 buffers", 5, buffers.size());    int i = 0;        ByteBuffer one = buffers.get(0);    Assert.assertSame("Should be a duplicate of the first array", one.array(), DATA.get(0).array());    Assert.assertEquals(8, one.remaining());    Assert.assertEquals(0, one.position());    Assert.assertEquals(8, one.limit());    Assert.assertEquals(9, one.capacity());    for (; i < 8; i += 1) {        Assert.assertEquals("Should produce correct values", i, one.get());    }        ByteBuffer two = buffers.get(1);    Assert.assertEquals(8, two.remaining());    Assert.assertEquals(0, two.position());    Assert.assertEquals(8, two.limit());    Assert.assertEquals(8, two.capacity());    for (; i < 16; i += 1) {        Assert.assertEquals("Should produce correct values", i, two.get());    }        ByteBuffer three = buffers.get(2);    Assert.assertSame("Should be a duplicate of the fourth array", three.array(), DATA.get(3).array());    Assert.assertEquals(8, three.remaining());    Assert.assertEquals(3, three.position());    Assert.assertEquals(11, three.limit());    Assert.assertEquals(12, three.capacity());    for (; i < 24; i += 1) {        Assert.assertEquals("Should produce correct values", i, three.get());    }        ByteBuffer four = buffers.get(3);    Assert.assertEquals(8, four.remaining());    Assert.assertEquals(0, four.position());    Assert.assertEquals(8, four.limit());    Assert.assertEquals(8, four.capacity());    for (; i < 32; i += 1) {        Assert.assertEquals("Should produce correct values", i, four.get());    }        ByteBuffer five = buffers.get(4);    Assert.assertEquals(3, five.remaining());    Assert.assertEquals(0, five.position());    Assert.assertEquals(3, five.limit());    Assert.assertEquals(3, five.capacity());    for (; i < 35; i += 1) {        Assert.assertEquals("Should produce correct values", i, five.get());    }}
public void parquet-mr_f4315_0() throws Exception
{    ByteBufferInputStream stream = newStream();    List<ByteBuffer> buffers = stream.sliceBuffers(stream.available());    List<ByteBuffer> nonEmptyBuffers = new ArrayList<>();    for (ByteBuffer buffer : DATA) {        if (buffer.remaining() > 0) {            nonEmptyBuffers.add(buffer);        }    }    Assert.assertEquals("Should return duplicates of all non-empty buffers", nonEmptyBuffers, buffers);}
protected ByteBufferInputStream parquet-mr_f4316_0()
{    return new SingleBufferInputStream(DATA);}
protected void parquet-mr_f4317_0()
{    Assert.assertEquals("Position should not change", 0, DATA.position());    Assert.assertEquals("Limit should not change", DATA.array().length, DATA.limit());}
public void parquet-mr_f4318_0() throws Exception
{    ByteBufferInputStream stream = newStream();    int length = stream.available();    List<ByteBuffer> buffers = new ArrayList<>();        while (stream.available() > 0) {        int bytesToSlice = Math.min(stream.available(), 8);        buffers.add(stream.slice(bytesToSlice));    }    Assert.assertEquals("Position should be at end", length, stream.position());    Assert.assertEquals("Should produce 5 buffers", 5, buffers.size());    int i = 0;    ByteBuffer one = buffers.get(0);    Assert.assertSame("Should use the same backing array", one.array(), DATA.array());    Assert.assertEquals(8, one.remaining());    Assert.assertEquals(0, one.position());    Assert.assertEquals(8, one.limit());    Assert.assertEquals(35, one.capacity());    for (; i < 8; i += 1) {        Assert.assertEquals("Should produce correct values", i, one.get());    }    ByteBuffer two = buffers.get(1);    Assert.assertSame("Should use the same backing array", two.array(), DATA.array());    Assert.assertEquals(8, two.remaining());    Assert.assertEquals(8, two.position());    Assert.assertEquals(16, two.limit());    Assert.assertEquals(35, two.capacity());    for (; i < 16; i += 1) {        Assert.assertEquals("Should produce correct values", i, two.get());    }        ByteBuffer three = buffers.get(2);    Assert.assertSame("Should use the same backing array", three.array(), DATA.array());    Assert.assertEquals(8, three.remaining());    Assert.assertEquals(16, three.position());    Assert.assertEquals(24, three.limit());    Assert.assertEquals(35, three.capacity());    for (; i < 24; i += 1) {        Assert.assertEquals("Should produce correct values", i, three.get());    }        ByteBuffer four = buffers.get(3);    Assert.assertSame("Should use the same backing array", four.array(), DATA.array());    Assert.assertEquals(8, four.remaining());    Assert.assertEquals(24, four.position());    Assert.assertEquals(32, four.limit());    Assert.assertEquals(35, four.capacity());    for (; i < 32; i += 1) {        Assert.assertEquals("Should produce correct values", i, four.get());    }        ByteBuffer five = buffers.get(4);    Assert.assertSame("Should use the same backing array", five.array(), DATA.array());    Assert.assertEquals(3, five.remaining());    Assert.assertEquals(32, five.position());    Assert.assertEquals(35, five.limit());    Assert.assertEquals(35, five.capacity());    for (; i < 35; i += 1) {        Assert.assertEquals("Should produce correct values", i, five.get());    }}
public void parquet-mr_f4319_0() throws Exception
{    ByteBufferInputStream stream = newStream();    List<ByteBuffer> buffers = stream.sliceBuffers(stream.available());    Assert.assertEquals("Should return duplicates of all non-empty buffers", Collections.singletonList(DATA), buffers);}
public void parquet-mr_f4320_0()
{    assertEquals(Arrays.asList("foo"), Strings.expandGlob("foo"));}
public void parquet-mr_f4321_0()
{    assertEquals(Arrays.asList(""), Strings.expandGlob(""));    assertEquals(Arrays.asList(""), Strings.expandGlob("{}"));    assertEquals(Arrays.asList("a"), Strings.expandGlob("a{}"));    assertEquals(Arrays.asList("ab"), Strings.expandGlob("a{}b"));    assertEquals(Arrays.asList("a"), Strings.expandGlob("{}a"));    assertEquals(Arrays.asList("a"), Strings.expandGlob("a{}"));    assertEquals(Arrays.asList("", ""), Strings.expandGlob("{,}"));    assertEquals(Arrays.asList("ab", "a", "ac"), Strings.expandGlob("a{b,{},c}"));}
public void parquet-mr_f4322_0()
{    assertEquals(Arrays.asList("foobar", "foobaz"), Strings.expandGlob("foo{bar,baz}"));    assertEquals(Arrays.asList("startfooend", "startbarend"), Strings.expandGlob("start{foo,bar}end"));    assertEquals(Arrays.asList("fooend", "barend"), Strings.expandGlob("{foo,bar}end"));    assertEquals(Arrays.asList("startfooenda", "startfooendb", "startfooendc", "startfooendd", "startbarenda", "startbarendb", "startbarendc", "startbarendd"), Strings.expandGlob("start{foo,bar}end{a,b,c,d}"));    assertEquals(Arrays.asList("xa", "xb", "xc", "ya", "yb", "yc"), Strings.expandGlob("{x,y}{a,b,c}"));    assertEquals(Arrays.asList("x", "y", "z"), Strings.expandGlob("{x,y,z}"));}
public void parquet-mr_f4323_0()
{    assertEquals(Arrays.asList("startoneend", "startpretwopostend", "startprethreepostend", "startfourend", "startfiveend", "a", "b", "foox", "fooy"), Strings.expandGlob("{start{one,pre{two,three}post,{four,five}}end,a,b,foo{x,y}}"));}
public void parquet-mr_f4324_0()
{    assertEquals(Arrays.asList("x", "y", "z"), Strings.expandGlob("{{x,y,z}}"));    assertEquals(Arrays.asList("x", "y", "z"), Strings.expandGlob("{{{x,y,z}}}"));    assertEquals(Arrays.asList("startx", "starta", "startb", "starty"), Strings.expandGlob("start{x,{a,b},y}"));}
public void parquet-mr_f4325_0()
{    try {        Strings.expandGlob("foo,bar");        fail("This should throw");    } catch (GlobParseException e) {        Assert.assertEquals("Unexpected comma outside of a {} group:\n" + "foo,bar\n" + "---^", e.getMessage());    }}
public void parquet-mr_f4326_0()
{        assertEquals(Arrays.asList("foobar", "foo", "foobaz"), Strings.expandGlob("foo{bar,,baz}"));    assertEquals(Arrays.asList("foo", "foobar", "foobaz"), Strings.expandGlob("foo{,bar,baz}"));    assertEquals(Arrays.asList("foobar", "foobaz", "foo"), Strings.expandGlob("foo{bar,baz,}"));        assertEquals(Arrays.asList("foobar", "foo", "foo", "foobaz"), Strings.expandGlob("foo{bar,,,baz}"));    assertEquals(Arrays.asList("foo", "foo", "foobar", "foobaz"), Strings.expandGlob("foo{,,bar,baz}"));    assertEquals(Arrays.asList("foobar", "foobaz", "foo", "foo"), Strings.expandGlob("foo{bar,baz,,}"));        assertEquals(Arrays.asList("x", "y", "", "a", "b"), Strings.expandGlob("{{x,y},,{a,b}}"));}
private void parquet-mr_f4327_0(String s)
{    String expected = "Not enough close braces in: ";    try {        Strings.expandGlob(s);        fail("this should throw");    } catch (GlobParseException e) {        Assert.assertEquals(expected, e.getMessage().substring(0, expected.length()));    }}
private void parquet-mr_f4328_0(String s)
{    String expected = "Unexpected closing }:";    try {        Strings.expandGlob(s);        fail("this should throw");    } catch (GlobParseException e) {        Assert.assertEquals(expected, e.getMessage().substring(0, expected.length()));    }}
public void parquet-mr_f4329_0()
{    assertNotEnoughCloseBraces("{");    assertNotEnoughCloseBraces("{}{}{}{{}{}{");    assertNotEnoughCloseBraces("foo{bar");    assertNotEnoughCloseBraces("foo{{bar}");    assertNotEnoughCloseBraces("foo{}{{bar}");    assertTooManyCloseBraces("{}}{");    assertTooManyCloseBraces("}");    assertTooManyCloseBraces("{}{}{}}{}{}{");    assertTooManyCloseBraces("foo}bar");    assertTooManyCloseBraces("foo}}bar}");    assertTooManyCloseBraces("foo{}{{bar}}}");}
private static void parquet-mr_f4330_0(WildcardPath wp, String... strings)
{    for (String s : strings) {        if (!wp.matches(s)) {            fail(String.format("String '%s' was expected to match '%s'", s, wp));        }    }}
private static void parquet-mr_f4331_0(WildcardPath wp, String... strings)
{    for (String s : strings) {        if (wp.matches(s)) {            fail(String.format("String '%s' was not expected to match '%s'", s, wp));        }    }}
public void parquet-mr_f4332_0()
{    WildcardPath wp = new WildcardPath("", "foo", '.');    assertMatches(wp, "foo", "foo.x", "foo.x.y");    assertDoesNotMatch(wp, "xfoo", "xfoox", "fooa.x.y");}
public void parquet-mr_f4333_0()
{    WildcardPath wp = new WildcardPath("", "*", '.');    assertMatches(wp, "", ".", "hi", "foo.bar", "*", "foo.");}
public void parquet-mr_f4334_0()
{    WildcardPath wp = new WildcardPath("", "x.y.z", '.');    assertMatches(wp, "x.y.z", "x.y.z.bar", "x.y.z.bar.baz.bop");    assertDoesNotMatch(wp, "x.y.zzzz", "x.y.b", "x.y.a.z", "x.y.zhi.z");}
public void parquet-mr_f4335_0()
{    WildcardPath wp = new WildcardPath("", "", '.');    assertMatches(wp, "");    assertDoesNotMatch(wp, "x");}
public void parquet-mr_f4336_0()
{    WildcardPath wp = new WildcardPath("", "foo**bar", '.');    assertMatches(wp, "foobar", "fooxyzbar", "foo.x.y.z.bar");    assertDoesNotMatch(wp, "fobar", "hi", "foobazr");    wp = new WildcardPath("", "foo********bar", '.');    assertMatches(wp, "foobar", "fooxyzbar", "foo.x.y.z.bar");    assertDoesNotMatch(wp, "fobar", "hi", "foobazr");}
public void parquet-mr_f4337_0()
{    WildcardPath wp = new WildcardPath("", "*x.y.z", '.');    assertMatches(wp, "a.b.c.x.y.z", "x.y.z", "zoopx.y.z", "zoopx.y.z.child");    assertDoesNotMatch(wp, "a.b.c.x.y", "xy.z", "hi");    wp = new WildcardPath("", "*.x.y.z", '.');    assertMatches(wp, "a.b.c.x.y.z", "foo.x.y.z", "foo.x.y.z.child");    assertDoesNotMatch(wp, "x.y.z", "a.b.c.x.y", "xy.z", "hi", "zoopx.y.z", "zoopx.y.z.child");    wp = new WildcardPath("", "x.y.z*", '.');    assertMatches(wp, "x.y.z", "x.y.z.foo", "x.y.zoo", "x.y.zoo.bar");    assertDoesNotMatch(wp, "a.b.c.x.y.z", "foo.x.y.z", "hi");    wp = new WildcardPath("", "x.y.z.*", '.');    assertMatches(wp, "x.y.z.foo", "x.y.z.bar.baz");    assertDoesNotMatch(wp, "x.y.z", "a.b.c.x.y.z", "x.y.zoo", "foo.x.y.z", "hi", "x.y.zoo.bar");}
public void parquet-mr_f4338_0()
{    WildcardPath wp = new WildcardPath("", "*.street", '.');    assertMatches(wp, "home.address.street", "home.address.street.number", "work.address.street", "work.address.street.foo", "street.street", "street.street.street.street", "thing.street.thing");    assertDoesNotMatch(wp, "home.address.street_2", "home.address.street_2.number", "work.addressstreet", "work.addressstreet.foo", "", "x.y.z.street2", "x.y.z.street2.z");    wp = new WildcardPath("", "x.y.*_stat.average", '.');    assertMatches(wp, "x.y.z_stat.average", "x.y.foo_stat.average", "x.y.z.a.b_stat.average", "x.y.z.a.b_stat.average.child", "x.y.z._stat.average");    assertDoesNotMatch(wp, "x.y.z_stats.average", "x.y.z_stat.averages", "x.y_stat.average", "x.yyy.foo_stat.average");    wp = new WildcardPath("", "x.y.pre*.bar", '.');    assertMatches(wp, "x.y.pre.bar", "x.y.preabc.bar", "x.y.prebar.bar");    assertDoesNotMatch(wp, "x.y.pre.baraaaa", "x.y.preabc.baraaaa");}
public synchronized int parquet-mr_f4339_0(byte[] b, int off, int len)
{    if (current < lengths.length) {        if (len <= lengths[current]) {                        int bytesRead = super.read(b, off, len);            lengths[current] -= bytesRead;            return bytesRead;        } else {            int bytesRead = super.read(b, off, lengths[current]);            current += 1;            return bytesRead;        }    } else {        return super.read(b, off, len);    }}
public long parquet-mr_f4340_0()
{    return this.pos;}
public void parquet-mr_f4341_0() throws Exception
{    byte[] buffer = new byte[5];    MockInputStream stream = new MockInputStream();    DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);    Assert.assertArrayEquals("Byte array contents should match", Arrays.copyOfRange(TEST_ARRAY, 0, 5), buffer);    Assert.assertEquals("Stream position should reflect bytes read", 5, stream.getPos());}
public void parquet-mr_f4342_0() throws Exception
{    byte[] buffer = new byte[5];    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);    Assert.assertArrayEquals("Byte array contents should match", Arrays.copyOfRange(TEST_ARRAY, 0, 5), buffer);    Assert.assertEquals("Stream position should reflect bytes read", 5, stream.getPos());}
public void parquet-mr_f4343_0() throws Exception
{    final byte[] buffer = new byte[10];    final MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);    Assert.assertArrayEquals("Byte array contents should match", TEST_ARRAY, buffer);    Assert.assertEquals("Stream position should reflect bytes read", 10, stream.getPos());    TestUtils.assertThrows("Should throw EOFException if no more bytes left", EOFException.class, (Callable<Void>) () -> {        DelegatingSeekableInputStream.readFully(stream, buffer, 0, 1);        return null;    });}
public void parquet-mr_f4344_0() throws Exception
{    final byte[] buffer = new byte[11];    final MockInputStream stream = new MockInputStream(2, 3, 3);    TestUtils.assertThrows("Should throw EOFException if no more bytes left", EOFException.class, (Callable<Void>) () -> {        DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);        return null;    });    Assert.assertArrayEquals("Should have consumed bytes", TEST_ARRAY, Arrays.copyOfRange(buffer, 0, 10));    Assert.assertEquals("Stream position should reflect bytes read", 10, stream.getPos());}
public void parquet-mr_f4345_0() throws IOException
{    byte[] buffer = new byte[10];    MockInputStream stream = new MockInputStream();    DelegatingSeekableInputStream.readFully(stream, buffer, 2, 5);    Assert.assertArrayEquals("Byte array contents should match", Arrays.copyOfRange(TEST_ARRAY, 0, 5), Arrays.copyOfRange(buffer, 2, 7));    Assert.assertEquals("Stream position should reflect bytes read", 5, stream.getPos());}
public void parquet-mr_f4346_0() throws IOException
{    byte[] buffer = new byte[0];    MockInputStream stream = new MockInputStream();    DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);    Assert.assertEquals("Stream position should reflect bytes read", 0, stream.getPos());}
public void parquet-mr_f4347_0() throws IOException
{    byte[] buffer = new byte[10];    MockInputStream stream = new MockInputStream(2, 2, 3);    DelegatingSeekableInputStream.readFully(stream, buffer, 2, 5);    Assert.assertArrayEquals("Byte array contents should match", Arrays.copyOfRange(TEST_ARRAY, 0, 5), Arrays.copyOfRange(buffer, 2, 7));    Assert.assertEquals("Stream position should reflect bytes read", 5, stream.getPos());}
public void parquet-mr_f4348_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(20);    MockInputStream stream = new MockInputStream();    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, len);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(-1, len);    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f4349_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(5);    MockInputStream stream = new MockInputStream();    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(5, len);    Assert.assertEquals(5, readBuffer.position());    Assert.assertEquals(5, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(0, len);    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 5), readBuffer);}
public void parquet-mr_f4350_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(10);    MockInputStream stream = new MockInputStream(2, 3, 3);    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(2, len);    Assert.assertEquals(2, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(3, len);    Assert.assertEquals(5, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(3, len);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(2, len);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f4351_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(20);    readBuffer.position(10);    readBuffer.mark();    MockInputStream stream = new MockInputStream(8);    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(8, len);    Assert.assertEquals(18, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(2, len);    Assert.assertEquals(20, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(-1, len);    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f4352_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(20);    readBuffer.limit(8);    MockInputStream stream = new MockInputStream(7);    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(7, len);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(1, len);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(0, len);    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
public void parquet-mr_f4353_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(20);    readBuffer.position(5);    readBuffer.limit(13);    readBuffer.mark();    MockInputStream stream = new MockInputStream(7);    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(7, len);    Assert.assertEquals(12, readBuffer.position());    Assert.assertEquals(13, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(1, len);    Assert.assertEquals(13, readBuffer.position());    Assert.assertEquals(13, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(0, len);    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
public void parquet-mr_f4354_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);    MockInputStream stream = new MockInputStream();    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, len);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(-1, len);    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f4355_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(5);    MockInputStream stream = new MockInputStream();    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(5, len);    Assert.assertEquals(5, readBuffer.position());    Assert.assertEquals(5, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(0, len);    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 5), readBuffer);}
public void parquet-mr_f4356_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    MockInputStream stream = new MockInputStream(2, 3, 3);    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(2, len);    Assert.assertEquals(2, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(3, len);    Assert.assertEquals(5, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(3, len);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(2, len);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f4357_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);    readBuffer.position(10);    readBuffer.mark();    MockInputStream stream = new MockInputStream(8);    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(8, len);    Assert.assertEquals(18, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(2, len);    Assert.assertEquals(20, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(-1, len);    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f4358_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(20);    readBuffer.limit(8);    MockInputStream stream = new MockInputStream(7);    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(7, len);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(1, len);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(0, len);    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
public void parquet-mr_f4359_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);    readBuffer.position(5);    readBuffer.limit(13);    readBuffer.mark();    MockInputStream stream = new MockInputStream(7);    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(7, len);    Assert.assertEquals(12, readBuffer.position());    Assert.assertEquals(13, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(1, len);    Assert.assertEquals(13, readBuffer.position());    Assert.assertEquals(13, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(0, len);    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
public void parquet-mr_f4360_0() throws Exception
{        byte[] temp = new byte[2];    ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    MockInputStream stream = new MockInputStream(2, 3, 3);    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(2, len);    Assert.assertEquals(2, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(3, len);    Assert.assertEquals(5, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(3, len);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(2, len);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(-1, len);    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f4361_0() throws Exception
{        byte[] temp = new byte[2];    ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);    readBuffer.position(5);    readBuffer.limit(13);    readBuffer.mark();    MockInputStream stream = new MockInputStream(7);    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(7, len);    Assert.assertEquals(12, readBuffer.position());    Assert.assertEquals(13, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(1, len);    Assert.assertEquals(13, readBuffer.position());    Assert.assertEquals(13, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(0, len);    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
public void parquet-mr_f4362_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(8);    MockInputStream stream = new MockInputStream();    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
public void parquet-mr_f4363_0() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocate(20);    final MockInputStream stream = new MockInputStream();    TestUtils.assertThrows("Should throw EOFException", EOFException.class, () -> {        DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);        return null;    });    Assert.assertEquals(0, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());}
public void parquet-mr_f4364_0() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocate(10);    MockInputStream stream = new MockInputStream();        DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());        DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f4365_0() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocate(10);    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f4366_0() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocate(10);    readBuffer.position(3);    readBuffer.mark();    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
public void parquet-mr_f4367_0() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocate(10);    readBuffer.limit(7);    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f4368_0() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocate(10);    readBuffer.position(3);    readBuffer.limit(7);    readBuffer.mark();    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 4), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
public void parquet-mr_f4369_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(8);    MockInputStream stream = new MockInputStream();    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
public void parquet-mr_f4370_0() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);    final MockInputStream stream = new MockInputStream();    TestUtils.assertThrows("Should throw EOFException", EOFException.class, () -> {        DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());        return null;    });                            Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());}
public void parquet-mr_f4371_0() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    MockInputStream stream = new MockInputStream();        DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());        DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f4372_0() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f4373_0() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    readBuffer.position(3);    readBuffer.mark();    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
public void parquet-mr_f4374_0() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    readBuffer.limit(7);    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f4375_0() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    readBuffer.position(3);    readBuffer.limit(7);    readBuffer.mark();    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 4), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
public void parquet-mr_f4376_0() throws Exception
{        byte[] temp = new byte[2];    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    readBuffer.position(3);    readBuffer.limit(7);    readBuffer.mark();    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 4), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
public void parquet-mr_f4377_0()
{    assertTrue(new SemanticVersion(1, 8, 1).compareTo(new SemanticVersion(1, 8, 1)) == 0);    assertTrue(new SemanticVersion(1, 8, 0).compareTo(new SemanticVersion(1, 8, 1)) < 0);    assertTrue(new SemanticVersion(1, 8, 2).compareTo(new SemanticVersion(1, 8, 1)) > 0);    assertTrue(new SemanticVersion(1, 8, 1).compareTo(new SemanticVersion(1, 8, 1)) == 0);    assertTrue(new SemanticVersion(1, 8, 0).compareTo(new SemanticVersion(1, 8, 1)) < 0);    assertTrue(new SemanticVersion(1, 8, 2).compareTo(new SemanticVersion(1, 8, 1)) > 0);    assertTrue(new SemanticVersion(1, 7, 0).compareTo(new SemanticVersion(1, 8, 0)) < 0);    assertTrue(new SemanticVersion(1, 9, 0).compareTo(new SemanticVersion(1, 8, 0)) > 0);    assertTrue(new SemanticVersion(0, 0, 0).compareTo(new SemanticVersion(1, 0, 0)) < 0);    assertTrue(new SemanticVersion(2, 0, 0).compareTo(new SemanticVersion(1, 0, 0)) > 0);    assertTrue(new SemanticVersion(1, 8, 100).compareTo(new SemanticVersion(1, 9, 0)) < 0);    assertTrue(new SemanticVersion(1, 8, 0).compareTo(new SemanticVersion(1, 8, 0, true)) > 0);    assertTrue(new SemanticVersion(1, 8, 0, true).compareTo(new SemanticVersion(1, 8, 0, true)) == 0);    assertTrue(new SemanticVersion(1, 8, 0, true).compareTo(new SemanticVersion(1, 8, 0)) < 0);}
public void parquet-mr_f4378_0() throws Exception
{    List<String> examples = Arrays.asList("1.0.0-alpha", "1.0.0-alpha.1", "1.0.0-alpha.beta", "1.0.0-beta", "1.0.0-beta.2", "1.0.0-beta.11", "1.0.0-rc.1", "1.0.0");    for (int i = 0; i < examples.size() - 1; i += 1) {        assertLessThan(examples.get(i), examples.get(i + 1));        assertEqualTo(examples.get(i), examples.get(i));    }        assertEqualTo(examples.get(examples.size() - 1), examples.get(examples.size() - 1));}
public void parquet-mr_f4379_0() throws Exception
{    assertEqualTo("1.0.0-alpha+001", "1.0.0-alpha+001");    assertEqualTo("1.0.0-alpha", "1.0.0-alpha+001");    assertEqualTo("1.0.0+20130313144700", "1.0.0+20130313144700");    assertEqualTo("1.0.0", "1.0.0+20130313144700");    assertEqualTo("1.0.0-beta+exp.sha.5114f85", "1.0.0-beta+exp.sha.5114f85");    assertEqualTo("1.0.0-beta", "1.0.0-beta+exp.sha.5114f85");}
public void parquet-mr_f4380_0() throws Exception
{        assertLessThan("1.0.0rc0-alpha+001", "1.0.0-alpha");}
public void parquet-mr_f4381_0() throws Exception
{    assertEqualTo("1.5.0-cdh5.5.0", "1.5.0-cdh5.5.0");    assertLessThan("1.5.0-cdh5.5.0", "1.5.0-cdh5.5.1");    assertLessThan("1.5.0-cdh5.5.0", "1.5.0-cdh5.5.1-SNAPSHOT");    assertLessThan("1.5.0-cdh5.5.0", "1.5.0-cdh5.6.0");    assertLessThan("1.5.0-cdh5.5.0", "1.5.0-cdh6.0.0");    assertLessThan("1.5.0-cdh5.5.0", "1.5.0");        assertLessThan("1.5.0-cdh5.5.0", "1.5.0-cdh5.5.0-SNAPSHOT");}
public void parquet-mr_f4382_0() throws Exception
{    assertEquals(new SemanticVersion(1, 8, 0), SemanticVersion.parse("1.8.0"));    assertEquals(new SemanticVersion(1, 8, 0, true), SemanticVersion.parse("1.8.0rc3"));    assertEquals(new SemanticVersion(1, 8, 0, "rc3", "SNAPSHOT", null), SemanticVersion.parse("1.8.0rc3-SNAPSHOT"));    assertEquals(new SemanticVersion(1, 8, 0, null, "SNAPSHOT", null), SemanticVersion.parse("1.8.0-SNAPSHOT"));    assertEquals(new SemanticVersion(1, 5, 0, null, "cdh5.5.0", null), SemanticVersion.parse("1.5.0-cdh5.5.0"));}
private static void parquet-mr_f4383_0(String a, String b) throws SemanticVersion.SemanticVersionParseException
{    assertTrue(a + " should be < " + b, SemanticVersion.parse(a).compareTo(SemanticVersion.parse(b)) < 0);    assertTrue(b + " should be > " + a, SemanticVersion.parse(b).compareTo(SemanticVersion.parse(a)) > 0);}
private static void parquet-mr_f4384_0(String a, String b) throws SemanticVersion.SemanticVersionParseException
{    assertTrue(a + " should equal " + b, SemanticVersion.parse(a).compareTo(SemanticVersion.parse(b)) == 0);}
public void parquet-mr_f4385_0()
{    try {        Preconditions.checkArgument(true, "Test message: %s %s", 12, null);    } catch (IllegalArgumentException e) {        Assert.fail("Should not throw exception when isValid is true");    }    try {        Preconditions.checkArgument(false, "Test message: %s %s", 12, null);        Assert.fail("Should throw exception when isValid is false");    } catch (IllegalArgumentException e) {        Assert.assertEquals("Should format message", "Test message: 12 null", e.getMessage());    }}
public void parquet-mr_f4386_0()
{    try {        Preconditions.checkState(true, "Test message: %s %s", 12, null);    } catch (IllegalStateException e) {        Assert.fail("Should not throw exception when isValid is true");    }    try {        Preconditions.checkState(false, "Test message: %s %s", 12, null);        Assert.fail("Should throw exception when isValid is false");    } catch (IllegalStateException e) {        Assert.assertEquals("Should format message", "Test message: 12 null", e.getMessage());    }}
public static void parquet-mr_f4387_0(String message, Class<? extends Exception> expected, Callable callable)
{    try {        callable.call();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        try {            Assert.assertEquals(message, expected, actual.getClass());        } catch (AssertionError e) {            e.addSuppressed(actual);            throw e;        }    }}
public static void parquet-mr_f4388_0(String message, Class<? extends Exception> expected, Runnable runnable)
{    try {        runnable.run();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        try {            Assert.assertEquals(message, expected, actual.getClass());        } catch (AssertionError e) {            e.addSuppressed(actual);            throw e;        }    }}
public static Concatenator parquet-mr_f4389_0(String sep)
{    return new Concatenator(sep);}
private void parquet-mr_f4390_0(String sep)
{    this.sep = sep;}
public String parquet-mr_f4391_0(String left, String right)
{    return left + sep + right;}
public String parquet-mr_f4392_0(String left, String middle, String right)
{    return left + sep + middle + sep + right;}
public String parquet-mr_f4393_0(Exception e) throws Exception
{    throw e;}
public String parquet-mr_f4394_0(String... strings)
{    if (strings.length >= 1) {        StringBuilder sb = new StringBuilder();        sb.append(strings[0]);        for (int i = 1; i < strings.length; i += 1) {            sb.append(sep);            sb.append(strings[i]);        }        return sb.toString();    }    return null;}
public static String parquet-mr_f4395_0(String... strings)
{    return new Concatenator().concat(strings);}
public void parquet-mr_f4396_0()
{    final DynConstructors.Builder builder = new DynConstructors.Builder();    TestUtils.assertThrows("Checked build should throw NoSuchMethodException", NoSuchMethodException.class, (Callable) builder::buildChecked);    TestUtils.assertThrows("Normal build should throw RuntimeException", RuntimeException.class, (Runnable) builder::build);}
public void parquet-mr_f4397_0()
{    final DynConstructors.Builder builder = new DynConstructors.Builder().impl("not.a.RealClass");    TestUtils.assertThrows("Checked build should throw NoSuchMethodException", NoSuchMethodException.class, (Callable) builder::buildChecked);    TestUtils.assertThrows("Normal build should throw RuntimeException", RuntimeException.class, (Callable) builder::build);}
public void parquet-mr_f4398_0()
{    final DynConstructors.Builder builder = new DynConstructors.Builder().impl(Concatenator.class, String.class, String.class);    TestUtils.assertThrows("Checked build should throw NoSuchMethodException", NoSuchMethodException.class, (Callable) builder::buildChecked);    TestUtils.assertThrows("Normal build should throw RuntimeException", RuntimeException.class, (Callable) builder::build);}
public void parquet-mr_f4399_0() throws Exception
{    final DynConstructors.Ctor<Concatenator> sepCtor = new DynConstructors.Builder().impl("not.a.RealClass", String.class).impl(Concatenator.class, String.class).impl(Concatenator.class).buildChecked();    Concatenator dashCat = sepCtor.newInstanceChecked("-");    Assert.assertEquals("Should construct with the 1-arg version", "a-b", dashCat.concat("a", "b"));    TestUtils.assertThrows("Should complain about extra arguments", IllegalArgumentException.class, () -> sepCtor.newInstanceChecked("/", "-"));    TestUtils.assertThrows("Should complain about extra arguments", IllegalArgumentException.class, () -> sepCtor.newInstance("/", "-"));    DynConstructors.Ctor<Concatenator> defaultCtor = new DynConstructors.Builder().impl("not.a.RealClass", String.class).impl(Concatenator.class).impl(Concatenator.class, String.class).buildChecked();    Concatenator cat = defaultCtor.newInstanceChecked();    Assert.assertEquals("Should construct with the no-arg version", "ab", cat.concat("a", "b"));}
public void parquet-mr_f4400_0() throws Exception
{    final SomeCheckedException exc = new SomeCheckedException();    final DynConstructors.Ctor<Concatenator> sepCtor = new DynConstructors.Builder().impl("not.a.RealClass", String.class).impl(Concatenator.class, Exception.class).buildChecked();    TestUtils.assertThrows("Should re-throw the exception", SomeCheckedException.class, () -> sepCtor.newInstanceChecked(exc));    TestUtils.assertThrows("Should wrap the exception in RuntimeException", RuntimeException.class, () -> sepCtor.newInstance(exc));}
public void parquet-mr_f4401_0() throws Exception
{    final DynConstructors.Ctor<Concatenator> sepCtor = new DynConstructors.Builder().impl(Concatenator.class.getName(), String.class).buildChecked();    Assert.assertNotNull("Should find 1-arg constructor", sepCtor.newInstance("-"));}
public void parquet-mr_f4402_0() throws Exception
{    TestUtils.assertThrows("Should fail to find hidden method", NoSuchMethodException.class, () -> new DynMethods.Builder("setSeparator").impl(Concatenator.class, char.class).buildChecked());    final DynConstructors.Ctor<Concatenator> sepCtor = new DynConstructors.Builder().hiddenImpl(Concatenator.class.getName(), char.class).buildChecked();    Assert.assertNotNull("Should find hidden ctor with hiddenImpl", sepCtor);    Concatenator slashCat = sepCtor.newInstanceChecked('/');    Assert.assertEquals("Should use separator /", "a/b", slashCat.concat("a", "b"));}
public void parquet-mr_f4403_0() throws Exception
{    final DynConstructors.Ctor<Concatenator> ctor = new DynConstructors.Builder().impl(Concatenator.class.getName()).buildChecked();    Assert.assertTrue("Should always be static", ctor.isStatic());    TestUtils.assertThrows("Should complain that method is static", IllegalStateException.class, () -> ctor.bind(null));}
public void parquet-mr_f4404_0() throws Exception
{    final DynMethods.UnboundMethod ctor = new DynConstructors.Builder().impl(Concatenator.class.getName()).buildChecked();    TestUtils.assertThrows("Should complain that target must be null", IllegalArgumentException.class, () -> ctor.invokeChecked("a"));    TestUtils.assertThrows("Should complain that target must be null", IllegalArgumentException.class, () -> ctor.invoke("a"));    Assert.assertNotNull("Should allow invokeChecked(null, ...)", ctor.invokeChecked(null));    Assert.assertNotNull("Should allow invoke(null, ...)", ctor.invoke(null));}
public void parquet-mr_f4405_0()
{    final DynMethods.Builder builder = new DynMethods.Builder("concat");    TestUtils.assertThrows("Checked build should throw NoSuchMethodException", NoSuchMethodException.class, (Callable) builder::buildChecked);    TestUtils.assertThrows("Normal build should throw RuntimeException", RuntimeException.class, (Callable) builder::build);}
public void parquet-mr_f4406_0()
{    final DynMethods.Builder builder = new DynMethods.Builder("concat").impl("not.a.RealClass", String.class, String.class);    TestUtils.assertThrows("Checked build should throw NoSuchMethodException", NoSuchMethodException.class, (Callable) builder::buildChecked);    TestUtils.assertThrows("Normal build should throw RuntimeException", RuntimeException.class, (Runnable) builder::build);}
public void parquet-mr_f4407_0()
{    final DynMethods.Builder builder = new DynMethods.Builder("concat").impl(Concatenator.class, "cat2strings", String.class, String.class);    TestUtils.assertThrows("Checked build should throw NoSuchMethodException", NoSuchMethodException.class, (Callable) builder::buildChecked);    TestUtils.assertThrows("Normal build should throw RuntimeException", RuntimeException.class, (Runnable) builder::build);}
public void parquet-mr_f4408_0() throws Exception
{    Concatenator obj = new Concatenator("-");    DynMethods.UnboundMethod cat2 = new DynMethods.Builder("concat").impl("not.a.RealClass", String.class, String.class).impl(Concatenator.class, String.class, String.class).impl(Concatenator.class, String.class, String.class, String.class).buildChecked();    Assert.assertEquals("Should call the 2-arg version successfully", "a-b", cat2.invoke(obj, "a", "b"));    Assert.assertEquals("Should ignore extra arguments", "a-b", cat2.invoke(obj, "a", "b", "c"));    DynMethods.UnboundMethod cat3 = new DynMethods.Builder("concat").impl("not.a.RealClass", String.class, String.class).impl(Concatenator.class, String.class, String.class, String.class).impl(Concatenator.class, String.class, String.class).build();    Assert.assertEquals("Should call the 3-arg version successfully", "a-b-c", cat3.invoke(obj, "a", "b", "c"));    Assert.assertEquals("Should call the 3-arg version null padding", "a-b-null", cat3.invoke(obj, "a", "b"));}
public void parquet-mr_f4409_0() throws Exception
{    DynMethods.UnboundMethod cat = new DynMethods.Builder("concat").impl(Concatenator.class, String[].class).buildChecked();    Assert.assertEquals("Should use the varargs version", "abcde", cat.invokeChecked(new Concatenator(), (Object) new String[] { "a", "b", "c", "d", "e" }));    Assert.assertEquals("Should use the varargs version", "abcde", cat.bind(new Concatenator()).invokeChecked((Object) new String[] { "a", "b", "c", "d", "e" }));}
public void parquet-mr_f4410_0() throws Exception
{    final Concatenator obj = new Concatenator("-");    final DynMethods.UnboundMethod cat = new DynMethods.Builder("concat").impl("not.a.RealClass", String.class, String.class).impl(Concatenator.class, String.class, String.class).buildChecked();    TestUtils.assertThrows("Should fail if non-string arguments are passed", IllegalArgumentException.class, () -> cat.invoke(obj, 3, 4));    TestUtils.assertThrows("Should fail if non-string arguments are passed", IllegalArgumentException.class, () -> cat.invokeChecked(obj, 3, 4));}
public void parquet-mr_f4411_0() throws Exception
{    final SomeCheckedException exc = new SomeCheckedException();    final Concatenator obj = new Concatenator("-");    final DynMethods.UnboundMethod cat = new DynMethods.Builder("concat").impl("not.a.RealClass", String.class, String.class).impl(Concatenator.class, Exception.class).buildChecked();    TestUtils.assertThrows("Should re-throw the exception", SomeCheckedException.class, () -> cat.invokeChecked(obj, exc));    TestUtils.assertThrows("Should wrap the exception in RuntimeException", RuntimeException.class, () -> cat.invoke(obj, exc));}
public void parquet-mr_f4412_0() throws Exception
{    Concatenator obj = new Concatenator("-");    DynMethods.UnboundMethod cat = new DynMethods.Builder("cat").impl(Concatenator.class, "concat", String.class, String.class).buildChecked();    Assert.assertEquals("Should find 2-arg concat method", "a-b", cat.invoke(obj, "a", "b"));}
public void parquet-mr_f4413_0() throws Exception
{    Concatenator obj = new Concatenator("-");    DynMethods.UnboundMethod cat = new DynMethods.Builder("concat").impl(Concatenator.class.getName(), String.class, String.class).buildChecked();    Assert.assertEquals("Should find 2-arg concat method", "a-b", cat.invoke(obj, "a", "b"));}
public void parquet-mr_f4414_0() throws Exception
{    Concatenator obj = new Concatenator("-");    TestUtils.assertThrows("Should fail to find hidden method", NoSuchMethodException.class, () -> new DynMethods.Builder("setSeparator").impl(Concatenator.class, String.class).buildChecked());    DynMethods.UnboundMethod changeSep = new DynMethods.Builder("setSeparator").hiddenImpl(Concatenator.class, String.class).buildChecked();    Assert.assertNotNull("Should find hidden method with hiddenImpl", changeSep);    changeSep.invokeChecked(obj, "/");    Assert.assertEquals("Should use separator / instead of -", "a/b", obj.concat("a", "b"));}
public void parquet-mr_f4415_0() throws Exception
{    DynMethods.UnboundMethod cat = new DynMethods.Builder("concat").impl(Concatenator.class, String.class, String.class).buildChecked();        DynMethods.BoundMethod dashCat = cat.bind(new Concatenator("-"));    DynMethods.BoundMethod underCat = cat.bind(new Concatenator("_"));    Assert.assertEquals("Should use '-' object without passing", "a-b", dashCat.invoke("a", "b"));    Assert.assertEquals("Should use '_' object without passing", "a_b", underCat.invoke("a", "b"));    DynMethods.BoundMethod slashCat = new DynMethods.Builder("concat").impl(Concatenator.class, String.class, String.class).buildChecked(new Concatenator("/"));    Assert.assertEquals("Should use bound object from builder without passing", "a/b", slashCat.invoke("a", "b"));}
public void parquet-mr_f4416_0() throws Exception
{    final DynMethods.Builder builder = new DynMethods.Builder("cat").impl(Concatenator.class, String[].class);    TestUtils.assertThrows("Should complain that method is static", IllegalStateException.class, () -> builder.buildChecked(new Concatenator()));    TestUtils.assertThrows("Should complain that method is static", IllegalStateException.class, () -> builder.build(new Concatenator()));    final DynMethods.UnboundMethod staticCat = builder.buildChecked();    Assert.assertTrue("Should be static", staticCat.isStatic());    TestUtils.assertThrows("Should complain that method is static", IllegalStateException.class, () -> staticCat.bind(new Concatenator()));}
public void parquet-mr_f4417_0() throws Exception
{    DynMethods.StaticMethod staticCat = new DynMethods.Builder("cat").impl(Concatenator.class, String[].class).buildStaticChecked();    Assert.assertEquals("Should call varargs static method cat(String...)", "abcde", staticCat.invokeChecked((Object) new String[] { "a", "b", "c", "d", "e" }));}
public void parquet-mr_f4418_0() throws Exception
{    final DynMethods.Builder builder = new DynMethods.Builder("concat").impl(Concatenator.class, String.class, String.class);    TestUtils.assertThrows("Should complain that method is not static", IllegalStateException.class, builder::buildStatic);    TestUtils.assertThrows("Should complain that method is not static", IllegalStateException.class, builder::buildStaticChecked);    final DynMethods.UnboundMethod cat2 = builder.buildChecked();    Assert.assertFalse("concat(String,String) should not be static", cat2.isStatic());    TestUtils.assertThrows("Should complain that method is not static", IllegalStateException.class, cat2::asStatic);}
public void parquet-mr_f4419_0() throws Exception
{    final DynMethods.Builder builder = new DynMethods.Builder("newConcatenator").ctorImpl(Concatenator.class, String.class).impl(Concatenator.class, String.class);    DynMethods.UnboundMethod newConcatenator = builder.buildChecked();    Assert.assertTrue("Should find constructor implementation", newConcatenator instanceof DynConstructors.Ctor);    Assert.assertTrue("Constructor should be a static method", newConcatenator.isStatic());    Assert.assertFalse("Constructor should not be NOOP", newConcatenator.isNoop());        TestUtils.assertThrows("Should complain that ctor method is static", IllegalStateException.class, () -> builder.buildChecked(new Concatenator()));    TestUtils.assertThrows("Should complain that ctor method is static", IllegalStateException.class, () -> builder.build(new Concatenator()));    Concatenator concatenator = newConcatenator.asStatic().invoke("*");    Assert.assertEquals("Should function as a concatenator", "a*b", concatenator.concat("a", "b"));    concatenator = newConcatenator.asStatic().invokeChecked("@");    Assert.assertEquals("Should function as a concatenator", "a@b", concatenator.concat("a", "b"));}
public void parquet-mr_f4420_0() throws Exception
{    DynMethods.UnboundMethod newConcatenator = new DynMethods.Builder("newConcatenator").impl(Concatenator.class, String.class).ctorImpl(Concatenator.class, String.class).buildChecked();    Assert.assertFalse("Should find factory method before constructor method", newConcatenator instanceof DynConstructors.Ctor);}
public void parquet-mr_f4421_0() throws Exception
{        DynMethods.UnboundMethod noop = new DynMethods.Builder("concat").impl("not.a.RealClass", String.class, String.class).orNoop().buildChecked();    Assert.assertTrue("No implementation found, should return NOOP", noop.isNoop());    Assert.assertNull("NOOP should always return null", noop.invoke(new Concatenator(), "a"));    Assert.assertNull("NOOP can be called with null", noop.invoke(null, "a"));    Assert.assertNull("NOOP can be bound", noop.bind(new Concatenator()).invoke("a"));    Assert.assertNull("NOOP can be bound to null", noop.bind(null).invoke("a"));    Assert.assertNull("NOOP can be static", noop.asStatic().invoke("a"));}
private void parquet-mr_f4422_0(String v)
{    try {        org.semver.Version.parse(v);    } catch (RuntimeException e) {        throw new RuntimeException(v + " is not a valid semver!", e);    }}
public void parquet-mr_f4423_0()
{    assertVersionValid(Version.VERSION_NUMBER);}
public void parquet-mr_f4424_0() throws Exception
{    ParsedVersion version = VersionParser.parse(Version.FULL_VERSION);    assertVersionValid(version.version);    assertEquals(Version.VERSION_NUMBER, version.version);    assertEquals("parquet-mr", version.application);}
public void parquet-mr_f4425_0() throws Exception
{    assertEquals(new ParsedVersion("parquet-mr", "1.6.0", "abcd"), VersionParser.parse("parquet-mr version 1.6.0 (build abcd)"));    assertEquals(new ParsedVersion("parquet-mr", "1.6.22rc99-SNAPSHOT", "abcd"), VersionParser.parse("parquet-mr version 1.6.22rc99-SNAPSHOT (build abcd)"));    try {        VersionParser.parse("unparseable string");        fail("this should throw");    } catch (VersionParseException e) {        }        assertEquals(new ParsedVersion("parquet-mr", null, "abcd"), VersionParser.parse("parquet-mr version (build abcd)"));    assertEquals(new ParsedVersion("parquet-mr", null, "abcd"), VersionParser.parse("parquet-mr version  (build abcd)"));        assertEquals(new ParsedVersion("parquet-mr", "1.6.0", null), VersionParser.parse("parquet-mr version 1.6.0 (build )"));    assertEquals(new ParsedVersion("parquet-mr", "1.6.0", null), VersionParser.parse("parquet-mr version 1.6.0 (build)"));    assertEquals(new ParsedVersion("parquet-mr", null, null), VersionParser.parse("parquet-mr version (build)"));    assertEquals(new ParsedVersion("parquet-mr", null, null), VersionParser.parse("parquet-mr version (build )"));        assertEquals(new ParsedVersion("parquet-mr", "1.6.0", null), VersionParser.parse("parquet-mr version 1.6.0"));    assertEquals(new ParsedVersion("parquet-mr", "1.8.0rc4", null), VersionParser.parse("parquet-mr version 1.8.0rc4"));    assertEquals(new ParsedVersion("parquet-mr", "1.8.0rc4-SNAPSHOT", null), VersionParser.parse("parquet-mr version 1.8.0rc4-SNAPSHOT"));    assertEquals(new ParsedVersion("parquet-mr", null, null), VersionParser.parse("parquet-mr version"));        assertEquals(new ParsedVersion("parquet-mr", "1.6.0", null), VersionParser.parse("parquet-mr     version    1.6.0"));    assertEquals(new ParsedVersion("parquet-mr", "1.8.0rc4", null), VersionParser.parse("parquet-mr     version    1.8.0rc4"));    assertEquals(new ParsedVersion("parquet-mr", "1.8.0rc4-SNAPSHOT", null), VersionParser.parse("parquet-mr      version    1.8.0rc4-SNAPSHOT  "));    assertEquals(new ParsedVersion("parquet-mr", null, null), VersionParser.parse("parquet-mr      version"));    assertEquals(new ParsedVersion("parquet-mr", "1.6.0", null), VersionParser.parse("parquet-mr version 1.6.0 (  build )"));    assertEquals(new ParsedVersion("parquet-mr", "1.6.0", null), VersionParser.parse("parquet-mr     version 1.6.0 (    build)"));    assertEquals(new ParsedVersion("parquet-mr", null, null), VersionParser.parse("parquet-mr     version (    build)"));    assertEquals(new ParsedVersion("parquet-mr", null, null), VersionParser.parse("parquet-mr    version    (build    )"));}
public static BitPackingWriter parquet-mr_f4426_0(int bitLength, OutputStream out)
{    switch(bitLength) {        case 0:            return new ZeroBitPackingWriter();        case 1:            return new OneBitPackingWriter(out);        case 2:            return new TwoBitPackingWriter(out);        case 3:            return new ThreeBitPackingWriter(out);        case 4:            return new FourBitPackingWriter(out);        case 5:            return new FiveBitPackingWriter(out);        case 6:            return new SixBitPackingWriter(out);        case 7:            return new SevenBitPackingWriter(out);        case 8:            return new EightBitPackingWriter(out);        default:            throw new UnsupportedOperationException("only support up to 8 for now");    }}
public static BitPackingReader parquet-mr_f4427_0(int bitLength, InputStream in, long valueCount)
{    switch(bitLength) {        case 0:            return new ZeroBitPackingReader();        case 1:            return new OneBitPackingReader(in);        case 2:            return new TwoBitPackingReader(in);        case 3:            return new ThreeBitPackingReader(in, valueCount);        case 4:            return new FourBitPackingReader(in);        case 5:            return new FiveBitPackingReader(in, valueCount);        case 6:            return new SixBitPackingReader(in, valueCount);        case 7:            return new SevenBitPackingReader(in, valueCount);        case 8:            return new EightBitPackingReader(in);        default:            throw new UnsupportedOperationException("only support up to 8 for now");    }}
 void parquet-mr_f4428_0(int numberOfBits, int buffer, OutputStream out) throws IOException
{    int padding = numberOfBits % 8 == 0 ? 0 : 8 - (numberOfBits % 8);    buffer = buffer << padding;    int numberOfBytes = (numberOfBits + padding) / 8;    for (int i = (numberOfBytes - 1) * 8; i >= 0; i -= 8) {        out.write((buffer >>> i) & 0xFF);    }}
 void parquet-mr_f4429_0(int numberOfBits, long buffer, OutputStream out) throws IOException
{    int padding = numberOfBits % 8 == 0 ? 0 : 8 - (numberOfBits % 8);    buffer = buffer << padding;    int numberOfBytes = (numberOfBits + padding) / 8;    for (int i = (numberOfBytes - 1) * 8; i >= 0; i -= 8) {        out.write((int) (buffer >>> i) & 0xFF);    }}
 int parquet-mr_f4430_0(int bitsCount)
{    return BytesUtils.paddedByteCountFromBits(bitsCount);}
public void parquet-mr_f4431_0(int val) throws IOException
{}
public void parquet-mr_f4432_0()
{}
public int parquet-mr_f4433_0() throws IOException
{    return 0;}
public void parquet-mr_f4434_0(int val) throws IOException
{    buffer = buffer << 1;    buffer |= val;    ++count;    if (count == 8) {        out.write(buffer);        buffer = 0;        count = 0;    }}
public void parquet-mr_f4435_0() throws IOException
{    while (count != 0) {        write(0);    }        out = null;}
public int parquet-mr_f4436_0() throws IOException
{    if (count == 0) {        buffer = in.read();        count = 8;    }    int result = (buffer >> (count - 1)) & 1;    --count;    return result;}
public void parquet-mr_f4437_0(int val) throws IOException
{    buffer = buffer << 2;    buffer |= val;    ++count;    if (count == 4) {        out.write(buffer);        buffer = 0;        count = 0;    }}
public void parquet-mr_f4438_0() throws IOException
{    while (count != 0) {        write(0);    }        out = null;}
public int parquet-mr_f4439_0() throws IOException
{    if (count == 0) {        buffer = in.read();        count = 4;    }    int result = (buffer >> ((count - 1) * 2)) & 3;    --count;    return result;}
public void parquet-mr_f4440_0(int val) throws IOException
{    buffer = buffer << 3;    buffer |= val;    ++count;    if (count == 8) {        out.write((buffer >>> 16) & 0xFF);        out.write((buffer >>> 8) & 0xFF);        out.write((buffer >>> 0) & 0xFF);        buffer = 0;        count = 0;    }}
public void parquet-mr_f4441_0() throws IOException
{    if (count != 0) {        int numberOfBits = count * 3;        finish(numberOfBits, buffer, out);        buffer = 0;        count = 0;    }        out = null;}
public int parquet-mr_f4442_0() throws IOException
{    if (count == 0) {        if (valueCount - totalRead < 8) {            buffer = 0;            int bitsToRead = 3 * (int) (valueCount - totalRead);            int bytesToRead = alignToBytes(bitsToRead);            for (int i = 3 - 1; i >= 3 - bytesToRead; i--) {                buffer |= in.read() << (i * 8);            }            count = 8;            totalRead = valueCount;        } else {            buffer = (in.read() << 16) + (in.read() << 8) + in.read();            count = 8;            totalRead += 8;        }    }    int result = (buffer >> ((count - 1) * 3)) & 7;    --count;    return result;}
public void parquet-mr_f4443_0(int val) throws IOException
{    buffer = buffer << 4;    buffer |= val;    ++count;    if (count == 2) {        out.write(buffer);        buffer = 0;        count = 0;    }}
public void parquet-mr_f4444_0() throws IOException
{    while (count != 0) {                write(0);    }        out = null;}
public int parquet-mr_f4445_0() throws IOException
{    if (count == 0) {        buffer = in.read();        count = 2;    }    int result = (buffer >> ((count - 1) * 4)) & 15;    --count;    return result;}
public void parquet-mr_f4446_0(int val) throws IOException
{    buffer = buffer << 5;    buffer |= val;    ++count;    if (count == 8) {        out.write((int) (buffer >>> 32) & 0xFF);        out.write((int) (buffer >>> 24) & 0xFF);        out.write((int) (buffer >>> 16) & 0xFF);        out.write((int) (buffer >>> 8) & 0xFF);        out.write((int) (buffer >>> 0) & 0xFF);        buffer = 0;        count = 0;    }}
public void parquet-mr_f4447_0() throws IOException
{    if (count != 0) {        int numberOfBits = count * 5;        finish(numberOfBits, buffer, out);        buffer = 0;        count = 0;    }        out = null;}
public int parquet-mr_f4448_0() throws IOException
{    if (count == 0) {        if (valueCount - totalRead < 8) {            buffer = 0;            int bitsToRead = 5 * (int) (valueCount - totalRead);            int bytesToRead = alignToBytes(bitsToRead);            for (int i = 5 - 1; i >= 5 - bytesToRead; i--) {                buffer |= (((long) in.read()) & 255) << (i * 8);            }            count = 8;            totalRead = valueCount;        } else {            buffer = ((((long) in.read()) & 255) << 32) + ((((long) in.read()) & 255) << 24) + (in.read() << 16) + (in.read() << 8) + in.read();            count = 8;            totalRead += 8;        }    }    int result = (((int) (buffer >> ((count - 1) * 5))) & 31);    --count;    return result;}
public void parquet-mr_f4449_0(int val) throws IOException
{    buffer = buffer << 6;    buffer |= val;    ++count;    if (count == 4) {        out.write((buffer >>> 16) & 0xFF);        out.write((buffer >>> 8) & 0xFF);        out.write((buffer >>> 0) & 0xFF);        buffer = 0;        count = 0;    }}
public void parquet-mr_f4450_0() throws IOException
{    if (count != 0) {        int numberOfBits = count * 6;        finish(numberOfBits, buffer, out);        buffer = 0;        count = 0;    }        out = null;}
public int parquet-mr_f4451_0() throws IOException
{    if (count == 0) {        if (valueCount - totalRead < 4) {            buffer = 0;            int bitsToRead = 6 * (int) (valueCount - totalRead);            int bytesToRead = alignToBytes(bitsToRead);            for (int i = 3 - 1; i >= 3 - bytesToRead; i--) {                buffer |= in.read() << (i * 8);            }            count = 4;            totalRead = valueCount;        } else {            buffer = (in.read() << 16) + (in.read() << 8) + in.read();            count = 4;            totalRead += 4;        }    }    int result = (buffer >> ((count - 1) * 6)) & 63;    --count;    return result;}
public void parquet-mr_f4452_0(int val) throws IOException
{    buffer = buffer << 7;    buffer |= val;    ++count;    if (count == 8) {        out.write((int) (buffer >>> 48) & 0xFF);        out.write((int) (buffer >>> 40) & 0xFF);        out.write((int) (buffer >>> 32) & 0xFF);        out.write((int) (buffer >>> 24) & 0xFF);        out.write((int) (buffer >>> 16) & 0xFF);        out.write((int) (buffer >>> 8) & 0xFF);        out.write((int) (buffer >>> 0) & 0xFF);        buffer = 0;        count = 0;    }}
public void parquet-mr_f4453_0() throws IOException
{    if (count != 0) {        int numberOfBits = count * 7;        finish(numberOfBits, buffer, out);        buffer = 0;        count = 0;    }        out = null;}
public int parquet-mr_f4454_0() throws IOException
{    if (count == 0) {        if (valueCount - totalRead < 8) {            buffer = 0;            int bitsToRead = 7 * (int) (valueCount - totalRead);            int bytesToRead = alignToBytes(bitsToRead);            for (int i = 7 - 1; i >= 7 - bytesToRead; i--) {                buffer |= (((long) in.read()) & 255) << (i * 8);            }            count = 8;            totalRead = valueCount;        } else {            buffer = ((((long) in.read()) & 255) << 48) + ((((long) in.read()) & 255) << 40) + ((((long) in.read()) & 255) << 32) + ((((long) in.read()) & 255) << 24) + (in.read() << 16) + (in.read() << 8) + in.read();            count = 8;            totalRead += 8;        }    }    int result = (((int) (buffer >> ((count - 1) * 7))) & 127);    --count;    return result;}
public void parquet-mr_f4455_0(int val) throws IOException
{    out.write(val);}
public void parquet-mr_f4456_0() throws IOException
{        out = null;}
public int parquet-mr_f4457_0() throws IOException
{    return in.read();}
public void parquet-mr_f4458_0(int value) throws IOException
{    input[inputSize] = value;    ++inputSize;    if (inputSize == VALUES_WRITTEN_AT_A_TIME) {        pack();        if (packedPosition == slabSize) {            slabs.add(BytesInput.from(packed));            totalFullSlabSize += slabSize;            if (slabSize < bitWidth * MAX_SLAB_SIZE_MULT) {                slabSize *= 2;            }            initPackedSlab();        }    }}
private void parquet-mr_f4459_0()
{    packer.pack8Values(input, 0, packed, packedPosition);    packedPosition += bitWidth;    totalValues += inputSize;    inputSize = 0;}
private void parquet-mr_f4460_0()
{    packed = new byte[slabSize];    packedPosition = 0;}
public BytesInput parquet-mr_f4461_1() throws IOException
{    int packedByteLength = packedPosition + BytesUtils.paddedByteCountFromBits(inputSize * bitWidth);        if (inputSize > 0) {        for (int i = inputSize; i < input.length; i++) {            input[i] = 0;        }        pack();    }    return concat(concat(slabs), BytesInput.from(packed, 0, packedByteLength));}
public long parquet-mr_f4462_0()
{    return BytesUtils.paddedByteCountFromBits((totalValues + inputSize) * bitWidth);}
public long parquet-mr_f4463_0()
{    return totalFullSlabSize + packed.length + input.length * 4;}
public String parquet-mr_f4464_0(String prefix)
{    return String.format("%s ByteBitPacking %d slabs, %d bytes", prefix, slabs.size(), getAllocatedSize());}
 int parquet-mr_f4465_0()
{    return slabs.size() + 1;}
public final int parquet-mr_f4466_0()
{    return bitWidth;}
public void parquet-mr_f4467_0(final byte[] input, final int inPos, final int[] output, final int outPos)
{    unpack8Values(ByteBuffer.wrap(input), inPos, output, outPos);}
public void parquet-mr_f4468_0(byte[] input, int inPos, int[] output, int outPos)
{    unpack32Values(ByteBuffer.wrap(input), inPos, output, outPos);}
public final int parquet-mr_f4469_0()
{    return bitWidth;}
public final int parquet-mr_f4470_0()
{    return bitWidth;}
private static IntPackerFactory parquet-mr_f4471_0(String name)
{    return (IntPackerFactory) getStaticField("org.apache.parquet.column.values.bitpacking." + name, "factory");}
private static BytePackerFactory parquet-mr_f4472_0(String name)
{    return (BytePackerFactory) getStaticField("org.apache.parquet.column.values.bitpacking." + name, "factory");}
private static BytePackerForLongFactory parquet-mr_f4473_0(String name)
{    return (BytePackerForLongFactory) getStaticField("org.apache.parquet.column.values.bitpacking." + name, "factory");}
private static Object parquet-mr_f4474_0(String className, String fieldName)
{    try {        return Class.forName(className).getField(fieldName).get(null);    } catch (IllegalArgumentException e) {        throw new RuntimeException(e);    } catch (IllegalAccessException e) {        throw new RuntimeException(e);    } catch (NoSuchFieldException e) {        throw new RuntimeException(e);    } catch (SecurityException e) {        throw new RuntimeException(e);    } catch (ClassNotFoundException e) {        throw new RuntimeException(e);    }}
public IntPacker parquet-mr_f4475_0(int width)
{    return beIntPackerFactory.newIntPacker(width);}
public BytePacker parquet-mr_f4476_0(int width)
{    return beBytePackerFactory.newBytePacker(width);}
public BytePackerForLong parquet-mr_f4477_0(int width)
{    return beBytePackerForLongFactory.newBytePackerForLong(width);}
public IntPacker parquet-mr_f4478_0(int width)
{    return leIntPackerFactory.newIntPacker(width);}
public BytePacker parquet-mr_f4479_0(int width)
{    return leBytePackerFactory.newBytePacker(width);}
public BytePackerForLong parquet-mr_f4480_0(int width)
{    return leBytePackerForLongFactory.newBytePackerForLong(width);}
public void parquet-mr_f4481_0() throws Throwable
{    int[] testVals = { Integer.MIN_VALUE, Integer.MAX_VALUE, 0, 100, 1000, 0xdaedbeef };    for (Integer testVal : testVals) {        BytesInput varInt = BytesInput.fromUnsignedVarInt(testVal);        byte[] rno = varInt.toByteArray();        int i = BytesUtils.readUnsignedVarInt(new ByteArrayInputStream(rno));        assertEquals((int) testVal, i);    }}
public void parquet-mr_f4482_0() throws Throwable
{    CapacityByteArrayOutputStream capacityByteArrayOutputStream = newCapacityBAOS(10);    final int expectedSize = 54;    for (int i = 0; i < expectedSize; i++) {        capacityByteArrayOutputStream.write(i);        assertEquals(i + 1, capacityByteArrayOutputStream.size());    }    validate(capacityByteArrayOutputStream, expectedSize);}
public void parquet-mr_f4483_0() throws Throwable
{    CapacityByteArrayOutputStream capacityByteArrayOutputStream = newCapacityBAOS(10);    int v = 23;    writeArraysOf3(capacityByteArrayOutputStream, v);    validate(capacityByteArrayOutputStream, v * 3);}
public void parquet-mr_f4484_0() throws Throwable
{    CapacityByteArrayOutputStream capacityByteArrayOutputStream = newCapacityBAOS(10);    for (int i = 0; i < 23; i++) {        byte[] toWrite = { (byte) (i * 3), (byte) (i * 3 + 1) };        capacityByteArrayOutputStream.write(toWrite);        capacityByteArrayOutputStream.write((byte) (i * 3 + 2));        assertEquals((i + 1) * 3, capacityByteArrayOutputStream.size());    }    validate(capacityByteArrayOutputStream, 23 * 3);}
protected CapacityByteArrayOutputStream parquet-mr_f4485_0(int initialSize)
{    return new CapacityByteArrayOutputStream(initialSize, 1000000, new HeapByteBufferAllocator());}
public void parquet-mr_f4486_0() throws Throwable
{    CapacityByteArrayOutputStream capacityByteArrayOutputStream = newCapacityBAOS(10);    for (int i = 0; i < 54; i++) {        capacityByteArrayOutputStream.write(i);        assertEquals(i + 1, capacityByteArrayOutputStream.size());    }    capacityByteArrayOutputStream.reset();    for (int i = 0; i < 54; i++) {        capacityByteArrayOutputStream.write(54 + i);        assertEquals(i + 1, capacityByteArrayOutputStream.size());    }    final byte[] byteArray = BytesInput.from(capacityByteArrayOutputStream).toByteArray();    assertEquals(54, byteArray.length);    for (int i = 0; i < 54; i++) {        assertEquals(i + " in " + Arrays.toString(byteArray), 54 + i, byteArray[i]);    }}
public void parquet-mr_f4487_0() throws Throwable
{    CapacityByteArrayOutputStream capacityByteArrayOutputStream = newCapacityBAOS(10);    int v = 23;    writeArraysOf3(capacityByteArrayOutputStream, v);    int n = v * 3;    byte[] toWrite = {     (byte) n, (byte) (n + 1), (byte) (n + 2), (byte) (n + 3), (byte) (n + 4), (byte) (n + 5), (byte) (n + 6), (byte) (n + 7), (byte) (n + 8), (byte) (n + 9), (byte) (n + 10), (byte) (n + 11), (byte) (n + 12), (byte) (n + 13), (byte) (n + 14), (byte) (n + 15), (byte) (n + 16), (byte) (n + 17), (byte) (n + 18), (byte) (n + 19), (byte) (n + 20) };    capacityByteArrayOutputStream.write(toWrite);    n = n + toWrite.length;    assertEquals(n, capacityByteArrayOutputStream.size());    validate(capacityByteArrayOutputStream, n);    capacityByteArrayOutputStream.reset();        capacityByteArrayOutputStream.write(toWrite);    assertEquals(toWrite.length, capacityByteArrayOutputStream.size());    byte[] byteArray = BytesInput.from(capacityByteArrayOutputStream).toByteArray();    assertEquals(toWrite.length, byteArray.length);    for (int i = 0; i < toWrite.length; i++) {        assertEquals(toWrite[i], byteArray[i]);    }}
public void parquet-mr_f4488_0() throws Throwable
{    CapacityByteArrayOutputStream capacityByteArrayOutputStream = newCapacityBAOS(10);    int it = 500;    int v = 23;    for (int j = 0; j < it; j++) {        for (int i = 0; i < v; i++) {            byte[] toWrite = { (byte) (i * 3), (byte) (i * 3 + 1), (byte) (i * 3 + 2) };            capacityByteArrayOutputStream.write(toWrite);            assertEquals((i + 1) * 3 + v * 3 * j, capacityByteArrayOutputStream.size());        }    }    byte[] byteArray = BytesInput.from(capacityByteArrayOutputStream).toByteArray();    assertEquals(v * 3 * it, byteArray.length);    for (int i = 0; i < v * 3 * it; i++) {        assertEquals(i % (v * 3), byteArray[i]);    }        assertTrue("slab count: " + capacityByteArrayOutputStream.getSlabCount(), capacityByteArrayOutputStream.getSlabCount() <= 20);    capacityByteArrayOutputStream.reset();    writeArraysOf3(capacityByteArrayOutputStream, v);    validate(capacityByteArrayOutputStream, v * 3);        assertTrue("slab count: " + capacityByteArrayOutputStream.getSlabCount(), capacityByteArrayOutputStream.getSlabCount() <= 2);}
public void parquet-mr_f4489_0() throws Throwable
{        {        CapacityByteArrayOutputStream cbaos = newCapacityBAOS(5);        cbaos.write(10);        assertEquals(0, cbaos.getCurrentIndex());        cbaos.setByte(0, (byte) 7);        ByteArrayOutputStream baos = new ByteArrayOutputStream();        cbaos.writeTo(baos);        assertEquals(7, baos.toByteArray()[0]);    }        {        CapacityByteArrayOutputStream cbaos = newCapacityBAOS(5);        cbaos.write(10);        cbaos.write(13);        cbaos.write(15);        cbaos.write(17);        assertEquals(3, cbaos.getCurrentIndex());        cbaos.write(19);        cbaos.setByte(3, (byte) 7);        ByteArrayOutputStream baos = new ByteArrayOutputStream();        cbaos.writeTo(baos);        assertArrayEquals(new byte[] { 10, 13, 15, 7, 19 }, baos.toByteArray());    }        {        CapacityByteArrayOutputStream cbaos = newCapacityBAOS(5);                for (int i = 0; i < 12; i++) {            cbaos.write(100 + i);        }        assertEquals(11, cbaos.getCurrentIndex());        cbaos.setByte(6, (byte) 7);        ByteArrayOutputStream baos = new ByteArrayOutputStream();        cbaos.writeTo(baos);        assertArrayEquals(new byte[] { 100, 101, 102, 103, 104, 105, 7, 107, 108, 109, 110, 111 }, baos.toByteArray());    }        {        CapacityByteArrayOutputStream cbaos = newCapacityBAOS(5);                for (int i = 0; i < 12; i++) {            cbaos.write(100 + i);        }        assertEquals(11, cbaos.getCurrentIndex());        cbaos.setByte(9, (byte) 7);        ByteArrayOutputStream baos = new ByteArrayOutputStream();        cbaos.writeTo(baos);        assertArrayEquals(new byte[] { 100, 101, 102, 103, 104, 105, 106, 107, 108, 7, 110, 111 }, baos.toByteArray());    }        {        CapacityByteArrayOutputStream cbaos = newCapacityBAOS(5);                for (int i = 0; i < 12; i++) {            cbaos.write(100 + i);        }        assertEquals(11, cbaos.getCurrentIndex());        cbaos.setByte(11, (byte) 7);        ByteArrayOutputStream baos = new ByteArrayOutputStream();        cbaos.writeTo(baos);        assertArrayEquals(new byte[] { 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 7 }, baos.toByteArray());    }}
private void parquet-mr_f4490_0(CapacityByteArrayOutputStream capacityByteArrayOutputStream, int n) throws IOException
{    for (int i = 0; i < n; i++) {        byte[] toWrite = { (byte) (i * 3), (byte) (i * 3 + 1), (byte) (i * 3 + 2) };        capacityByteArrayOutputStream.write(toWrite);        assertEquals((i + 1) * 3, capacityByteArrayOutputStream.size());    }}
private void parquet-mr_f4491_0(CapacityByteArrayOutputStream capacityByteArrayOutputStream, final int expectedSize) throws IOException
{    final byte[] byteArray = BytesInput.from(capacityByteArrayOutputStream).toByteArray();    assertEquals(expectedSize, byteArray.length);    for (int i = 0; i < expectedSize; i++) {        assertEquals(i, byteArray[i]);    }}
public void parquet-mr_f4492_0() throws IOException
{    int bitLength = 0;    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };    String expected = "";    validateEncodeDecode(bitLength, vals, expected);}
public void parquet-mr_f4493_0() throws IOException
{    int[] vals = { 0 };    String expected = "00000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f4494_0() throws IOException
{    int[] vals = { 1 };    String expected = "10000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f4495_0() throws IOException
{    int[] vals = { 0, 0 };    String expected = "00000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f4496_0() throws IOException
{    int[] vals = { 1, 1 };    String expected = "11000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f4497_0() throws IOException
{    int[] vals = { 1, 1, 1, 1, 1, 1, 1, 1, 1 };    String expected = "11111111 10000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f4498_0() throws IOException
{    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 0, 0 };    String expected = "00000000 00000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f4499_0() throws IOException
{    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 1 };    String expected = "00000001";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f4500_0() throws IOException
{    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 };    String expected = "00000000 01000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f4501_0() throws IOException
{    int[] vals = { 0, 1, 0, 0, 1, 1, 1, 0, 0, 1 };    String expected = "01001110 01000000";    validateEncodeDecode(1, vals, expected);}
public void parquet-mr_f4502_0() throws IOException
{    int[] vals = { 0, 1, 2, 3, 3, 3, 2, 1, 1, 0, 0, 0, 1 };    String expected = "00011011 11111001 01000000 01000000";    validateEncodeDecode(2, vals, expected);}
public void parquet-mr_f4503_0() throws IOException
{    int[] vals = { 0, 1, 2, 3, 4, 5, 6, 7, 1 };    String expected = "00000101 00111001 01110111 " + "00100000";    validateEncodeDecode(3, vals, expected);}
public void parquet-mr_f4504_0() throws IOException
{    int[] vals = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1 };    String expected = "00000001 00100011 01000101 01100111 10001001 10101011 11001101 11101111 00010000";    validateEncodeDecode(4, vals, expected);}
public void parquet-mr_f4505_0() throws IOException
{    int[] vals = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 1 };    String expected = "00000000 01000100 00110010 00010100 11000111 " + "01000010 01010100 10110110 00110101 11001111 " + "10000100 01100101 00111010 01010110 11010111 " + "11000110 01110101 10111110 01110111 11011111 " + "00001000";    validateEncodeDecode(5, vals, expected);}
public void parquet-mr_f4506_0() throws IOException
{    int[] vals = { 0, 28, 34, 35, 63, 1 };        String expected = "00000001 11001000 10100011 " + "11111100 00010000";    validateEncodeDecode(6, vals, expected);}
public void parquet-mr_f4507_0() throws IOException
{    int[] vals = { 0, 28, 34, 35, 63, 1, 125, 1, 1 };        String expected = "00000000 01110001 00010010 00110111 11100000 01111110 10000001 " + "00000010";    validateEncodeDecode(7, vals, expected);}
private void parquet-mr_f4508_1(int bitLength, int[] vals, String expected) throws IOException
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    BitPackingWriter w = BitPacking.getBitPackingWriter(bitLength, baos);    for (int i : vals) {        w.write(i);    }    w.finish();    byte[] bytes = baos.toByteArray();            Assert.assertEquals(expected, toString(bytes));    ByteArrayInputStream bais = new ByteArrayInputStream(bytes);    BitPackingReader r = BitPacking.createBitPackingReader(bitLength, bais, vals.length);    int[] result = new int[vals.length];    for (int i = 0; i < result.length; i++) {        result[i] = r.read();    }        assertArrayEquals(vals, result);}
public static String parquet-mr_f4509_0(int[] vals)
{    StringBuilder sb = new StringBuilder();    boolean first = true;    for (int i : vals) {        if (first) {            first = false;        } else {            sb.append(" ");        }        sb.append(i);    }    return sb.toString();}
public static String parquet-mr_f4510_0(long[] vals)
{    StringBuilder sb = new StringBuilder();    boolean first = true;    for (long i : vals) {        if (first) {            first = false;        } else {            sb.append(" ");        }        sb.append(i);    }    return sb.toString();}
public static String parquet-mr_f4511_0(byte[] bytes)
{    StringBuilder sb = new StringBuilder();    boolean first = true;    for (byte b : bytes) {        if (first) {            first = false;        } else {            sb.append(" ");        }        int i = b < 0 ? 256 + b : b;        String binaryString = Integer.toBinaryString(i);        for (int j = binaryString.length(); j < 8; ++j) {            sb.append("0");        }        sb.append(binaryString);    }    return sb.toString();}
public void parquet-mr_f4512_0()
{    for (int i = 0; i <= 32; i++) {        final ByteBasedBitPackingEncoder encoder = new ByteBasedBitPackingEncoder(i, Packer.BIG_ENDIAN);                final int totalValues = 191 * 1024 * 8 + 10;        for (int j = 0; j < totalValues; j++) {            try {                encoder.writeInt(j);            } catch (Exception e) {                throw new RuntimeException(i + ": error writing " + j, e);            }        }        assertEquals(BytesUtils.paddedByteCountFromBits(totalValues * i), encoder.getBufferSize());        assertEquals(i == 0 ? 1 : 9, encoder.getNumSlabs());    }}
public void parquet-mr_f4513_1()
{            for (int i = 1; i < 32; i++) {                int[] unpacked = new int[32];        int[] values = generateValues(i);        packUnpack(Packer.BIG_ENDIAN.newBytePacker(i), values, unpacked);                Assert.assertArrayEquals("width " + i, values, unpacked);    }}
public void parquet-mr_f4514_1()
{            for (int i = 1; i < 64; i++) {                long[] unpacked32 = new long[32];        long[] unpacked8 = new long[32];        long[] values = generateValuesLong(i);        packUnpack32(Packer.BIG_ENDIAN.newBytePackerForLong(i), values, unpacked32);                Assert.assertArrayEquals("width " + i, values, unpacked32);        packUnpack8(Packer.BIG_ENDIAN.newBytePackerForLong(i), values, unpacked8);                Assert.assertArrayEquals("width " + i, values, unpacked8);    }}
private void parquet-mr_f4515_1(BytePacker packer, int[] values, int[] unpacked)
{    byte[] packed = new byte[packer.getBitWidth() * 4];    packer.pack32Values(values, 0, packed, 0);        packer.unpack32Values(ByteBuffer.wrap(packed), 0, unpacked, 0);}
private void parquet-mr_f4516_1(BytePackerForLong packer, long[] values, long[] unpacked)
{    byte[] packed = new byte[packer.getBitWidth() * 4];    packer.pack32Values(values, 0, packed, 0);        packer.unpack32Values(packed, 0, unpacked, 0);}
private void parquet-mr_f4517_1(BytePackerForLong packer, long[] values, long[] unpacked)
{    byte[] packed = new byte[packer.getBitWidth() * 4];    for (int i = 0; i < 4; i++) {        packer.pack8Values(values, 8 * i, packed, packer.getBitWidth() * i);    }        for (int i = 0; i < 4; i++) {        packer.unpack8Values(packed, packer.getBitWidth() * i, unpacked, 8 * i);    }}
private int[] parquet-mr_f4518_1(int bitWidth)
{    int[] values = new int[32];    for (int j = 0; j < values.length; j++) {        values[j] = (int) (Math.random() * 100000) % (int) Math.pow(2, bitWidth);    }        return values;}
private long[] parquet-mr_f4519_1(int bitWidth)
{    long[] values = new long[32];    Random random = new Random(0);    for (int j = 0; j < values.length; j++) {        values[j] = random.nextLong() & ((1l << bitWidth) - 1l);    }        return values;}
public void parquet-mr_f4520_1() throws IOException
{            for (int i = 1; i < 8; i++) {                byte[] packed = new byte[i * 4];        int[] unpacked = new int[32];        int[] values = generateValues(i);                final BytePacker packer = Packer.BIG_ENDIAN.newBytePacker(i);        packer.pack32Values(values, 0, packed, 0);                        final ByteArrayOutputStream manualOut = new ByteArrayOutputStream();        final BitPackingWriter writer = BitPacking.getBitPackingWriter(i, manualOut);        for (int j = 0; j < values.length; j++) {            writer.write(values[j]);        }        final byte[] packedManualAsBytes = manualOut.toByteArray();                        final BitPackingReader reader = BitPacking.createBitPackingReader(i, new ByteArrayInputStream(packed), 32);        for (int j = 0; j < unpacked.length; j++) {            unpacked[j] = reader.read();        }                Assert.assertArrayEquals("width " + i, values, unpacked);    }}
public void parquet-mr_f4521_1() throws IOException
{    for (Packer pack : Packer.values()) {                        for (int i = 1; i < 32; i++) {                        int[] packed = new int[i];            int[] unpacked = new int[32];            int[] values = generateValues(i);                        final IntPacker packer = pack.newIntPacker(i);            packer.pack32Values(values, 0, packed, 0);                        final ByteArrayOutputStream lemireOut = new ByteArrayOutputStream();            for (int v : packed) {                switch(pack) {                    case LITTLE_ENDIAN:                        lemireOut.write((v >>> 0) & 0xFF);                        lemireOut.write((v >>> 8) & 0xFF);                        lemireOut.write((v >>> 16) & 0xFF);                        lemireOut.write((v >>> 24) & 0xFF);                        break;                    case BIG_ENDIAN:                        lemireOut.write((v >>> 24) & 0xFF);                        lemireOut.write((v >>> 16) & 0xFF);                        lemireOut.write((v >>> 8) & 0xFF);                        lemireOut.write((v >>> 0) & 0xFF);                        break;                }            }            final byte[] packedByLemireAsBytes = lemireOut.toByteArray();                                    final BytePacker bytePacker = pack.newBytePacker(i);            byte[] packedGenerated = new byte[i * 4];            bytePacker.pack32Values(values, 0, packedGenerated, 0);                        Assert.assertEquals(pack.name() + " width " + i, TestBitPacking.toString(packedByLemireAsBytes), TestBitPacking.toString(packedGenerated));            bytePacker.unpack32Values(ByteBuffer.wrap(packedByLemireAsBytes), 0, unpacked, 0);                        Assert.assertArrayEquals("width " + i, values, unpacked);        }    }}
public void parquet-mr_f4522_1()
{    for (Packer packer : Packer.values()) {                        for (int i = 1; i < 32; i++) {                        int[] values = generateValues(i);            int[] unpacked = new int[32];            {                packUnpack(packer.newIntPacker(i), values, unpacked);                                Assert.assertArrayEquals(packer.name() + " width " + i, values, unpacked);            }            {                packUnpack(packer.newBytePacker(i), values, unpacked);                                Assert.assertArrayEquals(packer.name() + " width " + i, values, unpacked);            }        }    }}
private void parquet-mr_f4523_0(IntPacker packer, int[] values, int[] unpacked)
{    int[] packed = new int[packer.getBitWidth()];    packer.pack32Values(values, 0, packed, 0);    packer.unpack32Values(packed, 0, unpacked, 0);}
private void parquet-mr_f4524_0(BytePacker packer, int[] values, int[] unpacked)
{    byte[] packed = new byte[packer.getBitWidth() * 4];    packer.pack32Values(values, 0, packed, 0);    packer.unpack32Values(ByteBuffer.wrap(packed), 0, unpacked, 0);}
private int[] parquet-mr_f4525_1(int bitWidth)
{    int[] values = new int[32];    for (int j = 0; j < values.length; j++) {        values[j] = (int) (Math.random() * 100000) % (int) Math.pow(2, bitWidth);    }        return values;}
public void parquet-mr_f4526_1() throws IOException
{            for (int i = 1; i < 8; i++) {                int[] packed = new int[i];        int[] unpacked = new int[32];        int[] values = generateValues(i);                final IntPacker packer = Packer.BIG_ENDIAN.newIntPacker(i);        packer.pack32Values(values, 0, packed, 0);                final ByteArrayOutputStream lemireOut = new ByteArrayOutputStream();        for (int v : packed) {            lemireOut.write((v >>> 24) & 0xFF);            lemireOut.write((v >>> 16) & 0xFF);            lemireOut.write((v >>> 8) & 0xFF);            lemireOut.write((v >>> 0) & 0xFF);        }        final byte[] packedByLemireAsBytes = lemireOut.toByteArray();                        final ByteArrayOutputStream manualOut = new ByteArrayOutputStream();        final BitPackingWriter writer = BitPacking.getBitPackingWriter(i, manualOut);        for (int j = 0; j < values.length; j++) {            writer.write(values[j]);        }        final byte[] packedManualAsBytes = manualOut.toByteArray();                        final BitPackingReader reader = BitPacking.createBitPackingReader(i, new ByteArrayInputStream(packedByLemireAsBytes), 32);        for (int j = 0; j < unpacked.length; j++) {            unpacked[j] = reader.read();        }                Assert.assertArrayEquals("width " + i, values, unpacked);    }}
public DelegatingFieldConsumer parquet-mr_f4527_0(TFieldIdEnum e, TypedConsumer typedConsumer)
{    Map<Short, TypedConsumer> newContexts = new HashMap<Short, TypedConsumer>(contexts);    newContexts.put(e.getThriftFieldId(), typedConsumer);    return new DelegatingFieldConsumer(defaultFieldEventConsumer, newContexts);}
public void parquet-mr_f4528_0(TProtocol protocol, EventBasedThriftReader reader, short id, byte type) throws TException
{    TypedConsumer delegate = contexts.get(id);    if (delegate != null) {        delegate.read(protocol, reader, type);    } else {        defaultFieldEventConsumer.consumeField(protocol, reader, id, type);    }}
public static DelegatingFieldConsumer parquet-mr_f4529_0()
{    return new DelegatingFieldConsumer();}
public static ListConsumer parquet-mr_f4530_0(Class<T> c, final Consumer<List<T>> consumer)
{    class ListConsumer implements Consumer<T> {        List<T> list;        @Override        public void consume(T t) {            list.add(t);        }    }    final ListConsumer co = new ListConsumer();    return new DelegatingListElementsConsumer(struct(c, co)) {        @Override        public void consumeList(TProtocol protocol, EventBasedThriftReader reader, TList tList) throws TException {            co.list = new ArrayList<T>();            super.consumeList(protocol, reader, tList);            consumer.consume(co.list);        }    };}
public void parquet-mr_f4531_0(T t)
{    list.add(t);}
public void parquet-mr_f4532_0(TProtocol protocol, EventBasedThriftReader reader, TList tList) throws TException
{    co.list = new ArrayList<T>();    super.consumeList(protocol, reader, tList);    consumer.consume(co.list);}
public static ListConsumer parquet-mr_f4533_0(TypedConsumer consumer)
{    return new DelegatingListElementsConsumer(consumer);}
public static StructConsumer parquet-mr_f4534_0(final Class<T> c, final Consumer<T> consumer)
{    return new TBaseStructConsumer<T>(c, consumer);}
public void parquet-mr_f4535_0(TProtocol protocol, EventBasedThriftReader reader, short id, byte type) throws TException
{    TProtocolUtil.skip(protocol, type);}
public void parquet-mr_f4536_0(TProtocol protocol, EventBasedThriftReader reader, byte elemType) throws TException
{    elementConsumer.read(protocol, reader, elemType);}
public void parquet-mr_f4537_0(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    T o = newObject();    o.read(protocol);    consumer.consume(o);}
protected T parquet-mr_f4538_0()
{    try {        return c.newInstance();    } catch (InstantiationException e) {        throw new RuntimeException(c.getName(), e);    } catch (IllegalAccessException e) {        throw new RuntimeException(c.getName(), e);    }}
public void parquet-mr_f4539_0(FieldConsumer c) throws TException
{    protocol.readStructBegin();    readStructContent(c);    protocol.readStructEnd();}
public void parquet-mr_f4540_0(FieldConsumer c) throws TException
{    TField field;    while (true) {        field = protocol.readFieldBegin();        if (field.type == TType.STOP) {            break;        }        c.consumeField(protocol, this, field.id, field.type);    }}
public void parquet-mr_f4541_0(SetConsumer eventConsumer, TSet tSet) throws TException
{    for (int i = 0; i < tSet.size; i++) {        eventConsumer.consumeElement(protocol, this, tSet.elemType);    }}
public void parquet-mr_f4542_0(MapConsumer eventConsumer, TMap tMap) throws TException
{    for (int i = 0; i < tMap.size; i++) {        eventConsumer.consumeEntry(protocol, this, tMap.keyType, tMap.valueType);    }}
public void parquet-mr_f4543_0(byte keyType, TypedConsumer keyConsumer, byte valueType, TypedConsumer valueConsumer) throws TException
{    keyConsumer.read(protocol, this, keyType);    valueConsumer.read(protocol, this, valueType);}
public void parquet-mr_f4544_0(ListConsumer eventConsumer, TList tList) throws TException
{    for (int i = 0; i < tList.size; i++) {        eventConsumer.consumeElement(protocol, this, tList.elemType);    }}
 final void parquet-mr_f4545_0(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consume(protocol.readDouble());}
 final void parquet-mr_f4546_0(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consume(protocol.readByte());}
 final void parquet-mr_f4547_0(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consume(protocol.readBool());}
 final void parquet-mr_f4548_0(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consume(protocol.readI32());}
 final void parquet-mr_f4549_0(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consume(protocol.readI64());}
 final void parquet-mr_f4550_0(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consume(protocol.readI16());}
 final void parquet-mr_f4551_0(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consume(protocol.readString());}
 final void parquet-mr_f4552_0(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consumeStruct(protocol, reader);}
 final void parquet-mr_f4553_0(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consumeList(protocol, reader, protocol.readListBegin());    protocol.readListEnd();}
public void parquet-mr_f4554_0(TProtocol protocol, EventBasedThriftReader reader, TList tList) throws TException
{    reader.readListContent(this, tList);}
 final void parquet-mr_f4555_0(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consumeSet(protocol, reader, protocol.readSetBegin());    protocol.readSetEnd();}
public void parquet-mr_f4556_0(TProtocol protocol, EventBasedThriftReader reader, TSet tSet) throws TException
{    reader.readSetContent(this, tSet);}
 final void parquet-mr_f4557_0(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consumeMap(protocol, reader, protocol.readMapBegin());    protocol.readMapEnd();}
public void parquet-mr_f4558_0(TProtocol protocol, EventBasedThriftReader reader, TMap tMap) throws TException
{    reader.readMapContent(this, tMap);}
public final void parquet-mr_f4559_0(TProtocol protocol, EventBasedThriftReader reader, byte type) throws TException
{    if (this.type != type) {        throw new TException("Incorrect type in stream. " + "Expected " + this.type + " but got " + type);    }    this.read(protocol, reader);}
public TTransport parquet-mr_f4560_0()
{    return delegate.getTransport();}
public void parquet-mr_f4561_0(TMessage message) throws TException
{    delegate.writeMessageBegin(message);}
public void parquet-mr_f4562_0() throws TException
{    delegate.writeMessageEnd();}
public int parquet-mr_f4563_0()
{    return delegate.hashCode();}
public void parquet-mr_f4564_0(TStruct struct) throws TException
{    delegate.writeStructBegin(struct);}
public void parquet-mr_f4565_0() throws TException
{    delegate.writeStructEnd();}
public void parquet-mr_f4566_0(TField field) throws TException
{    delegate.writeFieldBegin(field);}
public void parquet-mr_f4567_0() throws TException
{    delegate.writeFieldEnd();}
public void parquet-mr_f4568_0() throws TException
{    delegate.writeFieldStop();}
public void parquet-mr_f4569_0(TMap map) throws TException
{    delegate.writeMapBegin(map);}
public void parquet-mr_f4570_0() throws TException
{    delegate.writeMapEnd();}
public void parquet-mr_f4571_0(TList list) throws TException
{    delegate.writeListBegin(list);}
public void parquet-mr_f4572_0() throws TException
{    delegate.writeListEnd();}
public void parquet-mr_f4573_0(TSet set) throws TException
{    delegate.writeSetBegin(set);}
public void parquet-mr_f4574_0() throws TException
{    delegate.writeSetEnd();}
public void parquet-mr_f4575_0(boolean b) throws TException
{    delegate.writeBool(b);}
public void parquet-mr_f4576_0(byte b) throws TException
{    delegate.writeByte(b);}
public void parquet-mr_f4577_0(short i16) throws TException
{    delegate.writeI16(i16);}
public void parquet-mr_f4578_0(int i32) throws TException
{    delegate.writeI32(i32);}
public void parquet-mr_f4579_0(long i64) throws TException
{    delegate.writeI64(i64);}
public void parquet-mr_f4580_0(double dub) throws TException
{    delegate.writeDouble(dub);}
public void parquet-mr_f4581_0(String str) throws TException
{    delegate.writeString(str);}
public void parquet-mr_f4582_0(ByteBuffer buf) throws TException
{    delegate.writeBinary(buf);}
public TMessage parquet-mr_f4583_0() throws TException
{    return delegate.readMessageBegin();}
public void parquet-mr_f4584_0() throws TException
{    delegate.readMessageEnd();}
public TStruct parquet-mr_f4585_0() throws TException
{    return delegate.readStructBegin();}
public void parquet-mr_f4586_0() throws TException
{    delegate.readStructEnd();}
public TField parquet-mr_f4587_0() throws TException
{    return delegate.readFieldBegin();}
public void parquet-mr_f4588_0() throws TException
{    delegate.readFieldEnd();}
public TMap parquet-mr_f4589_0() throws TException
{    return delegate.readMapBegin();}
public void parquet-mr_f4590_0() throws TException
{    delegate.readMapEnd();}
public TList parquet-mr_f4591_0() throws TException
{    return delegate.readListBegin();}
public void parquet-mr_f4592_0() throws TException
{    delegate.readListEnd();}
public TSet parquet-mr_f4593_0() throws TException
{    return delegate.readSetBegin();}
public void parquet-mr_f4594_0() throws TException
{    delegate.readSetEnd();}
public boolean parquet-mr_f4595_0(Object obj)
{    return delegate.equals(obj);}
public boolean parquet-mr_f4596_0() throws TException
{    return delegate.readBool();}
public byte parquet-mr_f4597_0() throws TException
{    return delegate.readByte();}
public short parquet-mr_f4598_0() throws TException
{    return delegate.readI16();}
public int parquet-mr_f4599_0() throws TException
{    return delegate.readI32();}
public long parquet-mr_f4600_0() throws TException
{    return delegate.readI64();}
public double parquet-mr_f4601_0() throws TException
{    return delegate.readDouble();}
public String parquet-mr_f4602_0() throws TException
{        return delegate.readString().intern();}
public ByteBuffer parquet-mr_f4603_0() throws TException
{    return delegate.readBinary();}
public void parquet-mr_f4604_0()
{    delegate.reset();}
public String parquet-mr_f4605_0()
{    return delegate.toString();}
public static LogicalType parquet-mr_f4606_0(int scale, int precision)
{    return LogicalType.DECIMAL(new DecimalType(scale, precision));}
public static void parquet-mr_f4607_0(ColumnIndex columnIndex, OutputStream to) throws IOException
{    write(columnIndex, to);}
public static ColumnIndex parquet-mr_f4608_0(InputStream from) throws IOException
{    return read(from, new ColumnIndex());}
public static void parquet-mr_f4609_0(OffsetIndex offsetIndex, OutputStream to) throws IOException
{    write(offsetIndex, to);}
public static OffsetIndex parquet-mr_f4610_0(InputStream from) throws IOException
{    return read(from, new OffsetIndex());}
public static void parquet-mr_f4611_0(PageHeader pageHeader, OutputStream to) throws IOException
{    write(pageHeader, to);}
public static PageHeader parquet-mr_f4612_0(InputStream from) throws IOException
{    return read(from, new PageHeader());}
public static void parquet-mr_f4613_0(org.apache.parquet.format.FileMetaData fileMetadata, OutputStream to) throws IOException
{    write(fileMetadata, to);}
public static FileMetaData parquet-mr_f4614_0(InputStream from) throws IOException
{    return read(from, new FileMetaData());}
public static FileMetaData parquet-mr_f4615_0(InputStream from, boolean skipRowGroups) throws IOException
{    FileMetaData md = new FileMetaData();    if (skipRowGroups) {        readFileMetaData(from, new DefaultFileMetaDataConsumer(md), skipRowGroups);    } else {        read(from, md);    }    return md;}
public void parquet-mr_f4616_0(int version)
{    md.setVersion(version);}
public void parquet-mr_f4617_0(List<SchemaElement> schema)
{    md.setSchema(schema);}
public void parquet-mr_f4618_0(long numRows)
{    md.setNum_rows(numRows);}
public void parquet-mr_f4619_0(String createdBy)
{    md.setCreated_by(createdBy);}
public void parquet-mr_f4620_0(RowGroup rowGroup)
{    md.addToRow_groups(rowGroup);}
public void parquet-mr_f4621_0(KeyValue kv)
{    md.addToKey_value_metadata(kv);}
public static void parquet-mr_f4622_0(InputStream from, FileMetaDataConsumer consumer) throws IOException
{    readFileMetaData(from, consumer, false);}
public static void parquet-mr_f4623_0(InputStream from, final FileMetaDataConsumer consumer, boolean skipRowGroups) throws IOException
{    try {        DelegatingFieldConsumer eventConsumer = fieldConsumer().onField(VERSION, new I32Consumer() {            @Override            public void consume(int value) {                consumer.setVersion(value);            }        }).onField(SCHEMA, listOf(SchemaElement.class, new Consumer<List<SchemaElement>>() {            @Override            public void consume(List<SchemaElement> schema) {                consumer.setSchema(schema);            }        })).onField(NUM_ROWS, new I64Consumer() {            @Override            public void consume(long value) {                consumer.setNumRows(value);            }        }).onField(KEY_VALUE_METADATA, listElementsOf(struct(KeyValue.class, new Consumer<KeyValue>() {            @Override            public void consume(KeyValue kv) {                consumer.addKeyValueMetaData(kv);            }        }))).onField(CREATED_BY, new StringConsumer() {            @Override            public void consume(String value) {                consumer.setCreatedBy(value);            }        });        if (!skipRowGroups) {            eventConsumer = eventConsumer.onField(ROW_GROUPS, listElementsOf(struct(RowGroup.class, new Consumer<RowGroup>() {                @Override                public void consume(RowGroup rowGroup) {                    consumer.addRowGroup(rowGroup);                }            })));        }        new EventBasedThriftReader(protocol(from)).readStruct(eventConsumer);    } catch (TException e) {        throw new IOException("can not read FileMetaData: " + e.getMessage(), e);    }}
public void parquet-mr_f4624_0(int value)
{    consumer.setVersion(value);}
public void parquet-mr_f4625_0(List<SchemaElement> schema)
{    consumer.setSchema(schema);}
public void parquet-mr_f4626_0(long value)
{    consumer.setNumRows(value);}
public void parquet-mr_f4627_0(KeyValue kv)
{    consumer.addKeyValueMetaData(kv);}
public void parquet-mr_f4628_0(String value)
{    consumer.setCreatedBy(value);}
public void parquet-mr_f4629_0(RowGroup rowGroup)
{    consumer.addRowGroup(rowGroup);}
private static TProtocol parquet-mr_f4630_0(OutputStream to)
{    return protocol(new TIOStreamTransport(to));}
private static TProtocol parquet-mr_f4631_0(InputStream from)
{    return protocol(new TIOStreamTransport(from));}
private static InterningProtocol parquet-mr_f4632_0(TIOStreamTransport t)
{    return new InterningProtocol(new TCompactProtocol(t));}
private static T parquet-mr_f4633_0(InputStream from, T tbase) throws IOException
{    try {        tbase.read(protocol(from));        return tbase;    } catch (TException e) {        throw new IOException("can not read " + tbase.getClass() + ": " + e.getMessage(), e);    }}
private static void parquet-mr_f4634_0(TBase<?, ?> tbase, OutputStream to) throws IOException
{    try {        tbase.write(protocol(to));    } catch (TException e) {        throw new IOException("can not write " + tbase, e);    }}
public void parquet-mr_f4635_0() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    FileMetaData md = new FileMetaData(1, asList(new SchemaElement("foo")), 10, asList(new RowGroup(asList(new ColumnChunk(0), new ColumnChunk(1)), 10, 5), new RowGroup(asList(new ColumnChunk(2), new ColumnChunk(3)), 11, 5)));    writeFileMetaData(md, baos);    FileMetaData md2 = readFileMetaData(in(baos));    FileMetaData md3 = new FileMetaData();    readFileMetaData(in(baos), new DefaultFileMetaDataConsumer(md3));    FileMetaData md4 = new FileMetaData();    readFileMetaData(in(baos), new DefaultFileMetaDataConsumer(md4), true);    FileMetaData md5 = readFileMetaData(in(baos), true);    FileMetaData md6 = readFileMetaData(in(baos), false);    assertEquals(md, md2);    assertEquals(md, md3);    assertNull(md4.getRow_groups());    assertNull(md5.getRow_groups());    assertEquals(md4, md5);    md4.setRow_groups(md.getRow_groups());    md5.setRow_groups(md.getRow_groups());    assertEquals(md, md4);    assertEquals(md, md5);    assertEquals(md4, md5);    assertEquals(md, md6);}
private ByteArrayInputStream parquet-mr_f4636_0(ByteArrayOutputStream baos)
{    return new ByteArrayInputStream(baos.toByteArray());}
public static void parquet-mr_f4637_0(String[] args) throws Exception
{    String basePath = args[0];        generateScheme(false, true, basePath);        generateScheme(false, false, basePath);        generateScheme(true, true, basePath);        generateScheme(true, false, basePath);}
private static void parquet-mr_f4638_0(boolean isLong, boolean msbFirst, String basePath) throws IOException
{    String baseClassName = isLong ? CLASS_NAME_PREFIX_FOR_LONG : CLASS_NAME_PREFIX_FOR_INT;    String className = msbFirst ? (baseClassName + "BE") : (baseClassName + "LE");    int maxBits = isLong ? MAX_BITS_FOR_LONG : MAX_BITS_FOR_INT;    String nameSuffix = isLong ? "ForLong" : "";    final File file = new File(basePath + "/org/apache/parquet/column/values/bitpacking/" + className + ".java").getAbsoluteFile();    if (!file.getParentFile().exists()) {        file.getParentFile().mkdirs();    }    try (FileWriter fw = new FileWriter(file)) {        fw.append("package org.apache.parquet.column.values.bitpacking;\n");        fw.append("import java.nio.ByteBuffer;\n");        fw.append("\n");        fw.append("/**\n");        if (msbFirst) {            fw.append(" * Packs from the Most Significant Bit first\n");        } else {            fw.append(" * Packs from the Least Significant Bit first\n");        }        fw.append(" * \n");        fw.append(" * See ByteBasedBitPackingGenerator to make changes to this file\n");        fw.append(" * Automatically generated\n");        fw.append(" *\n");        fw.append(" */\n");        fw.append("public abstract class " + className + " {\n");        fw.append("\n");        fw.append("  private static final BytePacker" + nameSuffix + "[] packers = new BytePacker" + nameSuffix + "[" + (maxBits + 1) + "];\n");        fw.append("  static {\n");        for (int i = 0; i <= maxBits; i++) {            fw.append("    packers[" + i + "] = new Packer" + i + "();\n");        }        fw.append("  }\n");        fw.append("\n");        fw.append("  public static final BytePacker" + nameSuffix + "Factory factory = new BytePacker" + nameSuffix + "Factory() {\n");        fw.append("    public BytePacker" + nameSuffix + " newBytePacker" + nameSuffix + "(int bitWidth) {\n");        fw.append("      return packers[bitWidth];\n");        fw.append("    }\n");        fw.append("  };\n");        fw.append("\n");        for (int i = 0; i <= maxBits; i++) {            generateClass(fw, i, isLong, msbFirst);            fw.append("\n");        }        fw.append("}\n");    }}
private static void parquet-mr_f4639_0(FileWriter fw, int bitWidth, boolean isLong, boolean msbFirst) throws IOException
{    String nameSuffix = isLong ? "ForLong" : "";    fw.append("  private static final class Packer" + bitWidth + " extends BytePacker" + nameSuffix + " {\n");    fw.append("\n");    fw.append("    private Packer" + bitWidth + "() {\n");    fw.append("      super(" + bitWidth + ");\n");    fw.append("    }\n");    fw.append("\n");        generatePack(fw, bitWidth, 1, isLong, msbFirst);    generatePack(fw, bitWidth, 4, isLong, msbFirst);        generateUnpack(fw, bitWidth, 1, isLong, msbFirst, true);    generateUnpack(fw, bitWidth, 1, isLong, msbFirst, false);    generateUnpack(fw, bitWidth, 4, isLong, msbFirst, true);    generateUnpack(fw, bitWidth, 4, isLong, msbFirst, false);    fw.append("  }\n");}
private static ShiftMask parquet-mr_f4640_0(FileWriter fw, int bitWidth, boolean isLong, boolean msbFirst, int byteIndex, int valueIndex) throws IOException
{        int valueStartBitIndex = (valueIndex * bitWidth) - (8 * (byteIndex));    int valueEndBitIndex = ((valueIndex + 1) * bitWidth) - (8 * (byteIndex + 1));        int valueStartBitWanted;    int valueEndBitWanted;        int byteStartBitWanted;    int byteEndBitWanted;    int shift;    int widthWanted;    if (msbFirst) {        valueStartBitWanted = valueStartBitIndex < 0 ? bitWidth - 1 + valueStartBitIndex : bitWidth - 1;        valueEndBitWanted = valueEndBitIndex > 0 ? valueEndBitIndex : 0;        byteStartBitWanted = valueStartBitIndex < 0 ? 8 : 7 - valueStartBitIndex;        byteEndBitWanted = valueEndBitIndex > 0 ? 0 : -valueEndBitIndex;        shift = valueEndBitWanted - byteEndBitWanted;        widthWanted = Math.min(7, byteStartBitWanted) - Math.min(7, byteEndBitWanted) + 1;    } else {        valueStartBitWanted = bitWidth - 1 - (valueEndBitIndex > 0 ? valueEndBitIndex : 0);        valueEndBitWanted = bitWidth - 1 - (valueStartBitIndex < 0 ? bitWidth - 1 + valueStartBitIndex : bitWidth - 1);        byteStartBitWanted = 7 - (valueEndBitIndex > 0 ? 0 : -valueEndBitIndex);        byteEndBitWanted = 7 - (valueStartBitIndex < 0 ? 8 : 7 - valueStartBitIndex);        shift = valueStartBitWanted - byteStartBitWanted;        widthWanted = Math.max(0, byteStartBitWanted) - Math.max(0, byteEndBitWanted) + 1;    }    int maskWidth = widthWanted + Math.max(0, shift);    visualizeAlignment(fw, bitWidth, valueEndBitIndex, valueStartBitWanted, valueEndBitWanted, byteStartBitWanted, byteEndBitWanted, shift);    return new ShiftMask(shift, genMask(maskWidth, isLong));}
private static void parquet-mr_f4641_0(FileWriter fw, int bitWidth, int valueEndBitIndex, int valueStartBitWanted, int valueEndBitWanted, int byteStartBitWanted, int byteEndBitWanted, int shift) throws IOException
{        fw.append("//");    int buf = 2 + Math.max(0, bitWidth + 8);    for (int i = 0; i < buf; i++) {        fw.append(" ");    }    fw.append("[");    for (int i = 7; i >= 0; i--) {        if (i <= byteStartBitWanted && i >= byteEndBitWanted) {            fw.append(String.valueOf(i));        } else {            fw.append("_");        }    }    fw.append("]\n          //");    for (int i = 0; i < buf + (8 - bitWidth + shift); i++) {        fw.append(" ");    }    fw.append("[");    for (int i = bitWidth - 1; i >= 0; i--) {        if (i <= valueStartBitWanted && i >= valueEndBitWanted) {            fw.append(String.valueOf(i % 10));        } else {            fw.append("_");        }    }    fw.append("]\n");    fw.append("           ");}
private static void parquet-mr_f4642_0(FileWriter fw, int bitWidth, int batch, boolean isLong, boolean msbFirst) throws IOException
{    long mask = genMask(bitWidth, isLong);    String maskSuffix = isLong ? "L" : "";    String variableType = isLong ? VARIABLE_TYPE_FOR_LONG : VARIABLE_TYPE_FOR_INT;    fw.append("    public final void pack" + (batch * 8) + "Values(final " + variableType + "[] in, final int inPos, final byte[] out, final int outPos) {\n");    for (int byteIndex = 0; byteIndex < bitWidth * batch; ++byteIndex) {        fw.append("      out[" + align(byteIndex, 2) + " + outPos] = (byte)((\n");        int startIndex = (byteIndex * 8) / bitWidth;        int endIndex = ((byteIndex + 1) * 8 + bitWidth - 1) / bitWidth;        for (int valueIndex = startIndex; valueIndex < endIndex; valueIndex++) {            if (valueIndex == startIndex) {                fw.append("          ");            } else {                fw.append("\n        | ");            }            ShiftMask shiftMask = getShift(fw, bitWidth, isLong, msbFirst, byteIndex, valueIndex);                        String shiftString = "";            if (shiftMask.shift > 0) {                shiftString = " >>> " + shiftMask.shift;            } else if (shiftMask.shift < 0) {                shiftString = " <<  " + (-shiftMask.shift);            }            fw.append("((in[" + align(valueIndex, 2) + " + inPos] & " + mask + maskSuffix + ")" + shiftString + ")");        }        fw.append(") & 255);\n");    }    fw.append("    }\n");}
private static void parquet-mr_f4643_0(FileWriter fw, int bitWidth, int batch, boolean isLong, boolean msbFirst, boolean useByteArray) throws IOException
{    final String variableType = isLong ? VARIABLE_TYPE_FOR_LONG : VARIABLE_TYPE_FOR_INT;    final String bufferDataType = useByteArray ? "byte[]" : "ByteBuffer";    fw.append("    public final void unpack" + (batch * 8) + "Values(final " + bufferDataType + " in, " + "final int inPos, final " + variableType + "[] out, final int outPos) {\n");    if (bitWidth > 0) {        String maskSuffix = isLong ? "L" : "";        for (int valueIndex = 0; valueIndex < (batch * 8); ++valueIndex) {            fw.append("      out[" + align(valueIndex, 2) + " + outPos] =\n");            int startIndex = valueIndex * bitWidth / 8;            int endIndex = paddedByteCountFromBits((valueIndex + 1) * bitWidth);            for (int byteIndex = startIndex; byteIndex < endIndex; byteIndex++) {                if (byteIndex == startIndex) {                    fw.append("          ");                } else {                    fw.append("\n        | ");                }                ShiftMask shiftMask = getShift(fw, bitWidth, isLong, msbFirst, byteIndex, valueIndex);                                String shiftString = "";                if (shiftMask.shift < 0) {                    shiftString = ">>  " + (-shiftMask.shift);                } else if (shiftMask.shift > 0) {                    shiftString = "<<  " + shiftMask.shift;                }                final String byteAccess;                if (useByteArray) {                    byteAccess = "in[" + align(byteIndex, 2) + " + inPos]";                } else {                                        byteAccess = "in.get(" + align(byteIndex, 2) + " + inPos)";                }                                fw.append(" ((((" + variableType + ")" + byteAccess + ") " + shiftString + ") & " + shiftMask.mask + maskSuffix + ")");            }            fw.append(";\n");        }    }    fw.append("    }\n");}
private static long parquet-mr_f4644_0(int bitWidth, boolean isLong)
{    int maxBitWidth = isLong ? MAX_BITS_FOR_LONG : MAX_BITS_FOR_INT;    if (bitWidth >= maxBitWidth) {                return -1;    }    long mask = 0;    for (int i = 0; i < bitWidth; i++) {        mask <<= 1;        mask |= 1;    }    return mask;}
private static String parquet-mr_f4645_0(int value, int digits)
{    final String valueString = String.valueOf(value);    StringBuilder result = new StringBuilder();    for (int i = valueString.length(); i < digits; i++) {        result.append(" ");    }    result.append(valueString);    return result.toString();}
private static int parquet-mr_f4646_0(int bitLength)
{    return (bitLength + 7) / 8;}
public static void parquet-mr_f4647_0(String[] args) throws Exception
{    String basePath = args[0];    generateScheme(CLASS_NAME_PREFIX + "BE", true, basePath);    generateScheme(CLASS_NAME_PREFIX + "LE", false, basePath);}
private static void parquet-mr_f4648_0(String className, boolean msbFirst, String basePath) throws IOException
{    final File file = new File(basePath + "/org/apache/parquet/column/values/bitpacking/" + className + ".java").getAbsoluteFile();    if (!file.getParentFile().exists()) {        file.getParentFile().mkdirs();    }    try (FileWriter fw = new FileWriter(file)) {        fw.append("package org.apache.parquet.column.values.bitpacking;\n");        fw.append("\n");        fw.append("/**\n");        fw.append(" * Based on the original implementation at at https://github.com/lemire/JavaFastPFOR/blob/master/src/integercompression/BitPacking.java\n");        fw.append(" * Which is released under the\n");        fw.append(" * Apache License Version 2.0 http://www.apache.org/licenses/.\n");        fw.append(" * By Daniel Lemire, http://lemire.me/en/\n");        fw.append(" * \n");        fw.append(" * Scheme designed by D. Lemire\n");        if (msbFirst) {            fw.append(" * Adapted to pack from the Most Significant Bit first\n");        }        fw.append(" * \n");        fw.append(" * Automatically generated\n");        fw.append(" * @see IntBasedBitPackingGenerator\n");        fw.append(" *\n");        fw.append(" */\n");        fw.append("abstract class " + className + " {\n");        fw.append("\n");        fw.append("  private static final IntPacker[] packers = new IntPacker[32];\n");        fw.append("  static {\n");        for (int i = 0; i < 32; i++) {            fw.append("    packers[" + i + "] = new Packer" + i + "();\n");        }        fw.append("  }\n");        fw.append("\n");        fw.append("  public static final IntPackerFactory factory = new IntPackerFactory() {\n");        fw.append("    public IntPacker newIntPacker(int bitWidth) {\n");        fw.append("      return packers[bitWidth];\n");        fw.append("    }\n");        fw.append("  };\n");        fw.append("\n");        for (int i = 0; i < 32; i++) {            generateClass(fw, i, msbFirst);            fw.append("\n");        }        fw.append("}\n");    }}
private static void parquet-mr_f4649_0(FileWriter fw, int bitWidth, boolean msbFirst) throws IOException
{    int mask = 0;    for (int i = 0; i < bitWidth; i++) {        mask <<= 1;        mask |= 1;    }    fw.append("  private static final class Packer" + bitWidth + " extends IntPacker {\n");    fw.append("\n");    fw.append("    private Packer" + bitWidth + "() {\n");    fw.append("      super(" + bitWidth + ");\n");    fw.append("    }\n");    fw.append("\n");        fw.append("    public final void pack32Values(final int[] in, final int inPos, final int[] out, final int outPos) {\n");    for (int i = 0; i < bitWidth; ++i) {        fw.append("      out[" + align(i, 2) + " + outPos] =\n");        int startIndex = (i * 32) / bitWidth;        int endIndex = ((i + 1) * 32 + bitWidth - 1) / bitWidth;        for (int j = startIndex; j < endIndex; j++) {            if (j == startIndex) {                fw.append("          ");            } else {                fw.append("\n        | ");            }            String shiftString = getPackShiftString(bitWidth, i, startIndex, j, msbFirst);            fw.append("((in[" + align(j, 2) + " + inPos] & " + mask + ")" + shiftString + ")");        }        fw.append(";\n");    }    fw.append("    }\n");        fw.append("    public final void unpack32Values(final int[] in, final int inPos, final int[] out, final int outPos) {\n");    if (bitWidth > 0) {        for (int i = 0; i < 32; ++i) {            fw.append("      out[" + align(i, 2) + " + outPos] =");            int byteIndex = i * bitWidth / 32;            String shiftString = getUnpackShiftString(bitWidth, i, msbFirst);            fw.append(" ((in[" + align(byteIndex, 2) + " + inPos] " + shiftString + ") & " + mask + ")");            if (((i + 1) * bitWidth - 1) / 32 != byteIndex) {                                int bitsRead = ((i + 1) * bitWidth - 1) % 32 + 1;                fw.append(" | ((in[" + align(byteIndex + 1, 2) + " + inPos]");                if (msbFirst) {                    fw.append(") >>> " + align(32 - bitsRead, 2) + ")");                } else {                    int lowerMask = 0;                    for (int j = 0; j < bitsRead; j++) {                        lowerMask <<= 1;                        lowerMask |= 1;                    }                    fw.append(" & " + lowerMask + ") << " + align(bitWidth - bitsRead, 2) + ")");                }            }            fw.append(";\n");        }    }    fw.append("    }\n");    fw.append("  }\n");}
private static String parquet-mr_f4650_0(int bitWidth, int i, boolean msbFirst)
{    final int regularShift = i * bitWidth % 32;    String shiftString;    if (msbFirst) {        int shift = 32 - (regularShift + bitWidth);        if (shift < 0) {            shiftString = "<<  " + align(-shift, 2);        } else {            shiftString = ">>> " + align(shift, 2);        }    } else {        shiftString = ">>> " + align(regularShift, 2);    }    return shiftString;}
private static String parquet-mr_f4651_0(int bitWidth, int integerIndex, int startIndex, int valueIndex, boolean msbFirst)
{    String shiftString;    int regularShift = (valueIndex * bitWidth) % 32;    if (msbFirst) {                int shift = 32 - (regularShift + bitWidth);        if (valueIndex == startIndex && (integerIndex * 32) % bitWidth != 0) {                        shiftString = " <<  " + align(32 - (((valueIndex + 1) * bitWidth) % 32), 2);        } else if (shift < 0) {                        shiftString = " >>> " + align(-shift, 2);        } else {            shiftString = " <<  " + align(shift, 2);        }    } else {                if (valueIndex == startIndex && (integerIndex * 32) % bitWidth != 0) {                        shiftString = " >>> " + align(32 - regularShift, 2);        } else {            shiftString = " <<  " + align(regularShift, 2);        }    }    return shiftString;}
private static String parquet-mr_f4652_0(int value, int digits)
{    final String valueString = String.valueOf(value);    StringBuilder result = new StringBuilder();    for (int i = valueString.length(); i < digits; i++) {        result.append(" ");    }    result.append(valueString);    return result.toString();}
public static void parquet-mr_f4653_0(String[] args) throws Exception
{    IntBasedBitPackingGenerator.main(args);    ByteBasedBitPackingGenerator.main(args);}
public static void parquet-mr_f4654_0(String[] args) throws Exception
{    IncrementallyUpdatedFilterPredicateGenerator.main(args);}
public static void parquet-mr_f4655_0(String[] args) throws IOException
{    File srcFile = new File(args[0] + "/org/apache/parquet/filter2/recordlevel/IncrementallyUpdatedFilterPredicateBuilder.java");    srcFile = srcFile.getAbsoluteFile();    File parent = srcFile.getParentFile();    if (!parent.exists()) {        if (!parent.mkdirs()) {            throw new IOException("Couldn't mkdirs for " + parent);        }    }    new IncrementallyUpdatedFilterPredicateGenerator(srcFile).run();}
public void parquet-mr_f4656_0() throws IOException
{    add("package org.apache.parquet.filter2.recordlevel;\n" + "\n" + "import java.util.List;\n" + "\n" + "import org.apache.parquet.hadoop.metadata.ColumnPath;\n" + "import org.apache.parquet.filter2.predicate.Operators.Eq;\n" + "import org.apache.parquet.filter2.predicate.Operators.Gt;\n" + "import org.apache.parquet.filter2.predicate.Operators.GtEq;\n" + "import org.apache.parquet.filter2.predicate.Operators.LogicalNotUserDefined;\n" + "import org.apache.parquet.filter2.predicate.Operators.Lt;\n" + "import org.apache.parquet.filter2.predicate.Operators.LtEq;\n" + "import org.apache.parquet.filter2.predicate.Operators.NotEq;\n" + "import org.apache.parquet.filter2.predicate.Operators.UserDefined;\n" + "import org.apache.parquet.filter2.predicate.UserDefinedPredicate;\n" + "import org.apache.parquet.filter2.recordlevel.IncrementallyUpdatedFilterPredicate.ValueInspector;\n" + "import org.apache.parquet.io.api.Binary;\n" + "import org.apache.parquet.io.PrimitiveColumnIO;\n" + "import org.apache.parquet.schema.PrimitiveComparator;\n\n" + "/**\n" + " * This class is auto-generated by org.apache.parquet.filter2.IncrementallyUpdatedFilterPredicateGenerator\n" + " * Do not manually edit!\n" + " * See {@link IncrementallyUpdatedFilterPredicateBuilderBase}\n" + " */\n");    add("public class IncrementallyUpdatedFilterPredicateBuilder extends IncrementallyUpdatedFilterPredicateBuilderBase {\n\n");    add("  public IncrementallyUpdatedFilterPredicateBuilder(List<PrimitiveColumnIO> leaves) {\n" + "    super(leaves);\n" + "  }\n\n");    addVisitBegin("Eq");    for (TypeInfo info : TYPES) {        addEqNotEqCase(info, true);    }    addVisitEnd();    addVisitBegin("NotEq");    for (TypeInfo info : TYPES) {        addEqNotEqCase(info, false);    }    addVisitEnd();    addVisitBegin("Lt");    for (TypeInfo info : TYPES) {        addInequalityCase(info, "<");    }    addVisitEnd();    addVisitBegin("LtEq");    for (TypeInfo info : TYPES) {        addInequalityCase(info, "<=");    }    addVisitEnd();    addVisitBegin("Gt");    for (TypeInfo info : TYPES) {        addInequalityCase(info, ">");    }    addVisitEnd();    addVisitBegin("GtEq");    for (TypeInfo info : TYPES) {        addInequalityCase(info, ">=");    }    addVisitEnd();    add("  @Override\n" + "  public <T extends Comparable<T>, U extends UserDefinedPredicate<T>> IncrementallyUpdatedFilterPredicate visit(UserDefined<T, U> pred) {\n");    addUdpBegin();    for (TypeInfo info : TYPES) {        addUdpCase(info, false);    }    addVisitEnd();    add("  @Override\n" + "  public <T extends Comparable<T>, U extends UserDefinedPredicate<T>> IncrementallyUpdatedFilterPredicate visit(LogicalNotUserDefined<T, U> notPred) {\n" + "    UserDefined<T, U> pred = notPred.getUserDefined();\n");    addUdpBegin();    for (TypeInfo info : TYPES) {        addUdpCase(info, true);    }    addVisitEnd();    add("}\n");    writer.close();}
private void parquet-mr_f4657_0(String inVar) throws IOException
{    add("  @Override\n" + "  public <T extends Comparable<T>> IncrementallyUpdatedFilterPredicate visit(" + inVar + "<T> pred) {\n" + "    ColumnPath columnPath = pred.getColumn().getColumnPath();\n" + "    Class<T> clazz = pred.getColumn().getColumnType();\n" + "\n" + "    ValueInspector valueInspector = null;\n\n");}
private void parquet-mr_f4658_0() throws IOException
{    add("    if (valueInspector == null) {\n" + "      throw new IllegalArgumentException(\"Encountered unknown type \" + clazz);\n" + "    }\n" + "\n" + "    addValueInspector(columnPath, valueInspector);\n" + "    return valueInspector;\n" + "  }\n\n");}
private void parquet-mr_f4659_0(TypeInfo info, boolean isEq) throws IOException
{    add("    if (clazz.equals(" + info.className + ".class)) {\n" + "      if (pred.getValue() == null) {\n" + "        valueInspector = new ValueInspector() {\n" + "          @Override\n" + "          public void updateNull() {\n" + "            setResult(" + isEq + ");\n" + "          }\n" + "\n" + "          @Override\n" + "          public void update(" + info.primitiveName + " value) {\n" + "            setResult(" + !isEq + ");\n" + "          }\n" + "        };\n" + "      } else {\n" + "        final " + info.primitiveName + " target = (" + info.className + ") (Object) pred.getValue();\n" + "        final PrimitiveComparator<" + info.className + "> comparator = getComparator(columnPath);\n" + "\n" + "        valueInspector = new ValueInspector() {\n" + "          @Override\n" + "          public void updateNull() {\n" + "            setResult(" + !isEq + ");\n" + "          }\n" + "\n" + "          @Override\n" + "          public void update(" + info.primitiveName + " value) {\n");    add("            setResult(" + compareEquality("value", "target", isEq) + ");\n");    add("          }\n" + "        };\n" + "      }\n" + "    }\n\n");}
private void parquet-mr_f4660_0(TypeInfo info, String op) throws IOException
{    if (!info.supportsInequality) {        add("    if (clazz.equals(" + info.className + ".class)) {\n");        add("      throw new IllegalArgumentException(\"Operator " + op + " not supported for " + info.className + "\");\n");        add("    }\n\n");        return;    }    add("    if (clazz.equals(" + info.className + ".class)) {\n" + "      final " + info.primitiveName + " target = (" + info.className + ") (Object) pred.getValue();\n" + "      final PrimitiveComparator<" + info.className + "> comparator = getComparator(columnPath);\n" + "\n" + "      valueInspector = new ValueInspector() {\n" + "        @Override\n" + "        public void updateNull() {\n" + "          setResult(false);\n" + "        }\n" + "\n" + "        @Override\n" + "        public void update(" + info.primitiveName + " value) {\n");    add("          setResult(comparator.compare(value, target) " + op + " 0);\n");    add("        }\n" + "      };\n" + "    }\n\n");}
private void parquet-mr_f4661_0() throws IOException
{    add("    ColumnPath columnPath = pred.getColumn().getColumnPath();\n" + "    Class<T> clazz = pred.getColumn().getColumnType();\n" + "\n" + "    ValueInspector valueInspector = null;\n" + "\n" + "    final U udp = pred.getUserDefinedPredicate();\n" + "\n");}
private void parquet-mr_f4662_0(TypeInfo info, boolean invert) throws IOException
{    add("    if (clazz.equals(" + info.className + ".class)) {\n" + "      valueInspector = new ValueInspector() {\n" + "        @Override\n" + "        public void updateNull() {\n" + "          setResult(" + (invert ? "!" : "") + "udp.acceptsNullValue());\n" + "        }\n" + "\n" + "        @SuppressWarnings(\"unchecked\")\n" + "        @Override\n" + "        public void update(" + info.primitiveName + " value) {\n" + "          setResult(" + (invert ? "!" : "") + "udp.keep((T) (Object) value));\n" + "        }\n" + "      };\n" + "    }\n\n");}
private String parquet-mr_f4663_0(String var, String target, boolean eq)
{    return "comparator.compare(" + var + ", " + target + ")" + (eq ? " == 0 " : " != 0");}
private void parquet-mr_f4664_0(String s) throws IOException
{    writer.write(s);}
public static void parquet-mr_f4665_0(String[] args) throws Exception
{    VersionGenerator.main(args);}
public static void parquet-mr_f4666_0(String[] args) throws IOException
{    File srcFile = new File(args[0] + "/org/apache/parquet/Version.java");    srcFile = srcFile.getAbsoluteFile();    File parent = srcFile.getParentFile();    if (!parent.exists()) {        if (!parent.mkdirs()) {            throw new IOException("Couldn't mkdirs for " + parent);        }    }    new VersionGenerator(srcFile).run();}
public void parquet-mr_f4667_0() throws IOException
{    InputStream in = VersionGenerator.class.getResourceAsStream("/parquet-version.properties");    if (in == null) {        throw new IOException("/parquet-version.properties not found");    }    Properties props = new Properties();    try {        props.load(in);    } finally {        in.close();    }    add("package org.apache.parquet;\n" + "\n" + "/**\n" + " * This class is auto-generated by org.apache.parquet.version.VersionGenerator\n" + " * Do not manually edit!\n" + " */\n");    add("public class Version {\n");    add("  public static final String VERSION_NUMBER = \"");    add(props.getProperty("versionNumber"));    add("\";\n");    add("  public static final String FULL_VERSION = \"");    add(props.getProperty("fullVersion"));    add("\";\n\n");    add("  public static void main(String[] args) {\n");    add("    System.out.println(FULL_VERSION);\n");    add("  }\n");    add("}\n");    writer.close();}
private void parquet-mr_f4668_0(String s) throws IOException
{    writer.write(s);}
public static List<BlockMetaData> parquet-mr_f4669_0(Filter filter, List<BlockMetaData> blocks, MessageType schema)
{    checkNotNull(filter, "filter");    return filter.accept(new RowGroupFilter(blocks, schema));}
public static List<BlockMetaData> parquet-mr_f4670_0(List<FilterLevel> levels, Filter filter, List<BlockMetaData> blocks, ParquetFileReader reader)
{    checkNotNull(filter, "filter");    return filter.accept(new RowGroupFilter(levels, blocks, reader));}
public List<BlockMetaData> parquet-mr_f4671_0(FilterCompat.FilterPredicateCompat filterPredicateCompat)
{    FilterPredicate filterPredicate = filterPredicateCompat.getFilterPredicate();        SchemaCompatibilityValidator.validate(filterPredicate, schema);    List<BlockMetaData> filteredBlocks = new ArrayList<BlockMetaData>();    for (BlockMetaData block : blocks) {        boolean drop = false;        if (levels.contains(FilterLevel.STATISTICS)) {            drop = StatisticsFilter.canDrop(filterPredicate, block.getColumns());        }        if (!drop && levels.contains(FilterLevel.DICTIONARY)) {            drop = DictionaryFilter.canDrop(filterPredicate, block.getColumns(), reader.getDictionaryReader(block));        }        if (!drop) {            filteredBlocks.add(block);        }    }    return filteredBlocks;}
public List<BlockMetaData> parquet-mr_f4672_0(FilterCompat.UnboundRecordFilterCompat unboundRecordFilterCompat)
{    return blocks;}
public List<BlockMetaData> parquet-mr_f4673_0(NoOpFilter noOpFilter)
{    return blocks;}
public static boolean parquet-mr_f4674_0(FilterPredicate pred, List<ColumnChunkMetaData> columns, DictionaryPageReadStore dictionaries)
{    checkNotNull(pred, "pred");    checkNotNull(columns, "columns");    return pred.accept(new DictionaryFilter(columns, dictionaries));}
private ColumnChunkMetaData parquet-mr_f4675_0(ColumnPath columnPath)
{    return columns.get(columnPath);}
private Set<T> parquet-mr_f4676_1(ColumnChunkMetaData meta) throws IOException
{    ColumnDescriptor col = new ColumnDescriptor(meta.getPath().toArray(), meta.getPrimitiveType(), -1, -1);    DictionaryPage page = dictionaries.readDictionaryPage(col);        if (page == null) {        return null;    }    Dictionary dict = page.getEncoding().initDictionary(col, page);    IntFunction<Object> dictValueProvider;    PrimitiveTypeName type = meta.getPrimitiveType().getPrimitiveTypeName();    switch(type) {                case FIXED_LEN_BYTE_ARRAY:        case BINARY:            dictValueProvider = dict::decodeToBinary;            break;        case INT32:            dictValueProvider = dict::decodeToInt;            break;        case INT64:            dictValueProvider = dict::decodeToLong;            break;        case FLOAT:            dictValueProvider = dict::decodeToFloat;            break;        case DOUBLE:            dictValueProvider = dict::decodeToDouble;            break;        default:                        return null;    }    Set<T> dictSet = new HashSet<>();    for (int i = 0; i <= dict.getMaxId(); i++) {        dictSet.add((T) dictValueProvider.apply(i));    }    return dictSet;}
public Boolean parquet-mr_f4677_1(Eq<T> eq)
{    T value = eq.getValue();    if (value == null) {                return BLOCK_MIGHT_MATCH;    }    Column<T> filterColumn = eq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }        if (hasNonDictionaryPages(meta)) {        return BLOCK_MIGHT_MATCH;    }    try {        Set<T> dictSet = expandDictionary(meta);        if (dictSet != null && !dictSet.contains(value)) {            return BLOCK_CANNOT_MATCH;        }    } catch (IOException e) {            }        return BLOCK_MIGHT_MATCH;}
public Boolean parquet-mr_f4678_1(NotEq<T> notEq)
{    Column<T> filterColumn = notEq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    T value = notEq.getValue();    if (value == null && meta == null) {                return BLOCK_CANNOT_MATCH;    }    if (value == null) {                return BLOCK_MIGHT_MATCH;    }    if (meta == null) {                return BLOCK_MIGHT_MATCH;    }        if (hasNonDictionaryPages(meta)) {        return BLOCK_MIGHT_MATCH;    }    try {        Set<T> dictSet = expandDictionary(meta);        boolean mayContainNull = (meta.getStatistics() == null || !meta.getStatistics().isNumNullsSet() || meta.getStatistics().getNumNulls() > 0);        if (dictSet != null && dictSet.size() == 1 && dictSet.contains(value) && !mayContainNull) {            return BLOCK_CANNOT_MATCH;        }    } catch (IOException e) {            }    return BLOCK_MIGHT_MATCH;}
public Boolean parquet-mr_f4679_1(Lt<T> lt)
{    Column<T> filterColumn = lt.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }        if (hasNonDictionaryPages(meta)) {        return BLOCK_MIGHT_MATCH;    }    T value = lt.getValue();    try {        Set<T> dictSet = expandDictionary(meta);        if (dictSet == null) {            return BLOCK_MIGHT_MATCH;        }        Comparator<T> comparator = meta.getPrimitiveType().comparator();        for (T entry : dictSet) {            if (comparator.compare(value, entry) > 0) {                return BLOCK_MIGHT_MATCH;            }        }        return BLOCK_CANNOT_MATCH;    } catch (IOException e) {            }    return BLOCK_MIGHT_MATCH;}
public Boolean parquet-mr_f4680_1(LtEq<T> ltEq)
{    Column<T> filterColumn = ltEq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }        if (hasNonDictionaryPages(meta)) {        return BLOCK_MIGHT_MATCH;    }    T value = ltEq.getValue();    filterColumn.getColumnPath();    try {        Set<T> dictSet = expandDictionary(meta);        if (dictSet == null) {            return BLOCK_MIGHT_MATCH;        }        Comparator<T> comparator = meta.getPrimitiveType().comparator();        for (T entry : dictSet) {            if (comparator.compare(value, entry) >= 0) {                return BLOCK_MIGHT_MATCH;            }        }        return BLOCK_CANNOT_MATCH;    } catch (IOException e) {            }    return BLOCK_MIGHT_MATCH;}
public Boolean parquet-mr_f4681_1(Gt<T> gt)
{    Column<T> filterColumn = gt.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }        if (hasNonDictionaryPages(meta)) {        return BLOCK_MIGHT_MATCH;    }    T value = gt.getValue();    try {        Set<T> dictSet = expandDictionary(meta);        if (dictSet == null) {            return BLOCK_MIGHT_MATCH;        }        Comparator<T> comparator = meta.getPrimitiveType().comparator();        for (T entry : dictSet) {            if (comparator.compare(value, entry) < 0) {                return BLOCK_MIGHT_MATCH;            }        }        return BLOCK_CANNOT_MATCH;    } catch (IOException e) {            }    return BLOCK_MIGHT_MATCH;}
public Boolean parquet-mr_f4682_1(GtEq<T> gtEq)
{    Column<T> filterColumn = gtEq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }        if (hasNonDictionaryPages(meta)) {        return BLOCK_MIGHT_MATCH;    }    T value = gtEq.getValue();    filterColumn.getColumnPath();    try {        Set<T> dictSet = expandDictionary(meta);        if (dictSet == null) {            return BLOCK_MIGHT_MATCH;        }        Comparator<T> comparator = meta.getPrimitiveType().comparator();        for (T entry : dictSet) {            if (comparator.compare(value, entry) <= 0) {                return BLOCK_MIGHT_MATCH;            }        }        return BLOCK_CANNOT_MATCH;    } catch (IOException e) {            }    return BLOCK_MIGHT_MATCH;}
public Boolean parquet-mr_f4683_0(And and)
{    return and.getLeft().accept(this) || and.getRight().accept(this);}
public Boolean parquet-mr_f4684_0(Or or)
{    return or.getLeft().accept(this) && or.getRight().accept(this);}
public Boolean parquet-mr_f4685_0(Not not)
{    throw new IllegalArgumentException("This predicate contains a not! Did you forget to run this predicate through LogicalInverseRewriter? " + not);}
private Boolean parquet-mr_f4686_1(UserDefined<T, U> ud, boolean inverted)
{    Column<T> filterColumn = ud.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    U udp = ud.getUserDefinedPredicate();        if (meta == null) {        if (inverted) {            return udp.acceptsNullValue();        } else {            return !udp.acceptsNullValue();        }    }    if (hasNonDictionaryPages(meta)) {        return BLOCK_MIGHT_MATCH;    }    try {        Set<T> dictSet = expandDictionary(meta);        if (dictSet == null) {            return BLOCK_MIGHT_MATCH;        }        for (T entry : dictSet) {            boolean keep = udp.keep(entry);            if ((keep && !inverted) || (!keep && inverted))                return BLOCK_MIGHT_MATCH;        }        return BLOCK_CANNOT_MATCH;    } catch (IOException e) {            }    return BLOCK_MIGHT_MATCH;}
public Boolean parquet-mr_f4687_0(UserDefined<T, U> udp)
{    return visit(udp, false);}
public Boolean parquet-mr_f4688_0(LogicalNotUserDefined<T, U> udp)
{    return visit(udp.getUserDefined(), true);}
private static boolean parquet-mr_f4689_0(ColumnChunkMetaData meta)
{    EncodingStats stats = meta.getEncodingStats();    if (stats != null) {        return stats.hasNonDictionaryEncodedPages();    }        Set<Encoding> encodings = new HashSet<Encoding>(meta.getEncodings());    if (encodings.remove(Encoding.PLAIN_DICTIONARY)) {                                encodings.remove(Encoding.RLE);        encodings.remove(Encoding.BIT_PACKED);        if (encodings.isEmpty()) {                        return false;        }        return true;    } else {                return true;    }}
public static boolean parquet-mr_f4690_0(FilterPredicate pred, List<ColumnChunkMetaData> columns)
{    checkNotNull(pred, "pred");    checkNotNull(columns, "columns");    return pred.accept(new StatisticsFilter(columns));}
private ColumnChunkMetaData parquet-mr_f4691_0(ColumnPath columnPath)
{    return columns.get(columnPath);}
private boolean parquet-mr_f4692_0(ColumnChunkMetaData column)
{    return column.getStatistics().getNumNulls() == column.getValueCount();}
private boolean parquet-mr_f4693_0(ColumnChunkMetaData column)
{    return column.getStatistics().getNumNulls() > 0;}
public Boolean parquet-mr_f4694_0(Eq<T> eq)
{    Column<T> filterColumn = eq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    T value = eq.getValue();    if (meta == null) {                if (value != null) {                        return BLOCK_CANNOT_MATCH;        }        return BLOCK_MIGHT_MATCH;    }    Statistics<T> stats = meta.getStatistics();    if (stats.isEmpty()) {                return BLOCK_MIGHT_MATCH;    }    if (value == null) {                if (!stats.isNumNullsSet()) {            return BLOCK_MIGHT_MATCH;        }                return !hasNulls(meta);    }    if (isAllNulls(meta)) {                return BLOCK_CANNOT_MATCH;    }    if (!stats.hasNonNullValue()) {                return BLOCK_MIGHT_MATCH;    }        return stats.compareMinToValue(value) > 0 || stats.compareMaxToValue(value) < 0;}
public Boolean parquet-mr_f4695_0(NotEq<T> notEq)
{    Column<T> filterColumn = notEq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    T value = notEq.getValue();    if (meta == null) {        if (value == null) {                        return BLOCK_CANNOT_MATCH;        }        return BLOCK_MIGHT_MATCH;    }    Statistics<T> stats = meta.getStatistics();    if (stats.isEmpty()) {                return BLOCK_MIGHT_MATCH;    }    if (value == null) {                return isAllNulls(meta);    }    if (stats.isNumNullsSet() && hasNulls(meta)) {                return BLOCK_MIGHT_MATCH;    }    if (!stats.hasNonNullValue()) {                return BLOCK_MIGHT_MATCH;    }        return stats.compareMinToValue(value) == 0 && stats.compareMaxToValue(value) == 0;}
public Boolean parquet-mr_f4696_0(Lt<T> lt)
{    Column<T> filterColumn = lt.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }    Statistics<T> stats = meta.getStatistics();    if (stats.isEmpty()) {                return BLOCK_MIGHT_MATCH;    }    if (isAllNulls(meta)) {                return BLOCK_CANNOT_MATCH;    }    if (!stats.hasNonNullValue()) {                return BLOCK_MIGHT_MATCH;    }    T value = lt.getValue();        return stats.compareMinToValue(value) >= 0;}
public Boolean parquet-mr_f4697_0(LtEq<T> ltEq)
{    Column<T> filterColumn = ltEq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }    Statistics<T> stats = meta.getStatistics();    if (stats.isEmpty()) {                return BLOCK_MIGHT_MATCH;    }    if (isAllNulls(meta)) {                return BLOCK_CANNOT_MATCH;    }    if (!stats.hasNonNullValue()) {                return BLOCK_MIGHT_MATCH;    }    T value = ltEq.getValue();        return stats.compareMinToValue(value) > 0;}
public Boolean parquet-mr_f4698_0(Gt<T> gt)
{    Column<T> filterColumn = gt.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }    Statistics<T> stats = meta.getStatistics();    if (stats.isEmpty()) {                return BLOCK_MIGHT_MATCH;    }    if (isAllNulls(meta)) {                return BLOCK_CANNOT_MATCH;    }    if (!stats.hasNonNullValue()) {                return BLOCK_MIGHT_MATCH;    }    T value = gt.getValue();        return stats.compareMaxToValue(value) <= 0;}
public Boolean parquet-mr_f4699_0(GtEq<T> gtEq)
{    Column<T> filterColumn = gtEq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }    Statistics<T> stats = meta.getStatistics();    if (stats.isEmpty()) {                return BLOCK_MIGHT_MATCH;    }    if (isAllNulls(meta)) {                return BLOCK_CANNOT_MATCH;    }    if (!stats.hasNonNullValue()) {                return BLOCK_MIGHT_MATCH;    }    T value = gtEq.getValue();        return stats.compareMaxToValue(value) < 0;}
public Boolean parquet-mr_f4700_0(And and)
{        return and.getLeft().accept(this) || and.getRight().accept(this);}
public Boolean parquet-mr_f4701_0(Or or)
{        return or.getLeft().accept(this) && or.getRight().accept(this);}
public Boolean parquet-mr_f4702_0(Not not)
{    throw new IllegalArgumentException("This predicate contains a not! Did you forget to run this predicate through LogicalInverseRewriter? " + not);}
private Boolean parquet-mr_f4703_0(UserDefined<T, U> ud, boolean inverted)
{    Column<T> filterColumn = ud.getColumn();    ColumnChunkMetaData columnChunk = getColumnChunk(filterColumn.getColumnPath());    U udp = ud.getUserDefinedPredicate();    if (columnChunk == null) {                if (inverted) {            return udp.acceptsNullValue();        } else {            return !udp.acceptsNullValue();        }    }    Statistics<T> stats = columnChunk.getStatistics();    if (stats.isEmpty()) {                return BLOCK_MIGHT_MATCH;    }    if (isAllNulls(columnChunk)) {                if (inverted) {            return udp.acceptsNullValue();        } else {            return !udp.acceptsNullValue();        }    }    if (!stats.hasNonNullValue()) {                return BLOCK_MIGHT_MATCH;    }    org.apache.parquet.filter2.predicate.Statistics<T> udpStats = new org.apache.parquet.filter2.predicate.Statistics<T>(stats.genericGetMin(), stats.genericGetMax(), stats.comparator());    if (inverted) {        return udp.inverseCanDrop(udpStats);    } else {        return udp.canDrop(udpStats);    }}
public Boolean parquet-mr_f4704_0(UserDefined<T, U> ud)
{    return visit(ud, false);}
public Boolean parquet-mr_f4705_0(LogicalNotUserDefined<T, U> lnud)
{    return visit(lnud.getUserDefined(), true);}
public FileMetaData parquet-mr_f4706_0(int currentVersion, ParquetMetadata parquetMetadata)
{    List<BlockMetaData> blocks = parquetMetadata.getBlocks();    List<RowGroup> rowGroups = new ArrayList<RowGroup>();    long numRows = 0;    for (BlockMetaData block : blocks) {        numRows += block.getRowCount();        addRowGroup(parquetMetadata, rowGroups, block);    }    FileMetaData fileMetaData = new FileMetaData(currentVersion, toParquetSchema(parquetMetadata.getFileMetaData().getSchema()), numRows, rowGroups);    Set<Entry<String, String>> keyValues = parquetMetadata.getFileMetaData().getKeyValueMetaData().entrySet();    for (Entry<String, String> keyValue : keyValues) {        addKeyValue(fileMetaData, keyValue.getKey(), keyValue.getValue());    }    fileMetaData.setCreated_by(parquetMetadata.getFileMetaData().getCreatedBy());    fileMetaData.setColumn_orders(getColumnOrders(parquetMetadata.getFileMetaData().getSchema()));    return fileMetaData;}
private List<ColumnOrder> parquet-mr_f4707_0(MessageType schema)
{    List<ColumnOrder> columnOrders = new ArrayList<>();        for (int i = 0, n = schema.getPaths().size(); i < n; ++i) {        ColumnOrder columnOrder = new ColumnOrder();        columnOrder.setTYPE_ORDER(TYPE_DEFINED_ORDER);        columnOrders.add(columnOrder);    }    return columnOrders;}
 List<SchemaElement> parquet-mr_f4708_0(MessageType schema)
{    List<SchemaElement> result = new ArrayList<SchemaElement>();    addToList(result, schema);    return result;}
private void parquet-mr_f4709_0(final List<SchemaElement> result, org.apache.parquet.schema.Type field)
{    field.accept(new TypeVisitor() {        @Override        public void visit(PrimitiveType primitiveType) {            SchemaElement element = new SchemaElement(primitiveType.getName());            element.setRepetition_type(toParquetRepetition(primitiveType.getRepetition()));            element.setType(getType(primitiveType.getPrimitiveTypeName()));            if (primitiveType.getLogicalTypeAnnotation() != null) {                element.setConverted_type(convertToConvertedType(primitiveType.getLogicalTypeAnnotation()));                element.setLogicalType(convertToLogicalType(primitiveType.getLogicalTypeAnnotation()));            }            if (primitiveType.getDecimalMetadata() != null) {                element.setPrecision(primitiveType.getDecimalMetadata().getPrecision());                element.setScale(primitiveType.getDecimalMetadata().getScale());            }            if (primitiveType.getTypeLength() > 0) {                element.setType_length(primitiveType.getTypeLength());            }            if (primitiveType.getId() != null) {                element.setField_id(primitiveType.getId().intValue());            }            result.add(element);        }        @Override        public void visit(MessageType messageType) {            SchemaElement element = new SchemaElement(messageType.getName());            if (messageType.getId() != null) {                element.setField_id(messageType.getId().intValue());            }            visitChildren(result, messageType.asGroupType(), element);        }        @Override        public void visit(GroupType groupType) {            SchemaElement element = new SchemaElement(groupType.getName());            element.setRepetition_type(toParquetRepetition(groupType.getRepetition()));            if (groupType.getLogicalTypeAnnotation() != null) {                element.setConverted_type(convertToConvertedType(groupType.getLogicalTypeAnnotation()));                element.setLogicalType(convertToLogicalType(groupType.getLogicalTypeAnnotation()));            }            if (groupType.getId() != null) {                element.setField_id(groupType.getId().intValue());            }            visitChildren(result, groupType, element);        }        private void visitChildren(final List<SchemaElement> result, GroupType groupType, SchemaElement element) {            element.setNum_children(groupType.getFieldCount());            result.add(element);            for (org.apache.parquet.schema.Type field : groupType.getFields()) {                addToList(result, field);            }        }    });}
public void parquet-mr_f4710_0(PrimitiveType primitiveType)
{    SchemaElement element = new SchemaElement(primitiveType.getName());    element.setRepetition_type(toParquetRepetition(primitiveType.getRepetition()));    element.setType(getType(primitiveType.getPrimitiveTypeName()));    if (primitiveType.getLogicalTypeAnnotation() != null) {        element.setConverted_type(convertToConvertedType(primitiveType.getLogicalTypeAnnotation()));        element.setLogicalType(convertToLogicalType(primitiveType.getLogicalTypeAnnotation()));    }    if (primitiveType.getDecimalMetadata() != null) {        element.setPrecision(primitiveType.getDecimalMetadata().getPrecision());        element.setScale(primitiveType.getDecimalMetadata().getScale());    }    if (primitiveType.getTypeLength() > 0) {        element.setType_length(primitiveType.getTypeLength());    }    if (primitiveType.getId() != null) {        element.setField_id(primitiveType.getId().intValue());    }    result.add(element);}
public void parquet-mr_f4711_0(MessageType messageType)
{    SchemaElement element = new SchemaElement(messageType.getName());    if (messageType.getId() != null) {        element.setField_id(messageType.getId().intValue());    }    visitChildren(result, messageType.asGroupType(), element);}
public void parquet-mr_f4712_0(GroupType groupType)
{    SchemaElement element = new SchemaElement(groupType.getName());    element.setRepetition_type(toParquetRepetition(groupType.getRepetition()));    if (groupType.getLogicalTypeAnnotation() != null) {        element.setConverted_type(convertToConvertedType(groupType.getLogicalTypeAnnotation()));        element.setLogicalType(convertToLogicalType(groupType.getLogicalTypeAnnotation()));    }    if (groupType.getId() != null) {        element.setField_id(groupType.getId().intValue());    }    visitChildren(result, groupType, element);}
private void parquet-mr_f4713_0(final List<SchemaElement> result, GroupType groupType, SchemaElement element)
{    element.setNum_children(groupType.getFieldCount());    result.add(element);    for (org.apache.parquet.schema.Type field : groupType.getFields()) {        addToList(result, field);    }}
 LogicalType parquet-mr_f4714_0(LogicalTypeAnnotation logicalTypeAnnotation)
{    return logicalTypeAnnotation.accept(LOGICAL_TYPE_ANNOTATION_VISITOR).get();}
 ConvertedType parquet-mr_f4715_0(LogicalTypeAnnotation logicalTypeAnnotation)
{    return logicalTypeAnnotation.accept(CONVERTED_TYPE_CONVERTER_VISITOR).orElse(null);}
 static org.apache.parquet.format.TimeUnit parquet-mr_f4716_0(LogicalTypeAnnotation.TimeUnit unit)
{    switch(unit) {        case MICROS:            return org.apache.parquet.format.TimeUnit.MICROS(new MicroSeconds());        case MILLIS:            return org.apache.parquet.format.TimeUnit.MILLIS(new MilliSeconds());        case NANOS:            return TimeUnit.NANOS(new NanoSeconds());        default:            throw new RuntimeException("Unknown time unit " + unit);    }}
public Optional<ConvertedType> parquet-mr_f4717_0(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    return of(ConvertedType.UTF8);}
public Optional<ConvertedType> parquet-mr_f4718_0(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    return of(ConvertedType.MAP);}
public Optional<ConvertedType> parquet-mr_f4719_0(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    return of(ConvertedType.LIST);}
public Optional<ConvertedType> parquet-mr_f4720_0(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType)
{    return of(ConvertedType.ENUM);}
public Optional<ConvertedType> parquet-mr_f4721_0(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(ConvertedType.DECIMAL);}
public Optional<ConvertedType> parquet-mr_f4722_0(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    return of(ConvertedType.DATE);}
public Optional<ConvertedType> parquet-mr_f4723_0(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    if (!timeLogicalType.isAdjustedToUTC()) {        return empty();    }    switch(timeLogicalType.getUnit()) {        case MILLIS:            return of(ConvertedType.TIME_MILLIS);        case MICROS:            return of(ConvertedType.TIME_MICROS);        case NANOS:            return empty();        default:            throw new RuntimeException("Unknown converted type for " + timeLogicalType.toOriginalType());    }}
public Optional<ConvertedType> parquet-mr_f4724_0(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    if (!timestampLogicalType.isAdjustedToUTC()) {        return empty();    }    switch(timestampLogicalType.getUnit()) {        case MICROS:            return of(ConvertedType.TIMESTAMP_MICROS);        case MILLIS:            return of(ConvertedType.TIMESTAMP_MILLIS);        case NANOS:            return empty();        default:            throw new RuntimeException("Unknown converted type for " + timestampLogicalType.toOriginalType());    }}
public Optional<ConvertedType> parquet-mr_f4725_0(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    boolean signed = intLogicalType.isSigned();    switch(intLogicalType.getBitWidth()) {        case 8:            return of(signed ? ConvertedType.INT_8 : ConvertedType.UINT_8);        case 16:            return of(signed ? ConvertedType.INT_16 : ConvertedType.UINT_16);        case 32:            return of(signed ? ConvertedType.INT_32 : ConvertedType.UINT_32);        case 64:            return of(signed ? ConvertedType.INT_64 : ConvertedType.UINT_64);        default:            throw new RuntimeException("Unknown original type " + intLogicalType.toOriginalType());    }}
public Optional<ConvertedType> parquet-mr_f4726_0(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType)
{    return of(ConvertedType.JSON);}
public Optional<ConvertedType> parquet-mr_f4727_0(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType)
{    return of(ConvertedType.BSON);}
public Optional<ConvertedType> parquet-mr_f4728_0(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType)
{    return of(ConvertedType.INTERVAL);}
public Optional<ConvertedType> parquet-mr_f4729_0(LogicalTypeAnnotation.MapKeyValueTypeAnnotation mapKeyValueLogicalType)
{    return of(ConvertedType.MAP_KEY_VALUE);}
public Optional<LogicalType> parquet-mr_f4730_0(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    return of(LogicalType.STRING(new StringType()));}
public Optional<LogicalType> parquet-mr_f4731_0(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    return of(LogicalType.MAP(new MapType()));}
public Optional<LogicalType> parquet-mr_f4732_0(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    return of(LogicalType.LIST(new ListType()));}
public Optional<LogicalType> parquet-mr_f4733_0(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType)
{    return of(LogicalType.ENUM(new EnumType()));}
public Optional<LogicalType> parquet-mr_f4734_0(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(LogicalType.DECIMAL(new DecimalType(decimalLogicalType.getScale(), decimalLogicalType.getPrecision())));}
public Optional<LogicalType> parquet-mr_f4735_0(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    return of(LogicalType.DATE(new DateType()));}
public Optional<LogicalType> parquet-mr_f4736_0(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    return of(LogicalType.TIME(new TimeType(timeLogicalType.isAdjustedToUTC(), convertUnit(timeLogicalType.getUnit()))));}
public Optional<LogicalType> parquet-mr_f4737_0(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    return of(LogicalType.TIMESTAMP(new TimestampType(timestampLogicalType.isAdjustedToUTC(), convertUnit(timestampLogicalType.getUnit()))));}
public Optional<LogicalType> parquet-mr_f4738_0(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    return of(LogicalType.INTEGER(new IntType((byte) intLogicalType.getBitWidth(), intLogicalType.isSigned())));}
public Optional<LogicalType> parquet-mr_f4739_0(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType)
{    return of(LogicalType.JSON(new JsonType()));}
public Optional<LogicalType> parquet-mr_f4740_0(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType)
{    return of(LogicalType.BSON(new BsonType()));}
public Optional<LogicalType> parquet-mr_f4741_0(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType)
{    return of(LogicalType.UNKNOWN(new NullType()));}
public Optional<LogicalType> parquet-mr_f4742_0(LogicalTypeAnnotation.MapKeyValueTypeAnnotation mapKeyValueLogicalType)
{    return of(LogicalType.UNKNOWN(new NullType()));}
private void parquet-mr_f4743_0(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block)
{        List<ColumnChunkMetaData> columns = block.getColumns();    List<ColumnChunk> parquetColumns = new ArrayList<ColumnChunk>();    for (ColumnChunkMetaData columnMetaData : columns) {                ColumnChunk columnChunk = new ColumnChunk(columnMetaData.getFirstDataPageOffset());                columnChunk.file_path = block.getPath();        columnChunk.meta_data = new ColumnMetaData(getType(columnMetaData.getType()), toFormatEncodings(columnMetaData.getEncodings()), Arrays.asList(columnMetaData.getPath().toArray()), toFormatCodec(columnMetaData.getCodec()), columnMetaData.getValueCount(), columnMetaData.getTotalUncompressedSize(), columnMetaData.getTotalSize(), columnMetaData.getFirstDataPageOffset());        columnChunk.meta_data.dictionary_page_offset = columnMetaData.getDictionaryPageOffset();        if (!columnMetaData.getStatistics().isEmpty()) {            columnChunk.meta_data.setStatistics(toParquetStatistics(columnMetaData.getStatistics()));        }        if (columnMetaData.getEncodingStats() != null) {            columnChunk.meta_data.setEncoding_stats(convertEncodingStats(columnMetaData.getEncodingStats()));        }                        IndexReference columnIndexRef = columnMetaData.getColumnIndexReference();        if (columnIndexRef != null) {            columnChunk.setColumn_index_offset(columnIndexRef.getOffset());            columnChunk.setColumn_index_length(columnIndexRef.getLength());        }        IndexReference offsetIndexRef = columnMetaData.getOffsetIndexReference();        if (offsetIndexRef != null) {            columnChunk.setOffset_index_offset(offsetIndexRef.getOffset());            columnChunk.setOffset_index_length(offsetIndexRef.getLength());        }        parquetColumns.add(columnChunk);    }    RowGroup rowGroup = new RowGroup(parquetColumns, block.getTotalByteSize(), block.getRowCount());    rowGroups.add(rowGroup);}
private List<Encoding> parquet-mr_f4744_0(Set<org.apache.parquet.column.Encoding> encodings)
{    List<Encoding> converted = new ArrayList<Encoding>(encodings.size());    for (org.apache.parquet.column.Encoding encoding : encodings) {        converted.add(getEncoding(encoding));    }    return converted;}
 Set<org.apache.parquet.column.Encoding> parquet-mr_f4745_0(List<Encoding> encodings)
{    Set<org.apache.parquet.column.Encoding> converted = new HashSet<org.apache.parquet.column.Encoding>();    for (Encoding encoding : encodings) {        converted.add(getEncoding(encoding));    }        converted = Collections.unmodifiableSet(converted);        Set<org.apache.parquet.column.Encoding> cached = cachedEncodingSets.putIfAbsent(converted, converted);    if (cached == null) {                                cached = converted;    }    return cached;}
private CompressionCodecName parquet-mr_f4746_0(CompressionCodec codec)
{    return CompressionCodecName.valueOf(codec.toString());}
private CompressionCodec parquet-mr_f4747_0(CompressionCodecName codec)
{    return CompressionCodec.valueOf(codec.toString());}
public org.apache.parquet.column.Encoding parquet-mr_f4748_0(Encoding encoding)
{    return org.apache.parquet.column.Encoding.valueOf(encoding.name());}
public Encoding parquet-mr_f4749_0(org.apache.parquet.column.Encoding encoding)
{    return Encoding.valueOf(encoding.name());}
public EncodingStats parquet-mr_f4750_0(List<PageEncodingStats> stats)
{    if (stats == null) {        return null;    }    EncodingStats.Builder builder = new EncodingStats.Builder();    for (PageEncodingStats stat : stats) {        switch(stat.getPage_type()) {            case DATA_PAGE_V2:                builder.withV2Pages();                        case DATA_PAGE:                builder.addDataEncoding(getEncoding(stat.getEncoding()), stat.getCount());                break;            case DICTIONARY_PAGE:                builder.addDictEncoding(getEncoding(stat.getEncoding()), stat.getCount());                break;        }    }    return builder.build();}
public List<PageEncodingStats> parquet-mr_f4751_0(EncodingStats stats)
{    if (stats == null) {        return null;    }    List<PageEncodingStats> formatStats = new ArrayList<PageEncodingStats>();    for (org.apache.parquet.column.Encoding encoding : stats.getDictionaryEncodings()) {        formatStats.add(new PageEncodingStats(PageType.DICTIONARY_PAGE, getEncoding(encoding), stats.getNumDictionaryPagesEncodedAs(encoding)));    }    PageType dataPageType = (stats.usesV2Pages() ? PageType.DATA_PAGE_V2 : PageType.DATA_PAGE);    for (org.apache.parquet.column.Encoding encoding : stats.getDataEncodings()) {        formatStats.add(new PageEncodingStats(dataPageType, getEncoding(encoding), stats.getNumDataPagesEncodedAs(encoding)));    }    return formatStats;}
public static Statistics parquet-mr_f4752_0(org.apache.parquet.column.statistics.Statistics stats)
{    Statistics formatStats = new Statistics();        if (!stats.isEmpty() && stats.isSmallerThan(MAX_STATS_SIZE)) {        formatStats.setNull_count(stats.getNumNulls());        if (stats.hasNonNullValue()) {            byte[] min = stats.getMinBytes();            byte[] max = stats.getMaxBytes();                        if (sortOrder(stats.type()) == SortOrder.SIGNED || Arrays.equals(min, max)) {                formatStats.setMin(min);                formatStats.setMax(max);            }            if (isMinMaxStatsSupported(stats.type()) || Arrays.equals(min, max)) {                formatStats.setMin_value(min);                formatStats.setMax_value(max);            }        }    }    return formatStats;}
private static boolean parquet-mr_f4753_0(PrimitiveType type)
{    return type.columnOrder().getColumnOrderName() == ColumnOrderName.TYPE_DEFINED_ORDER;}
public static org.apache.parquet.column.statistics.Statistics parquet-mr_f4754_0(Statistics statistics, PrimitiveTypeName type)
{    return fromParquetStatistics(null, statistics, type);}
public static org.apache.parquet.column.statistics.Statistics parquet-mr_f4755_0(String createdBy, Statistics statistics, PrimitiveTypeName type)
{    return fromParquetStatisticsInternal(createdBy, statistics, new PrimitiveType(Repetition.OPTIONAL, type, "fake_type"), defaultSortOrder(type));}
 static org.apache.parquet.column.statistics.Statistics parquet-mr_f4756_0(String createdBy, Statistics formatStats, PrimitiveType type, SortOrder typeSortOrder)
{        org.apache.parquet.column.statistics.Statistics.Builder statsBuilder = org.apache.parquet.column.statistics.Statistics.getBuilderForReading(type);    if (formatStats != null) {                if (formatStats.isSetMin_value() && formatStats.isSetMax_value()) {            byte[] min = formatStats.min_value.array();            byte[] max = formatStats.max_value.array();            if (isMinMaxStatsSupported(type) || Arrays.equals(min, max)) {                statsBuilder.withMin(min);                statsBuilder.withMax(max);            }        } else {            boolean isSet = formatStats.isSetMax() && formatStats.isSetMin();            boolean maxEqualsMin = isSet ? Arrays.equals(formatStats.getMin(), formatStats.getMax()) : false;            boolean sortOrdersMatch = SortOrder.SIGNED == typeSortOrder;                        if (!CorruptStatistics.shouldIgnoreStatistics(createdBy, type.getPrimitiveTypeName()) && (sortOrdersMatch || maxEqualsMin)) {                if (isSet) {                    statsBuilder.withMin(formatStats.min.array());                    statsBuilder.withMax(formatStats.max.array());                }            }        }        if (formatStats.isSetNull_count()) {            statsBuilder.withNumNulls(formatStats.null_count);        }    }    return statsBuilder.build();}
public org.apache.parquet.column.statistics.Statistics parquet-mr_f4757_0(String createdBy, Statistics statistics, PrimitiveType type)
{    SortOrder expectedOrder = overrideSortOrderToSigned(type) ? SortOrder.SIGNED : sortOrder(type);    return fromParquetStatisticsInternal(createdBy, statistics, type, expectedOrder);}
private boolean parquet-mr_f4758_0(PrimitiveType type)
{                LogicalTypeAnnotation annotation = type.getLogicalTypeAnnotation();    return useSignedStringMinMax && PrimitiveTypeName.BINARY == type.getPrimitiveTypeName() && (annotation == null || STRING_TYPES.contains(annotation.getClass()));}
private static SortOrder parquet-mr_f4759_0(PrimitiveTypeName primitive)
{    switch(primitive) {        case BOOLEAN:        case INT32:        case INT64:        case FLOAT:        case DOUBLE:            return SortOrder.SIGNED;        case BINARY:        case FIXED_LEN_BYTE_ARRAY:            return SortOrder.UNSIGNED;    }    return SortOrder.UNKNOWN;}
private static SortOrder parquet-mr_f4760_0(PrimitiveType primitive)
{    LogicalTypeAnnotation annotation = primitive.getLogicalTypeAnnotation();    if (annotation != null) {        return annotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<SortOrder>() {            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {                return intLogicalType.isSigned() ? of(SortOrder.SIGNED) : of(SortOrder.UNSIGNED);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType) {                return of(SortOrder.UNKNOWN);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {                return of(SortOrder.SIGNED);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType) {                return of(SortOrder.UNSIGNED);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType) {                return of(SortOrder.UNSIGNED);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType) {                return of(SortOrder.UNSIGNED);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType) {                return of(SortOrder.UNSIGNED);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {                return of(SortOrder.UNKNOWN);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.MapKeyValueTypeAnnotation mapKeyValueLogicalType) {                return of(SortOrder.UNKNOWN);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType) {                return of(SortOrder.UNKNOWN);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType) {                return of(SortOrder.UNKNOWN);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {                return of(SortOrder.SIGNED);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {                return of(SortOrder.SIGNED);            }        }).orElse(defaultSortOrder(primitive.getPrimitiveTypeName()));    }    return defaultSortOrder(primitive.getPrimitiveTypeName());}
public Optional<SortOrder> parquet-mr_f4761_0(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    return intLogicalType.isSigned() ? of(SortOrder.SIGNED) : of(SortOrder.UNSIGNED);}
public Optional<SortOrder> parquet-mr_f4762_0(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType)
{    return of(SortOrder.UNKNOWN);}
public Optional<SortOrder> parquet-mr_f4763_0(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    return of(SortOrder.SIGNED);}
public Optional<SortOrder> parquet-mr_f4764_0(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType)
{    return of(SortOrder.UNSIGNED);}
public Optional<SortOrder> parquet-mr_f4765_0(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType)
{    return of(SortOrder.UNSIGNED);}
public Optional<SortOrder> parquet-mr_f4766_0(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType)
{    return of(SortOrder.UNSIGNED);}
public Optional<SortOrder> parquet-mr_f4767_0(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    return of(SortOrder.UNSIGNED);}
public Optional<SortOrder> parquet-mr_f4768_0(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(SortOrder.UNKNOWN);}
public Optional<SortOrder> parquet-mr_f4769_0(LogicalTypeAnnotation.MapKeyValueTypeAnnotation mapKeyValueLogicalType)
{    return of(SortOrder.UNKNOWN);}
public Optional<SortOrder> parquet-mr_f4770_0(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    return of(SortOrder.UNKNOWN);}
public Optional<SortOrder> parquet-mr_f4771_0(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    return of(SortOrder.UNKNOWN);}
public Optional<SortOrder> parquet-mr_f4772_0(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    return of(SortOrder.SIGNED);}
public Optional<SortOrder> parquet-mr_f4773_0(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    return of(SortOrder.SIGNED);}
public PrimitiveTypeName parquet-mr_f4774_0(Type type)
{    switch(type) {        case         BYTE_ARRAY:            return PrimitiveTypeName.BINARY;        case INT64:            return PrimitiveTypeName.INT64;        case INT32:            return PrimitiveTypeName.INT32;        case BOOLEAN:            return PrimitiveTypeName.BOOLEAN;        case FLOAT:            return PrimitiveTypeName.FLOAT;        case DOUBLE:            return PrimitiveTypeName.DOUBLE;        case INT96:            return PrimitiveTypeName.INT96;        case FIXED_LEN_BYTE_ARRAY:            return PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY;        default:            throw new RuntimeException("Unknown type " + type);    }}
 Type parquet-mr_f4775_0(PrimitiveTypeName type)
{    switch(type) {        case INT64:            return Type.INT64;        case INT32:            return Type.INT32;        case BOOLEAN:            return Type.BOOLEAN;        case BINARY:            return Type.BYTE_ARRAY;        case FLOAT:            return Type.FLOAT;        case DOUBLE:            return Type.DOUBLE;        case INT96:            return Type.INT96;        case FIXED_LEN_BYTE_ARRAY:            return Type.FIXED_LEN_BYTE_ARRAY;        default:            throw new RuntimeException("Unknown primitive type " + type);    }}
 LogicalTypeAnnotation parquet-mr_f4776_0(ConvertedType type, SchemaElement schemaElement)
{    switch(type) {        case UTF8:            return LogicalTypeAnnotation.stringType();        case MAP:            return LogicalTypeAnnotation.mapType();        case MAP_KEY_VALUE:            return LogicalTypeAnnotation.MapKeyValueTypeAnnotation.getInstance();        case LIST:            return LogicalTypeAnnotation.listType();        case ENUM:            return LogicalTypeAnnotation.enumType();        case DECIMAL:            int scale = (schemaElement == null ? 0 : schemaElement.scale);            int precision = (schemaElement == null ? 0 : schemaElement.precision);            return LogicalTypeAnnotation.decimalType(scale, precision);        case DATE:            return LogicalTypeAnnotation.dateType();        case TIME_MILLIS:            return LogicalTypeAnnotation.timeType(true, LogicalTypeAnnotation.TimeUnit.MILLIS);        case TIME_MICROS:            return LogicalTypeAnnotation.timeType(true, LogicalTypeAnnotation.TimeUnit.MICROS);        case TIMESTAMP_MILLIS:            return LogicalTypeAnnotation.timestampType(true, LogicalTypeAnnotation.TimeUnit.MILLIS);        case TIMESTAMP_MICROS:            return LogicalTypeAnnotation.timestampType(true, LogicalTypeAnnotation.TimeUnit.MICROS);        case INTERVAL:            return LogicalTypeAnnotation.IntervalLogicalTypeAnnotation.getInstance();        case INT_8:            return LogicalTypeAnnotation.intType(8, true);        case INT_16:            return LogicalTypeAnnotation.intType(16, true);        case INT_32:            return LogicalTypeAnnotation.intType(32, true);        case INT_64:            return LogicalTypeAnnotation.intType(64, true);        case UINT_8:            return LogicalTypeAnnotation.intType(8, false);        case UINT_16:            return LogicalTypeAnnotation.intType(16, false);        case UINT_32:            return LogicalTypeAnnotation.intType(32, false);        case UINT_64:            return LogicalTypeAnnotation.intType(64, false);        case JSON:            return LogicalTypeAnnotation.jsonType();        case BSON:            return LogicalTypeAnnotation.bsonType();        default:            throw new RuntimeException("Can't convert converted type to logical type, unknown converted type " + type);    }}
 LogicalTypeAnnotation parquet-mr_f4777_0(LogicalType type)
{    switch(type.getSetField()) {        case MAP:            return LogicalTypeAnnotation.mapType();        case BSON:            return LogicalTypeAnnotation.bsonType();        case DATE:            return LogicalTypeAnnotation.dateType();        case ENUM:            return LogicalTypeAnnotation.enumType();        case JSON:            return LogicalTypeAnnotation.jsonType();        case LIST:            return LogicalTypeAnnotation.listType();        case TIME:            TimeType time = type.getTIME();            return LogicalTypeAnnotation.timeType(time.isAdjustedToUTC, convertTimeUnit(time.unit));        case STRING:            return LogicalTypeAnnotation.stringType();        case DECIMAL:            DecimalType decimal = type.getDECIMAL();            return LogicalTypeAnnotation.decimalType(decimal.scale, decimal.precision);        case INTEGER:            IntType integer = type.getINTEGER();            return LogicalTypeAnnotation.intType(integer.bitWidth, integer.isSigned);        case UNKNOWN:            return null;        case TIMESTAMP:            TimestampType timestamp = type.getTIMESTAMP();            return LogicalTypeAnnotation.timestampType(timestamp.isAdjustedToUTC, convertTimeUnit(timestamp.unit));        default:            throw new RuntimeException("Unknown logical type " + type);    }}
private LogicalTypeAnnotation.TimeUnit parquet-mr_f4778_0(TimeUnit unit)
{    switch(unit.getSetField()) {        case MICROS:            return LogicalTypeAnnotation.TimeUnit.MICROS;        case MILLIS:            return LogicalTypeAnnotation.TimeUnit.MILLIS;        case NANOS:            return LogicalTypeAnnotation.TimeUnit.NANOS;        default:            throw new RuntimeException("Unknown time unit " + unit);    }}
private static void parquet-mr_f4779_0(FileMetaData fileMetaData, String key, String value)
{    KeyValue keyValue = new KeyValue(key);    keyValue.value = value;    fileMetaData.addToKey_value_metadata(keyValue);}
public static MetadataFilter parquet-mr_f4780_0(long startOffset, long endOffset)
{    return new RangeMetadataFilter(startOffset, endOffset);}
public static MetadataFilter parquet-mr_f4781_0(long... offsets)
{    Set<Long> set = new HashSet<Long>();    for (long offset : offsets) {        set.add(offset);    }    return new OffsetMetadataFilter(set);}
 T parquet-mr_f4782_0(MetadataFilterVisitor<T, E> visitor) throws E
{    return visitor.visit(this);}
public String parquet-mr_f4783_0()
{    return "NO_FILTER";}
 T parquet-mr_f4784_0(MetadataFilterVisitor<T, E> visitor) throws E
{    return visitor.visit(this);}
public String parquet-mr_f4785_0()
{    return "SKIP_ROW_GROUPS";}
 T parquet-mr_f4786_0(MetadataFilterVisitor<T, E> visitor) throws E
{    return visitor.visit(this);}
public boolean parquet-mr_f4787_0(long offset)
{    return offset >= this.startOffset && offset < this.endOffset;}
public String parquet-mr_f4788_0()
{    return "range(s:" + startOffset + ", e:" + endOffset + ")";}
public boolean parquet-mr_f4789_0(long offset)
{    return offsets.contains(offset);}
 T parquet-mr_f4790_0(MetadataFilterVisitor<T, E> visitor) throws E
{    return visitor.visit(this);}
public ParquetMetadata parquet-mr_f4791_0(InputStream from) throws IOException
{    return readParquetMetadata(from, NO_FILTER);}
 static FileMetaData parquet-mr_f4792_0(FileMetaData metaData, RangeMetadataFilter filter)
{    List<RowGroup> rowGroups = metaData.getRow_groups();    List<RowGroup> newRowGroups = new ArrayList<RowGroup>();    for (RowGroup rowGroup : rowGroups) {        long totalSize = 0;        long startIndex = getOffset(rowGroup.getColumns().get(0));        for (ColumnChunk col : rowGroup.getColumns()) {            totalSize += col.getMeta_data().getTotal_compressed_size();        }        long midPoint = startIndex + totalSize / 2;        if (filter.contains(midPoint)) {            newRowGroups.add(rowGroup);        }    }    metaData.setRow_groups(newRowGroups);    return metaData;}
 static FileMetaData parquet-mr_f4793_0(FileMetaData metaData, OffsetMetadataFilter filter)
{    List<RowGroup> rowGroups = metaData.getRow_groups();    List<RowGroup> newRowGroups = new ArrayList<RowGroup>();    for (RowGroup rowGroup : rowGroups) {        long startIndex = getOffset(rowGroup.getColumns().get(0));        if (filter.contains(startIndex)) {            newRowGroups.add(rowGroup);        }    }    metaData.setRow_groups(newRowGroups);    return metaData;}
 static long parquet-mr_f4794_0(RowGroup rowGroup)
{    return getOffset(rowGroup.getColumns().get(0));}
 static long parquet-mr_f4795_0(ColumnChunk columnChunk)
{    ColumnMetaData md = columnChunk.getMeta_data();    long offset = md.getData_page_offset();    if (md.isSetDictionary_page_offset() && offset > md.getDictionary_page_offset()) {        offset = md.getDictionary_page_offset();    }    return offset;}
public ParquetMetadata parquet-mr_f4796_1(final InputStream from, MetadataFilter filter) throws IOException
{    FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {        @Override        public FileMetaData visit(NoFilter filter) throws IOException {            return readFileMetaData(from);        }        @Override        public FileMetaData visit(SkipMetadataFilter filter) throws IOException {            return readFileMetaData(from, true);        }        @Override        public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {            return filterFileMetaDataByStart(readFileMetaData(from), filter);        }        @Override        public FileMetaData visit(RangeMetadataFilter filter) throws IOException {            return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);        }    });        ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);    if (LOG.isDebugEnabled())            return parquetMetadata;}
public FileMetaData parquet-mr_f4797_0(NoFilter filter) throws IOException
{    return readFileMetaData(from);}
public FileMetaData parquet-mr_f4798_0(SkipMetadataFilter filter) throws IOException
{    return readFileMetaData(from, true);}
public FileMetaData parquet-mr_f4799_0(OffsetMetadataFilter filter) throws IOException
{    return filterFileMetaDataByStart(readFileMetaData(from), filter);}
public FileMetaData parquet-mr_f4800_0(RangeMetadataFilter filter) throws IOException
{    return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);}
public ParquetMetadata parquet-mr_f4801_0(FileMetaData parquetMetadata) throws IOException
{    MessageType messageType = fromParquetSchema(parquetMetadata.getSchema(), parquetMetadata.getColumn_orders());    List<BlockMetaData> blocks = new ArrayList<BlockMetaData>();    List<RowGroup> row_groups = parquetMetadata.getRow_groups();    if (row_groups != null) {        for (RowGroup rowGroup : row_groups) {            BlockMetaData blockMetaData = new BlockMetaData();            blockMetaData.setRowCount(rowGroup.getNum_rows());            blockMetaData.setTotalByteSize(rowGroup.getTotal_byte_size());            List<ColumnChunk> columns = rowGroup.getColumns();            String filePath = columns.get(0).getFile_path();            for (ColumnChunk columnChunk : columns) {                if ((filePath == null && columnChunk.getFile_path() != null) || (filePath != null && !filePath.equals(columnChunk.getFile_path()))) {                    throw new ParquetDecodingException("all column chunks of the same row group must be in the same file for now");                }                ColumnMetaData metaData = columnChunk.meta_data;                ColumnPath path = getPath(metaData);                ColumnChunkMetaData column = ColumnChunkMetaData.get(path, messageType.getType(path.toArray()).asPrimitiveType(), fromFormatCodec(metaData.codec), convertEncodingStats(metaData.getEncoding_stats()), fromFormatEncodings(metaData.encodings), fromParquetStatistics(parquetMetadata.getCreated_by(), metaData.statistics, messageType.getType(path.toArray()).asPrimitiveType()), metaData.data_page_offset, metaData.dictionary_page_offset, metaData.num_values, metaData.total_compressed_size, metaData.total_uncompressed_size);                column.setColumnIndexReference(toColumnIndexReference(columnChunk));                column.setOffsetIndexReference(toOffsetIndexReference(columnChunk));                                                                blockMetaData.addColumn(column);            }            blockMetaData.setPath(filePath);            blocks.add(blockMetaData);        }    }    Map<String, String> keyValueMetaData = new HashMap<String, String>();    List<KeyValue> key_value_metadata = parquetMetadata.getKey_value_metadata();    if (key_value_metadata != null) {        for (KeyValue keyValue : key_value_metadata) {            keyValueMetaData.put(keyValue.key, keyValue.value);        }    }    return new ParquetMetadata(new org.apache.parquet.hadoop.metadata.FileMetaData(messageType, keyValueMetaData, parquetMetadata.getCreated_by()), blocks);}
private static IndexReference parquet-mr_f4802_0(ColumnChunk columnChunk)
{    if (columnChunk.isSetColumn_index_offset() && columnChunk.isSetColumn_index_length()) {        return new IndexReference(columnChunk.getColumn_index_offset(), columnChunk.getColumn_index_length());    }    return null;}
private static IndexReference parquet-mr_f4803_0(ColumnChunk columnChunk)
{    if (columnChunk.isSetOffset_index_offset() && columnChunk.isSetOffset_index_length()) {        return new IndexReference(columnChunk.getOffset_index_offset(), columnChunk.getOffset_index_length());    }    return null;}
private static ColumnPath parquet-mr_f4804_0(ColumnMetaData metaData)
{    String[] path = metaData.path_in_schema.toArray(new String[metaData.path_in_schema.size()]);    return ColumnPath.get(path);}
 MessageType parquet-mr_f4805_0(List<SchemaElement> schema, List<ColumnOrder> columnOrders)
{    Iterator<SchemaElement> iterator = schema.iterator();    SchemaElement root = iterator.next();    Types.MessageTypeBuilder builder = Types.buildMessage();    if (root.isSetField_id()) {        builder.id(root.field_id);    }    buildChildren(builder, iterator, root.getNum_children(), columnOrders, 0);    return builder.named(root.name);}
private void parquet-mr_f4806_1(Types.GroupBuilder builder, Iterator<SchemaElement> schema, int childrenCount, List<ColumnOrder> columnOrders, int columnCount)
{    for (int i = 0; i < childrenCount; i++) {        SchemaElement schemaElement = schema.next();                Types.Builder childBuilder;        if (schemaElement.type != null) {            Types.PrimitiveBuilder primitiveBuilder = builder.primitive(getPrimitive(schemaElement.type), fromParquetRepetition(schemaElement.repetition_type));            if (schemaElement.isSetType_length()) {                primitiveBuilder.length(schemaElement.type_length);            }            if (schemaElement.isSetPrecision()) {                primitiveBuilder.precision(schemaElement.precision);            }            if (schemaElement.isSetScale()) {                primitiveBuilder.scale(schemaElement.scale);            }            if (columnOrders != null) {                org.apache.parquet.schema.ColumnOrder columnOrder = fromParquetColumnOrder(columnOrders.get(columnCount));                                if (columnOrder.getColumnOrderName() == ColumnOrderName.TYPE_DEFINED_ORDER && (schemaElement.type == Type.INT96 || schemaElement.converted_type == ConvertedType.INTERVAL)) {                    columnOrder = org.apache.parquet.schema.ColumnOrder.undefined();                }                primitiveBuilder.columnOrder(columnOrder);            }            childBuilder = primitiveBuilder;        } else {            childBuilder = builder.group(fromParquetRepetition(schemaElement.repetition_type));            buildChildren((Types.GroupBuilder) childBuilder, schema, schemaElement.num_children, columnOrders, columnCount);        }        if (schemaElement.isSetLogicalType()) {            childBuilder.as(getLogicalTypeAnnotation(schemaElement.logicalType));        }        if (schemaElement.isSetConverted_type()) {            OriginalType originalType = getLogicalTypeAnnotation(schemaElement.converted_type, schemaElement).toOriginalType();            OriginalType newOriginalType = (schemaElement.isSetLogicalType() && getLogicalTypeAnnotation(schemaElement.logicalType) != null) ? getLogicalTypeAnnotation(schemaElement.logicalType).toOriginalType() : null;            if (!originalType.equals(newOriginalType)) {                if (newOriginalType != null) {                                    }                childBuilder.as(originalType);            }        }        if (schemaElement.isSetField_id()) {            childBuilder.id(schemaElement.field_id);        }        childBuilder.named(schemaElement.name);        ++columnCount;    }}
 FieldRepetitionType parquet-mr_f4807_0(Repetition repetition)
{    return FieldRepetitionType.valueOf(repetition.name());}
 Repetition parquet-mr_f4808_0(FieldRepetitionType repetition)
{    return Repetition.valueOf(repetition.name());}
private static org.apache.parquet.schema.ColumnOrder parquet-mr_f4809_0(ColumnOrder columnOrder)
{    if (columnOrder.isSetTYPE_ORDER()) {        return org.apache.parquet.schema.ColumnOrder.typeDefined();    }        return org.apache.parquet.schema.ColumnOrder.undefined();}
public void parquet-mr_f4810_0(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.Encoding rlEncoding, org.apache.parquet.column.Encoding dlEncoding, org.apache.parquet.column.Encoding valuesEncoding, OutputStream to) throws IOException
{    writePageHeader(newDataPageHeader(uncompressedSize, compressedSize, valueCount, rlEncoding, dlEncoding, valuesEncoding), to);}
public void parquet-mr_f4811_0(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.statistics.Statistics statistics, org.apache.parquet.column.Encoding rlEncoding, org.apache.parquet.column.Encoding dlEncoding, org.apache.parquet.column.Encoding valuesEncoding, OutputStream to) throws IOException
{    writePageHeader(newDataPageHeader(uncompressedSize, compressedSize, valueCount, rlEncoding, dlEncoding, valuesEncoding), to);}
private PageHeader parquet-mr_f4812_0(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.Encoding rlEncoding, org.apache.parquet.column.Encoding dlEncoding, org.apache.parquet.column.Encoding valuesEncoding)
{    PageHeader pageHeader = new PageHeader(PageType.DATA_PAGE, uncompressedSize, compressedSize);    pageHeader.setData_page_header(new DataPageHeader(valueCount, getEncoding(valuesEncoding), getEncoding(dlEncoding), getEncoding(rlEncoding)));    return pageHeader;}
private PageHeader parquet-mr_f4813_0(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.Encoding rlEncoding, org.apache.parquet.column.Encoding dlEncoding, org.apache.parquet.column.Encoding valuesEncoding, int crc)
{    PageHeader pageHeader = new PageHeader(PageType.DATA_PAGE, uncompressedSize, compressedSize);    pageHeader.setCrc(crc);    pageHeader.setData_page_header(new DataPageHeader(valueCount, getEncoding(valuesEncoding), getEncoding(dlEncoding), getEncoding(rlEncoding)));    return pageHeader;}
public void parquet-mr_f4814_0(int uncompressedSize, int compressedSize, int valueCount, int nullCount, int rowCount, org.apache.parquet.column.statistics.Statistics statistics, org.apache.parquet.column.Encoding dataEncoding, int rlByteLength, int dlByteLength, OutputStream to) throws IOException
{    writePageHeader(newDataPageV2Header(uncompressedSize, compressedSize, valueCount, nullCount, rowCount, dataEncoding, rlByteLength, dlByteLength), to);}
public void parquet-mr_f4815_0(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.Encoding rlEncoding, org.apache.parquet.column.Encoding dlEncoding, org.apache.parquet.column.Encoding valuesEncoding, OutputStream to) throws IOException
{    writePageHeader(newDataPageHeader(uncompressedSize, compressedSize, valueCount, rlEncoding, dlEncoding, valuesEncoding), to);}
public void parquet-mr_f4816_0(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.Encoding rlEncoding, org.apache.parquet.column.Encoding dlEncoding, org.apache.parquet.column.Encoding valuesEncoding, int crc, OutputStream to) throws IOException
{    writePageHeader(newDataPageHeader(uncompressedSize, compressedSize, valueCount, rlEncoding, dlEncoding, valuesEncoding, crc), to);}
public void parquet-mr_f4817_0(int uncompressedSize, int compressedSize, int valueCount, int nullCount, int rowCount, org.apache.parquet.column.Encoding dataEncoding, int rlByteLength, int dlByteLength, OutputStream to) throws IOException
{    writePageHeader(newDataPageV2Header(uncompressedSize, compressedSize, valueCount, nullCount, rowCount, dataEncoding, rlByteLength, dlByteLength), to);}
private PageHeader parquet-mr_f4818_0(int uncompressedSize, int compressedSize, int valueCount, int nullCount, int rowCount, org.apache.parquet.column.Encoding dataEncoding, int rlByteLength, int dlByteLength)
{        DataPageHeaderV2 dataPageHeaderV2 = new DataPageHeaderV2(valueCount, nullCount, rowCount, getEncoding(dataEncoding), dlByteLength, rlByteLength);    PageHeader pageHeader = new PageHeader(PageType.DATA_PAGE_V2, uncompressedSize, compressedSize);    pageHeader.setData_page_header_v2(dataPageHeaderV2);    return pageHeader;}
public void parquet-mr_f4819_0(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.Encoding valuesEncoding, OutputStream to) throws IOException
{    PageHeader pageHeader = new PageHeader(PageType.DICTIONARY_PAGE, uncompressedSize, compressedSize);    pageHeader.setDictionary_page_header(new DictionaryPageHeader(valueCount, getEncoding(valuesEncoding)));    writePageHeader(pageHeader, to);}
public void parquet-mr_f4820_0(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.Encoding valuesEncoding, int crc, OutputStream to) throws IOException
{    PageHeader pageHeader = new PageHeader(PageType.DICTIONARY_PAGE, uncompressedSize, compressedSize);    pageHeader.setCrc(crc);    pageHeader.setDictionary_page_header(new DictionaryPageHeader(valueCount, getEncoding(valuesEncoding)));    writePageHeader(pageHeader, to);}
private static BoundaryOrder parquet-mr_f4821_0(org.apache.parquet.internal.column.columnindex.BoundaryOrder boundaryOrder)
{    switch(boundaryOrder) {        case ASCENDING:            return BoundaryOrder.ASCENDING;        case DESCENDING:            return BoundaryOrder.DESCENDING;        case UNORDERED:            return BoundaryOrder.UNORDERED;        default:            throw new IllegalArgumentException("Unsupported boundary order: " + boundaryOrder);    }}
private static org.apache.parquet.internal.column.columnindex.BoundaryOrder parquet-mr_f4822_0(BoundaryOrder boundaryOrder)
{    switch(boundaryOrder) {        case ASCENDING:            return org.apache.parquet.internal.column.columnindex.BoundaryOrder.ASCENDING;        case DESCENDING:            return org.apache.parquet.internal.column.columnindex.BoundaryOrder.DESCENDING;        case UNORDERED:            return org.apache.parquet.internal.column.columnindex.BoundaryOrder.UNORDERED;        default:            throw new IllegalArgumentException("Unsupported boundary order: " + boundaryOrder);    }}
public static ColumnIndex parquet-mr_f4823_0(PrimitiveType type, org.apache.parquet.internal.column.columnindex.ColumnIndex columnIndex)
{    if (!isMinMaxStatsSupported(type) || columnIndex == null) {        return null;    }    ColumnIndex parquetColumnIndex = new ColumnIndex(columnIndex.getNullPages(), columnIndex.getMinValues(), columnIndex.getMaxValues(), toParquetBoundaryOrder(columnIndex.getBoundaryOrder()));    parquetColumnIndex.setNull_counts(columnIndex.getNullCounts());    return parquetColumnIndex;}
public static org.apache.parquet.internal.column.columnindex.ColumnIndex parquet-mr_f4824_0(PrimitiveType type, ColumnIndex parquetColumnIndex)
{    if (!isMinMaxStatsSupported(type)) {        return null;    }    return ColumnIndexBuilder.build(type, fromParquetBoundaryOrder(parquetColumnIndex.getBoundary_order()), parquetColumnIndex.getNull_pages(), parquetColumnIndex.getNull_counts(), parquetColumnIndex.getMin_values(), parquetColumnIndex.getMax_values());}
public static OffsetIndex parquet-mr_f4825_0(org.apache.parquet.internal.column.columnindex.OffsetIndex offsetIndex)
{    List<PageLocation> pageLocations = new ArrayList<>(offsetIndex.getPageCount());    for (int i = 0, n = offsetIndex.getPageCount(); i < n; ++i) {        pageLocations.add(new PageLocation(offsetIndex.getOffset(i), offsetIndex.getCompressedPageSize(i), offsetIndex.getFirstRowIndex(i)));    }    return new OffsetIndex(pageLocations);}
public static org.apache.parquet.internal.column.columnindex.OffsetIndex parquet-mr_f4826_0(OffsetIndex parquetOffsetIndex)
{    OffsetIndexBuilder builder = OffsetIndexBuilder.getBuilder();    for (PageLocation pageLocation : parquetOffsetIndex.getPage_locations()) {        builder.add(pageLocation.getOffset(), pageLocation.getCompressed_page_size(), pageLocation.getFirst_row_index());    }    return builder.build();}
public ReadSupport.ReadContext parquet-mr_f4827_0(InitContext context)
{    return delegate.init(context);}
public RecordMaterializer<T> parquet-mr_f4828_0(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema, ReadSupport.ReadContext readContext)
{    return delegate.prepareForRead(configuration, keyValueMetaData, fileSchema, readContext);}
public String parquet-mr_f4829_0()
{    return this.getClass().getName() + "(" + delegate.toString() + ")";}
public WriteSupport.WriteContext parquet-mr_f4830_0(Configuration configuration)
{    return delegate.init(configuration);}
public void parquet-mr_f4831_0(RecordConsumer recordConsumer)
{    delegate.prepareForWrite(recordConsumer);}
public void parquet-mr_f4832_0(T record)
{    delegate.write(record);}
public String parquet-mr_f4833_0()
{    return delegate.getName();}
public WriteSupport.FinalizedWriteContext parquet-mr_f4834_0()
{    return delegate.finalizeWrite();}
public String parquet-mr_f4835_0()
{    return getClass().getName() + "(" + delegate.toString() + ")";}
public Map<String, String> parquet-mr_f4836_0()
{    if (mergedKeyValueMetadata == null) {        Map<String, String> mergedKeyValues = new HashMap<String, String>();        for (Entry<String, Set<String>> entry : keyValueMetadata.entrySet()) {            if (entry.getValue().size() > 1) {                throw new RuntimeException("could not merge metadata: key " + entry.getKey() + " has conflicting values: " + entry.getValue());            }            mergedKeyValues.put(entry.getKey(), entry.getValue().iterator().next());        }        mergedKeyValueMetadata = mergedKeyValues;    }    return mergedKeyValueMetadata;}
public Configuration parquet-mr_f4837_0()
{    return configuration;}
public MessageType parquet-mr_f4838_0()
{    return fileSchema;}
public Map<String, Set<String>> parquet-mr_f4839_0()
{    return keyValueMetadata;}
public static MessageType parquet-mr_f4840_0(MessageType fileMessageType, String partialReadSchemaString)
{    if (partialReadSchemaString == null)        return fileMessageType;    MessageType requestedMessageType = MessageTypeParser.parseMessageType(partialReadSchemaString);    return getSchemaForRead(fileMessageType, requestedMessageType);}
public static MessageType parquet-mr_f4841_0(MessageType fileMessageType, MessageType projectedMessageType)
{    fileMessageType.checkContains(projectedMessageType);    return projectedMessageType;}
public ReadContext parquet-mr_f4842_0(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema)
{    throw new UnsupportedOperationException("Override init(InitContext)");}
public ReadContext parquet-mr_f4843_0(InitContext context)
{    return init(context.getConfiguration(), context.getMergedKeyValueMetaData(), context.getFileSchema());}
public MessageType parquet-mr_f4844_0()
{    return requestedSchema;}
public Map<String, String> parquet-mr_f4845_0()
{    return readSupportMetadata;}
public MessageType parquet-mr_f4846_0()
{    return schema;}
public Map<String, String> parquet-mr_f4847_0()
{    return extraMetaData;}
public Map<String, String> parquet-mr_f4848_0()
{    return extraMetaData;}
public String parquet-mr_f4849_0()
{    return null;}
public FinalizedWriteContext parquet-mr_f4850_0()
{    return new FinalizedWriteContext(new HashMap<String, String>());}
public static void parquet-mr_f4851_1(ByteBuffer buf)
{    if (cleanMethod != null) {        try {            cleanMethod.invoke(cleanerMethod.invoke(buf));        } catch (IllegalAccessException | IllegalArgumentException | InvocationTargetException | SecurityException e) {                    }    } else if (invokeCleanerMethod != null) {        try {            invokeCleanerMethod.invoke(unsafe, buf);        } catch (IllegalAccessException | IllegalArgumentException | InvocationTargetException | SecurityException e) {                    }    }}
public static CodecConfig parquet-mr_f4852_0(JobConf jobConf)
{    return new MapredCodecConfig(jobConf);}
public static CodecConfig parquet-mr_f4853_0(TaskAttemptContext context)
{    return new MapreduceCodecConfig(context);}
public static boolean parquet-mr_f4854_0(Configuration conf)
{    return conf.get(ParquetOutputFormat.COMPRESSION) != null;}
public static CompressionCodecName parquet-mr_f4855_0(Configuration configuration)
{    return CompressionCodecName.fromConf(configuration.get(ParquetOutputFormat.COMPRESSION, UNCOMPRESSED.name()));}
public CompressionCodecName parquet-mr_f4856_1()
{    CompressionCodecName codec;    Configuration configuration = getConfiguration();    if (isParquetCompressionSet(configuration)) {                codec = getParquetCompressionCodec(configuration);    } else if (isHadoopCompressionSet()) {                codec = getHadoopCompressionCodec();    } else {                codec = CompressionCodecName.UNCOMPRESSED;    }        return codec;}
private CompressionCodecName parquet-mr_f4857_1()
{    CompressionCodecName codec;    try {                Class<?> codecClass = getHadoopOutputCompressorClass(CompressionCodecName.UNCOMPRESSED.getHadoopCompressionCodecClass());                codec = CompressionCodecName.fromCompressionCodec(codecClass);    } catch (CompressionCodecNotSupportedException e) {                codec = CompressionCodecName.UNCOMPRESSED;    } catch (IllegalArgumentException e) {                codec = CompressionCodecName.UNCOMPRESSED;    }    return codec;}
public boolean parquet-mr_f4858_0()
{    return FileOutputFormat.getCompressOutput(context);}
public Class parquet-mr_f4859_0(Class defaultCodec)
{    return FileOutputFormat.getOutputCompressorClass(context, defaultCodec);}
public Configuration parquet-mr_f4860_0()
{    return ContextUtil.getConfiguration(context);}
public boolean parquet-mr_f4861_0()
{    return org.apache.hadoop.mapred.FileOutputFormat.getCompressOutput(conf);}
public Class parquet-mr_f4862_0(Class defaultCodec)
{    return org.apache.hadoop.mapred.FileOutputFormat.getOutputCompressorClass(conf, defaultCodec);}
public Configuration parquet-mr_f4863_0()
{    return conf;}
public void parquet-mr_f4864_0(byte[] b, int off, int len) throws IOException
{        if (compressor.finished()) {        throw new IOException("write beyond end of stream");    }    if ((off | len | (off + len) | (b.length - (off + len))) < 0) {        throw new IndexOutOfBoundsException();    } else if (len == 0) {        return;    }    compressor.setInput(b, off, len);}
public int parquet-mr_f4865_0(byte[] b, int off, int len) throws IOException
{    if (!inputHandled) {                while (true) {            int compressedBytes = getCompressedData();            if (compressedBytes == -1)                break;            decompressor.setInput(buffer, 0, compressedBytes);        }        inputHandled = true;    }    int decompressedBytes = decompressor.decompress(b, off, len);    if (decompressor.finished()) {        decompressor.reset();    }    return decompressedBytes;}
public void parquet-mr_f4866_0(Configuration conf)
{    this.conf = conf;}
public Configuration parquet-mr_f4867_0()
{    return conf;}
public Compressor parquet-mr_f4868_0()
{    return new SnappyCompressor();}
public Decompressor parquet-mr_f4869_0()
{    return new SnappyDecompressor();}
public CompressionInputStream parquet-mr_f4870_0(InputStream stream) throws IOException
{    return createInputStream(stream, createDecompressor());}
public CompressionInputStream parquet-mr_f4871_0(InputStream stream, Decompressor decompressor) throws IOException
{    return new NonBlockedDecompressorStream(stream, decompressor, conf.getInt(BUFFER_SIZE_CONFIG, 4 * 1024));}
public CompressionOutputStream parquet-mr_f4872_0(OutputStream stream) throws IOException
{    return createOutputStream(stream, createCompressor());}
public CompressionOutputStream parquet-mr_f4873_0(OutputStream stream, Compressor compressor) throws IOException
{    return new NonBlockedCompressorStream(stream, compressor, conf.getInt(BUFFER_SIZE_CONFIG, 4 * 1024));}
public Class<? extends Compressor> parquet-mr_f4874_0()
{    return SnappyCompressor.class;}
public Class<? extends Decompressor> parquet-mr_f4875_0()
{    return SnappyDecompressor.class;}
public String parquet-mr_f4876_0()
{    return ".snappy";}
public synchronized int parquet-mr_f4877_0(byte[] buffer, int off, int len) throws IOException
{    SnappyUtil.validateBuffer(buffer, off, len);    if (needsInput()) {                return 0;    }    if (!outputBuffer.hasRemaining()) {                int maxOutputSize = Snappy.maxCompressedLength(inputBuffer.position());        if (maxOutputSize > outputBuffer.capacity()) {            ByteBuffer oldBuffer = outputBuffer;            outputBuffer = ByteBuffer.allocateDirect(maxOutputSize);            CleanUtil.cleanDirectBuffer(oldBuffer);        }                outputBuffer.clear();        inputBuffer.limit(inputBuffer.position());        inputBuffer.position(0);        int size = Snappy.compress(inputBuffer, outputBuffer);        outputBuffer.limit(size);        inputBuffer.limit(0);        inputBuffer.rewind();    }        int numBytes = Math.min(len, outputBuffer.remaining());    outputBuffer.get(buffer, off, numBytes);    bytesWritten += numBytes;    return numBytes;}
public synchronized void parquet-mr_f4878_0(byte[] buffer, int off, int len)
{    SnappyUtil.validateBuffer(buffer, off, len);    Preconditions.checkArgument(!outputBuffer.hasRemaining(), "Output buffer should be empty. Caller must call compress()");    if (inputBuffer.capacity() - inputBuffer.position() < len) {        ByteBuffer tmp = ByteBuffer.allocateDirect(inputBuffer.position() + len);        inputBuffer.rewind();        tmp.put(inputBuffer);        ByteBuffer oldBuffer = inputBuffer;        inputBuffer = tmp;        CleanUtil.cleanDirectBuffer(oldBuffer);    } else {        inputBuffer.limit(inputBuffer.position() + len);    }        inputBuffer.put(buffer, off, len);    bytesRead += len;}
public void parquet-mr_f4879_0()
{    CleanUtil.cleanDirectBuffer(inputBuffer);    CleanUtil.cleanDirectBuffer(outputBuffer);}
public synchronized void parquet-mr_f4880_0()
{    finishCalled = true;}
public synchronized boolean parquet-mr_f4881_0()
{    return finishCalled && inputBuffer.position() == 0 && !outputBuffer.hasRemaining();}
public long parquet-mr_f4882_0()
{    return bytesRead;}
public long parquet-mr_f4883_0()
{    return bytesWritten;}
public synchronized boolean parquet-mr_f4884_0()
{    return !finishCalled;}
public void parquet-mr_f4885_0(Configuration c)
{    reset();}
public synchronized void parquet-mr_f4886_0()
{    finishCalled = false;    bytesRead = bytesWritten = 0;    inputBuffer.rewind();    outputBuffer.rewind();    inputBuffer.limit(0);    outputBuffer.limit(0);}
public void parquet-mr_f4887_0(byte[] dictionary, int off, int len)
{}
public synchronized int parquet-mr_f4888_0(byte[] buffer, int off, int len) throws IOException
{    SnappyUtil.validateBuffer(buffer, off, len);    if (inputBuffer.position() == 0 && !outputBuffer.hasRemaining()) {        return 0;    }    if (!outputBuffer.hasRemaining()) {        inputBuffer.rewind();        Preconditions.checkArgument(inputBuffer.position() == 0, "Invalid position of 0.");        Preconditions.checkArgument(outputBuffer.position() == 0, "Invalid position of 0.");                int decompressedSize = Snappy.uncompressedLength(inputBuffer);        if (decompressedSize > outputBuffer.capacity()) {            ByteBuffer oldBuffer = outputBuffer;            outputBuffer = ByteBuffer.allocateDirect(decompressedSize);            CleanUtil.cleanDirectBuffer(oldBuffer);        }                outputBuffer.clear();        int size = Snappy.uncompress(inputBuffer, outputBuffer);        outputBuffer.limit(size);                inputBuffer.clear();        inputBuffer.limit(0);        finished = true;    }        int numBytes = Math.min(len, outputBuffer.remaining());    outputBuffer.get(buffer, off, numBytes);    return numBytes;}
public synchronized void parquet-mr_f4889_0(byte[] buffer, int off, int len)
{    SnappyUtil.validateBuffer(buffer, off, len);    if (inputBuffer.capacity() - inputBuffer.position() < len) {        final ByteBuffer newBuffer = ByteBuffer.allocateDirect(inputBuffer.position() + len);        inputBuffer.rewind();        newBuffer.put(inputBuffer);        final ByteBuffer oldBuffer = inputBuffer;        inputBuffer = newBuffer;        CleanUtil.cleanDirectBuffer(oldBuffer);    } else {        inputBuffer.limit(inputBuffer.position() + len);    }    inputBuffer.put(buffer, off, len);}
public void parquet-mr_f4890_0()
{    CleanUtil.cleanDirectBuffer(inputBuffer);    CleanUtil.cleanDirectBuffer(outputBuffer);}
public synchronized boolean parquet-mr_f4891_0()
{    return finished && !outputBuffer.hasRemaining();}
public int parquet-mr_f4892_0()
{    return 0;}
public synchronized boolean parquet-mr_f4893_0()
{    return !inputBuffer.hasRemaining() && !outputBuffer.hasRemaining();}
public synchronized void parquet-mr_f4894_0()
{    finished = false;    inputBuffer.rewind();    outputBuffer.rewind();    inputBuffer.limit(0);    outputBuffer.limit(0);}
public boolean parquet-mr_f4895_0()
{    return false;}
public void parquet-mr_f4896_0(byte[] b, int off, int len)
{}
public static void parquet-mr_f4897_0(byte[] buffer, int off, int len)
{    Preconditions.checkNotNull(buffer, "buffer");    Preconditions.checkArgument(off >= 0 && len >= 0 && off <= buffer.length - len, "Invalid buffer offset or length: buffer.length=%s off=%s len=%s", buffer.length, off, len);}
public static CodecFactory parquet-mr_f4898_0(Configuration config, ByteBufferAllocator allocator, int pageSize)
{    return new DirectCodecFactory(config, allocator, pageSize);}
public BytesInput parquet-mr_f4899_0(BytesInput bytes, int uncompressedSize) throws IOException
{    final BytesInput decompressed;    if (codec != null) {        decompressor.reset();        InputStream is = codec.createInputStream(bytes.toInputStream(), decompressor);        decompressed = BytesInput.from(is, uncompressedSize);    } else {        decompressed = bytes;    }    return decompressed;}
public void parquet-mr_f4900_0(ByteBuffer input, int compressedSize, ByteBuffer output, int uncompressedSize) throws IOException
{    ByteBuffer decompressed = decompress(BytesInput.from(input), uncompressedSize).toByteBuffer();    output.put(decompressed);}
public void parquet-mr_f4901_0()
{    if (decompressor != null) {        CodecPool.returnDecompressor(decompressor);    }}
public BytesInput parquet-mr_f4902_0(BytesInput bytes) throws IOException
{    final BytesInput compressedBytes;    if (codec == null) {        compressedBytes = bytes;    } else {        compressedOutBuffer.reset();        if (compressor != null) {                        compressor.reset();        }        CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, compressor);        bytes.writeAllTo(cos);        cos.finish();        cos.close();        compressedBytes = BytesInput.from(compressedOutBuffer);    }    return compressedBytes;}
public void parquet-mr_f4903_0()
{    if (compressor != null) {        CodecPool.returnCompressor(compressor);    }}
public CompressionCodecName parquet-mr_f4904_0()
{    return codecName;}
public BytesCompressor parquet-mr_f4905_0(CompressionCodecName codecName)
{    BytesCompressor comp = compressors.get(codecName);    if (comp == null) {        comp = createCompressor(codecName);        compressors.put(codecName, comp);    }    return comp;}
public BytesDecompressor parquet-mr_f4906_0(CompressionCodecName codecName)
{    BytesDecompressor decomp = decompressors.get(codecName);    if (decomp == null) {        decomp = createDecompressor(codecName);        decompressors.put(codecName, decomp);    }    return decomp;}
protected BytesCompressor parquet-mr_f4907_0(CompressionCodecName codecName)
{    return new HeapBytesCompressor(codecName);}
protected BytesDecompressor parquet-mr_f4908_0(CompressionCodecName codecName)
{    return new HeapBytesDecompressor(codecName);}
protected CompressionCodec parquet-mr_f4909_0(CompressionCodecName codecName)
{    String codecClassName = codecName.getHadoopCompressionCodecClassName();    if (codecClassName == null) {        return null;    }    CompressionCodec codec = CODEC_BY_NAME.get(codecClassName);    if (codec != null) {        return codec;    }    try {        Class<?> codecClass = Class.forName(codecClassName);        codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, configuration);        CODEC_BY_NAME.put(codecClassName, codec);        return codec;    } catch (ClassNotFoundException e) {        throw new BadConfigurationException("Class " + codecClassName + " was not found", e);    }}
public void parquet-mr_f4910_0()
{    for (BytesCompressor compressor : compressors.values()) {        compressor.release();    }    compressors.clear();    for (BytesDecompressor decompressor : decompressors.values()) {        decompressor.release();    }    decompressors.clear();}
public long parquet-mr_f4911_0()
{    return valueCount;}
public DataPage parquet-mr_f4912_0()
{    if (compressedPages.isEmpty()) {        return null;    }    DataPage compressedPage = compressedPages.remove(0);    final int currentPageIndex = pageIndex++;    return compressedPage.accept(new DataPage.Visitor<DataPage>() {        @Override        public DataPage visit(DataPageV1 dataPageV1) {            try {                BytesInput decompressed = decompressor.decompress(dataPageV1.getBytes(), dataPageV1.getUncompressedSize());                final DataPageV1 decompressedPage;                if (offsetIndex == null) {                    decompressedPage = new DataPageV1(decompressed, dataPageV1.getValueCount(), dataPageV1.getUncompressedSize(), dataPageV1.getStatistics(), dataPageV1.getRlEncoding(), dataPageV1.getDlEncoding(), dataPageV1.getValueEncoding());                } else {                    long firstRowIndex = offsetIndex.getFirstRowIndex(currentPageIndex);                    decompressedPage = new DataPageV1(decompressed, dataPageV1.getValueCount(), dataPageV1.getUncompressedSize(), firstRowIndex, Math.toIntExact(offsetIndex.getLastRowIndex(currentPageIndex, rowCount) - firstRowIndex + 1), dataPageV1.getStatistics(), dataPageV1.getRlEncoding(), dataPageV1.getDlEncoding(), dataPageV1.getValueEncoding());                }                if (dataPageV1.getCrc().isPresent()) {                    decompressedPage.setCrc(dataPageV1.getCrc().getAsInt());                }                return decompressedPage;            } catch (IOException e) {                throw new ParquetDecodingException("could not decompress page", e);            }        }        @Override        public DataPage visit(DataPageV2 dataPageV2) {            if (!dataPageV2.isCompressed()) {                if (offsetIndex == null) {                    return dataPageV2;                } else {                    return DataPageV2.uncompressed(dataPageV2.getRowCount(), dataPageV2.getNullCount(), dataPageV2.getValueCount(), offsetIndex.getFirstRowIndex(currentPageIndex), dataPageV2.getRepetitionLevels(), dataPageV2.getDefinitionLevels(), dataPageV2.getDataEncoding(), dataPageV2.getData(), dataPageV2.getStatistics());                }            }            try {                int uncompressedSize = Math.toIntExact(dataPageV2.getUncompressedSize() - dataPageV2.getDefinitionLevels().size() - dataPageV2.getRepetitionLevels().size());                BytesInput decompressed = decompressor.decompress(dataPageV2.getData(), uncompressedSize);                if (offsetIndex == null) {                    return DataPageV2.uncompressed(dataPageV2.getRowCount(), dataPageV2.getNullCount(), dataPageV2.getValueCount(), dataPageV2.getRepetitionLevels(), dataPageV2.getDefinitionLevels(), dataPageV2.getDataEncoding(), decompressed, dataPageV2.getStatistics());                } else {                    return DataPageV2.uncompressed(dataPageV2.getRowCount(), dataPageV2.getNullCount(), dataPageV2.getValueCount(), offsetIndex.getFirstRowIndex(currentPageIndex), dataPageV2.getRepetitionLevels(), dataPageV2.getDefinitionLevels(), dataPageV2.getDataEncoding(), decompressed, dataPageV2.getStatistics());                }            } catch (IOException e) {                throw new ParquetDecodingException("could not decompress page", e);            }        }    });}
public DataPage parquet-mr_f4913_0(DataPageV1 dataPageV1)
{    try {        BytesInput decompressed = decompressor.decompress(dataPageV1.getBytes(), dataPageV1.getUncompressedSize());        final DataPageV1 decompressedPage;        if (offsetIndex == null) {            decompressedPage = new DataPageV1(decompressed, dataPageV1.getValueCount(), dataPageV1.getUncompressedSize(), dataPageV1.getStatistics(), dataPageV1.getRlEncoding(), dataPageV1.getDlEncoding(), dataPageV1.getValueEncoding());        } else {            long firstRowIndex = offsetIndex.getFirstRowIndex(currentPageIndex);            decompressedPage = new DataPageV1(decompressed, dataPageV1.getValueCount(), dataPageV1.getUncompressedSize(), firstRowIndex, Math.toIntExact(offsetIndex.getLastRowIndex(currentPageIndex, rowCount) - firstRowIndex + 1), dataPageV1.getStatistics(), dataPageV1.getRlEncoding(), dataPageV1.getDlEncoding(), dataPageV1.getValueEncoding());        }        if (dataPageV1.getCrc().isPresent()) {            decompressedPage.setCrc(dataPageV1.getCrc().getAsInt());        }        return decompressedPage;    } catch (IOException e) {        throw new ParquetDecodingException("could not decompress page", e);    }}
public DataPage parquet-mr_f4914_0(DataPageV2 dataPageV2)
{    if (!dataPageV2.isCompressed()) {        if (offsetIndex == null) {            return dataPageV2;        } else {            return DataPageV2.uncompressed(dataPageV2.getRowCount(), dataPageV2.getNullCount(), dataPageV2.getValueCount(), offsetIndex.getFirstRowIndex(currentPageIndex), dataPageV2.getRepetitionLevels(), dataPageV2.getDefinitionLevels(), dataPageV2.getDataEncoding(), dataPageV2.getData(), dataPageV2.getStatistics());        }    }    try {        int uncompressedSize = Math.toIntExact(dataPageV2.getUncompressedSize() - dataPageV2.getDefinitionLevels().size() - dataPageV2.getRepetitionLevels().size());        BytesInput decompressed = decompressor.decompress(dataPageV2.getData(), uncompressedSize);        if (offsetIndex == null) {            return DataPageV2.uncompressed(dataPageV2.getRowCount(), dataPageV2.getNullCount(), dataPageV2.getValueCount(), dataPageV2.getRepetitionLevels(), dataPageV2.getDefinitionLevels(), dataPageV2.getDataEncoding(), decompressed, dataPageV2.getStatistics());        } else {            return DataPageV2.uncompressed(dataPageV2.getRowCount(), dataPageV2.getNullCount(), dataPageV2.getValueCount(), offsetIndex.getFirstRowIndex(currentPageIndex), dataPageV2.getRepetitionLevels(), dataPageV2.getDefinitionLevels(), dataPageV2.getDataEncoding(), decompressed, dataPageV2.getStatistics());        }    } catch (IOException e) {        throw new ParquetDecodingException("could not decompress page", e);    }}
public DictionaryPage parquet-mr_f4915_0()
{    if (compressedDictionaryPage == null) {        return null;    }    try {        DictionaryPage decompressedPage = new DictionaryPage(decompressor.decompress(compressedDictionaryPage.getBytes(), compressedDictionaryPage.getUncompressedSize()), compressedDictionaryPage.getDictionarySize(), compressedDictionaryPage.getEncoding());        if (compressedDictionaryPage.getCrc().isPresent()) {            decompressedPage.setCrc(compressedDictionaryPage.getCrc().getAsInt());        }        return decompressedPage;    } catch (IOException e) {        throw new ParquetDecodingException("Could not decompress dictionary page", e);    }}
public long parquet-mr_f4916_0()
{    return rowCount;}
public PageReader parquet-mr_f4917_0(ColumnDescriptor path)
{    if (!readers.containsKey(path)) {        throw new IllegalArgumentException(path + " is not in the store: " + readers.keySet() + " " + rowCount);    }    return readers.get(path);}
public DictionaryPage parquet-mr_f4918_0(ColumnDescriptor descriptor)
{    return readers.get(descriptor).readDictionaryPage();}
public Optional<PrimitiveIterator.OfLong> parquet-mr_f4919_0()
{    return rowRanges == null ? Optional.empty() : Optional.of(rowRanges.iterator());}
 void parquet-mr_f4920_0(ColumnDescriptor path, ColumnChunkPageReader reader)
{    if (readers.put(path, reader) != null) {        throw new RuntimeException(path + " was added twice");    }}
public void parquet-mr_f4921_0(BytesInput bytesInput, int valueCount, Statistics<?> statistics, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{        columnIndexBuilder = ColumnIndexBuilder.getNoOpBuilder();    offsetIndexBuilder = OffsetIndexBuilder.getNoOpBuilder();    writePage(bytesInput, valueCount, -1, statistics, rlEncoding, dlEncoding, valuesEncoding);}
public void parquet-mr_f4922_0(BytesInput bytes, int valueCount, int rowCount, Statistics statistics, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{    long uncompressedSize = bytes.size();    if (uncompressedSize > Integer.MAX_VALUE) {        throw new ParquetEncodingException("Cannot write page larger than Integer.MAX_VALUE bytes: " + uncompressedSize);    }    BytesInput compressedBytes = compressor.compress(bytes);    long compressedSize = compressedBytes.size();    if (compressedSize > Integer.MAX_VALUE) {        throw new ParquetEncodingException("Cannot write compressed page larger than Integer.MAX_VALUE bytes: " + compressedSize);    }    tempOutputStream.reset();    if (pageWriteChecksumEnabled) {        crc.reset();        crc.update(compressedBytes.toByteArray());        parquetMetadataConverter.writeDataPageV1Header((int) uncompressedSize, (int) compressedSize, valueCount, rlEncoding, dlEncoding, valuesEncoding, (int) crc.getValue(), tempOutputStream);    } else {        parquetMetadataConverter.writeDataPageV1Header((int) uncompressedSize, (int) compressedSize, valueCount, rlEncoding, dlEncoding, valuesEncoding, tempOutputStream);    }    this.uncompressedLength += uncompressedSize;    this.compressedLength += compressedSize;    this.totalValueCount += valueCount;    this.pageCount += 1;        if (totalStatistics == null) {        totalStatistics = statistics.copy();    } else {        totalStatistics.mergeStatistics(statistics);    }    columnIndexBuilder.add(statistics);    offsetIndexBuilder.add(toIntWithCheck(tempOutputStream.size() + compressedSize), rowCount);            buf.collect(BytesInput.concat(BytesInput.from(tempOutputStream), compressedBytes));    rlEncodings.add(rlEncoding);    dlEncodings.add(dlEncoding);    dataEncodings.add(valuesEncoding);}
public void parquet-mr_f4923_0(int rowCount, int nullCount, int valueCount, BytesInput repetitionLevels, BytesInput definitionLevels, Encoding dataEncoding, BytesInput data, Statistics<?> statistics) throws IOException
{    int rlByteLength = toIntWithCheck(repetitionLevels.size());    int dlByteLength = toIntWithCheck(definitionLevels.size());    int uncompressedSize = toIntWithCheck(data.size() + repetitionLevels.size() + definitionLevels.size());        BytesInput compressedData = compressor.compress(data);    int compressedSize = toIntWithCheck(compressedData.size() + repetitionLevels.size() + definitionLevels.size());    tempOutputStream.reset();    parquetMetadataConverter.writeDataPageV2Header(uncompressedSize, compressedSize, valueCount, nullCount, rowCount, dataEncoding, rlByteLength, dlByteLength, tempOutputStream);    this.uncompressedLength += uncompressedSize;    this.compressedLength += compressedSize;    this.totalValueCount += valueCount;    this.pageCount += 1;        if (totalStatistics == null) {        totalStatistics = statistics.copy();    } else {        totalStatistics.mergeStatistics(statistics);    }    columnIndexBuilder.add(statistics);    offsetIndexBuilder.add(toIntWithCheck((long) tempOutputStream.size() + compressedSize), rowCount);            buf.collect(BytesInput.concat(BytesInput.from(tempOutputStream), repetitionLevels, definitionLevels, compressedData));    dataEncodings.add(dataEncoding);}
private int parquet-mr_f4924_0(long size)
{    if (size > Integer.MAX_VALUE) {        throw new ParquetEncodingException("Cannot write page larger than " + Integer.MAX_VALUE + " bytes: " + size);    }    return (int) size;}
public long parquet-mr_f4925_0()
{    return buf.size();}
public void parquet-mr_f4926_1(ParquetFileWriter writer) throws IOException
{    writer.writeColumnChunk(path, totalValueCount, compressor.getCodecName(), dictionaryPage, buf, uncompressedLength, compressedLength, totalStatistics, columnIndexBuilder, offsetIndexBuilder, rlEncodings, dlEncodings, dataEncodings);    if (LOG.isDebugEnabled()) {            }    rlEncodings.clear();    dlEncodings.clear();    dataEncodings.clear();    pageCount = 0;}
public long parquet-mr_f4927_0()
{    return buf.size();}
public void parquet-mr_f4928_0(DictionaryPage dictionaryPage) throws IOException
{    if (this.dictionaryPage != null) {        throw new ParquetEncodingException("Only one dictionary page is allowed");    }    BytesInput dictionaryBytes = dictionaryPage.getBytes();    int uncompressedSize = (int) dictionaryBytes.size();    BytesInput compressedBytes = compressor.compress(dictionaryBytes);    this.dictionaryPage = new DictionaryPage(BytesInput.copy(compressedBytes), uncompressedSize, dictionaryPage.getDictionarySize(), dictionaryPage.getEncoding());}
public String parquet-mr_f4929_0(String prefix)
{    return buf.memUsageString(prefix + " ColumnChunkPageWriter");}
public PageWriter parquet-mr_f4930_0(ColumnDescriptor path)
{    return writers.get(path);}
public void parquet-mr_f4931_0(ParquetFileWriter writer) throws IOException
{    for (ColumnDescriptor path : schema.getColumns()) {        ColumnChunkPageWriter pageWriter = writers.get(path);        pageWriter.writeToFileWriter(writer);    }}
 long parquet-mr_f4932_0()
{    return offset;}
 long parquet-mr_f4933_0()
{    return length;}
private boolean parquet-mr_f4934_0(long offset, int length)
{    if (this.offset + this.length == offset) {        this.length += length;        return true;    } else {        return false;    }}
public int parquet-mr_f4935_0()
{    return indexMap.length;}
public long parquet-mr_f4936_0(int pageIndex)
{    return offsetIndex.getOffset(indexMap[pageIndex]);}
public int parquet-mr_f4937_0(int pageIndex)
{    return offsetIndex.getCompressedPageSize(indexMap[pageIndex]);}
public long parquet-mr_f4938_0(int pageIndex)
{    return offsetIndex.getFirstRowIndex(indexMap[pageIndex]);}
public long parquet-mr_f4939_0(int pageIndex, long totalRowCount)
{    int nextIndex = indexMap[pageIndex] + 1;    return (nextIndex >= offsetIndex.getPageCount() ? totalRowCount : offsetIndex.getFirstRowIndex(nextIndex)) - 1;}
public String parquet-mr_f4940_0()
{    try (Formatter formatter = new Formatter()) {        formatter.format("%-12s  %20s  %16s  %20s\n", "", "offset", "compressed size", "first row index");        for (int i = 0, n = offsetIndex.getPageCount(); i < n; ++i) {            int index = Arrays.binarySearch(indexMap, i);            boolean isHidden = index < 0;            formatter.format("%spage-%-5d  %20d  %16d  %20d\n", isHidden ? "- " : "  ", isHidden ? i : index, offsetIndex.getOffset(i), offsetIndex.getCompressedPageSize(i), offsetIndex.getFirstRowIndex(i));        }        return formatter.toString();    }}
 static OffsetIndex parquet-mr_f4941_0(OffsetIndex offsetIndex, RowRanges rowRanges, long totalRowCount)
{    IntList indexMap = new IntArrayList();    for (int i = 0, n = offsetIndex.getPageCount(); i < n; ++i) {        long from = offsetIndex.getFirstRowIndex(i);        if (rowRanges.isOverlapping(from, offsetIndex.getLastRowIndex(i, totalRowCount))) {            indexMap.add(i);        }    }    return new FilteredOffsetIndex(offsetIndex, indexMap.toIntArray());}
 static List<OffsetRange> parquet-mr_f4942_0(OffsetIndex offsetIndex, ColumnChunkMetaData cm, long firstPageOffset)
{    List<OffsetRange> ranges = new ArrayList<>();    int n = offsetIndex.getPageCount();    if (n > 0) {        OffsetRange currentRange = null;                long rowGroupOffset = cm.getStartingPos();        if (rowGroupOffset < firstPageOffset) {            currentRange = new OffsetRange(rowGroupOffset, (int) (firstPageOffset - rowGroupOffset));            ranges.add(currentRange);        }        for (int i = 0; i < n; ++i) {            long offset = offsetIndex.getOffset(i);            int length = offsetIndex.getCompressedPageSize(i);            if (currentRange == null || !currentRange.extend(offset, length)) {                currentRange = new OffsetRange(offset, length);                ranges.add(currentRange);            }        }    }    return ranges;}
public ColumnIndex parquet-mr_f4943_1()
{    if (!columnIndexRead) {        try {            columnIndex = reader.readColumnIndex(meta);        } catch (IOException e) {                                            }        columnIndexRead = true;    }    return columnIndex;}
public OffsetIndex parquet-mr_f4944_0()
{    return offsetIndex;}
public ColumnIndex parquet-mr_f4945_0()
{    return null;}
public OffsetIndex parquet-mr_f4946_0()
{    return null;}
public ColumnIndex parquet-mr_f4947_0(ColumnPath column)
{    return null;}
public OffsetIndex parquet-mr_f4948_0(ColumnPath column)
{    throw new MissingOffsetIndexException(column);}
 static ColumnIndexStore parquet-mr_f4949_0(ParquetFileReader reader, BlockMetaData block, Set<ColumnPath> paths)
{    try {        return new ColumnIndexStoreImpl(reader, block, paths);    } catch (MissingOffsetIndexException e) {        return EMPTY;    }}
public ColumnIndex parquet-mr_f4950_0(ColumnPath column)
{    return store.getOrDefault(column, MISSING_INDEX_STORE).getColumnIndex();}
public OffsetIndex parquet-mr_f4951_0(ColumnPath column)
{    return store.getOrDefault(column, MISSING_INDEX_STORE).getOffsetIndex();}
 void parquet-mr_f4952_0(ColumnChunkPageReadStore rowGroup)
{    this.rowGroup = rowGroup;}
public DictionaryPage parquet-mr_f4953_0(ColumnDescriptor descriptor)
{    if (rowGroup != null) {                return rowGroup.readDictionaryPage(descriptor);    }    String dotPath = Strings.join(descriptor.getPath(), ".");    ColumnChunkMetaData column = columns.get(dotPath);    if (column == null) {        throw new ParquetDecodingException("Cannot load dictionary, unknown column: " + dotPath);    }    if (cache.containsKey(dotPath)) {        return cache.get(dotPath);    }    try {        synchronized (cache) {                        if (!cache.containsKey(dotPath)) {                DictionaryPage dict = hasDictionaryPage(column) ? reader.readDictionary(column) : null;                                                                cache.put(dotPath, reusableCopy(dict));            }        }        return cache.get(dotPath);    } catch (IOException e) {        throw new ParquetDecodingException("Failed to read dictionary", e);    }}
private static DictionaryPage parquet-mr_f4954_0(DictionaryPage dict)
{    if (dict == null) {        return null;    }    try {        return new DictionaryPage(BytesInput.from(dict.getBytes().toByteArray()), dict.getDictionarySize(), dict.getEncoding());    } catch (IOException e) {        throw new ParquetDecodingException("Cannot read dictionary", e);    }}
private boolean parquet-mr_f4955_0(ColumnChunkMetaData column)
{    EncodingStats stats = column.getEncodingStats();    if (stats != null) {                return stats.hasDictionaryPages() && stats.hasDictionaryEncodedPages();    }    Set<Encoding> encodings = column.getEncodings();    return (encodings.contains(PLAIN_DICTIONARY) || encodings.contains(RLE_DICTIONARY));}
private ByteBuffer parquet-mr_f4956_0(ByteBuffer buffer, int size)
{    if (buffer == null) {        buffer = allocator.allocate(size);    } else if (buffer.capacity() >= size) {        buffer.clear();    } else {        release(buffer);        buffer = allocator.allocate(size);    }    return buffer;}
 ByteBuffer parquet-mr_f4957_0(ByteBuffer buffer)
{    if (buffer != null) {        allocator.release(buffer);    }    return null;}
protected BytesCompressor parquet-mr_f4958_0(final CompressionCodecName codecName)
{    CompressionCodec codec = getCodec(codecName);    if (codec == null) {        return new NoopCompressor();    } else if (codecName == CompressionCodecName.SNAPPY) {                return new SnappyCompressor();    } else {                return new HeapBytesCompressor(codecName);    }}
protected BytesDecompressor parquet-mr_f4959_0(final CompressionCodecName codecName)
{    CompressionCodec codec = getCodec(codecName);    if (codec == null) {        return new NoopDecompressor();    } else if (codecName == CompressionCodecName.SNAPPY) {        return new SnappyDecompressor();    } else if (DirectCodecPool.INSTANCE.codec(codec).supportsDirectDecompression()) {        return new FullDirectDecompressor(codecName);    } else {        return new IndirectDecompressor(codec);    }}
public void parquet-mr_f4960_0()
{    release();}
public BytesInput parquet-mr_f4961_0(BytesInput bytes, int uncompressedSize) throws IOException
{    decompressor.reset();    byte[] inputBytes = bytes.toByteArray();    decompressor.setInput(inputBytes, 0, inputBytes.length);    byte[] output = new byte[uncompressedSize];    decompressor.decompress(output, 0, uncompressedSize);    return BytesInput.from(output);}
public void parquet-mr_f4962_0(ByteBuffer input, int compressedSize, ByteBuffer output, int uncompressedSize) throws IOException
{    decompressor.reset();    byte[] inputBytes = new byte[compressedSize];    input.position(0);    input.get(inputBytes);    decompressor.setInput(inputBytes, 0, inputBytes.length);    byte[] outputBytes = new byte[uncompressedSize];    decompressor.decompress(outputBytes, 0, uncompressedSize);    output.clear();    output.put(outputBytes);}
public void parquet-mr_f4963_0()
{    DirectCodecPool.INSTANCE.returnDecompressor(decompressor);}
public BytesInput parquet-mr_f4964_0(BytesInput compressedBytes, int uncompressedSize) throws IOException
{    return extraDecompressor.decompress(compressedBytes, uncompressedSize);}
public void parquet-mr_f4965_0(ByteBuffer input, int compressedSize, ByteBuffer output, int uncompressedSize) throws IOException
{    output.clear();    try {        DECOMPRESS_METHOD.invoke(decompressor, (ByteBuffer) input.limit(compressedSize), (ByteBuffer) output.limit(uncompressedSize));    } catch (IllegalAccessException e) {        throw new DirectCodecPool.ParquetCompressionCodecException(e);    } catch (InvocationTargetException e) {        throw new DirectCodecPool.ParquetCompressionCodecException(e);    }    output.position(uncompressedSize);}
public void parquet-mr_f4966_0()
{    DirectCodecPool.INSTANCE.returnDirectDecompressor(decompressor);    extraDecompressor.release();}
public void parquet-mr_f4967_0(ByteBuffer input, int compressedSize, ByteBuffer output, int uncompressedSize) throws IOException
{    Preconditions.checkArgument(compressedSize == uncompressedSize, "Non-compressed data did not have matching compressed and uncompressed sizes.");    output.clear();    output.put((ByteBuffer) input.duplicate().position(0).limit(compressedSize));}
public BytesInput parquet-mr_f4968_0(BytesInput bytes, int uncompressedSize) throws IOException
{    return bytes;}
public void parquet-mr_f4969_0()
{}
public BytesInput parquet-mr_f4970_0(BytesInput bytes, int uncompressedSize) throws IOException
{    return extraDecompressor.decompress(bytes, uncompressedSize);}
public void parquet-mr_f4971_0(ByteBuffer src, int compressedSize, ByteBuffer dst, int uncompressedSize) throws IOException
{    dst.clear();    int size = Snappy.uncompress(src, dst);    dst.limit(size);}
public void parquet-mr_f4972_0()
{}
public BytesInput parquet-mr_f4973_0(BytesInput bytes) throws IOException
{    int maxOutputSize = Snappy.maxCompressedLength((int) bytes.size());    ByteBuffer bufferIn = bytes.toByteBuffer();    outgoing = ensure(outgoing, maxOutputSize);    final int size;    if (bufferIn.isDirect()) {        size = Snappy.compress(bufferIn, outgoing);    } else {                this.incoming = ensure(this.incoming, (int) bytes.size());        this.incoming.put(bufferIn);        this.incoming.flip();        size = Snappy.compress(this.incoming, outgoing);    }    outgoing.limit(size);    return BytesInput.from(outgoing);}
public CompressionCodecName parquet-mr_f4974_0()
{    return CompressionCodecName.SNAPPY;}
public void parquet-mr_f4975_0()
{    outgoing = DirectCodecFactory.this.release(outgoing);    incoming = DirectCodecFactory.this.release(incoming);}
public BytesInput parquet-mr_f4976_0(BytesInput bytes) throws IOException
{    return bytes;}
public CompressionCodecName parquet-mr_f4977_0()
{    return CompressionCodecName.UNCOMPRESSED;}
public void parquet-mr_f4978_0()
{}
public Object parquet-mr_f4979_0() throws Exception
{    return codec.createCompressor();}
public Object parquet-mr_f4980_0() throws Exception
{    return codec.createDecompressor();}
public Object parquet-mr_f4981_0() throws Exception
{    return CREATE_DIRECT_DECOMPRESSOR_METHOD.invoke(DIRECT_DECOMPRESSION_CODEC_CLASS);}
public Object parquet-mr_f4982_0()
{    Preconditions.checkArgument(supportDirectDecompressor, "Tried to get a direct Decompressor from a non-direct codec.");    try {        return directDecompressorPool.borrowObject();    } catch (Exception e) {        throw new ParquetCompressionCodecException(e);    }}
public boolean parquet-mr_f4983_0()
{    return supportDirectDecompressor;}
public Decompressor parquet-mr_f4984_0()
{    return borrow(decompressorPool);}
public Compressor parquet-mr_f4985_0()
{    return borrow(compressorPool);}
public CodecPool parquet-mr_f4986_0(CompressionCodec codec)
{    CodecPool pools = codecs.get(codec);    if (pools == null) {        synchronized (this) {            pools = codecs.get(codec);            if (pools == null) {                pools = new CodecPool(codec);                codecs.put(codec, pools);            }        }    }    return pools;}
private void parquet-mr_f4987_0(Object obj, Map<Class<?>, GenericObjectPool> pools)
{    try {        GenericObjectPool pool = pools.get(obj.getClass());        if (pool == null) {            throw new IllegalStateException("Received unexpected compressor or decompressor, " + "cannot be returned to any available pool: " + obj.getClass().getSimpleName());        }        pool.returnObject(obj);    } catch (Exception e) {        throw new ParquetCompressionCodecException(e);    }}
public T parquet-mr_f4988_0(GenericObjectPool pool)
{    try {        return (T) pool.borrowObject();    } catch (Exception e) {        throw new ParquetCompressionCodecException(e);    }}
public void parquet-mr_f4989_0(Compressor compressor)
{    returnToPool(compressor, cPools);}
public void parquet-mr_f4990_0(Decompressor decompressor)
{    returnToPool(decompressor, dePools);}
public void parquet-mr_f4991_0(Object decompressor)
{    returnToPool(decompressor, directDePools);}
public static void parquet-mr_f4992_0(Job job, MessageType schema)
{    GroupWriteSupport.setSchema(schema, ContextUtil.getConfiguration(job));}
public static MessageType parquet-mr_f4993_0(Job job)
{    return GroupWriteSupport.getSchema(ContextUtil.getConfiguration(job));}
public static Builder parquet-mr_f4994_0(Path file)
{    return new Builder(file);}
public static Builder parquet-mr_f4995_0(OutputFile file)
{    return new Builder(file);}
public Builder parquet-mr_f4996_0(MessageType type)
{    this.type = type;    return this;}
public Builder parquet-mr_f4997_0(Map<String, String> extraMetaData)
{    this.extraMetaData = extraMetaData;    return this;}
protected Builder parquet-mr_f4998_0()
{    return this;}
protected WriteSupport<Group> parquet-mr_f4999_0(Configuration conf)
{    return new GroupWriteSupport(type, extraMetaData);}
public org.apache.parquet.hadoop.api.ReadSupport.ReadContext parquet-mr_f5000_0(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema)
{    String partialSchemaString = configuration.get(ReadSupport.PARQUET_READ_SCHEMA);    MessageType requestedProjection = getSchemaForRead(fileSchema, partialSchemaString);    return new ReadContext(requestedProjection);}
public RecordMaterializer<Group> parquet-mr_f5001_0(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema, org.apache.parquet.hadoop.api.ReadSupport.ReadContext readContext)
{    return new GroupRecordConverter(readContext.getRequestedSchema());}
public static void parquet-mr_f5002_0(MessageType schema, Configuration configuration)
{    configuration.set(PARQUET_EXAMPLE_SCHEMA, schema.toString());}
public static MessageType parquet-mr_f5003_0(Configuration configuration)
{    return parseMessageType(checkNotNull(configuration.get(PARQUET_EXAMPLE_SCHEMA), PARQUET_EXAMPLE_SCHEMA));}
public String parquet-mr_f5004_0()
{    return "example";}
public org.apache.parquet.hadoop.api.WriteSupport.WriteContext parquet-mr_f5005_0(Configuration configuration)
{        if (schema == null) {        schema = getSchema(configuration);    }    return new WriteContext(schema, this.extraMetaData);}
public void parquet-mr_f5006_0(RecordConsumer recordConsumer)
{    groupWriter = new GroupWriter(recordConsumer, schema);}
public void parquet-mr_f5007_0(Group record)
{    groupWriter.write(record);}
public Path parquet-mr_f5008_0()
{    return file;}
public ParquetMetadata parquet-mr_f5009_0()
{    return parquetMetadata;}
public String parquet-mr_f5010_0()
{    return "Footer{" + file + ", " + parquetMetadata + "}";}
public void parquet-mr_f5012_0() throws IOException
{    if (reader != null) {        reader.close();    }}
public Void parquet-mr_f5013_0() throws IOException, InterruptedException
{    return null;}
public T parquet-mr_f5014_0() throws IOException, InterruptedException
{    return currentValue;}
public float parquet-mr_f5015_0() throws IOException, InterruptedException
{    return (float) current / total;}
public void parquet-mr_f5016_1(ParquetFileReader reader, ParquetReadOptions options)
{        Configuration conf = new Configuration();    if (options instanceof HadoopReadOptions) {        conf = ((HadoopReadOptions) options).getConf();    }    for (String property : options.getPropertyNames()) {        conf.set(property, options.getProperty(property));    }        this.reader = reader;    FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();    this.fileSchema = parquetFileMetadata.getSchema();    Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();    ReadSupport.ReadContext readContext = readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));    this.columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());    this.requestedSchema = readContext.getRequestedSchema();    this.columnCount = requestedSchema.getPaths().size();    this.recordConverter = readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);    this.strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);    this.total = reader.getFilteredRecordCount();    this.unmaterializableRecordCounter = new UnmaterializableRecordCounter(options, total);    this.filterRecords = options.useRecordFilter();    reader.setRequestedSchema(requestedSchema);    }
public void parquet-mr_f5017_1(ParquetFileReader reader, Configuration configuration) throws IOException
{        this.reader = reader;    FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();    this.fileSchema = parquetFileMetadata.getSchema();    Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();    ReadSupport.ReadContext readContext = readSupport.init(new InitContext(configuration, toSetMultiMap(fileMetadata), fileSchema));    this.columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());    this.requestedSchema = readContext.getRequestedSchema();    this.columnCount = requestedSchema.getPaths().size();    this.recordConverter = readSupport.prepareForRead(configuration, fileMetadata, fileSchema, readContext);    this.strictTypeChecking = configuration.getBoolean(STRICT_TYPE_CHECKING, true);    this.total = reader.getFilteredRecordCount();    this.unmaterializableRecordCounter = new UnmaterializableRecordCounter(configuration, total);    this.filterRecords = configuration.getBoolean(RECORD_FILTERING_ENABLED, true);    reader.setRequestedSchema(requestedSchema);    }
public boolean parquet-mr_f5018_1() throws IOException, InterruptedException
{    boolean recordFound = false;    while (!recordFound) {                if (current >= total) {            return false;        }        try {            checkRead();            current++;            try {                currentValue = recordReader.read();            } catch (RecordMaterializationException e) {                                unmaterializableRecordCounter.incErrors(e);                                continue;            }            if (recordReader.shouldSkipCurrentRecord()) {                                                continue;            }            if (currentValue == null) {                                current = totalCountLoadedSoFar;                                continue;            }            recordFound = true;                    } catch (RuntimeException e) {            throw new ParquetDecodingException(format("Can not read value at %d in block %d in file %s", current, currentBlock, reader.getPath()), e);        }    }    return true;}
private static Map<K, Set<V>> parquet-mr_f5019_0(Map<K, V> map)
{    Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();    for (Map.Entry<K, V> entry : map.entrySet()) {        Set<V> set = new HashSet<V>();        set.add(entry.getValue());        setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));    }    return Collections.unmodifiableMap(setMultiMap);}
public ParquetMetadata parquet-mr_f5020_0()
{    return parquetFileWriter.getFooter();}
private void parquet-mr_f5021_0()
{    pageStore = new ColumnChunkPageWriteStore(compressor, schema, props.getAllocator(), props.getColumnIndexTruncateLength(), props.getPageWriteChecksumEnabled());    columnStore = props.newColumnWriteStore(schema, pageStore);    MessageColumnIO columnIO = new ColumnIOFactory(validating).getColumnIO(schema);    this.recordConsumer = columnIO.getRecordWriter(columnStore);    writeSupport.prepareForWrite(recordConsumer);}
public void parquet-mr_f5022_0() throws IOException, InterruptedException
{    if (!closed) {        flushRowGroupToStore();        FinalizedWriteContext finalWriteContext = writeSupport.finalizeWrite();        Map<String, String> finalMetadata = new HashMap<String, String>(extraMetaData);        String modelName = writeSupport.getName();        if (modelName != null) {            finalMetadata.put(ParquetWriter.OBJECT_MODEL_NAME_PROP, modelName);        }        finalMetadata.putAll(finalWriteContext.getExtraMetaData());        parquetFileWriter.end(finalMetadata);        closed = true;    }}
public void parquet-mr_f5023_0(T value) throws IOException, InterruptedException
{    writeSupport.write(value);    ++recordCount;    checkBlockSizeReached();}
public long parquet-mr_f5024_0()
{    return lastRowGroupEndPos + columnStore.getBufferedSize();}
private void parquet-mr_f5025_1() throws IOException
{    if (recordCount >= recordCountForNextMemCheck) {                long memSize = columnStore.getBufferedSize();        long recordSize = memSize / recordCount;                if (memSize > (nextRowGroupSize - 2 * recordSize)) {                        flushRowGroupToStore();            initStore();            recordCountForNextMemCheck = min(max(MINIMUM_RECORD_COUNT_FOR_CHECK, recordCount / 2), MAXIMUM_RECORD_COUNT_FOR_CHECK);            this.lastRowGroupEndPos = parquetFileWriter.getPos();        } else {            recordCountForNextMemCheck = min(            max(MINIMUM_RECORD_COUNT_FOR_CHECK, (recordCount + (long) (nextRowGroupSize / ((float) recordSize))) / 2),             recordCount + MAXIMUM_RECORD_COUNT_FOR_CHECK);                    }    }}
private void parquet-mr_f5026_1() throws IOException
{    recordConsumer.flush();        if (columnStore.getAllocatedSize() > (3 * rowGroupSizeThreshold)) {            }    if (recordCount > 0) {        parquetFileWriter.startBlock(recordCount);        columnStore.flush();        pageStore.flushToFileWriter(parquetFileWriter);        recordCount = 0;        parquetFileWriter.endBlock();        this.nextRowGroupSize = Math.min(parquetFileWriter.getNextRowGroupSize(), rowGroupSizeThreshold);    }    columnStore = null;    pageStore = null;}
 long parquet-mr_f5027_0()
{    return rowGroupSizeThreshold;}
 void parquet-mr_f5028_0(long rowGroupSizeThreshold)
{    this.rowGroupSizeThreshold = rowGroupSizeThreshold;}
 MessageType parquet-mr_f5029_0()
{    return this.schema;}
public boolean parquet-mr_f5030_1(final Map.Entry<K, V> eldest)
{    boolean result = size() > maxSize;    if (result) {        if (LOG.isDebugEnabled()) {                    }    }    return result;}
public V parquet-mr_f5031_1(final K key)
{    V oldValue = cacheMap.remove(key);    if (oldValue != null) {            }    return oldValue;}
public void parquet-mr_f5032_1(final K key, final V newValue)
{    if (newValue == null || !newValue.isCurrent(key)) {        if (LOG.isWarnEnabled()) {                    }        return;    }    V oldValue = cacheMap.get(key);    if (oldValue != null && oldValue.isNewerThan(newValue)) {        if (LOG.isWarnEnabled()) {                    }        return;    }        oldValue = cacheMap.put(key, newValue);    if (LOG.isDebugEnabled()) {        if (oldValue == null) {                    } else {                    }    }}
public void parquet-mr_f5033_0()
{    cacheMap.clear();}
public V parquet-mr_f5034_1(final K key)
{    V value = cacheMap.get(key);        if (value != null && !value.isCurrent(key)) {                remove(key);        return null;    }    return value;}
public int parquet-mr_f5035_0()
{    return cacheMap.size();}
public void parquet-mr_f5036_0(T object)
{    this.object = object;}
public T parquet-mr_f5037_0()
{    return object;}
public RecordReader<Void, Container<V>> parquet-mr_f5038_0(InputSplit split, JobConf job, Reporter reporter) throws IOException
{    return new RecordReaderWrapper<V>(split, job, reporter);}
public InputSplit[] parquet-mr_f5039_0(JobConf job, int numSplits) throws IOException
{    if (isTaskSideMetaData(job)) {        return super.getSplits(job, numSplits);    }    List<Footer> footers = getFooters(job);    List<ParquetInputSplit> splits = realInputFormat.getSplits(job, footers);    if (splits == null) {        return null;    }    InputSplit[] resultSplits = new InputSplit[splits.size()];    int i = 0;    for (ParquetInputSplit split : splits) {        resultSplits[i++] = new ParquetInputSplitWrapper(split);    }    return resultSplits;}
public List<Footer> parquet-mr_f5040_0(JobConf job) throws IOException
{    return realInputFormat.getFooters(job, asList(super.listStatus(job)));}
public void parquet-mr_f5041_0() throws IOException
{    realReader.close();}
public Void parquet-mr_f5042_0()
{    return null;}
public Container<V> parquet-mr_f5043_0()
{    return valueContainer;}
public long parquet-mr_f5044_0() throws IOException
{    return (long) (splitLen * getProgress());}
public float parquet-mr_f5045_0() throws IOException
{    try {        return realReader.getProgress();    } catch (InterruptedException e) {        Thread.interrupted();        throw new IOException(e);    }}
public boolean parquet-mr_f5046_0(Void key, Container<V> value) throws IOException
{    if (eof) {        return false;    }    if (firstRecord) {                firstRecord = false;        return true;    }    try {        if (realReader.nextKeyValue()) {            if (value != null)                value.set(realReader.getCurrentValue());            return true;        }    } catch (InterruptedException e) {        throw new IOException(e);    }        eof = true;    return false;}
public static boolean parquet-mr_f5047_0(JobConf job)
{    return job.getBoolean(ParquetInputFormat.TASK_SIDE_METADATA, TRUE);}
public long parquet-mr_f5048_0() throws IOException
{    return realSplit.getLength();}
public String[] parquet-mr_f5049_0() throws IOException
{    return realSplit.getLocations();}
public void parquet-mr_f5050_0(DataInput in) throws IOException
{    realSplit = new ParquetInputSplit();    realSplit.readFields(in);}
public void parquet-mr_f5051_0(DataOutput out) throws IOException
{    realSplit.write(out);}
public static void parquet-mr_f5052_0(Configuration configuration, Class<?> writeSupportClass)
{    configuration.set(ParquetOutputFormat.WRITE_SUPPORT_CLASS, writeSupportClass.getName());}
public static void parquet-mr_f5053_0(Configuration configuration, int blockSize)
{    configuration.setInt(ParquetOutputFormat.BLOCK_SIZE, blockSize);}
public static void parquet-mr_f5054_0(Configuration configuration, int pageSize)
{    configuration.setInt(ParquetOutputFormat.PAGE_SIZE, pageSize);}
public static void parquet-mr_f5055_0(Configuration configuration, CompressionCodecName compression)
{    configuration.set(ParquetOutputFormat.COMPRESSION, compression.name());}
public static void parquet-mr_f5056_0(Configuration configuration, boolean enableDictionary)
{    configuration.setBoolean(ParquetOutputFormat.ENABLE_DICTIONARY, enableDictionary);}
public static void parquet-mr_f5057_0(JobConf jobConf)
{    jobConf.setOutputFormat(DeprecatedParquetOutputFormat.class);    jobConf.setOutputCommitter(MapredParquetOutputCommitter.class);}
private CompressionCodecName parquet-mr_f5058_0(final JobConf conf)
{    return CodecConfig.from(conf).getCodec();}
private static Path parquet-mr_f5059_0(JobConf conf, String name, String extension)
{    String file = getUniqueName(conf, name) + extension;    return new Path(getWorkOutputPath(conf), file);}
public RecordWriter<Void, V> parquet-mr_f5060_0(FileSystem fs, JobConf conf, String name, Progressable progress) throws IOException
{    return new RecordWriterWrapper(realOutputFormat, fs, conf, name, progress);}
public void parquet-mr_f5061_0(Reporter reporter) throws IOException
{    try {        realWriter.close(null);    } catch (InterruptedException e) {        Thread.interrupted();        throw new IOException(e);    }}
public void parquet-mr_f5062_0(Void key, V value) throws IOException
{    try {        realWriter.write(key, value);    } catch (InterruptedException e) {        Thread.interrupted();        throw new IOException(e);    }}
public void parquet-mr_f5063_0(JobContext jobContext) throws IOException
{    super.commitJob(jobContext);    Configuration conf = ContextUtil.getConfiguration(jobContext);    Path outputPath = FileOutputFormat.getOutputPath(new JobConf(conf));    ParquetOutputCommitter.writeMetaDataFile(conf, outputPath);}
private void parquet-mr_f5064_0(float ratio)
{    if (ratio <= 0 || ratio > 1) {        throw new IllegalArgumentException("The configured memory pool ratio " + ratio + " is " + "not between 0 and 1.");    }}
 synchronized void parquet-mr_f5065_0(InternalParquetRecordWriter writer, Long allocation)
{    Long oldValue = writerList.get(writer);    if (oldValue == null) {        writerList.put(writer, allocation);    } else {        throw new IllegalArgumentException("[BUG] The Parquet Memory Manager should not add an " + "instance of InternalParquetRecordWriter more than once. The Manager already contains " + "the writer: " + writer);    }    updateAllocation();}
 synchronized void parquet-mr_f5066_0(InternalParquetRecordWriter writer)
{    if (writerList.containsKey(writer)) {        writerList.remove(writer);    }    if (!writerList.isEmpty()) {        updateAllocation();    }}
private void parquet-mr_f5067_1()
{    long totalAllocations = 0;    for (Long allocation : writerList.values()) {        totalAllocations += allocation;    }    if (totalAllocations <= totalMemoryPool) {        scale = 1.0;    } else {        scale = (double) totalMemoryPool / totalAllocations;                for (Runnable callBack : callBacks.values()) {                        callBack.run();        }    }    int maxColCount = 0;    for (InternalParquetRecordWriter w : writerList.keySet()) {        maxColCount = Math.max(w.getSchema().getColumns().size(), maxColCount);    }    for (Map.Entry<InternalParquetRecordWriter, Long> entry : writerList.entrySet()) {        long newSize = (long) Math.floor(entry.getValue() * scale);        if (scale < 1.0 && minMemoryAllocation > 0 && newSize < minMemoryAllocation) {            throw new ParquetRuntimeException(String.format("New Memory allocation %d bytes" + " is smaller than the minimum allocation size of %d bytes.", newSize, minMemoryAllocation)) {            };        }        entry.getKey().setRowGroupSizeThreshold(newSize);            }}
 long parquet-mr_f5068_0()
{    return totalMemoryPool;}
 Map<InternalParquetRecordWriter, Long> parquet-mr_f5069_0()
{    return writerList;}
 float parquet-mr_f5070_0()
{    return memoryPoolRatio;}
public void parquet-mr_f5071_0(String callBackName, Runnable callBack)
{    Preconditions.checkNotNull(callBackName, "callBackName");    Preconditions.checkNotNull(callBack, "callBack");    if (callBacks.containsKey(callBackName)) {        throw new IllegalArgumentException("The callBackName " + callBackName + " is duplicated and has been registered already.");    } else {        callBacks.put(callBackName, callBack);    }}
 Map<String, Runnable> parquet-mr_f5072_0()
{    return Collections.unmodifiableMap(callBacks);}
 double parquet-mr_f5073_0()
{    return scale;}
public void parquet-mr_f5074_0(String path)
{    this.path = path;}
public String parquet-mr_f5075_0()
{    return path;}
public long parquet-mr_f5076_0()
{    return rowCount;}
public void parquet-mr_f5077_0(long rowCount)
{    this.rowCount = rowCount;}
public long parquet-mr_f5078_0()
{    return totalByteSize;}
public void parquet-mr_f5079_0(long totalByteSize)
{    this.totalByteSize = totalByteSize;}
public void parquet-mr_f5080_0(ColumnChunkMetaData column)
{    columns.add(column);}
public List<ColumnChunkMetaData> parquet-mr_f5081_0()
{    return Collections.unmodifiableList(columns);}
public long parquet-mr_f5082_0()
{    return getColumns().get(0).getStartingPos();}
public String parquet-mr_f5083_0()
{    return "BlockMetaData{" + rowCount + ", " + totalByteSize + " " + columns + "}";}
public long parquet-mr_f5084_0()
{    long totalSize = 0;    for (ColumnChunkMetaData col : getColumns()) {        totalSize += col.getTotalSize();    }    return totalSize;}
public static ColumnChunkMetaData parquet-mr_f5085_0(ColumnPath path, PrimitiveTypeName type, CompressionCodecName codec, Set<Encoding> encodings, long firstDataPage, long dictionaryPageOffset, long valueCount, long totalSize, long totalUncompressedSize)
{    return get(path, type, codec, null, encodings, new BooleanStatistics(), firstDataPage, dictionaryPageOffset, valueCount, totalSize, totalUncompressedSize);}
public static ColumnChunkMetaData parquet-mr_f5086_0(ColumnPath path, PrimitiveTypeName type, CompressionCodecName codec, Set<Encoding> encodings, Statistics statistics, long firstDataPage, long dictionaryPageOffset, long valueCount, long totalSize, long totalUncompressedSize)
{    return get(path, type, codec, null, encodings, statistics, firstDataPage, dictionaryPageOffset, valueCount, totalSize, totalUncompressedSize);}
public static ColumnChunkMetaData parquet-mr_f5087_0(ColumnPath path, PrimitiveTypeName type, CompressionCodecName codec, EncodingStats encodingStats, Set<Encoding> encodings, Statistics statistics, long firstDataPage, long dictionaryPageOffset, long valueCount, long totalSize, long totalUncompressedSize)
{    return get(path, Types.optional(type).named("fake_type"), codec, encodingStats, encodings, statistics, firstDataPage, dictionaryPageOffset, valueCount, totalSize, totalUncompressedSize);}
public static ColumnChunkMetaData parquet-mr_f5088_0(ColumnPath path, PrimitiveType type, CompressionCodecName codec, EncodingStats encodingStats, Set<Encoding> encodings, Statistics statistics, long firstDataPage, long dictionaryPageOffset, long valueCount, long totalSize, long totalUncompressedSize)
{        if (positiveLongFitsInAnInt(firstDataPage) && positiveLongFitsInAnInt(dictionaryPageOffset) && positiveLongFitsInAnInt(valueCount) && positiveLongFitsInAnInt(totalSize) && positiveLongFitsInAnInt(totalUncompressedSize)) {        return new IntColumnChunkMetaData(path, type, codec, encodingStats, encodings, statistics, firstDataPage, dictionaryPageOffset, valueCount, totalSize, totalUncompressedSize);    } else {        return new LongColumnChunkMetaData(path, type, codec, encodingStats, encodings, statistics, firstDataPage, dictionaryPageOffset, valueCount, totalSize, totalUncompressedSize);    }}
public long parquet-mr_f5089_0()
{    long dictionaryPageOffset = getDictionaryPageOffset();    long firstDataPageOffset = getFirstDataPageOffset();    if (dictionaryPageOffset > 0 && dictionaryPageOffset < firstDataPageOffset) {                return dictionaryPageOffset;    }    return firstDataPageOffset;}
protected static boolean parquet-mr_f5090_0(long value)
{    return (value >= 0) && (value + Integer.MIN_VALUE <= Integer.MAX_VALUE);}
public CompressionCodecName parquet-mr_f5091_0()
{    return properties.getCodec();}
public ColumnPath parquet-mr_f5092_0()
{    return properties.getPath();}
public PrimitiveTypeName parquet-mr_f5093_0()
{    return properties.getType();}
public PrimitiveType parquet-mr_f5094_0()
{    return properties.getPrimitiveType();}
public IndexReference parquet-mr_f5095_0()
{    return columnIndexReference;}
public void parquet-mr_f5096_0(IndexReference indexReference)
{    this.columnIndexReference = indexReference;}
public IndexReference parquet-mr_f5097_0()
{    return offsetIndexReference;}
public void parquet-mr_f5098_0(IndexReference offsetIndexReference)
{    this.offsetIndexReference = offsetIndexReference;}
public Set<Encoding> parquet-mr_f5099_0()
{    return properties.getEncodings();}
public EncodingStats parquet-mr_f5100_0()
{    return encodingStats;}
public String parquet-mr_f5101_0()
{    return "ColumnMetaData{" + properties.toString() + ", " + getFirstDataPageOffset() + "}";}
private int parquet-mr_f5102_0(long value)
{    if (!ColumnChunkMetaData.positiveLongFitsInAnInt(value)) {        throw new IllegalArgumentException("value should be positive and fit in an int: " + value);    }    return (int) (value + Integer.MIN_VALUE);}
private long parquet-mr_f5103_0(int value)
{    return (long) value - Integer.MIN_VALUE;}
public long parquet-mr_f5104_0()
{    return intToPositiveLong(firstDataPage);}
public long parquet-mr_f5105_0()
{    return intToPositiveLong(dictionaryPageOffset);}
public long parquet-mr_f5106_0()
{    return intToPositiveLong(valueCount);}
public long parquet-mr_f5107_0()
{    return intToPositiveLong(totalUncompressedSize);}
public long parquet-mr_f5108_0()
{    return intToPositiveLong(totalSize);}
public Statistics parquet-mr_f5109_0()
{    return statistics;}
public long parquet-mr_f5110_0()
{    return firstDataPageOffset;}
public long parquet-mr_f5111_0()
{    return dictionaryPageOffset;}
public long parquet-mr_f5112_0()
{    return valueCount;}
public long parquet-mr_f5113_0()
{    return totalUncompressedSize;}
public long parquet-mr_f5114_0()
{    return totalSize;}
public Statistics parquet-mr_f5115_0()
{    return statistics;}
public static ColumnChunkProperties parquet-mr_f5116_0(ColumnPath path, PrimitiveTypeName type, CompressionCodecName codec, Set<Encoding> encodings)
{    return get(path, new PrimitiveType(Type.Repetition.OPTIONAL, type, ""), codec, encodings);}
public static ColumnChunkProperties parquet-mr_f5117_0(ColumnPath path, PrimitiveType type, CompressionCodecName codec, Set<Encoding> encodings)
{    return properties.canonicalize(new ColumnChunkProperties(codec, path, type, encodings));}
public CompressionCodecName parquet-mr_f5118_0()
{    return codec;}
public ColumnPath parquet-mr_f5119_0()
{    return path;}
public PrimitiveTypeName parquet-mr_f5120_0()
{    return type.getPrimitiveTypeName();}
public PrimitiveType parquet-mr_f5121_0()
{    return type;}
public Set<Encoding> parquet-mr_f5122_0()
{    return encodings;}
public boolean parquet-mr_f5123_0(Object obj)
{    if (obj instanceof ColumnChunkProperties) {        ColumnChunkProperties other = (ColumnChunkProperties) obj;        return other.codec == codec && other.path.equals(path) && other.type.equals(type) && equals(other.encodings, encodings);    }    return false;}
private boolean parquet-mr_f5124_0(Set<Encoding> a, Set<Encoding> b)
{    return a.size() == b.size() && a.containsAll(b);}
public int parquet-mr_f5125_0()
{    return codec.hashCode() ^ path.hashCode() ^ type.hashCode() ^ Arrays.hashCode(encodings.toArray());}
public String parquet-mr_f5126_0()
{    return codec + " " + path + " " + type + "  " + encodings;}
public static EncodingList parquet-mr_f5127_0(List<Encoding> encodings)
{    return encodingLists.canonicalize(new EncodingList(encodings));}
public boolean parquet-mr_f5128_0(Object obj)
{    if (obj instanceof EncodingList) {        List<org.apache.parquet.column.Encoding> other = ((EncodingList) obj).encodings;        final int size = other.size();        if (size != encodings.size()) {            return false;        }        for (int i = 0; i < size; i++) {            if (!other.get(i).equals(encodings.get(i))) {                return false;            }        }        return true;    }    return false;}
public int parquet-mr_f5129_0()
{    int result = 1;    for (org.apache.parquet.column.Encoding element : encodings) result = 31 * result + (element == null ? 0 : element.hashCode());    return result;}
public List<Encoding> parquet-mr_f5130_0()
{    return encodings;}
public Iterator<Encoding> parquet-mr_f5131_0()
{    return encodings.iterator();}
public int parquet-mr_f5132_0()
{    return encodings.size();}
public MessageType parquet-mr_f5133_0()
{    return schema;}
public String parquet-mr_f5134_0()
{    return "FileMetaData{schema: " + schema + ", metadata: " + keyValueMetaData + "}";}
public Map<String, String> parquet-mr_f5135_0()
{    return keyValueMetaData;}
public String parquet-mr_f5136_0()
{    return createdBy;}
public MessageType parquet-mr_f5137_0()
{    return schema;}
public String parquet-mr_f5138_0()
{    return "GlobalMetaData{schema: " + schema + ", metadata: " + keyValueMetaData + "}";}
public Map<String, Set<String>> parquet-mr_f5139_0()
{    return keyValueMetaData;}
public Set<String> parquet-mr_f5140_0()
{    return createdBy;}
public FileMetaData parquet-mr_f5141_0()
{    String createdByString = createdBy.size() == 1 ? createdBy.iterator().next() : createdBy.toString();    Map<String, String> mergedKeyValues = new HashMap<String, String>();    for (Entry<String, Set<String>> entry : keyValueMetaData.entrySet()) {        if (entry.getValue().size() > 1) {            throw new RuntimeException("could not merge metadata: key " + entry.getKey() + " has conflicting values: " + entry.getValue());        }        mergedKeyValues.put(entry.getKey(), entry.getValue().iterator().next());    }    return new FileMetaData(schema, mergedKeyValues, createdByString);}
public static String parquet-mr_f5142_0(ParquetMetadata parquetMetaData)
{    return toJSON(parquetMetaData, false);}
public static String parquet-mr_f5143_0(ParquetMetadata parquetMetaData)
{    return toJSON(parquetMetaData, true);}
private static String parquet-mr_f5144_0(ParquetMetadata parquetMetaData, boolean isPrettyPrint)
{    StringWriter stringWriter = new StringWriter();    try {        if (isPrettyPrint) {            objectMapper.writerWithDefaultPrettyPrinter().writeValue(stringWriter, parquetMetaData);        } else {            objectMapper.writeValue(stringWriter, parquetMetaData);        }    } catch (JsonGenerationException e) {        throw new RuntimeException(e);    } catch (JsonMappingException e) {        throw new RuntimeException(e);    } catch (IOException e) {        throw new RuntimeException(e);    }    return stringWriter.toString();}
public static ParquetMetadata parquet-mr_f5145_0(String json)
{    try {        return objectMapper.readValue(new StringReader(json), ParquetMetadata.class);    } catch (JsonParseException e) {        throw new RuntimeException(e);    } catch (JsonMappingException e) {        throw new RuntimeException(e);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public List<BlockMetaData> parquet-mr_f5146_0()
{    return blocks;}
public FileMetaData parquet-mr_f5147_0()
{    return fileMetaData;}
public String parquet-mr_f5148_0()
{    return "ParquetMetaData{" + fileMetaData + ", blocks: " + blocks + "}";}
public static List<Footer> parquet-mr_f5149_0(Configuration configuration, List<FileStatus> partFiles) throws IOException
{    return readAllFootersInParallelUsingSummaryFiles(configuration, partFiles, false);}
private static MetadataFilter parquet-mr_f5150_0(boolean skipRowGroups)
{    return skipRowGroups ? SKIP_ROW_GROUPS : NO_FILTER;}
public static List<Footer> parquet-mr_f5151_1(final Configuration configuration, final Collection<FileStatus> partFiles, final boolean skipRowGroups) throws IOException
{        Set<Path> parents = new HashSet<Path>();    for (FileStatus part : partFiles) {        parents.add(part.getPath().getParent());    }        List<Callable<Map<Path, Footer>>> summaries = new ArrayList<Callable<Map<Path, Footer>>>();    for (final Path path : parents) {        summaries.add(() -> {            ParquetMetadata mergedMetadata = readSummaryMetadata(configuration, path, skipRowGroups);            if (mergedMetadata != null) {                final List<Footer> footers;                if (skipRowGroups) {                    footers = new ArrayList<Footer>();                    for (FileStatus f : partFiles) {                        footers.add(new Footer(f.getPath(), mergedMetadata));                    }                } else {                    footers = footersFromSummaryFile(path, mergedMetadata);                }                Map<Path, Footer> map = new HashMap<Path, Footer>();                for (Footer footer : footers) {                                        footer = new Footer(new Path(path, footer.getFile().getName()), footer.getParquetMetadata());                    map.put(footer.getFile(), footer);                }                return map;            } else {                return Collections.emptyMap();            }        });    }    Map<Path, Footer> cache = new HashMap<Path, Footer>();    try {        List<Map<Path, Footer>> footersFromSummaries = runAllInParallel(configuration.getInt(PARQUET_READ_PARALLELISM, 5), summaries);        for (Map<Path, Footer> footers : footersFromSummaries) {            cache.putAll(footers);        }    } catch (ExecutionException e) {        throw new IOException("Error reading summaries", e);    }        List<Footer> result = new ArrayList<Footer>(partFiles.size());    List<FileStatus> toRead = new ArrayList<FileStatus>();    for (FileStatus part : partFiles) {        Footer f = cache.get(part.getPath());        if (f != null) {            result.add(f);        } else {            toRead.add(part);        }    }    if (toRead.size() > 0) {                        result.addAll(readAllFootersInParallel(configuration, toRead, skipRowGroups));    }    return result;}
private static List<T> parquet-mr_f5152_1(int parallelism, List<Callable<T>> toRun) throws ExecutionException
{        ExecutorService threadPool = Executors.newFixedThreadPool(parallelism);    try {        List<Future<T>> futures = new ArrayList<Future<T>>();        for (Callable<T> callable : toRun) {            futures.add(threadPool.submit(callable));        }        List<T> result = new ArrayList<T>(toRun.size());        for (Future<T> future : futures) {            try {                result.add(future.get());            } catch (InterruptedException e) {                throw new RuntimeException("The thread was interrupted", e);            }        }        return result;    } finally {        threadPool.shutdownNow();    }}
public static List<Footer> parquet-mr_f5153_0(final Configuration configuration, List<FileStatus> partFiles) throws IOException
{    return readAllFootersInParallel(configuration, partFiles, false);}
public static List<Footer> parquet-mr_f5154_0(final Configuration configuration, List<FileStatus> partFiles, final boolean skipRowGroups) throws IOException
{    List<Callable<Footer>> footers = new ArrayList<Callable<Footer>>();    for (final FileStatus currentFile : partFiles) {        footers.add(() -> {            try {                return new Footer(currentFile.getPath(), readFooter(configuration, currentFile, filter(skipRowGroups)));            } catch (IOException e) {                throw new IOException("Could not read footer for file " + currentFile, e);            }        });    }    try {        return runAllInParallel(configuration.getInt(PARQUET_READ_PARALLELISM, 5), footers);    } catch (ExecutionException e) {        throw new IOException("Could not read footer: " + e.getMessage(), e.getCause());    }}
public static List<Footer> parquet-mr_f5155_0(Configuration configuration, FileStatus fileStatus, boolean skipRowGroups) throws IOException
{    List<FileStatus> statuses = listFiles(configuration, fileStatus);    return readAllFootersInParallel(configuration, statuses, skipRowGroups);}
public static List<Footer> parquet-mr_f5156_0(Configuration configuration, FileStatus fileStatus) throws IOException
{    return readAllFootersInParallel(configuration, fileStatus, false);}
public static List<Footer> parquet-mr_f5157_0(Configuration configuration, Path path) throws IOException
{    return readFooters(configuration, status(configuration, path));}
private static FileStatus parquet-mr_f5158_0(Configuration configuration, Path path) throws IOException
{    return path.getFileSystem(configuration).getFileStatus(path);}
public static List<Footer> parquet-mr_f5159_0(Configuration configuration, FileStatus pathStatus) throws IOException
{    return readFooters(configuration, pathStatus, false);}
public static List<Footer> parquet-mr_f5160_0(Configuration configuration, FileStatus pathStatus, boolean skipRowGroups) throws IOException
{    List<FileStatus> files = listFiles(configuration, pathStatus);    return readAllFootersInParallelUsingSummaryFiles(configuration, files, skipRowGroups);}
private static List<FileStatus> parquet-mr_f5161_0(Configuration conf, FileStatus fileStatus) throws IOException
{    if (fileStatus.isDir()) {        FileSystem fs = fileStatus.getPath().getFileSystem(conf);        FileStatus[] list = fs.listStatus(fileStatus.getPath(), HiddenFileFilter.INSTANCE);        List<FileStatus> result = new ArrayList<FileStatus>();        for (FileStatus sub : list) {            result.addAll(listFiles(conf, sub));        }        return result;    } else {        return Arrays.asList(fileStatus);    }}
public static List<Footer> parquet-mr_f5162_0(Configuration configuration, FileStatus summaryStatus) throws IOException
{    final Path parent = summaryStatus.getPath().getParent();    ParquetMetadata mergedFooters = readFooter(configuration, summaryStatus, filter(false));    return footersFromSummaryFile(parent, mergedFooters);}
 static ParquetMetadata parquet-mr_f5163_1(Configuration configuration, Path basePath, boolean skipRowGroups) throws IOException
{    Path metadataFile = new Path(basePath, PARQUET_METADATA_FILE);    Path commonMetaDataFile = new Path(basePath, PARQUET_COMMON_METADATA_FILE);    FileSystem fileSystem = basePath.getFileSystem(configuration);    if (skipRowGroups && fileSystem.exists(commonMetaDataFile)) {                        return readFooter(configuration, commonMetaDataFile, filter(skipRowGroups));    } else if (fileSystem.exists(metadataFile)) {                return readFooter(configuration, metadataFile, filter(skipRowGroups));    } else {        return null;    }}
 static List<Footer> parquet-mr_f5164_0(final Path parent, ParquetMetadata mergedFooters)
{    Map<Path, ParquetMetadata> footers = new HashMap<Path, ParquetMetadata>();    List<BlockMetaData> blocks = mergedFooters.getBlocks();    for (BlockMetaData block : blocks) {        String path = block.getPath();        Path fullPath = new Path(parent, path);        ParquetMetadata current = footers.get(fullPath);        if (current == null) {            current = new ParquetMetadata(mergedFooters.getFileMetaData(), new ArrayList<BlockMetaData>());            footers.put(fullPath, current);        }        current.getBlocks().add(block);    }    List<Footer> result = new ArrayList<Footer>();    for (Entry<Path, ParquetMetadata> entry : footers.entrySet()) {        result.add(new Footer(entry.getKey(), entry.getValue()));    }    return result;}
public static final ParquetMetadata parquet-mr_f5165_0(Configuration configuration, Path file) throws IOException
{    return readFooter(configuration, file, NO_FILTER);}
public static ParquetMetadata parquet-mr_f5166_0(Configuration configuration, Path file, MetadataFilter filter) throws IOException
{    return readFooter(HadoopInputFile.fromPath(file, configuration), filter);}
public static final ParquetMetadata parquet-mr_f5167_0(Configuration configuration, FileStatus file) throws IOException
{    return readFooter(configuration, file, NO_FILTER);}
public static final ParquetMetadata parquet-mr_f5168_0(Configuration configuration, FileStatus file, MetadataFilter filter) throws IOException
{    return readFooter(HadoopInputFile.fromStatus(file, configuration), filter);}
public static final ParquetMetadata parquet-mr_f5169_0(InputFile file, MetadataFilter filter) throws IOException
{    ParquetReadOptions options;    if (file instanceof HadoopInputFile) {        options = HadoopReadOptions.builder(((HadoopInputFile) file).getConfiguration()).withMetadataFilter(filter).build();    } else {        options = ParquetReadOptions.builder().withMetadataFilter(filter).build();    }    try (SeekableInputStream in = file.newStream()) {        return readFooter(file, options, in);    }}
private static final ParquetMetadata parquet-mr_f5170_0(InputFile file, ParquetReadOptions options, SeekableInputStream f) throws IOException
{    ParquetMetadataConverter converter = new ParquetMetadataConverter(options);    return readFooter(file, options, f, converter);}
private static final ParquetMetadata parquet-mr_f5171_1(InputFile file, ParquetReadOptions options, SeekableInputStream f, ParquetMetadataConverter converter) throws IOException
{    long fileLen = file.getLength();    String filePath = file.toString();        int FOOTER_LENGTH_SIZE = 4;    if (fileLen < MAGIC.length + FOOTER_LENGTH_SIZE + MAGIC.length) {                throw new RuntimeException(filePath + " is not a Parquet file (too small length: " + fileLen + ")");    }    long footerLengthIndex = fileLen - FOOTER_LENGTH_SIZE - MAGIC.length;        f.seek(footerLengthIndex);    int footerLength = readIntLittleEndian(f);    byte[] magic = new byte[MAGIC.length];    f.readFully(magic);    if (!Arrays.equals(MAGIC, magic)) {        throw new RuntimeException(filePath + " is not a Parquet file. expected magic number at tail " + Arrays.toString(MAGIC) + " but found " + Arrays.toString(magic));    }    long footerIndex = footerLengthIndex - footerLength;        if (footerIndex < MAGIC.length || footerIndex >= footerLengthIndex) {        throw new RuntimeException("corrupted file: the footer index is not within the file: " + footerIndex);    }    f.seek(footerIndex);            ByteBuffer footerBytesBuffer = ByteBuffer.allocate(footerLength);    f.readFully(footerBytesBuffer);        footerBytesBuffer.flip();    InputStream footerBytesStream = ByteBufferInputStream.wrap(footerBytesBuffer);    return converter.readParquetMetadata(footerBytesStream, options.getMetadataFilter());}
public static ParquetFileReader parquet-mr_f5172_0(Configuration conf, Path file) throws IOException
{    return new ParquetFileReader(HadoopInputFile.fromPath(file, conf), HadoopReadOptions.builder(conf).build());}
public static ParquetFileReader parquet-mr_f5173_0(Configuration conf, Path file, MetadataFilter filter) throws IOException
{    return open(HadoopInputFile.fromPath(file, conf), HadoopReadOptions.builder(conf).withMetadataFilter(filter).build());}
public static ParquetFileReader parquet-mr_f5174_0(Configuration conf, Path file, ParquetMetadata footer) throws IOException
{    return new ParquetFileReader(conf, file, footer);}
public static ParquetFileReader parquet-mr_f5175_0(InputFile file) throws IOException
{    return new ParquetFileReader(file, ParquetReadOptions.builder().build());}
public static ParquetFileReader parquet-mr_f5176_0(InputFile file, ParquetReadOptions options) throws IOException
{    return new ParquetFileReader(file, options);}
private static List<T> parquet-mr_f5177_0(int size)
{    return Stream.generate(() -> (T) null).limit(size).collect(Collectors.toCollection(ArrayList<T>::new));}
public ParquetMetadata parquet-mr_f5178_0()
{    if (footer == null) {        try {                        this.footer = readFooter(file, options, f, converter);        } catch (IOException e) {            throw new ParquetDecodingException("Unable to read file footer", e);        }    }    return footer;}
public FileMetaData parquet-mr_f5179_0()
{    if (fileMetaData != null) {        return fileMetaData;    }    return getFooter().getFileMetaData();}
public long parquet-mr_f5180_0()
{    long total = 0;    for (BlockMetaData block : blocks) {        total += block.getRowCount();    }    return total;}
 long parquet-mr_f5181_0()
{    if (!options.useColumnIndexFilter()) {        return getRecordCount();    }    long total = 0;    for (int i = 0, n = blocks.size(); i < n; ++i) {        total += getRowRanges(i).rowCount();    }    return total;}
public Path parquet-mr_f5182_0()
{    return new Path(file.toString());}
public String parquet-mr_f5183_0()
{    return file.toString();}
private List<BlockMetaData> parquet-mr_f5184_0(List<BlockMetaData> blocks) throws IOException
{        List<RowGroupFilter.FilterLevel> levels = new ArrayList<>();    if (options.useStatsFilter()) {        levels.add(STATISTICS);    }    if (options.useDictionaryFilter()) {        levels.add(DICTIONARY);    }    FilterCompat.Filter recordFilter = options.getRecordFilter();    if (recordFilter != null) {        return RowGroupFilter.filterRowGroups(levels, recordFilter, blocks, this);    }    return blocks;}
public List<BlockMetaData> parquet-mr_f5185_0()
{    return blocks;}
public void parquet-mr_f5186_0(MessageType projection)
{    paths.clear();    for (ColumnDescriptor col : projection.getColumns()) {        paths.put(ColumnPath.get(col.getPath()), col);    }}
public void parquet-mr_f5187_0(ParquetFileWriter writer) throws IOException
{    writer.appendRowGroups(f, blocks, true);}
public PageReadStore parquet-mr_f5188_0() throws IOException
{    if (currentBlock == blocks.size()) {        return null;    }    BlockMetaData block = blocks.get(currentBlock);    if (block.getRowCount() == 0) {        throw new RuntimeException("Illegal row group of 0 rows");    }    this.currentRowGroup = new ColumnChunkPageReadStore(block.getRowCount());        List<ConsecutivePartList> allParts = new ArrayList<ConsecutivePartList>();    ConsecutivePartList currentParts = null;    for (ColumnChunkMetaData mc : block.getColumns()) {        ColumnPath pathKey = mc.getPath();        BenchmarkCounter.incrementTotalBytes(mc.getTotalSize());        ColumnDescriptor columnDescriptor = paths.get(pathKey);        if (columnDescriptor != null) {            long startingPos = mc.getStartingPos();                        if (currentParts == null || currentParts.endPos() != startingPos) {                currentParts = new ConsecutivePartList(startingPos);                allParts.add(currentParts);            }            currentParts.addChunk(new ChunkDescriptor(columnDescriptor, mc, startingPos, (int) mc.getTotalSize()));        }    }        ChunkListBuilder builder = new ChunkListBuilder();    for (ConsecutivePartList consecutiveChunks : allParts) {        consecutiveChunks.readAll(f, builder);    }    for (Chunk chunk : builder.build()) {        currentRowGroup.addColumn(chunk.descriptor.col, chunk.readAllPages());    }        if (nextDictionaryReader != null) {        nextDictionaryReader.setRowGroup(currentRowGroup);    }    advanceToNextBlock();    return currentRowGroup;}
public PageReadStore parquet-mr_f5189_0() throws IOException
{    if (currentBlock == blocks.size()) {        return null;    }    if (!options.useColumnIndexFilter()) {        return readNextRowGroup();    }    BlockMetaData block = blocks.get(currentBlock);    if (block.getRowCount() == 0) {        throw new RuntimeException("Illegal row group of 0 rows");    }    ColumnIndexStore ciStore = getColumnIndexStore(currentBlock);    RowRanges rowRanges = getRowRanges(currentBlock);    long rowCount = rowRanges.rowCount();    if (rowCount == 0) {                advanceToNextBlock();        return readNextFilteredRowGroup();    }    if (rowCount == block.getRowCount()) {                return readNextRowGroup();    }    this.currentRowGroup = new ColumnChunkPageReadStore(rowRanges);        ChunkListBuilder builder = new ChunkListBuilder();    List<ConsecutivePartList> allParts = new ArrayList<ConsecutivePartList>();    ConsecutivePartList currentParts = null;    for (ColumnChunkMetaData mc : block.getColumns()) {        ColumnPath pathKey = mc.getPath();        ColumnDescriptor columnDescriptor = paths.get(pathKey);        if (columnDescriptor != null) {            OffsetIndex offsetIndex = ciStore.getOffsetIndex(mc.getPath());            OffsetIndex filteredOffsetIndex = filterOffsetIndex(offsetIndex, rowRanges, block.getRowCount());            for (OffsetRange range : calculateOffsetRanges(filteredOffsetIndex, mc, offsetIndex.getOffset(0))) {                BenchmarkCounter.incrementTotalBytes(range.getLength());                long startingPos = range.getOffset();                                if (currentParts == null || currentParts.endPos() != startingPos) {                    currentParts = new ConsecutivePartList(startingPos);                    allParts.add(currentParts);                }                ChunkDescriptor chunkDescriptor = new ChunkDescriptor(columnDescriptor, mc, startingPos, (int) range.getLength());                currentParts.addChunk(chunkDescriptor);                builder.setOffsetIndex(chunkDescriptor, filteredOffsetIndex);            }        }    }        for (ConsecutivePartList consecutiveChunks : allParts) {        consecutiveChunks.readAll(f, builder);    }    for (Chunk chunk : builder.build()) {        currentRowGroup.addColumn(chunk.descriptor.col, chunk.readAllPages());    }        if (nextDictionaryReader != null) {        nextDictionaryReader.setRowGroup(currentRowGroup);    }    advanceToNextBlock();    return currentRowGroup;}
private ColumnIndexStore parquet-mr_f5190_0(int blockIndex)
{    ColumnIndexStore ciStore = blockIndexStores.get(blockIndex);    if (ciStore == null) {        ciStore = ColumnIndexStoreImpl.create(this, blocks.get(blockIndex), paths.keySet());        blockIndexStores.set(blockIndex, ciStore);    }    return ciStore;}
private RowRanges parquet-mr_f5191_0(int blockIndex)
{    RowRanges rowRanges = blockRowRanges.get(blockIndex);    if (rowRanges == null) {        rowRanges = ColumnIndexFilter.calculateRowRanges(options.getRecordFilter(), getColumnIndexStore(blockIndex), paths.keySet(), blocks.get(blockIndex).getRowCount());        blockRowRanges.set(blockIndex, rowRanges);    }    return rowRanges;}
public boolean parquet-mr_f5192_0()
{    return advanceToNextBlock();}
private boolean parquet-mr_f5193_0()
{    if (currentBlock == blocks.size()) {        return false;    }        ++currentBlock;    this.nextDictionaryReader = null;    return true;}
public DictionaryPageReadStore parquet-mr_f5194_0()
{    if (nextDictionaryReader == null && currentBlock < blocks.size()) {        this.nextDictionaryReader = getDictionaryReader(blocks.get(currentBlock));    }    return nextDictionaryReader;}
public DictionaryPageReader parquet-mr_f5195_0(BlockMetaData block)
{    return new DictionaryPageReader(this, block);}
 DictionaryPage parquet-mr_f5196_0(ColumnChunkMetaData meta) throws IOException
{    if (!meta.getEncodings().contains(Encoding.PLAIN_DICTIONARY) && !meta.getEncodings().contains(Encoding.RLE_DICTIONARY)) {        return null;    }        if (f.getPos() != meta.getStartingPos()) {        f.seek(meta.getStartingPos());    }    PageHeader pageHeader = Util.readPageHeader(f);    if (!pageHeader.isSetDictionary_page_header()) {                return null;    }    DictionaryPage compressedPage = readCompressedDictionary(pageHeader, f);    BytesInputDecompressor decompressor = options.getCodecFactory().getDecompressor(meta.getCodec());    return new DictionaryPage(decompressor.decompress(compressedPage.getBytes(), compressedPage.getUncompressedSize()), compressedPage.getDictionarySize(), compressedPage.getEncoding());}
private DictionaryPage parquet-mr_f5197_0(PageHeader pageHeader, SeekableInputStream fin) throws IOException
{    DictionaryPageHeader dictHeader = pageHeader.getDictionary_page_header();    int uncompressedPageSize = pageHeader.getUncompressed_page_size();    int compressedPageSize = pageHeader.getCompressed_page_size();    byte[] dictPageBytes = new byte[compressedPageSize];    fin.readFully(dictPageBytes);    BytesInput bin = BytesInput.from(dictPageBytes);    return new DictionaryPage(bin, uncompressedPageSize, dictHeader.getNum_values(), converter.getEncoding(dictHeader.getEncoding()));}
public ColumnIndex parquet-mr_f5198_0(ColumnChunkMetaData column) throws IOException
{    IndexReference ref = column.getColumnIndexReference();    if (ref == null) {        return null;    }    f.seek(ref.getOffset());    return ParquetMetadataConverter.fromParquetColumnIndex(column.getPrimitiveType(), Util.readColumnIndex(f));}
public OffsetIndex parquet-mr_f5199_0(ColumnChunkMetaData column) throws IOException
{    IndexReference ref = column.getOffsetIndexReference();    if (ref == null) {        return null;    }    f.seek(ref.getOffset());    return ParquetMetadataConverter.fromParquetOffsetIndex(Util.readOffsetIndex(f));}
public void parquet-mr_f5200_0() throws IOException
{    try {        if (f != null) {            f.close();        }    } finally {        options.getCodecFactory().release();    }}
 void parquet-mr_f5201_0(ChunkDescriptor descriptor, List<ByteBuffer> buffers, SeekableInputStream f)
{    ChunkData data = map.get(descriptor);    if (data == null) {        data = new ChunkData();        map.put(descriptor, data);    }    data.buffers.addAll(buffers);    lastDescriptor = descriptor;    this.f = f;}
 void parquet-mr_f5202_0(ChunkDescriptor descriptor, OffsetIndex offsetIndex)
{    ChunkData data = map.get(descriptor);    if (data == null) {        data = new ChunkData();        map.put(descriptor, data);    }    data.offsetIndex = offsetIndex;}
 List<Chunk> parquet-mr_f5203_0()
{    List<Chunk> chunks = new ArrayList<>();    for (Entry<ChunkDescriptor, ChunkData> entry : map.entrySet()) {        ChunkDescriptor descriptor = entry.getKey();        ChunkData data = entry.getValue();        if (descriptor.equals(lastDescriptor)) {                        chunks.add(new WorkaroundChunk(lastDescriptor, data.buffers, f, data.offsetIndex));        } else {            chunks.add(new Chunk(descriptor, data.buffers, data.offsetIndex));        }    }    return chunks;}
protected PageHeader parquet-mr_f5204_0() throws IOException
{    return Util.readPageHeader(stream);}
private void parquet-mr_f5205_0(int referenceCrc, byte[] bytes, String exceptionMsg)
{    crc.reset();    crc.update(bytes);    if (crc.getValue() != ((long) referenceCrc & 0xffffffffL)) {        throw new ParquetDecodingException(exceptionMsg);    }}
public ColumnChunkPageReader parquet-mr_f5206_1() throws IOException
{    List<DataPage> pagesInChunk = new ArrayList<DataPage>();    DictionaryPage dictionaryPage = null;    PrimitiveType type = getFileMetaData().getSchema().getType(descriptor.col.getPath()).asPrimitiveType();    long valuesCountReadSoFar = 0;    int dataPageCountReadSoFar = 0;    while (hasMorePages(valuesCountReadSoFar, dataPageCountReadSoFar)) {        PageHeader pageHeader = readPageHeader();        int uncompressedPageSize = pageHeader.getUncompressed_page_size();        int compressedPageSize = pageHeader.getCompressed_page_size();        final BytesInput pageBytes;        switch(pageHeader.type) {            case DICTIONARY_PAGE:                                if (dictionaryPage != null) {                    throw new ParquetDecodingException("more than one dictionary page in column " + descriptor.col);                }                pageBytes = this.readAsBytesInput(compressedPageSize);                if (options.usePageChecksumVerification() && pageHeader.isSetCrc()) {                    verifyCrc(pageHeader.getCrc(), pageBytes.toByteArray(), "could not verify dictionary page integrity, CRC checksum verification failed");                }                DictionaryPageHeader dicHeader = pageHeader.getDictionary_page_header();                dictionaryPage = new DictionaryPage(pageBytes, uncompressedPageSize, dicHeader.getNum_values(), converter.getEncoding(dicHeader.getEncoding()));                                if (pageHeader.isSetCrc()) {                    dictionaryPage.setCrc(pageHeader.getCrc());                }                break;            case DATA_PAGE:                DataPageHeader dataHeaderV1 = pageHeader.getData_page_header();                pageBytes = this.readAsBytesInput(compressedPageSize);                if (options.usePageChecksumVerification() && pageHeader.isSetCrc()) {                    verifyCrc(pageHeader.getCrc(), pageBytes.toByteArray(), "could not verify page integrity, CRC checksum verification failed");                }                DataPageV1 dataPageV1 = new DataPageV1(pageBytes, dataHeaderV1.getNum_values(), uncompressedPageSize, converter.fromParquetStatistics(getFileMetaData().getCreatedBy(), dataHeaderV1.getStatistics(), type), converter.getEncoding(dataHeaderV1.getRepetition_level_encoding()), converter.getEncoding(dataHeaderV1.getDefinition_level_encoding()), converter.getEncoding(dataHeaderV1.getEncoding()));                                if (pageHeader.isSetCrc()) {                    dataPageV1.setCrc(pageHeader.getCrc());                }                pagesInChunk.add(dataPageV1);                valuesCountReadSoFar += dataHeaderV1.getNum_values();                ++dataPageCountReadSoFar;                break;            case DATA_PAGE_V2:                DataPageHeaderV2 dataHeaderV2 = pageHeader.getData_page_header_v2();                int dataSize = compressedPageSize - dataHeaderV2.getRepetition_levels_byte_length() - dataHeaderV2.getDefinition_levels_byte_length();                pagesInChunk.add(new DataPageV2(dataHeaderV2.getNum_rows(), dataHeaderV2.getNum_nulls(), dataHeaderV2.getNum_values(), this.readAsBytesInput(dataHeaderV2.getRepetition_levels_byte_length()), this.readAsBytesInput(dataHeaderV2.getDefinition_levels_byte_length()), converter.getEncoding(dataHeaderV2.getEncoding()), this.readAsBytesInput(dataSize), uncompressedPageSize, converter.fromParquetStatistics(getFileMetaData().getCreatedBy(), dataHeaderV2.getStatistics(), type), dataHeaderV2.isIs_compressed()));                valuesCountReadSoFar += dataHeaderV2.getNum_values();                ++dataPageCountReadSoFar;                break;            default:                                stream.skipFully(compressedPageSize);                break;        }    }    if (offsetIndex == null && valuesCountReadSoFar != descriptor.metadata.getValueCount()) {                throw new IOException("Expected " + descriptor.metadata.getValueCount() + " values in column chunk at " + getPath() + " offset " + descriptor.metadata.getFirstDataPageOffset() + " but got " + valuesCountReadSoFar + " values instead over " + pagesInChunk.size() + " pages ending at file offset " + (descriptor.fileOffset + stream.position()));    }    BytesInputDecompressor decompressor = options.getCodecFactory().getDecompressor(descriptor.metadata.getCodec());    return new ColumnChunkPageReader(decompressor, pagesInChunk, dictionaryPage, offsetIndex, blocks.get(currentBlock).getRowCount());}
private boolean parquet-mr_f5207_0(long valuesCountReadSoFar, int dataPageCountReadSoFar)
{    return offsetIndex == null ? valuesCountReadSoFar < descriptor.metadata.getValueCount() : dataPageCountReadSoFar < offsetIndex.getPageCount();}
public BytesInput parquet-mr_f5208_0(int size) throws IOException
{    return BytesInput.from(stream.sliceBuffers(size));}
protected PageHeader parquet-mr_f5209_1() throws IOException
{    PageHeader pageHeader;        stream.mark(8192);    try {        pageHeader = Util.readPageHeader(stream);    } catch (IOException e) {                                                        stream.reset();                        pageHeader = Util.readPageHeader(new SequenceInputStream(stream, f));    }    return pageHeader;}
public BytesInput parquet-mr_f5210_1(int size) throws IOException
{    int available = stream.available();    if (size > available) {                                        int missingBytes = size - available;                List<ByteBuffer> buffers = new ArrayList<>();        buffers.addAll(stream.sliceBuffers(available));        ByteBuffer lastBuffer = ByteBuffer.allocate(missingBytes);        f.readFully(lastBuffer);        buffers.add(lastBuffer);        return BytesInput.from(buffers);    }    return super.readAsBytesInput(size);}
public int parquet-mr_f5211_0()
{    return col.hashCode();}
public boolean parquet-mr_f5212_0(Object obj)
{    if (this == obj) {        return true;    } else if (obj instanceof ChunkDescriptor) {        return col.equals(((ChunkDescriptor) obj).col);    } else {        return false;    }}
public void parquet-mr_f5213_0(ChunkDescriptor descriptor)
{    chunks.add(descriptor);    length += descriptor.size;}
public void parquet-mr_f5214_0(SeekableInputStream f, ChunkListBuilder builder) throws IOException
{    List<Chunk> result = new ArrayList<Chunk>(chunks.size());    f.seek(offset);    int fullAllocations = length / options.getMaxAllocationSize();    int lastAllocationSize = length % options.getMaxAllocationSize();    int numAllocations = fullAllocations + (lastAllocationSize > 0 ? 1 : 0);    List<ByteBuffer> buffers = new ArrayList<>(numAllocations);    for (int i = 0; i < fullAllocations; i += 1) {        buffers.add(options.getAllocator().allocate(options.getMaxAllocationSize()));    }    if (lastAllocationSize > 0) {        buffers.add(options.getAllocator().allocate(lastAllocationSize));    }    for (ByteBuffer buffer : buffers) {        f.readFully(buffer);        buffer.flip();    }        BenchmarkCounter.incrementBytesRead(length);    ByteBufferInputStream stream = ByteBufferInputStream.wrap(buffers);    for (int i = 0; i < chunks.size(); i++) {        ChunkDescriptor descriptor = chunks.get(i);        builder.add(descriptor, stream.sliceBuffers(descriptor.size), f);    }}
public long parquet-mr_f5215_0()
{    return offset + length;}
 STATE parquet-mr_f5216_0() throws IOException
{    return error();}
 STATE parquet-mr_f5217_0() throws IOException
{    return error();}
 STATE parquet-mr_f5218_0() throws IOException
{    return error();}
 STATE parquet-mr_f5219_0() throws IOException
{    return error();}
 STATE parquet-mr_f5220_0() throws IOException
{    return error();}
 STATE parquet-mr_f5221_0() throws IOException
{    return error();}
 STATE parquet-mr_f5222_0() throws IOException
{    return error();}
private final STATE parquet-mr_f5223_0() throws IOException
{    throw new IOException("The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: " + this.name());}
 STATE parquet-mr_f5224_0()
{    return STARTED;}
 STATE parquet-mr_f5225_0()
{    return BLOCK;}
 STATE parquet-mr_f5226_0()
{    return ENDED;}
 STATE parquet-mr_f5227_0()
{    return COLUMN;}
 STATE parquet-mr_f5228_0()
{    return STARTED;}
 STATE parquet-mr_f5229_0()
{    return BLOCK;}
 STATE parquet-mr_f5230_0()
{    return this;}
public void parquet-mr_f5231_1() throws IOException
{    state = state.start();        out.write(MAGIC);}
public void parquet-mr_f5232_1(long recordCount) throws IOException
{    state = state.startBlock();            alignment.alignForRowGroup(out);    currentBlock = new BlockMetaData();    currentRecordCount = recordCount;    currentColumnIndexes = new ArrayList<>();    currentOffsetIndexes = new ArrayList<>();}
public void parquet-mr_f5233_0(ColumnDescriptor descriptor, long valueCount, CompressionCodecName compressionCodecName) throws IOException
{    state = state.startColumn();    encodingStatsBuilder.clear();    currentEncodings = new HashSet<Encoding>();    currentChunkPath = ColumnPath.get(descriptor.getPath());    currentChunkType = descriptor.getPrimitiveType();    currentChunkCodec = compressionCodecName;    currentChunkValueCount = valueCount;    currentChunkFirstDataPage = out.getPos();    compressedLength = 0;    uncompressedLength = 0;        currentStatistics = null;    columnIndexBuilder = ColumnIndexBuilder.getBuilder(currentChunkType, columnIndexTruncateLength);    offsetIndexBuilder = OffsetIndexBuilder.getBuilder();    firstPageOffset = -1;}
public void parquet-mr_f5234_1(DictionaryPage dictionaryPage) throws IOException
{    state = state.write();        currentChunkDictionaryPageOffset = out.getPos();    int uncompressedSize = dictionaryPage.getUncompressedSize();        int compressedPageSize = (int) dictionaryPage.getBytes().size();    if (pageWriteChecksumEnabled) {        crc.reset();        crc.update(dictionaryPage.getBytes().toByteArray());        metadataConverter.writeDictionaryPageHeader(uncompressedSize, compressedPageSize, dictionaryPage.getDictionarySize(), dictionaryPage.getEncoding(), (int) crc.getValue(), out);    } else {        metadataConverter.writeDictionaryPageHeader(uncompressedSize, compressedPageSize, dictionaryPage.getDictionarySize(), dictionaryPage.getEncoding(), out);    }    long headerSize = out.getPos() - currentChunkDictionaryPageOffset;    this.uncompressedLength += uncompressedSize + headerSize;    this.compressedLength += compressedPageSize + headerSize;        dictionaryPage.getBytes().writeAllTo(out);    encodingStatsBuilder.addDictEncoding(dictionaryPage.getEncoding());    currentEncodings.add(dictionaryPage.getEncoding());}
public void parquet-mr_f5235_1(int valueCount, int uncompressedPageSize, BytesInput bytes, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{    state = state.write();        offsetIndexBuilder = OffsetIndexBuilder.getNoOpBuilder();    columnIndexBuilder = ColumnIndexBuilder.getNoOpBuilder();    long beforeHeader = out.getPos();        int compressedPageSize = (int) bytes.size();    metadataConverter.writeDataPageV1Header(uncompressedPageSize, compressedPageSize, valueCount, rlEncoding, dlEncoding, valuesEncoding, out);    long headerSize = out.getPos() - beforeHeader;    this.uncompressedLength += uncompressedPageSize + headerSize;    this.compressedLength += compressedPageSize + headerSize;        bytes.writeAllTo(out);    encodingStatsBuilder.addDataEncoding(valuesEncoding);    currentEncodings.add(rlEncoding);    currentEncodings.add(dlEncoding);    currentEncodings.add(valuesEncoding);}
public void parquet-mr_f5236_0(int valueCount, int uncompressedPageSize, BytesInput bytes, Statistics statistics, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{        offsetIndexBuilder = OffsetIndexBuilder.getNoOpBuilder();    columnIndexBuilder = ColumnIndexBuilder.getNoOpBuilder();    innerWriteDataPage(valueCount, uncompressedPageSize, bytes, statistics, rlEncoding, dlEncoding, valuesEncoding);}
public void parquet-mr_f5237_0(int valueCount, int uncompressedPageSize, BytesInput bytes, Statistics statistics, long rowCount, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{    long beforeHeader = out.getPos();    innerWriteDataPage(valueCount, uncompressedPageSize, bytes, statistics, rlEncoding, dlEncoding, valuesEncoding);    offsetIndexBuilder.add((int) (out.getPos() - beforeHeader), rowCount);}
private void parquet-mr_f5238_1(int valueCount, int uncompressedPageSize, BytesInput bytes, Statistics statistics, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{    state = state.write();    long beforeHeader = out.getPos();    if (firstPageOffset == -1) {        firstPageOffset = beforeHeader;    }        int compressedPageSize = (int) bytes.size();    if (pageWriteChecksumEnabled) {        crc.reset();        crc.update(bytes.toByteArray());        metadataConverter.writeDataPageV1Header(uncompressedPageSize, compressedPageSize, valueCount, rlEncoding, dlEncoding, valuesEncoding, (int) crc.getValue(), out);    } else {        metadataConverter.writeDataPageV1Header(uncompressedPageSize, compressedPageSize, valueCount, rlEncoding, dlEncoding, valuesEncoding, out);    }    long headerSize = out.getPos() - beforeHeader;    this.uncompressedLength += uncompressedPageSize + headerSize;    this.compressedLength += compressedPageSize + headerSize;        bytes.writeAllTo(out);        if (currentStatistics == null) {        currentStatistics = statistics.copy();    } else {        currentStatistics.mergeStatistics(statistics);    }    columnIndexBuilder.add(statistics);    encodingStatsBuilder.addDataEncoding(valuesEncoding);    currentEncodings.add(rlEncoding);    currentEncodings.add(dlEncoding);    currentEncodings.add(valuesEncoding);}
 void parquet-mr_f5239_1(ColumnDescriptor descriptor, long valueCount, CompressionCodecName compressionCodecName, DictionaryPage dictionaryPage, BytesInput bytes, long uncompressedTotalPageSize, long compressedTotalPageSize, Statistics<?> totalStats, ColumnIndexBuilder columnIndexBuilder, OffsetIndexBuilder offsetIndexBuilder, Set<Encoding> rlEncodings, Set<Encoding> dlEncodings, List<Encoding> dataEncodings) throws IOException
{    startColumn(descriptor, valueCount, compressionCodecName);    state = state.write();    if (dictionaryPage != null) {        writeDictionaryPage(dictionaryPage);    }        long headersSize = bytes.size() - compressedTotalPageSize;    this.uncompressedLength += uncompressedTotalPageSize + headersSize;    this.compressedLength += compressedTotalPageSize + headersSize;        firstPageOffset = out.getPos();    bytes.writeAllTo(out);    encodingStatsBuilder.addDataEncodings(dataEncodings);    if (rlEncodings.isEmpty()) {        encodingStatsBuilder.withV2Pages();    }    currentEncodings.addAll(rlEncodings);    currentEncodings.addAll(dlEncodings);    currentEncodings.addAll(dataEncodings);    currentStatistics = totalStats;    this.columnIndexBuilder = columnIndexBuilder;    this.offsetIndexBuilder = offsetIndexBuilder;    endColumn();}
public void parquet-mr_f5240_1() throws IOException
{    state = state.endColumn();        if (columnIndexBuilder.getMinMaxSize() > columnIndexBuilder.getPageCount() * MAX_STATS_SIZE) {        currentColumnIndexes.add(null);    } else {        currentColumnIndexes.add(columnIndexBuilder.build());    }    currentOffsetIndexes.add(offsetIndexBuilder.build(firstPageOffset));    currentBlock.addColumn(ColumnChunkMetaData.get(currentChunkPath, currentChunkType, currentChunkCodec, encodingStatsBuilder.build(), currentEncodings, currentStatistics, currentChunkFirstDataPage, currentChunkDictionaryPageOffset, currentChunkValueCount, compressedLength, uncompressedLength));    this.currentBlock.setTotalByteSize(currentBlock.getTotalByteSize() + uncompressedLength);    this.uncompressedLength = 0;    this.compressedLength = 0;    columnIndexBuilder = null;    offsetIndexBuilder = null;}
public void parquet-mr_f5241_1() throws IOException
{    state = state.endBlock();        currentBlock.setRowCount(currentRecordCount);    blocks.add(currentBlock);    columnIndexes.add(currentColumnIndexes);    offsetIndexes.add(currentOffsetIndexes);    currentColumnIndexes = null;    currentOffsetIndexes = null;    currentBlock = null;}
public void parquet-mr_f5242_0(Configuration conf, Path file) throws IOException
{    ParquetFileReader.open(conf, file).appendTo(this);}
public void parquet-mr_f5243_0(InputFile file) throws IOException
{    try (ParquetFileReader reader = ParquetFileReader.open(file)) {        reader.appendTo(this);    }}
public void parquet-mr_f5244_0(FSDataInputStream file, List<BlockMetaData> rowGroups, boolean dropColumns) throws IOException
{    appendRowGroups(HadoopStreams.wrap(file), rowGroups, dropColumns);}
public void parquet-mr_f5245_0(SeekableInputStream file, List<BlockMetaData> rowGroups, boolean dropColumns) throws IOException
{    for (BlockMetaData block : rowGroups) {        appendRowGroup(file, block, dropColumns);    }}
public void parquet-mr_f5246_0(FSDataInputStream from, BlockMetaData rowGroup, boolean dropColumns) throws IOException
{    appendRowGroup(HadoopStreams.wrap(from), rowGroup, dropColumns);}
public void parquet-mr_f5247_0(SeekableInputStream from, BlockMetaData rowGroup, boolean dropColumns) throws IOException
{    startBlock(rowGroup.getRowCount());    Map<String, ColumnChunkMetaData> columnsToCopy = new HashMap<String, ColumnChunkMetaData>();    for (ColumnChunkMetaData chunk : rowGroup.getColumns()) {        columnsToCopy.put(chunk.getPath().toDotString(), chunk);    }    List<ColumnChunkMetaData> columnsInOrder = new ArrayList<ColumnChunkMetaData>();    for (ColumnDescriptor descriptor : schema.getColumns()) {        String path = ColumnPath.get(descriptor.getPath()).toDotString();        ColumnChunkMetaData chunk = columnsToCopy.remove(path);        if (chunk != null) {            columnsInOrder.add(chunk);        } else {            throw new IllegalArgumentException(String.format("Missing column '%s', cannot copy row group: %s", path, rowGroup));        }    }        if (!dropColumns && !columnsToCopy.isEmpty()) {        throw new IllegalArgumentException(String.format("Columns cannot be copied (missing from target schema): %s", Strings.join(columnsToCopy.keySet(), ", ")));    }        long start = -1;    long length = 0;    long blockUncompressedSize = 0L;    for (int i = 0; i < columnsInOrder.size(); i += 1) {        ColumnChunkMetaData chunk = columnsInOrder.get(i);                long newChunkStart = out.getPos() + length;                if (start < 0) {                        start = chunk.getStartingPos();        }        length += chunk.getTotalSize();        if ((i + 1) == columnsInOrder.size() || columnsInOrder.get(i + 1).getStartingPos() != (start + length)) {                        copy(from, out, start, length);                        start = -1;            length = 0;        }                        currentColumnIndexes.add(null);        currentOffsetIndexes.add(null);        currentBlock.addColumn(ColumnChunkMetaData.get(chunk.getPath(), chunk.getPrimitiveType(), chunk.getCodec(), chunk.getEncodingStats(), chunk.getEncodings(), chunk.getStatistics(), newChunkStart, newChunkStart, chunk.getValueCount(), chunk.getTotalSize(), chunk.getTotalUncompressedSize()));        blockUncompressedSize += chunk.getTotalUncompressedSize();    }    currentBlock.setTotalByteSize(blockUncompressedSize);    endBlock();}
private static void parquet-mr_f5248_1(SeekableInputStream from, PositionOutputStream to, long start, long length) throws IOException
{        from.seek(start);    long bytesCopied = 0;    byte[] buffer = COPY_BUFFER.get();    while (bytesCopied < length) {        long bytesLeft = length - bytesCopied;        int bytesRead = from.read(buffer, 0, (buffer.length < bytesLeft ? buffer.length : (int) bytesLeft));        if (bytesRead < 0) {            throw new IllegalArgumentException("Unexpected end of input file at " + start + bytesCopied);        }        to.write(buffer, 0, bytesRead);        bytesCopied += bytesRead;    }}
public void parquet-mr_f5249_1(Map<String, String> extraMetaData) throws IOException
{    state = state.end();    serializeColumnIndexes(columnIndexes, blocks, out);    serializeOffsetIndexes(offsetIndexes, blocks, out);        this.footer = new ParquetMetadata(new FileMetaData(schema, extraMetaData, Version.FULL_VERSION), blocks);    serializeFooter(footer, out);    out.close();}
private static void parquet-mr_f5250_1(List<List<ColumnIndex>> columnIndexes, List<BlockMetaData> blocks, PositionOutputStream out) throws IOException
{        for (int bIndex = 0, bSize = blocks.size(); bIndex < bSize; ++bIndex) {        List<ColumnChunkMetaData> columns = blocks.get(bIndex).getColumns();        List<ColumnIndex> blockColumnIndexes = columnIndexes.get(bIndex);        for (int cIndex = 0, cSize = columns.size(); cIndex < cSize; ++cIndex) {            ColumnChunkMetaData column = columns.get(cIndex);            org.apache.parquet.format.ColumnIndex columnIndex = ParquetMetadataConverter.toParquetColumnIndex(column.getPrimitiveType(), blockColumnIndexes.get(cIndex));            if (columnIndex == null) {                continue;            }            long offset = out.getPos();            Util.writeColumnIndex(columnIndex, out);            column.setColumnIndexReference(new IndexReference(offset, (int) (out.getPos() - offset)));        }    }}
private static void parquet-mr_f5251_1(List<List<OffsetIndex>> offsetIndexes, List<BlockMetaData> blocks, PositionOutputStream out) throws IOException
{        for (int bIndex = 0, bSize = blocks.size(); bIndex < bSize; ++bIndex) {        List<ColumnChunkMetaData> columns = blocks.get(bIndex).getColumns();        List<OffsetIndex> blockOffsetIndexes = offsetIndexes.get(bIndex);        for (int cIndex = 0, cSize = columns.size(); cIndex < cSize; ++cIndex) {            OffsetIndex offsetIndex = blockOffsetIndexes.get(cIndex);            if (offsetIndex == null) {                continue;            }            ColumnChunkMetaData column = columns.get(cIndex);            long offset = out.getPos();            Util.writeOffsetIndex(ParquetMetadataConverter.toParquetOffsetIndex(offsetIndex), out);            column.setOffsetIndexReference(new IndexReference(offset, (int) (out.getPos() - offset)));        }    }}
private static void parquet-mr_f5252_1(ParquetMetadata footer, PositionOutputStream out) throws IOException
{    long footerIndex = out.getPos();    org.apache.parquet.format.FileMetaData parquetMetadata = metadataConverter.toParquetMetadata(CURRENT_VERSION, footer);    writeFileMetaData(parquetMetadata, out);        BytesUtils.writeIntLittleEndian(out, (int) (out.getPos() - footerIndex));    out.write(MAGIC);}
public ParquetMetadata parquet-mr_f5253_0()
{    Preconditions.checkState(state == STATE.ENDED, "Cannot return unfinished footer.");    return footer;}
public static ParquetMetadata parquet-mr_f5254_0(List<Path> files, Configuration conf) throws IOException
{    Preconditions.checkArgument(!files.isEmpty(), "Cannot merge an empty list of metadata");    GlobalMetaData globalMetaData = null;    List<BlockMetaData> blocks = new ArrayList<BlockMetaData>();    for (Path p : files) {        ParquetMetadata pmd = ParquetFileReader.readFooter(conf, p, ParquetMetadataConverter.NO_FILTER);        FileMetaData fmd = pmd.getFileMetaData();        globalMetaData = mergeInto(fmd, globalMetaData, true);        blocks.addAll(pmd.getBlocks());    }        return new ParquetMetadata(globalMetaData.merge(), blocks);}
public static void parquet-mr_f5255_0(List<Path> files, Path outputPath, Configuration conf) throws IOException
{    ParquetMetadata merged = mergeMetadataFiles(files, conf);    writeMetadataFile(outputPath, merged, outputPath.getFileSystem(conf));}
public static void parquet-mr_f5256_0(Configuration configuration, Path outputPath, List<Footer> footers) throws IOException
{    writeMetadataFile(configuration, outputPath, footers, JobSummaryLevel.ALL);}
public static void parquet-mr_f5257_0(Configuration configuration, Path outputPath, List<Footer> footers, JobSummaryLevel level) throws IOException
{    Preconditions.checkArgument(level == JobSummaryLevel.ALL || level == JobSummaryLevel.COMMON_ONLY, "Unsupported level: " + level);    FileSystem fs = outputPath.getFileSystem(configuration);    outputPath = outputPath.makeQualified(fs);    ParquetMetadata metadataFooter = mergeFooters(outputPath, footers);    if (level == JobSummaryLevel.ALL) {        writeMetadataFile(outputPath, metadataFooter, fs, PARQUET_METADATA_FILE);    }    metadataFooter.getBlocks().clear();    writeMetadataFile(outputPath, metadataFooter, fs, PARQUET_COMMON_METADATA_FILE);}
private static void parquet-mr_f5258_0(Path outputPathRoot, ParquetMetadata metadataFooter, FileSystem fs, String parquetMetadataFile) throws IOException
{    Path metaDataPath = new Path(outputPathRoot, parquetMetadataFile);    writeMetadataFile(metaDataPath, metadataFooter, fs);}
private static void parquet-mr_f5259_0(Path outputPath, ParquetMetadata metadataFooter, FileSystem fs) throws IOException
{    PositionOutputStream metadata = HadoopStreams.wrap(fs.create(outputPath));    metadata.write(MAGIC);    serializeFooter(metadataFooter, metadata);    metadata.close();}
 static ParquetMetadata parquet-mr_f5260_0(Path root, List<Footer> footers)
{    String rootPath = root.toUri().getPath();    GlobalMetaData fileMetaData = null;    List<BlockMetaData> blocks = new ArrayList<BlockMetaData>();    for (Footer footer : footers) {        String footerPath = footer.getFile().toUri().getPath();        if (!footerPath.startsWith(rootPath)) {            throw new ParquetEncodingException(footerPath + " invalid: all the files must be contained in the root " + root);        }        footerPath = footerPath.substring(rootPath.length());        while (footerPath.startsWith("/")) {            footerPath = footerPath.substring(1);        }        fileMetaData = mergeInto(footer.getParquetMetadata().getFileMetaData(), fileMetaData);        for (BlockMetaData block : footer.getParquetMetadata().getBlocks()) {            block.setPath(footerPath);            blocks.add(block);        }    }    return new ParquetMetadata(fileMetaData.merge(), blocks);}
public long parquet-mr_f5261_0() throws IOException
{    return out.getPos();}
public long parquet-mr_f5262_0() throws IOException
{    return alignment.nextRowGroupSize(out);}
 static GlobalMetaData parquet-mr_f5263_0(List<Footer> footers)
{    return getGlobalMetaData(footers, true);}
 static GlobalMetaData parquet-mr_f5264_0(List<Footer> footers, boolean strict)
{    GlobalMetaData fileMetaData = null;    for (Footer footer : footers) {        ParquetMetadata currentMetadata = footer.getParquetMetadata();        fileMetaData = mergeInto(currentMetadata.getFileMetaData(), fileMetaData, strict);    }    return fileMetaData;}
 static GlobalMetaData parquet-mr_f5265_0(FileMetaData toMerge, GlobalMetaData mergedMetadata)
{    return mergeInto(toMerge, mergedMetadata, true);}
 static GlobalMetaData parquet-mr_f5266_0(FileMetaData toMerge, GlobalMetaData mergedMetadata, boolean strict)
{    MessageType schema = null;    Map<String, Set<String>> newKeyValues = new HashMap<String, Set<String>>();    Set<String> createdBy = new HashSet<String>();    if (mergedMetadata != null) {        schema = mergedMetadata.getSchema();        newKeyValues.putAll(mergedMetadata.getKeyValueMetaData());        createdBy.addAll(mergedMetadata.getCreatedBy());    }    if ((schema == null && toMerge.getSchema() != null) || (schema != null && !schema.equals(toMerge.getSchema()))) {        schema = mergeInto(toMerge.getSchema(), schema, strict);    }    for (Entry<String, String> entry : toMerge.getKeyValueMetaData().entrySet()) {        Set<String> values = newKeyValues.get(entry.getKey());        if (values == null) {            values = new LinkedHashSet<String>();            newKeyValues.put(entry.getKey(), values);        }        values.add(entry.getValue());    }    createdBy.add(toMerge.getCreatedBy());    return new GlobalMetaData(schema, newKeyValues, createdBy);}
 static MessageType parquet-mr_f5267_0(MessageType toMerge, MessageType mergedSchema)
{    return mergeInto(toMerge, mergedSchema, true);}
 static MessageType parquet-mr_f5268_0(MessageType toMerge, MessageType mergedSchema, boolean strict)
{    if (mergedSchema == null) {        return toMerge;    }    return mergedSchema.union(toMerge, strict);}
public static NoAlignment parquet-mr_f5269_0(long rowGroupSize)
{    return new NoAlignment(rowGroupSize);}
public void parquet-mr_f5270_0(PositionOutputStream out)
{}
public long parquet-mr_f5271_0(PositionOutputStream out)
{    return rowGroupSize;}
public static PaddingAlignment parquet-mr_f5272_0(long dfsBlockSize, long rowGroupSize, int maxPaddingSize)
{    return new PaddingAlignment(dfsBlockSize, rowGroupSize, maxPaddingSize);}
public void parquet-mr_f5273_1(PositionOutputStream out) throws IOException
{    long remaining = dfsBlockSize - (out.getPos() % dfsBlockSize);    if (isPaddingNeeded(remaining)) {                for (; remaining > 0; remaining -= zeros.length) {            out.write(zeros, 0, (int) Math.min((long) zeros.length, remaining));        }    }}
public long parquet-mr_f5274_0(PositionOutputStream out) throws IOException
{    if (maxPaddingSize <= 0) {        return rowGroupSize;    }    long remaining = dfsBlockSize - (out.getPos() % dfsBlockSize);    if (isPaddingNeeded(remaining)) {        return rowGroupSize;    }    return Math.min(remaining, rowGroupSize);}
protected boolean parquet-mr_f5275_0(long remaining)
{    return (remaining <= maxPaddingSize);}
public static void parquet-mr_f5276_0(Job job, boolean taskSideMetadata)
{    ContextUtil.getConfiguration(job).setBoolean(TASK_SIDE_METADATA, taskSideMetadata);}
public static boolean parquet-mr_f5277_0(Configuration configuration)
{    return configuration.getBoolean(TASK_SIDE_METADATA, TRUE);}
public static void parquet-mr_f5278_0(Job job, Class<?> readSupportClass)
{    ContextUtil.getConfiguration(job).set(READ_SUPPORT_CLASS, readSupportClass.getName());}
public static void parquet-mr_f5279_0(Job job, Class<? extends UnboundRecordFilter> filterClass)
{    Configuration conf = ContextUtil.getConfiguration(job);    checkArgument(getFilterPredicate(conf) == null, "You cannot provide an UnboundRecordFilter after providing a FilterPredicate");    conf.set(UNBOUND_RECORD_FILTER, filterClass.getName());}
public static Class<?> parquet-mr_f5280_0(Configuration configuration)
{    return ConfigurationUtil.getClassFromConfig(configuration, UNBOUND_RECORD_FILTER, UnboundRecordFilter.class);}
private static UnboundRecordFilter parquet-mr_f5281_0(Configuration configuration)
{    Class<?> clazz = ConfigurationUtil.getClassFromConfig(configuration, UNBOUND_RECORD_FILTER, UnboundRecordFilter.class);    if (clazz == null) {        return null;    }    try {        UnboundRecordFilter unboundRecordFilter = (UnboundRecordFilter) clazz.newInstance();        if (unboundRecordFilter instanceof Configurable) {            ((Configurable) unboundRecordFilter).setConf(configuration);        }        return unboundRecordFilter;    } catch (InstantiationException e) {        throw new BadConfigurationException("could not instantiate unbound record filter class", e);    } catch (IllegalAccessException e) {        throw new BadConfigurationException("could not instantiate unbound record filter class", e);    }}
public static void parquet-mr_f5282_0(JobConf conf, Class<?> readSupportClass)
{    conf.set(READ_SUPPORT_CLASS, readSupportClass.getName());}
public static Class<?> parquet-mr_f5283_0(Configuration configuration)
{    return ConfigurationUtil.getClassFromConfig(configuration, READ_SUPPORT_CLASS, ReadSupport.class);}
public static void parquet-mr_f5284_0(Configuration configuration, FilterPredicate filterPredicate)
{    checkArgument(getUnboundRecordFilter(configuration) == null, "You cannot provide a FilterPredicate after providing an UnboundRecordFilter");    configuration.set(FILTER_PREDICATE + ".human.readable", filterPredicate.toString());    try {        SerializationUtil.writeObjectToConfAsBase64(FILTER_PREDICATE, filterPredicate, configuration);    } catch (IOException e) {        throw new RuntimeException(e);    }}
private static FilterPredicate parquet-mr_f5285_0(Configuration configuration)
{    try {        return SerializationUtil.readObjectFromConfAsBase64(FILTER_PREDICATE, configuration);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public static Filter parquet-mr_f5286_0(Configuration conf)
{    return FilterCompat.get(getFilterPredicate(conf), getUnboundRecordFilterInstance(conf));}
public RecordReader<Void, T> parquet-mr_f5287_0(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException
{    Configuration conf = ContextUtil.getConfiguration(taskAttemptContext);    ReadSupport<T> readSupport = getReadSupport(conf);    return new ParquetRecordReader<T>(readSupport, getFilter(conf));}
 ReadSupport<T> parquet-mr_f5288_0(Configuration configuration)
{    return getReadSupportInstance(readSupportClass == null ? (Class<? extends ReadSupport<T>>) getReadSupportClass(configuration) : readSupportClass);}
public static ReadSupport<T> parquet-mr_f5289_0(Configuration configuration)
{    return getReadSupportInstance((Class<? extends ReadSupport<T>>) getReadSupportClass(configuration));}
 static ReadSupport<T> parquet-mr_f5290_0(Class<? extends ReadSupport<T>> readSupportClass)
{    try {        return readSupportClass.newInstance();    } catch (InstantiationException e) {        throw new BadConfigurationException("could not instantiate read support class", e);    } catch (IllegalAccessException e) {        throw new BadConfigurationException("could not instantiate read support class", e);    }}
protected boolean parquet-mr_f5291_0(JobContext context, Path filename)
{    return ContextUtil.getConfiguration(context).getBoolean(SPLIT_FILES, true);}
public List<InputSplit> parquet-mr_f5292_0(JobContext jobContext) throws IOException
{    Configuration configuration = ContextUtil.getConfiguration(jobContext);    List<InputSplit> splits = new ArrayList<InputSplit>();    if (isTaskSideMetaData(configuration)) {                for (InputSplit split : super.getSplits(jobContext)) {            Preconditions.checkArgument(split instanceof FileSplit, "Cannot wrap non-FileSplit: " + split);            splits.add(ParquetInputSplit.from((FileSplit) split));        }        return splits;    } else {        splits.addAll(getSplits(configuration, getFooters(jobContext)));    }    return splits;}
public List<ParquetInputSplit> parquet-mr_f5293_0(Configuration configuration, List<Footer> footers) throws IOException
{    boolean strictTypeChecking = configuration.getBoolean(STRICT_TYPE_CHECKING, true);    final long maxSplitSize = configuration.getLong("mapred.max.split.size", Long.MAX_VALUE);    final long minSplitSize = Math.max(getFormatMinSplitSize(), configuration.getLong("mapred.min.split.size", 0L));    if (maxSplitSize < 0 || minSplitSize < 0) {        throw new ParquetDecodingException("maxSplitSize or minSplitSize should not be negative: maxSplitSize = " + maxSplitSize + "; minSplitSize = " + minSplitSize);    }    GlobalMetaData globalMetaData = ParquetFileWriter.getGlobalMetaData(footers, strictTypeChecking);    ReadContext readContext = getReadSupport(configuration).init(new InitContext(configuration, globalMetaData.getKeyValueMetaData(), globalMetaData.getSchema()));    return new ClientSideMetadataSplitStrategy().getSplits(configuration, footers, maxSplitSize, minSplitSize, readContext);}
protected List<FileStatus> parquet-mr_f5294_0(JobContext jobContext) throws IOException
{    return getAllFileRecursively(super.listStatus(jobContext), ContextUtil.getConfiguration(jobContext));}
private static List<FileStatus> parquet-mr_f5295_1(List<FileStatus> files, Configuration conf) throws IOException
{    List<FileStatus> result = new ArrayList<FileStatus>();    for (FileStatus file : files) {        if (file.isDir()) {            Path p = file.getPath();            FileSystem fs = p.getFileSystem(conf);            staticAddInputPathRecursively(result, fs, p, HiddenFileFilter.INSTANCE);        } else {            result.add(file);        }    }        return result;}
private static void parquet-mr_f5296_0(List<FileStatus> result, FileSystem fs, Path path, PathFilter inputFilter) throws IOException
{    for (FileStatus stat : fs.listStatus(path, inputFilter)) {        if (stat.isDir()) {            staticAddInputPathRecursively(result, fs, stat.getPath(), inputFilter);        } else {            result.add(stat);        }    }}
public List<Footer> parquet-mr_f5297_1(JobContext jobContext) throws IOException
{    List<FileStatus> statuses = listStatus(jobContext);    if (statuses.isEmpty()) {        return Collections.emptyList();    }    Configuration config = ContextUtil.getConfiguration(jobContext);            Map<FileStatusWrapper, Footer> footersMap = new LinkedHashMap<FileStatusWrapper, Footer>();    Set<FileStatus> missingStatuses = new HashSet<FileStatus>();    Map<Path, FileStatusWrapper> missingStatusesMap = new HashMap<Path, FileStatusWrapper>(missingStatuses.size());    if (footersCache == null) {        footersCache = new LruCache<FileStatusWrapper, FootersCacheValue>(Math.max(statuses.size(), MIN_FOOTER_CACHE_SIZE));    }    for (FileStatus status : statuses) {        FileStatusWrapper statusWrapper = new FileStatusWrapper(status);        FootersCacheValue cacheEntry = footersCache.getCurrentValue(statusWrapper);        if (LOG.isDebugEnabled()) {                    }        if (cacheEntry != null) {            footersMap.put(statusWrapper, cacheEntry.getFooter());        } else {            footersMap.put(statusWrapper, null);            missingStatuses.add(status);            missingStatusesMap.put(status.getPath(), statusWrapper);        }    }        if (!missingStatuses.isEmpty()) {        List<Footer> newFooters = getFooters(config, missingStatuses);        for (Footer newFooter : newFooters) {                                                            FileStatusWrapper fileStatus = missingStatusesMap.get(newFooter.getFile());            footersCache.put(fileStatus, new FootersCacheValue(fileStatus, newFooter));        }    }    List<Footer> footers = new ArrayList<Footer>(statuses.size());    for (Entry<FileStatusWrapper, Footer> footerEntry : footersMap.entrySet()) {        Footer footer = footerEntry.getValue();        if (footer == null) {                        footers.add(footersCache.getCurrentValue(footerEntry.getKey()).getFooter());        } else {            footers.add(footer);        }    }    return footers;}
public List<Footer> parquet-mr_f5298_0(Configuration configuration, List<FileStatus> statuses) throws IOException
{    return getFooters(configuration, (Collection<FileStatus>) statuses);}
public List<Footer> parquet-mr_f5299_1(Configuration configuration, Collection<FileStatus> statuses) throws IOException
{        boolean taskSideMetaData = isTaskSideMetaData(configuration);    return ParquetFileReader.readAllFootersInParallelUsingSummaryFiles(configuration, statuses, taskSideMetaData);}
public GlobalMetaData parquet-mr_f5300_0(JobContext jobContext) throws IOException
{    return ParquetFileWriter.getGlobalMetaData(getFooters(jobContext));}
public boolean parquet-mr_f5301_1(FileStatusWrapper key)
{    long currentModTime = key.getModificationTime();    boolean isCurrent = modificationTime >= currentModTime;    if (LOG.isDebugEnabled() && !isCurrent) {            }    return isCurrent;}
public Footer parquet-mr_f5302_0()
{    return footer;}
public boolean parquet-mr_f5303_0(FootersCacheValue otherValue)
{    return otherValue == null || modificationTime > otherValue.modificationTime;}
public Path parquet-mr_f5304_0()
{    return footer.getFile();}
public long parquet-mr_f5305_0()
{    return status.getModificationTime();}
public int parquet-mr_f5306_0()
{    return status.hashCode();}
public boolean parquet-mr_f5307_0(Object other)
{    return other instanceof FileStatusWrapper && status.equals(((FileStatusWrapper) other).status);}
public String parquet-mr_f5308_0()
{    return status.getPath().toString();}
public int parquet-mr_f5309_0(BlockLocation b1, BlockLocation b2)
{    return Long.signum(b1.getOffset() - b2.getOffset());}
private long parquet-mr_f5310_0(int hdfsBlockIndex)
{    BlockLocation hdfsBlock = hdfsBlocks[hdfsBlockIndex];    return hdfsBlock.getOffset() + hdfsBlock.getLength() - 1;}
private boolean parquet-mr_f5311_0(BlockMetaData rowGroupMetadata)
{    boolean isNewHdfsBlock = false;    long rowGroupMidPoint = rowGroupMetadata.getStartingPos() + (rowGroupMetadata.getCompressedSize() / 2);        while (rowGroupMidPoint > getHDFSBlockEndingPosition(currentMidPointHDFSBlockIndex)) {        isNewHdfsBlock = true;        currentMidPointHDFSBlockIndex++;        if (currentMidPointHDFSBlockIndex >= hdfsBlocks.length)            throw new ParquetDecodingException("the row group is not in hdfs blocks in the file: midpoint of row groups is " + rowGroupMidPoint + ", the end of the hdfs block is " + getHDFSBlockEndingPosition(currentMidPointHDFSBlockIndex - 1));    }    while (rowGroupMetadata.getStartingPos() > getHDFSBlockEndingPosition(currentStartHdfsBlockIndex)) {        currentStartHdfsBlockIndex++;        if (currentStartHdfsBlockIndex >= hdfsBlocks.length)            throw new ParquetDecodingException("The row group does not start in this file: row group offset is " + rowGroupMetadata.getStartingPos() + " but the end of hdfs blocks of file is " + getHDFSBlockEndingPosition(currentStartHdfsBlockIndex));    }    return isNewHdfsBlock;}
public BlockLocation parquet-mr_f5312_0()
{    return hdfsBlocks[currentStartHdfsBlockIndex];}
private void parquet-mr_f5313_0(BlockMetaData rowGroup)
{    this.rowGroups.add(rowGroup);    this.compressedByteSize += rowGroup.getCompressedSize();}
public long parquet-mr_f5314_0()
{    return compressedByteSize;}
public List<BlockMetaData> parquet-mr_f5315_0()
{    return rowGroups;}
 int parquet-mr_f5316_0()
{    return rowGroups.size();}
public ParquetInputSplit parquet-mr_f5317_0(FileStatus fileStatus, String requestedSchema, Map<String, String> readSupportMetadata) throws IOException
{    MessageType requested = MessageTypeParser.parseMessageType(requestedSchema);    long length = 0;    for (BlockMetaData block : this.getRowGroups()) {        List<ColumnChunkMetaData> columns = block.getColumns();        for (ColumnChunkMetaData column : columns) {            if (requested.containsPath(column.getPath().toArray())) {                length += column.getTotalSize();            }        }    }    BlockMetaData lastRowGroup = this.getRowGroups().get(this.getRowGroupCount() - 1);    long end = lastRowGroup.getStartingPos() + lastRowGroup.getTotalByteSize();    long[] rowGroupOffsets = new long[this.getRowGroupCount()];    for (int i = 0; i < rowGroupOffsets.length; i++) {        rowGroupOffsets[i] = this.getRowGroups().get(i).getStartingPos();    }    return new ParquetInputSplit(fileStatus.getPath(), hdfsBlock.getOffset(), end, length, hdfsBlock.getHosts(), rowGroupOffsets);}
 List<ParquetInputSplit> parquet-mr_f5318_1(Configuration configuration, List<Footer> footers, long maxSplitSize, long minSplitSize, ReadContext readContext) throws IOException
{    List<ParquetInputSplit> splits = new ArrayList<ParquetInputSplit>();    Filter filter = ParquetInputFormat.getFilter(configuration);    long rowGroupsDropped = 0;    long totalRowGroups = 0;    for (Footer footer : footers) {        final Path file = footer.getFile();                FileSystem fs = file.getFileSystem(configuration);        FileStatus fileStatus = fs.getFileStatus(file);        ParquetMetadata parquetMetaData = footer.getParquetMetadata();        List<BlockMetaData> blocks = parquetMetaData.getBlocks();        List<BlockMetaData> filteredBlocks;        totalRowGroups += blocks.size();        filteredBlocks = RowGroupFilter.filterRowGroups(filter, blocks, parquetMetaData.getFileMetaData().getSchema());        rowGroupsDropped += blocks.size() - filteredBlocks.size();        if (filteredBlocks.isEmpty()) {            continue;        }        BlockLocation[] fileBlockLocations = fs.getFileBlockLocations(fileStatus, 0, fileStatus.getLen());        splits.addAll(generateSplits(filteredBlocks, fileBlockLocations, fileStatus, readContext.getRequestedSchema().toString(), readContext.getReadSupportMetadata(), minSplitSize, maxSplitSize));    }    if (rowGroupsDropped > 0 && totalRowGroups > 0) {        int percentDropped = (int) ((((double) rowGroupsDropped) / totalRowGroups) * 100);            } else {            }    return splits;}
 static List<ParquetInputSplit> parquet-mr_f5319_0(List<BlockMetaData> rowGroupBlocks, BlockLocation[] hdfsBlocksArray, FileStatus fileStatus, String requestedSchema, Map<String, String> readSupportMetadata, long minSplitSize, long maxSplitSize) throws IOException
{    List<SplitInfo> splitRowGroups = generateSplitInfo(rowGroupBlocks, hdfsBlocksArray, minSplitSize, maxSplitSize);        List<ParquetInputSplit> resultSplits = new ArrayList<ParquetInputSplit>();    for (SplitInfo splitInfo : splitRowGroups) {        ParquetInputSplit split = splitInfo.getParquetInputSplit(fileStatus, requestedSchema, readSupportMetadata);        resultSplits.add(split);    }    return resultSplits;}
 static List<SplitInfo> parquet-mr_f5320_0(List<BlockMetaData> rowGroupBlocks, BlockLocation[] hdfsBlocksArray, long minSplitSize, long maxSplitSize)
{    List<SplitInfo> splitRowGroups;    if (maxSplitSize < minSplitSize || maxSplitSize < 0 || minSplitSize < 0) {        throw new ParquetDecodingException("maxSplitSize and minSplitSize should be positive and max should be greater or equal to the minSplitSize: maxSplitSize = " + maxSplitSize + "; minSplitSize is " + minSplitSize);    }    HDFSBlocks hdfsBlocks = new HDFSBlocks(hdfsBlocksArray);    hdfsBlocks.checkBelongingToANewHDFSBlock(rowGroupBlocks.get(0));    SplitInfo currentSplit = new SplitInfo(hdfsBlocks.getCurrentBlock());        splitRowGroups = new ArrayList<SplitInfo>();        checkSorted(rowGroupBlocks);    for (BlockMetaData rowGroupMetadata : rowGroupBlocks) {        if ((hdfsBlocks.checkBelongingToANewHDFSBlock(rowGroupMetadata) && currentSplit.getCompressedByteSize() >= minSplitSize && currentSplit.getCompressedByteSize() > 0) || currentSplit.getCompressedByteSize() >= maxSplitSize) {                                    splitRowGroups.add(currentSplit);            currentSplit = new SplitInfo(hdfsBlocks.getCurrentBlock());        }        currentSplit.addRowGroup(rowGroupMetadata);    }    if (currentSplit.getRowGroupCount() > 0) {        splitRowGroups.add(currentSplit);    }    return splitRowGroups;}
private static void parquet-mr_f5321_0(List<BlockMetaData> rowGroupBlocks)
{    long previousOffset = 0L;    for (BlockMetaData rowGroup : rowGroupBlocks) {        long currentOffset = rowGroup.getStartingPos();        if (currentOffset < previousOffset) {            throw new ParquetDecodingException("row groups are not sorted: previous row groups starts at " + previousOffset + ", current row group starts at " + currentOffset);        }    }}
private static long parquet-mr_f5322_0(List<BlockMetaData> blocks, String requestedSchema)
{    MessageType requested = MessageTypeParser.parseMessageType(requestedSchema);    long length = 0;    for (BlockMetaData block : blocks) {        List<ColumnChunkMetaData> columns = block.getColumns();        for (ColumnChunkMetaData column : columns) {            if (requested.containsPath(column.getPath().toArray())) {                length += column.getTotalSize();            }        }    }    return length;}
private static long[] parquet-mr_f5323_0(List<BlockMetaData> blocks)
{    long[] offsets = new long[blocks.size()];    for (int i = 0; i < offsets.length; i++) {        offsets[i] = blocks.get(i).getStartingPos();    }    return offsets;}
public List<BlockMetaData> parquet-mr_f5324_0()
{    throw new UnsupportedOperationException("Splits no longer have row group metadata, see PARQUET-234");}
 static ParquetInputSplit parquet-mr_f5325_0(FileSplit split) throws IOException
{    return new ParquetInputSplit(split.getPath(), split.getStart(), split.getStart() + split.getLength(), split.getLength(), split.getLocations(), null);}
 static ParquetInputSplit parquet-mr_f5326_0(org.apache.hadoop.mapred.FileSplit split) throws IOException
{    return new ParquetInputSplit(split.getPath(), split.getStart(), split.getStart() + split.getLength(), split.getLength(), split.getLocations(), null);}
 String parquet-mr_f5327_0()
{    throw new UnsupportedOperationException("Splits no longer have the requested schema, see PARQUET-234");}
public String parquet-mr_f5328_0()
{    throw new UnsupportedOperationException("Splits no longer have the file schema, see PARQUET-234");}
public long parquet-mr_f5329_0()
{    return end;}
public Map<String, String> parquet-mr_f5330_0()
{    throw new UnsupportedOperationException("Splits no longer have file metadata, see PARQUET-234");}
 Map<String, String> parquet-mr_f5331_0()
{    throw new UnsupportedOperationException("Splits no longer have read-support metadata, see PARQUET-234");}
public long[] parquet-mr_f5332_0()
{    return rowGroupOffsets;}
public String parquet-mr_f5333_0()
{    String hosts;    try {        hosts = Arrays.toString(getLocations());    } catch (Exception e) {                hosts = "(" + e + ")";    }    return this.getClass().getSimpleName() + "{" + "part: " + getPath() + " start: " + getStart() + " end: " + getEnd() + " length: " + getLength() + " hosts: " + hosts + (rowGroupOffsets == null ? "" : (" row groups: " + Arrays.toString(rowGroupOffsets))) + "}";}
public void parquet-mr_f5334_0(DataInput hin) throws IOException
{    byte[] bytes = readArray(hin);    DataInputStream in = new DataInputStream(new GZIPInputStream(new ByteArrayInputStream(bytes)));    super.readFields(in);    this.end = in.readLong();    if (in.readBoolean()) {        this.rowGroupOffsets = new long[in.readInt()];        for (int i = 0; i < rowGroupOffsets.length; i++) {            rowGroupOffsets[i] = in.readLong();        }    }    in.close();}
public void parquet-mr_f5335_0(DataOutput hout) throws IOException
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutputStream out = new DataOutputStream(new GZIPOutputStream(baos));    super.write(out);    out.writeLong(end);    out.writeBoolean(rowGroupOffsets != null);    if (rowGroupOffsets != null) {        out.writeInt(rowGroupOffsets.length);        for (long o : rowGroupOffsets) {            out.writeLong(o);        }    }    out.close();    writeArray(hout, baos.toByteArray());}
private static void parquet-mr_f5336_0(DataOutput out, byte[] bytes) throws IOException
{    out.writeInt(bytes.length);    out.write(bytes, 0, bytes.length);}
private static byte[] parquet-mr_f5337_0(DataInput in) throws IOException
{    int len = in.readInt();    byte[] bytes = new byte[len];    in.readFully(bytes);    return bytes;}
public void parquet-mr_f5338_0(JobContext jobContext) throws IOException
{    super.commitJob(jobContext);    Configuration configuration = ContextUtil.getConfiguration(jobContext);    writeMetaDataFile(configuration, outputPath);}
public static void parquet-mr_f5339_1(Configuration configuration, Path outputPath)
{    JobSummaryLevel level = ParquetOutputFormat.getJobSummaryLevel(configuration);    if (level == JobSummaryLevel.NONE) {        return;    }    try {        final FileSystem fileSystem = outputPath.getFileSystem(configuration);        FileStatus outputStatus = fileSystem.getFileStatus(outputPath);        List<Footer> footers;        switch(level) {            case ALL:                                footers = ParquetFileReader.readAllFootersInParallel(configuration, outputStatus, false);                break;            case COMMON_ONLY:                                footers = ParquetFileReader.readAllFootersInParallel(configuration, outputStatus, true);                break;            default:                throw new IllegalArgumentException("Unrecognized job summary level: " + level);        }                if (footers.isEmpty()) {            return;        }        try {            ParquetFileWriter.writeMetadataFile(configuration, outputPath, footers, level);        } catch (Exception e) {                        final Path metadataPath = new Path(outputPath, ParquetFileWriter.PARQUET_METADATA_FILE);            try {                if (fileSystem.exists(metadataPath)) {                    fileSystem.delete(metadataPath, true);                }            } catch (Exception e2) {                            }            try {                final Path commonMetadataPath = new Path(outputPath, ParquetFileWriter.PARQUET_COMMON_METADATA_FILE);                if (fileSystem.exists(commonMetadataPath)) {                    fileSystem.delete(commonMetadataPath, true);                }            } catch (Exception e2) {                            }        }    } catch (Exception e) {            }}
public static JobSummaryLevel parquet-mr_f5340_1(Configuration conf)
{    String level = conf.get(JOB_SUMMARY_LEVEL);    String deprecatedFlag = conf.get(ENABLE_JOB_SUMMARY);    if (deprecatedFlag != null) {            }    if (level != null && deprecatedFlag != null) {            }    if (level != null) {        return JobSummaryLevel.valueOf(level.toUpperCase());    }    if (deprecatedFlag != null) {        return Boolean.valueOf(deprecatedFlag) ? JobSummaryLevel.ALL : JobSummaryLevel.NONE;    }    return JobSummaryLevel.ALL;}
public static void parquet-mr_f5341_0(Job job, Class<?> writeSupportClass)
{    getConfiguration(job).set(WRITE_SUPPORT_CLASS, writeSupportClass.getName());}
public static void parquet-mr_f5342_0(JobConf job, Class<?> writeSupportClass)
{    job.set(WRITE_SUPPORT_CLASS, writeSupportClass.getName());}
public static Class<?> parquet-mr_f5343_0(Configuration configuration)
{    final String className = configuration.get(WRITE_SUPPORT_CLASS);    if (className == null) {        return null;    }    final Class<?> writeSupportClass = ConfigurationUtil.getClassFromConfig(configuration, WRITE_SUPPORT_CLASS, WriteSupport.class);    return writeSupportClass;}
public static void parquet-mr_f5344_0(Job job, int blockSize)
{    getConfiguration(job).setInt(BLOCK_SIZE, blockSize);}
public static void parquet-mr_f5345_0(Job job, int pageSize)
{    getConfiguration(job).setInt(PAGE_SIZE, pageSize);}
public static void parquet-mr_f5346_0(Job job, int pageSize)
{    getConfiguration(job).setInt(DICTIONARY_PAGE_SIZE, pageSize);}
public static void parquet-mr_f5347_0(Job job, CompressionCodecName compression)
{    getConfiguration(job).set(COMPRESSION, compression.name());}
public static void parquet-mr_f5348_0(Job job, boolean enableDictionary)
{    getConfiguration(job).setBoolean(ENABLE_DICTIONARY, enableDictionary);}
public static boolean parquet-mr_f5349_0(JobContext jobContext)
{    return getEnableDictionary(getConfiguration(jobContext));}
public static int parquet-mr_f5350_0(JobContext jobContext)
{    return getBlockSize(getConfiguration(jobContext));}
public static int parquet-mr_f5351_0(JobContext jobContext)
{    return getPageSize(getConfiguration(jobContext));}
public static int parquet-mr_f5352_0(JobContext jobContext)
{    return getDictionaryPageSize(getConfiguration(jobContext));}
public static CompressionCodecName parquet-mr_f5353_0(JobContext jobContext)
{    return getCompression(getConfiguration(jobContext));}
public static boolean parquet-mr_f5354_0(JobContext jobContext)
{    return isCompressionSet(getConfiguration(jobContext));}
public static void parquet-mr_f5355_0(JobContext jobContext, boolean validating)
{    setValidation(getConfiguration(jobContext), validating);}
public static boolean parquet-mr_f5356_0(JobContext jobContext)
{    return getValidation(getConfiguration(jobContext));}
public static boolean parquet-mr_f5357_0(Configuration configuration)
{    return configuration.getBoolean(ENABLE_DICTIONARY, ParquetProperties.DEFAULT_IS_DICTIONARY_ENABLED);}
public static int parquet-mr_f5358_0(Configuration configuration)
{    return configuration.getInt(MIN_ROW_COUNT_FOR_PAGE_SIZE_CHECK, ParquetProperties.DEFAULT_MINIMUM_RECORD_COUNT_FOR_CHECK);}
public static int parquet-mr_f5359_0(Configuration configuration)
{    return configuration.getInt(MAX_ROW_COUNT_FOR_PAGE_SIZE_CHECK, ParquetProperties.DEFAULT_MAXIMUM_RECORD_COUNT_FOR_CHECK);}
public static boolean parquet-mr_f5360_0(Configuration configuration)
{    return configuration.getBoolean(ESTIMATE_PAGE_SIZE_CHECK, ParquetProperties.DEFAULT_ESTIMATE_ROW_COUNT_FOR_PAGE_SIZE_CHECK);}
public static int parquet-mr_f5361_0(Configuration configuration)
{    return configuration.getInt(BLOCK_SIZE, DEFAULT_BLOCK_SIZE);}
public static long parquet-mr_f5362_0(Configuration configuration)
{    return configuration.getLong(BLOCK_SIZE, DEFAULT_BLOCK_SIZE);}
public static int parquet-mr_f5363_0(Configuration configuration)
{    return configuration.getInt(PAGE_SIZE, ParquetProperties.DEFAULT_PAGE_SIZE);}
public static int parquet-mr_f5364_0(Configuration configuration)
{    return configuration.getInt(DICTIONARY_PAGE_SIZE, ParquetProperties.DEFAULT_DICTIONARY_PAGE_SIZE);}
public static WriterVersion parquet-mr_f5365_0(Configuration configuration)
{    String writerVersion = configuration.get(WRITER_VERSION, ParquetProperties.DEFAULT_WRITER_VERSION.toString());    return WriterVersion.fromString(writerVersion);}
public static CompressionCodecName parquet-mr_f5366_0(Configuration configuration)
{    return CodecConfig.getParquetCompressionCodec(configuration);}
public static boolean parquet-mr_f5367_0(Configuration configuration)
{    return CodecConfig.isParquetCompressionSet(configuration);}
public static void parquet-mr_f5368_0(Configuration configuration, boolean validating)
{    configuration.setBoolean(VALIDATION, validating);}
public static boolean parquet-mr_f5369_0(Configuration configuration)
{    return configuration.getBoolean(VALIDATION, false);}
private CompressionCodecName parquet-mr_f5370_0(TaskAttemptContext taskAttemptContext)
{    return CodecConfig.from(taskAttemptContext).getCodec();}
public static void parquet-mr_f5371_0(JobContext jobContext, int maxPaddingSize)
{    setMaxPaddingSize(getConfiguration(jobContext), maxPaddingSize);}
public static void parquet-mr_f5372_0(Configuration conf, int maxPaddingSize)
{    conf.setInt(MAX_PADDING_BYTES, maxPaddingSize);}
private static int parquet-mr_f5373_0(Configuration conf)
{    return conf.getInt(MAX_PADDING_BYTES, ParquetWriter.MAX_PADDING_SIZE_DEFAULT);}
public static void parquet-mr_f5374_0(JobContext jobContext, int length)
{    setColumnIndexTruncateLength(getConfiguration(jobContext), length);}
public static void parquet-mr_f5375_0(Configuration conf, int length)
{    conf.setInt(COLUMN_INDEX_TRUNCATE_LENGTH, length);}
private static int parquet-mr_f5376_0(Configuration conf)
{    return conf.getInt(COLUMN_INDEX_TRUNCATE_LENGTH, ParquetProperties.DEFAULT_COLUMN_INDEX_TRUNCATE_LENGTH);}
public static void parquet-mr_f5377_0(JobContext jobContext, int rowCount)
{    setPageRowCountLimit(getConfiguration(jobContext), rowCount);}
public static void parquet-mr_f5378_0(Configuration conf, int rowCount)
{    conf.setInt(PAGE_ROW_COUNT_LIMIT, rowCount);}
private static int parquet-mr_f5379_0(Configuration conf)
{    return conf.getInt(PAGE_ROW_COUNT_LIMIT, ParquetProperties.DEFAULT_PAGE_ROW_COUNT_LIMIT);}
public static void parquet-mr_f5380_0(JobContext jobContext, boolean val)
{    setPageWriteChecksumEnabled(getConfiguration(jobContext), val);}
public static void parquet-mr_f5381_0(Configuration conf, boolean val)
{    conf.setBoolean(PAGE_WRITE_CHECKSUM_ENABLED, val);}
public static boolean parquet-mr_f5382_0(Configuration conf)
{    return conf.getBoolean(PAGE_WRITE_CHECKSUM_ENABLED, ParquetProperties.DEFAULT_PAGE_WRITE_CHECKSUM_ENABLED);}
public RecordWriter<Void, T> parquet-mr_f5383_0(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException
{    return getRecordWriter(taskAttemptContext, Mode.CREATE);}
public RecordWriter<Void, T> parquet-mr_f5384_0(TaskAttemptContext taskAttemptContext, Mode mode) throws IOException, InterruptedException
{    final Configuration conf = getConfiguration(taskAttemptContext);    CompressionCodecName codec = getCodec(taskAttemptContext);    String extension = codec.getExtension() + ".parquet";    Path file = getDefaultWorkFile(taskAttemptContext, extension);    return getRecordWriter(conf, file, codec, mode);}
public RecordWriter<Void, T> parquet-mr_f5385_0(TaskAttemptContext taskAttemptContext, Path file) throws IOException, InterruptedException
{    return getRecordWriter(taskAttemptContext, file, Mode.CREATE);}
public RecordWriter<Void, T> parquet-mr_f5386_0(TaskAttemptContext taskAttemptContext, Path file, Mode mode) throws IOException, InterruptedException
{    return getRecordWriter(getConfiguration(taskAttemptContext), file, getCodec(taskAttemptContext), mode);}
public RecordWriter<Void, T> parquet-mr_f5387_0(Configuration conf, Path file, CompressionCodecName codec) throws IOException, InterruptedException
{    return getRecordWriter(conf, file, codec, Mode.CREATE);}
public RecordWriter<Void, T> parquet-mr_f5388_1(Configuration conf, Path file, CompressionCodecName codec, Mode mode) throws IOException, InterruptedException
{    final WriteSupport<T> writeSupport = getWriteSupport(conf);    ParquetProperties props = ParquetProperties.builder().withPageSize(getPageSize(conf)).withDictionaryPageSize(getDictionaryPageSize(conf)).withDictionaryEncoding(getEnableDictionary(conf)).withWriterVersion(getWriterVersion(conf)).estimateRowCountForPageSizeCheck(getEstimatePageSizeCheck(conf)).withMinRowCountForPageSizeCheck(getMinRowCountForPageSizeCheck(conf)).withMaxRowCountForPageSizeCheck(getMaxRowCountForPageSizeCheck(conf)).withColumnIndexTruncateLength(getColumnIndexTruncateLength(conf)).withPageRowCountLimit(getPageRowCountLimit(conf)).withPageWriteChecksumEnabled(getPageWriteChecksumEnabled(conf)).build();    long blockSize = getLongBlockSize(conf);    int maxPaddingSize = getMaxPaddingSize(conf);    boolean validating = getValidation(conf);    if (LOG.isInfoEnabled()) {                                                                                                            }    WriteContext init = writeSupport.init(conf);    ParquetFileWriter w = new ParquetFileWriter(HadoopOutputFile.fromPath(file, conf), init.getSchema(), mode, blockSize, maxPaddingSize, props.getColumnIndexTruncateLength(), props.getPageWriteChecksumEnabled());    w.start();    float maxLoad = conf.getFloat(ParquetOutputFormat.MEMORY_POOL_RATIO, MemoryManager.DEFAULT_MEMORY_POOL_RATIO);    long minAllocation = conf.getLong(ParquetOutputFormat.MIN_MEMORY_ALLOCATION, MemoryManager.DEFAULT_MIN_MEMORY_ALLOCATION);    synchronized (ParquetOutputFormat.class) {        if (memoryManager == null) {            memoryManager = new MemoryManager(maxLoad, minAllocation);        }    }    if (memoryManager.getMemoryPoolRatio() != maxLoad) {            }    return new ParquetRecordWriter<T>(w, writeSupport, init.getSchema(), init.getExtraMetaData(), blockSize, codec, validating, props, memoryManager, conf);}
public WriteSupport<T> parquet-mr_f5389_0(Configuration configuration)
{    if (writeSupport != null)        return writeSupport;    Class<?> writeSupportClass = getWriteSupportClass(configuration);    try {        return (WriteSupport<T>) checkNotNull(writeSupportClass, "writeSupportClass").newInstance();    } catch (InstantiationException e) {        throw new BadConfigurationException("could not instantiate write support class: " + writeSupportClass, e);    } catch (IllegalAccessException e) {        throw new BadConfigurationException("could not instantiate write support class: " + writeSupportClass, e);    }}
public OutputCommitter parquet-mr_f5390_0(TaskAttemptContext context) throws IOException
{    if (committer == null) {        Path output = getOutputPath(context);        committer = new ParquetOutputCommitter(output, context);    }    return committer;}
public static synchronized MemoryManager parquet-mr_f5391_0()
{    return memoryManager;}
public T parquet-mr_f5392_0() throws IOException
{    try {        if (reader != null && reader.nextKeyValue()) {            return reader.getCurrentValue();        } else {            initReader();            return reader == null ? null : read();        }    } catch (InterruptedException e) {        throw new IOException(e);    }}
private void parquet-mr_f5393_0() throws IOException
{    if (reader != null) {        reader.close();        reader = null;    }    if (filesIterator.hasNext()) {        InputFile file = filesIterator.next();        ParquetFileReader fileReader = ParquetFileReader.open(file, options);        reader = new InternalParquetRecordReader<>(readSupport, options.getRecordFilter());        reader.initialize(fileReader, options);    }}
public void parquet-mr_f5394_0() throws IOException
{    if (reader != null) {        reader.close();    }}
public static Builder<T> parquet-mr_f5395_0(InputFile file) throws IOException
{    return new Builder<>(file);}
public static Builder<T> parquet-mr_f5396_0(ReadSupport<T> readSupport, Path path)
{    return new Builder<>(readSupport, path);}
public Builder<T> parquet-mr_f5397_0(Configuration conf)
{    this.conf = checkNotNull(conf, "conf");            this.optionsBuilder = HadoopReadOptions.builder(conf);    if (filter != null) {        optionsBuilder.withRecordFilter(filter);    }    return this;}
public Builder<T> parquet-mr_f5398_0(Filter filter)
{    this.filter = filter;    optionsBuilder.withRecordFilter(filter);    return this;}
public Builder<T> parquet-mr_f5399_0(boolean useSignedStringMinMax)
{    optionsBuilder.useSignedStringMinMax(useSignedStringMinMax);    return this;}
public Builder<T> parquet-mr_f5400_0()
{    optionsBuilder.useSignedStringMinMax();    return this;}
public Builder<T> parquet-mr_f5401_0(boolean useStatsFilter)
{    optionsBuilder.useStatsFilter(useStatsFilter);    return this;}
public Builder<T> parquet-mr_f5402_0()
{    optionsBuilder.useStatsFilter();    return this;}
public Builder<T> parquet-mr_f5403_0(boolean useDictionaryFilter)
{    optionsBuilder.useDictionaryFilter(useDictionaryFilter);    return this;}
public Builder<T> parquet-mr_f5404_0()
{    optionsBuilder.useDictionaryFilter();    return this;}
public Builder<T> parquet-mr_f5405_0(boolean useRecordFilter)
{    optionsBuilder.useRecordFilter(useRecordFilter);    return this;}
public Builder<T> parquet-mr_f5406_0()
{    optionsBuilder.useRecordFilter();    return this;}
public Builder<T> parquet-mr_f5407_0(boolean useColumnIndexFilter)
{    optionsBuilder.useColumnIndexFilter(useColumnIndexFilter);    return this;}
public Builder<T> parquet-mr_f5408_0()
{    optionsBuilder.useColumnIndexFilter();    return this;}
public Builder<T> parquet-mr_f5409_0(boolean usePageChecksumVerification)
{    optionsBuilder.usePageChecksumVerification(usePageChecksumVerification);    return this;}
public Builder<T> parquet-mr_f5410_0()
{    optionsBuilder.usePageChecksumVerification();    return this;}
public Builder<T> parquet-mr_f5411_0(long start, long end)
{    optionsBuilder.withRange(start, end);    return this;}
public Builder<T> parquet-mr_f5412_0(CompressionCodecFactory codecFactory)
{    optionsBuilder.withCodecFactory(codecFactory);    return this;}
public Builder<T> parquet-mr_f5413_0(String key, String value)
{    optionsBuilder.set(key, value);    return this;}
protected ReadSupport<T> parquet-mr_f5414_0()
{        Preconditions.checkArgument(readSupport != null, "[BUG] Classes that extend Builder should override getReadSupport()");    return readSupport;}
public ParquetReader<T> parquet-mr_f5415_0() throws IOException
{    ParquetReadOptions options = optionsBuilder.build();    if (path != null) {        FileSystem fs = path.getFileSystem(conf);        FileStatus stat = fs.getFileStatus(path);        if (stat.isFile()) {            return new ParquetReader<>(Collections.singletonList((InputFile) HadoopInputFile.fromStatus(stat, conf)), options, getReadSupport());        } else {            List<InputFile> files = new ArrayList<>();            for (FileStatus fileStatus : fs.listStatus(path, HiddenFileFilter.INSTANCE)) {                files.add(HadoopInputFile.fromStatus(fileStatus, conf));            }            return new ParquetReader<T>(files, options, getReadSupport());        }    } else {        return new ParquetReader<>(Collections.singletonList(file), options, getReadSupport());    }}
public void parquet-mr_f5416_0() throws IOException
{    internalReader.close();}
public Void parquet-mr_f5417_0() throws IOException, InterruptedException
{    return null;}
public T parquet-mr_f5418_0() throws IOException, InterruptedException
{    return internalReader.getCurrentValue();}
public float parquet-mr_f5419_0() throws IOException, InterruptedException
{    return internalReader.getProgress();}
public void parquet-mr_f5420_1(InputSplit inputSplit, TaskAttemptContext context) throws IOException, InterruptedException
{    if (ContextUtil.hasCounterMethod(context)) {        BenchmarkCounter.initCounterFromContext(context);    } else {            }    initializeInternalReader(toParquetSplit(inputSplit), ContextUtil.getConfiguration(context));}
public void parquet-mr_f5421_0(InputSplit inputSplit, Configuration configuration, Reporter reporter) throws IOException, InterruptedException
{    BenchmarkCounter.initCounterFromReporter(reporter, configuration);    initializeInternalReader(toParquetSplit(inputSplit), configuration);}
private void parquet-mr_f5422_0(ParquetInputSplit split, Configuration configuration) throws IOException
{    Path path = split.getPath();    long[] rowGroupOffsets = split.getRowGroupOffsets();        ParquetReadOptions.Builder optionsBuilder = HadoopReadOptions.builder(configuration);    if (rowGroupOffsets != null) {        optionsBuilder.withOffsets(rowGroupOffsets);    } else {        optionsBuilder.withRange(split.getStart(), split.getEnd());    }        ParquetFileReader reader = ParquetFileReader.open(HadoopInputFile.fromPath(path, configuration), optionsBuilder.build());    if (rowGroupOffsets != null) {                List<BlockMetaData> blocks = reader.getFooter().getBlocks();        if (blocks.size() != rowGroupOffsets.length) {            throw new IllegalStateException("All of the offsets in the split should be found in the file." + " expected: " + Arrays.toString(rowGroupOffsets) + " found: " + blocks);        }    }    if (!reader.getRowGroups().isEmpty()) {        checkDeltaByteArrayProblem(reader.getFooter().getFileMetaData(), configuration, reader.getRowGroups().get(0));    }    internalReader.initialize(reader, configuration);}
private void parquet-mr_f5423_0(FileMetaData meta, Configuration conf, BlockMetaData block)
{        if (conf.getBoolean(ParquetInputFormat.SPLIT_FILES, true)) {                Set<Encoding> encodings = new HashSet<Encoding>();        for (ColumnChunkMetaData column : block.getColumns()) {            encodings.addAll(column.getEncodings());        }        for (Encoding encoding : encodings) {            if (CorruptDeltaByteArrays.requiresSequentialReads(meta.getCreatedBy(), encoding)) {                throw new ParquetDecodingException("Cannot read data due to " + "PARQUET-246: to read safely, set " + SPLIT_FILES + " to false");            }        }    }}
public boolean parquet-mr_f5424_0() throws IOException, InterruptedException
{    return internalReader.nextKeyValue();}
private ParquetInputSplit parquet-mr_f5425_0(InputSplit split) throws IOException
{    if (split instanceof ParquetInputSplit) {        return (ParquetInputSplit) split;    } else if (split instanceof FileSplit) {        return ParquetInputSplit.from((FileSplit) split);    } else if (split instanceof org.apache.hadoop.mapred.FileSplit) {        return ParquetInputSplit.from((org.apache.hadoop.mapred.FileSplit) split);    } else {        throw new IllegalArgumentException("Invalid split (not a FileSplit or ParquetInputSplit): " + split);    }}
public void parquet-mr_f5426_0(TaskAttemptContext context) throws IOException, InterruptedException
{    try {        internalWriter.close();        } finally {        if (codecFactory != null) {            codecFactory.release();        }        if (memoryManager != null) {            memoryManager.removeWriter(internalWriter);        }    }}
public void parquet-mr_f5427_0(Void key, T value) throws IOException, InterruptedException
{    internalWriter.write(value);}
public void parquet-mr_f5428_0(T object) throws IOException
{    try {        writer.write(object);    } catch (InterruptedException e) {        throw new IOException(e);    }}
public void parquet-mr_f5429_0() throws IOException
{    try {        writer.close();    } catch (InterruptedException e) {        throw new IOException(e);    } finally {                codecFactory.release();    }}
public ParquetMetadata parquet-mr_f5430_0()
{    return writer.getFooter();}
public long parquet-mr_f5431_0()
{    return writer.getDataSize();}
public SELF parquet-mr_f5432_0(Configuration conf)
{    this.conf = conf;    return self();}
public SELF parquet-mr_f5433_0(ParquetFileWriter.Mode mode)
{    this.mode = mode;    return self();}
public SELF parquet-mr_f5434_0(CompressionCodecName codecName)
{    this.codecName = codecName;    return self();}
public SELF parquet-mr_f5435_0(int rowGroupSize)
{    this.rowGroupSize = rowGroupSize;    return self();}
public SELF parquet-mr_f5436_0(int pageSize)
{    encodingPropsBuilder.withPageSize(pageSize);    return self();}
public SELF parquet-mr_f5437_0(int rowCount)
{    encodingPropsBuilder.withPageRowCountLimit(rowCount);    return self();}
public SELF parquet-mr_f5438_0(int dictionaryPageSize)
{    encodingPropsBuilder.withDictionaryPageSize(dictionaryPageSize);    return self();}
public SELF parquet-mr_f5439_0(int maxPaddingSize)
{    this.maxPaddingSize = maxPaddingSize;    return self();}
public SELF parquet-mr_f5440_0()
{    encodingPropsBuilder.withDictionaryEncoding(true);    return self();}
public SELF parquet-mr_f5441_0(boolean enableDictionary)
{    encodingPropsBuilder.withDictionaryEncoding(enableDictionary);    return self();}
public SELF parquet-mr_f5442_0()
{    this.enableValidation = true;    return self();}
public SELF parquet-mr_f5443_0(boolean enableValidation)
{    this.enableValidation = enableValidation;    return self();}
public SELF parquet-mr_f5444_0(WriterVersion version)
{    encodingPropsBuilder.withWriterVersion(version);    return self();}
public SELF parquet-mr_f5445_0()
{    encodingPropsBuilder.withPageWriteChecksumEnabled(true);    return self();}
public SELF parquet-mr_f5446_0(boolean enablePageWriteChecksum)
{    encodingPropsBuilder.withPageWriteChecksumEnabled(enablePageWriteChecksum);    return self();}
public SELF parquet-mr_f5447_0(String property, String value)
{    conf.set(property, value);    return self();}
public ParquetWriter<T> parquet-mr_f5448_0() throws IOException
{    if (file != null) {        return new ParquetWriter<>(file, mode, getWriteSupport(conf), codecName, rowGroupSize, enableValidation, conf, maxPaddingSize, encodingPropsBuilder.build());    } else {        return new ParquetWriter<>(HadoopOutputFile.fromPath(path, conf), mode, getWriteSupport(conf), codecName, rowGroupSize, enableValidation, conf, maxPaddingSize, encodingPropsBuilder.build());    }}
public static void parquet-mr_f5449_0(String[] args) throws Exception
{    if (args.length != 1) {        System.err.println("usage PrintFooter <path>");        return;    }    Path path = new Path(new URI(args[0]));    final Configuration configuration = new Configuration();    final FileSystem fs = path.getFileSystem(configuration);    FileStatus fileStatus = fs.getFileStatus(path);    Path summary = new Path(fileStatus.getPath(), PARQUET_METADATA_FILE);    if (fileStatus.isDir() && fs.exists(summary)) {        System.out.println("reading summary file");        FileStatus summaryStatus = fs.getFileStatus(summary);        List<Footer> readSummaryFile = ParquetFileReader.readSummaryFile(configuration, summaryStatus);        for (Footer footer : readSummaryFile) {            add(footer.getParquetMetadata());        }    } else {        List<FileStatus> statuses;        if (fileStatus.isDir()) {            System.out.println("listing files in " + fileStatus.getPath());            statuses = Arrays.asList(fs.listStatus(fileStatus.getPath(), HiddenFileFilter.INSTANCE));        } else {            statuses = new ArrayList<FileStatus>();            statuses.add(fileStatus);        }        System.out.println("opening " + statuses.size() + " files");        int i = 0;        ExecutorService threadPool = Executors.newFixedThreadPool(5);        try {            long t0 = System.currentTimeMillis();            Deque<Future<ParquetMetadata>> footers = new LinkedBlockingDeque<Future<ParquetMetadata>>();            for (final FileStatus currentFile : statuses) {                footers.add(threadPool.submit(() -> {                    try {                        return ParquetFileReader.readFooter(configuration, currentFile, NO_FILTER);                    } catch (Exception e) {                        throw new ParquetDecodingException("could not read footer", e);                    }                }));            }            int previousPercent = 0;            int n = 60;            System.out.print("0% [");            for (int j = 0; j < n; j++) {                System.out.print(" ");            }            System.out.print("] 100%");            for (int j = 0; j < n + 6; j++) {                System.out.print('\b');            }            while (!footers.isEmpty()) {                Future<ParquetMetadata> futureFooter = footers.removeFirst();                if (!futureFooter.isDone()) {                    footers.addLast(futureFooter);                    continue;                }                ParquetMetadata footer = futureFooter.get();                int currentPercent = (++i * n / statuses.size());                while (currentPercent > previousPercent) {                    System.out.print("*");                    previousPercent++;                }                add(footer);            }            System.out.println("");            long t1 = System.currentTimeMillis();            System.out.println("read all footers in " + (t1 - t0) + " ms");        } finally {            threadPool.shutdownNow();        }    }    Set<Entry<ColumnDescriptor, ColStats>> entries = stats.entrySet();    long total = 0;    long totalUnc = 0;    for (Entry<ColumnDescriptor, ColStats> entry : entries) {        ColStats colStats = entry.getValue();        total += colStats.allStats.total;        totalUnc += colStats.uncStats.total;    }    for (Entry<ColumnDescriptor, ColStats> entry : entries) {        ColStats colStats = entry.getValue();        System.out.println(entry.getKey() + " " + percent(colStats.allStats.total, total) + "% of all space " + colStats);    }    System.out.println("number of blocks: " + blockCount);    System.out.println("total data size: " + humanReadable(total) + " (raw " + humanReadable(totalUnc) + ")");    System.out.println("total record: " + humanReadable(recordCount));    System.out.println("average block size: " + humanReadable(total / blockCount) + " (raw " + humanReadable(totalUnc / blockCount) + ")");    System.out.println("average record count: " + humanReadable(recordCount / blockCount));}
private static void parquet-mr_f5450_0(ParquetMetadata footer)
{    for (BlockMetaData blockMetaData : footer.getBlocks()) {        ++blockCount;        MessageType schema = footer.getFileMetaData().getSchema();        recordCount += blockMetaData.getRowCount();        List<ColumnChunkMetaData> columns = blockMetaData.getColumns();        for (ColumnChunkMetaData columnMetaData : columns) {            ColumnDescriptor desc = schema.getColumnDescription(columnMetaData.getPath().toArray());            add(desc, columnMetaData.getValueCount(), columnMetaData.getTotalSize(), columnMetaData.getTotalUncompressedSize(), columnMetaData.getEncodings(), columnMetaData.getStatistics());        }    }}
private static void parquet-mr_f5451_0(String message, long total, long totalUnc)
{    System.out.println("total " + message + ": " + humanReadable(total) + " (raw " + humanReadable(totalUnc) + " saved " + percentComp(totalUnc, total) + "%)");}
private static float parquet-mr_f5452_0(long raw, long compressed)
{    return percent(raw - compressed, raw);}
private static float parquet-mr_f5453_0(long numerator, long denominator)
{    return ((float) ((numerator) * 1000 / denominator)) / 10;}
private static String parquet-mr_f5454_0(long size)
{    if (size < 1000) {        return String.valueOf(size);    }    long currentSize = size;    long previousSize = size * 1000;    int count = 0;    String[] unit = { "", "K", "M", "G", "T", "P" };    while (currentSize >= 1000) {        previousSize = currentSize;        currentSize = currentSize / 1000;        ++count;    }    return ((float) previousSize / 1000) + unit[count];}
public void parquet-mr_f5455_0(long length)
{    min = Math.min(length, min);    max = Math.max(length, max);    total += length;}
public String parquet-mr_f5456_0(int blocks)
{    return "min: " + humanReadable(min) + " max: " + humanReadable(max) + " average: " + humanReadable(total / blocks) + " total: " + humanReadable(total);}
public void parquet-mr_f5457_0(long valueCount, long size, long uncSize, Collection<Encoding> encodings, Statistics colValuesStats)
{    ++blocks;    valueCountStats.add(valueCount);    allStats.add(size);    uncStats.add(uncSize);    this.encodings.addAll(encodings);    this.colValuesStats = colValuesStats;}
public String parquet-mr_f5458_0()
{    long raw = uncStats.total;    long compressed = allStats.total;    return encodings + " " + allStats.toString(blocks) + " (raw data: " + humanReadable(raw) + (raw == 0 ? "" : " saving " + (raw - compressed) * 100 / raw + "%") + ")\n" + "  values: " + valueCountStats.toString(blocks) + "\n" + "  uncompressed: " + uncStats.toString(blocks) + "\n" + "  column values statistics: " + colValuesStats.toString();}
private static void parquet-mr_f5459_0(ColumnDescriptor desc, long valueCount, long size, long uncSize, Collection<Encoding> encodings, Statistics colValuesStats)
{    ColStats colStats = stats.get(desc);    if (colStats == null) {        colStats = new ColStats();        stats.put(desc, colStats);    }    colStats.add(valueCount, size, uncSize, encodings, colValuesStats);}
public void parquet-mr_f5460_1(RecordMaterializationException cause) throws ParquetDecodingException
{    numErrors++;        if (numErrors > 0 && errorThreshold <= 0) {                throw new ParquetDecodingException("Error while decoding records", cause);    }    double errRate = numErrors / (double) totalNumRecords;    if (errRate > errorThreshold) {        String message = String.format("Decoding error rate of at least %s/%s crosses configured threshold of %s", numErrors, totalNumRecords, errorThreshold);                throw new ParquetDecodingException(message, cause);    }}
private static float parquet-mr_f5461_0(ParquetReadOptions options, String key, float defaultValue)
{    String value = options.getProperty(key);    if (value != null) {        return Float.valueOf(value);    } else {        return defaultValue;    }}
public static Class<?> parquet-mr_f5462_0(Configuration configuration, String configName, Class<?> assignableFrom)
{    final String className = configuration.get(configName);    if (className == null) {        return null;    }    try {        final Class<?> foundClass = configuration.getClassByName(className);        if (!assignableFrom.isAssignableFrom(foundClass)) {            throw new BadConfigurationException("class " + className + " set in job conf at " + configName + " is not a subclass of " + assignableFrom.getCanonicalName());        }        return foundClass;    } catch (ClassNotFoundException e) {        throw new BadConfigurationException("could not instantiate class " + className + " set in job conf at " + configName, e);    }}
public static JobContext parquet-mr_f5463_0(Configuration conf, JobID jobId)
{    try {        return (JobContext) JOB_CONTEXT_CONSTRUCTOR.newInstance(conf, jobId);    } catch (InstantiationException e) {        throw new IllegalArgumentException("Can't instantiate JobContext", e);    } catch (IllegalAccessException e) {        throw new IllegalArgumentException("Can't instantiate JobContext", e);    } catch (InvocationTargetException e) {        throw new IllegalArgumentException("Can't instantiate JobContext", e);    }}
public static TaskAttemptContext parquet-mr_f5464_0(Configuration conf, TaskAttemptID taskAttemptId)
{    try {        return (TaskAttemptContext) TASK_CONTEXT_CONSTRUCTOR.newInstance(conf, taskAttemptId);    } catch (InstantiationException e) {        throw new IllegalArgumentException("Can't instantiate TaskAttemptContext", e);    } catch (IllegalAccessException e) {        throw new IllegalArgumentException("Can't instantiate TaskAttemptContext", e);    } catch (InvocationTargetException e) {        throw new IllegalArgumentException("Can't instantiate TaskAttemptContext", e);    }}
public static Counter parquet-mr_f5465_0(String name, String displayName, long value)
{    try {        return (Counter) GENERIC_COUNTER_CONSTRUCTOR.newInstance(name, displayName, value);    } catch (InstantiationException e) {        throw new IllegalArgumentException("Can't instantiate Counter", e);    } catch (IllegalAccessException e) {        throw new IllegalArgumentException("Can't instantiate Counter", e);    } catch (InvocationTargetException e) {        throw new IllegalArgumentException("Can't instantiate Counter", e);    }}
public static Configuration parquet-mr_f5466_0(JobContext context)
{    try {        return (Configuration) GET_CONFIGURATION_METHOD.invoke(context);    } catch (IllegalAccessException e) {        throw new IllegalArgumentException("Can't invoke method", e);    } catch (InvocationTargetException e) {        throw new IllegalArgumentException("Can't invoke method", e);    }}
public static Counter parquet-mr_f5467_0(TaskAttemptContext context, String groupName, String counterName)
{    Method counterMethod = findCounterMethod(context);    return (Counter) invoke(counterMethod, context, groupName, counterName);}
public static boolean parquet-mr_f5468_0(TaskAttemptContext context)
{    return findCounterMethod(context) != null;}
private static Method parquet-mr_f5469_0(TaskAttemptContext context)
{    if (context != null) {        if (COUNTER_METHODS_BY_CLASS.containsKey(context.getClass())) {            return COUNTER_METHODS_BY_CLASS.get(context.getClass());        }        try {            Method method = context.getClass().getMethod("getCounter", String.class, String.class);            if (method.getReturnType().isAssignableFrom(Counter.class)) {                COUNTER_METHODS_BY_CLASS.put(context.getClass(), method);                return method;            }        } catch (NoSuchMethodException e) {            return null;        }    }    return null;}
private static Object parquet-mr_f5470_0(Method method, Object obj, Object... args)
{    try {        return method.invoke(obj, args);    } catch (IllegalAccessException e) {        throw new IllegalArgumentException("Can't invoke method " + method.getName(), e);    } catch (InvocationTargetException e) {        throw new IllegalArgumentException("Can't invoke method " + method.getName(), e);    }}
public static void parquet-mr_f5471_0(Counter counter, long increment)
{    invoke(INCREMENT_COUNTER_METHOD, counter, increment);}
public static void parquet-mr_f5472_0(TaskAttemptContext context)
{    counterLoader = new MapReduceCounterLoader(context);    loadCounters();}
public static void parquet-mr_f5473_0(Reporter reporter, Configuration configuration)
{    counterLoader = new MapRedCounterLoader(reporter, configuration);    loadCounters();}
private static void parquet-mr_f5474_0()
{    bytesReadCounter = getCounterWhenFlagIsSet(COUNTER_GROUP_NAME, BYTES_READ_COUNTER_NAME, ENABLE_BYTES_READ_COUNTER);    totalBytesCounter = getCounterWhenFlagIsSet(COUNTER_GROUP_NAME, BYTES_TOTAL_COUNTER_NAME, ENABLE_BYTES_TOTAL_COUNTER);    timeCounter = getCounterWhenFlagIsSet(COUNTER_GROUP_NAME, TIME_READ_COUNTER_NAME, ENABLE_TIME_READ_COUNTER);}
private static ICounter parquet-mr_f5475_0(String groupName, String counterName, String counterFlag)
{    return counterLoader.getCounterByNameAndFlag(groupName, counterName, counterFlag);}
public static void parquet-mr_f5476_0(long val)
{    totalBytesCounter.increment(val);}
public static long parquet-mr_f5477_0()
{    return totalBytesCounter.getCount();}
public static void parquet-mr_f5478_0(long val)
{    bytesReadCounter.increment(val);}
public static long parquet-mr_f5479_0()
{    return bytesReadCounter.getCount();}
public static void parquet-mr_f5480_0(long val)
{    timeCounter.increment(val);}
public static long parquet-mr_f5481_0()
{    return timeCounter.getCount();}
public void parquet-mr_f5482_0(long val)
{}
public long parquet-mr_f5483_0()
{    return 0;}
public void parquet-mr_f5484_0(long val)
{    adaptee.increment(val);}
public long parquet-mr_f5485_0()
{    return adaptee.getCounter();}
public ICounter parquet-mr_f5486_0(String groupName, String counterName, String counterFlag)
{    if (conf.getBoolean(counterFlag, true)) {        Counters.Counter counter = reporter.getCounter(groupName, counterName);        if (counter != null) {            return new MapRedCounterAdapter(reporter.getCounter(groupName, counterName));        }    }    return new BenchmarkCounter.NullCounter();}
public void parquet-mr_f5487_0(long val)
{    ContextUtil.incrementCounter(adaptee, val);}
public long parquet-mr_f5488_0()
{        return adaptee.getValue();}
public ICounter parquet-mr_f5489_0(String groupName, String counterName, String counterFlag)
{    if (ContextUtil.getConfiguration(context).getBoolean(counterFlag, true)) {        return new MapReduceCounterAdapter(ContextUtil.getCounter(context, groupName, counterName));    } else {        return new BenchmarkCounter.NullCounter();    }}
public long parquet-mr_f5490_0() throws IOException
{    return stream.getPos();}
public void parquet-mr_f5491_0(long newPos) throws IOException
{    stream.seek(newPos);}
public void parquet-mr_f5492_0(byte[] bytes) throws IOException
{    stream.readFully(bytes, 0, bytes.length);}
public void parquet-mr_f5493_0(byte[] bytes, int start, int len) throws IOException
{    stream.readFully(bytes);}
public void parquet-mr_f5494_0() throws IOException
{    stream.close();}
public long parquet-mr_f5495_0() throws IOException
{    return stream.getPos();}
public void parquet-mr_f5496_0(long newPos) throws IOException
{    stream.seek(newPos);}
public void parquet-mr_f5497_0(byte[] bytes, int start, int len) throws IOException
{    stream.readFully(bytes);}
public int parquet-mr_f5498_0(ByteBuffer buf) throws IOException
{    return stream.read(buf);}
public void parquet-mr_f5499_0(ByteBuffer buf) throws IOException
{    readFully(reader, buf);}
public int parquet-mr_f5500_0(ByteBuffer buf) throws IOException
{    return stream.read(buf);}
public static void parquet-mr_f5501_0(Reader reader, ByteBuffer buf) throws IOException
{        while (buf.hasRemaining()) {        int readCount = reader.read(buf);        if (readCount == -1) {                        throw new EOFException("Reached the end of stream. Still have: " + buf.remaining() + " bytes left");        }    }}
public static CompressionCodecFactory parquet-mr_f5502_0(int sizeHint)
{    return new CodecFactory(new Configuration(), sizeHint);}
public static CompressionCodecFactory parquet-mr_f5503_0(Configuration conf, int sizeHint)
{    return new CodecFactory(conf, sizeHint);}
public static CompressionCodecFactory parquet-mr_f5504_0(Configuration conf, ByteBufferAllocator allocator, int sizeHint)
{    return CodecFactory.createDirectCodecFactory(conf, allocator, sizeHint);}
public static HadoopInputFile parquet-mr_f5505_0(Path path, Configuration conf) throws IOException
{    FileSystem fs = path.getFileSystem(conf);    return new HadoopInputFile(fs, fs.getFileStatus(path), conf);}
public static HadoopInputFile parquet-mr_f5506_0(FileStatus stat, Configuration conf) throws IOException
{    FileSystem fs = stat.getPath().getFileSystem(conf);    return new HadoopInputFile(fs, stat, conf);}
public Configuration parquet-mr_f5507_0()
{    return conf;}
public long parquet-mr_f5508_0()
{    return stat.getLen();}
public SeekableInputStream parquet-mr_f5509_0() throws IOException
{    return HadoopStreams.wrap(fs.open(stat.getPath()));}
public String parquet-mr_f5510_0()
{    return stat.getPath().toString();}
public static Set<String> parquet-mr_f5511_0()
{    return BLOCK_FS_SCHEMES;}
private static boolean parquet-mr_f5512_0(FileSystem fs)
{    return BLOCK_FS_SCHEMES.contains(fs.getUri().getScheme());}
public static HadoopOutputFile parquet-mr_f5513_0(Path path, Configuration conf) throws IOException
{    FileSystem fs = path.getFileSystem(conf);    return new HadoopOutputFile(fs, fs.makeQualified(path), conf);}
public Configuration parquet-mr_f5514_0()
{    return conf;}
public PositionOutputStream parquet-mr_f5515_0(long blockSizeHint) throws IOException
{    return HadoopStreams.wrap(fs.create(path, false, /* do not overwrite */    DFS_BUFFER_SIZE_DEFAULT, fs.getDefaultReplication(path), Math.max(fs.getDefaultBlockSize(path), blockSizeHint)));}
public PositionOutputStream parquet-mr_f5516_0(long blockSizeHint) throws IOException
{    return HadoopStreams.wrap(fs.create(path, true, /* overwrite if exists */    DFS_BUFFER_SIZE_DEFAULT, fs.getDefaultReplication(path), Math.max(fs.getDefaultBlockSize(path), blockSizeHint)));}
public boolean parquet-mr_f5517_0()
{    return supportsBlockSize(fs);}
public long parquet-mr_f5518_0()
{    return fs.getDefaultBlockSize(path);}
public String parquet-mr_f5519_0()
{    return path.toString();}
public long parquet-mr_f5520_0() throws IOException
{    return wrapped.getPos();}
public void parquet-mr_f5521_0(int b) throws IOException
{    wrapped.write(b);}
public void parquet-mr_f5522_0(byte[] b) throws IOException
{    wrapped.write(b);}
public void parquet-mr_f5523_0(byte[] b, int off, int len) throws IOException
{    wrapped.write(b, off, len);}
public void parquet-mr_f5524_0() throws IOException
{    wrapped.hsync();}
public void parquet-mr_f5525_0() throws IOException
{    wrapped.flush();}
public void parquet-mr_f5526_0() throws IOException
{    wrapped.close();}
public static SeekableInputStream parquet-mr_f5527_1(FSDataInputStream stream)
{    Preconditions.checkNotNull(stream, "Cannot wrap a null input stream");    if (byteBufferReadableClass != null && h2SeekableConstructor != null && byteBufferReadableClass.isInstance(stream.getWrappedStream())) {        try {            return h2SeekableConstructor.newInstance(stream);        } catch (InstantiationException e) {                        return new H1SeekableInputStream(stream);        } catch (IllegalAccessException e) {                        return new H1SeekableInputStream(stream);        } catch (InvocationTargetException e) {            throw new ParquetDecodingException("Could not instantiate H2SeekableInputStream", e.getTargetException());        }    } else {        return new H1SeekableInputStream(stream);    }}
private static Class<?> parquet-mr_f5528_0()
{    try {        return Class.forName("org.apache.hadoop.fs.ByteBufferReadable");    } catch (ClassNotFoundException e) {        return null;    } catch (NoClassDefFoundError e) {        return null;    }}
private static Class<SeekableInputStream> parquet-mr_f5529_0()
{    try {        return (Class<SeekableInputStream>) Class.forName("org.apache.parquet.hadoop.util.H2SeekableInputStream");    } catch (ClassNotFoundException e) {        return null;    } catch (NoClassDefFoundError e) {        return null;    }}
private static Constructor<SeekableInputStream> parquet-mr_f5530_0()
{    Class<SeekableInputStream> h2SeekableClass = getH2SeekableClass();    if (h2SeekableClass != null) {        try {            return h2SeekableClass.getConstructor(FSDataInputStream.class);        } catch (NoSuchMethodException e) {            return null;        }    }    return null;}
public static PositionOutputStream parquet-mr_f5531_0(FSDataOutputStream stream)
{    Preconditions.checkNotNull(stream, "Cannot wrap a null output stream");    return new HadoopPositionOutputStream(stream);}
public boolean parquet-mr_f5532_0(Path p)
{    final char c = p.getName().charAt(0);    return c != '.' && c != '_';}
public static void parquet-mr_f5533_0(String key, Object obj, Configuration conf) throws IOException
{    try (ByteArrayOutputStream baos = new ByteArrayOutputStream()) {        try (GZIPOutputStream gos = new GZIPOutputStream(baos);            ObjectOutputStream oos = new ObjectOutputStream(gos)) {            oos.writeObject(obj);        }        conf.set(key, new String(Base64.getMimeEncoder().encode(baos.toByteArray()), StandardCharsets.UTF_8));    }}
public static T parquet-mr_f5534_0(String key, Configuration conf) throws IOException
{    String b64 = conf.get(key);    if (b64 == null) {        return null;    }    byte[] bytes = Base64.getMimeDecoder().decode(b64.getBytes(StandardCharsets.UTF_8));    try (ByteArrayInputStream bais = new ByteArrayInputStream(bytes);        GZIPInputStream gis = new GZIPInputStream(bais);        ObjectInputStream ois = new ObjectInputStream(gis)) {        return (T) ois.readObject();    } catch (ClassNotFoundException e) {        throw new IOException("Could not read object from config with key " + key, e);    } catch (ClassCastException e) {        throw new IOException("Could not cast object read from config with key " + key, e);    }}
public String parquet-mr_f5535_0(String property)
{    String value = super.getProperty(property);    if (value != null) {        return value;    }    return conf.get(property);}
public Configuration parquet-mr_f5536_0()
{    return conf;}
public static Builder parquet-mr_f5537_0(Configuration conf)
{    return new Builder(conf);}
public ParquetReadOptions parquet-mr_f5538_0()
{    return new HadoopReadOptions(useSignedStringMinMax, useStatsFilter, useDictionaryFilter, useRecordFilter, useColumnIndexFilter, usePageChecksumVerification, recordFilter, metadataFilter, codecFactory, allocator, maxAllocationSize, properties, conf);}
public long parquet-mr_f5539_0()
{    return offset;}
public int parquet-mr_f5540_0()
{    return length;}
public boolean parquet-mr_f5541_0()
{    return useSignedStringMinMax;}
public boolean parquet-mr_f5542_0()
{    return useStatsFilter;}
public boolean parquet-mr_f5543_0()
{    return useDictionaryFilter;}
public boolean parquet-mr_f5544_0()
{    return useRecordFilter;}
public boolean parquet-mr_f5545_0()
{    return useColumnIndexFilter;}
public boolean parquet-mr_f5546_0()
{    return usePageChecksumVerification;}
public FilterCompat.Filter parquet-mr_f5547_0()
{    return recordFilter;}
public ParquetMetadataConverter.MetadataFilter parquet-mr_f5548_0()
{    return metadataFilter;}
public CompressionCodecFactory parquet-mr_f5549_0()
{    return codecFactory;}
public ByteBufferAllocator parquet-mr_f5550_0()
{    return allocator;}
public int parquet-mr_f5551_0()
{    return maxAllocationSize;}
public Set<String> parquet-mr_f5552_0()
{    return properties.keySet();}
public String parquet-mr_f5553_0(String property)
{    return properties.get(property);}
public boolean parquet-mr_f5554_0(String property, boolean defaultValue)
{    if (properties.containsKey(property)) {        return Boolean.valueOf(properties.get(property));    } else {        return defaultValue;    }}
public static Builder parquet-mr_f5555_0()
{    return new Builder();}
public Builder parquet-mr_f5556_0(boolean useSignedStringMinMax)
{    this.useSignedStringMinMax = useSignedStringMinMax;    return this;}
public Builder parquet-mr_f5557_0()
{    this.useSignedStringMinMax = true;    return this;}
public Builder parquet-mr_f5558_0(boolean useStatsFilter)
{    this.useStatsFilter = useStatsFilter;    return this;}
public Builder parquet-mr_f5559_0()
{    this.useStatsFilter = true;    return this;}
public Builder parquet-mr_f5560_0(boolean useDictionaryFilter)
{    this.useDictionaryFilter = useDictionaryFilter;    return this;}
public Builder parquet-mr_f5561_0()
{    this.useDictionaryFilter = true;    return this;}
public Builder parquet-mr_f5562_0(boolean useRecordFilter)
{    this.useRecordFilter = useRecordFilter;    return this;}
public Builder parquet-mr_f5563_0()
{    this.useRecordFilter = true;    return this;}
public Builder parquet-mr_f5564_0(boolean useColumnIndexFilter)
{    this.useColumnIndexFilter = useColumnIndexFilter;    return this;}
public Builder parquet-mr_f5565_0()
{    return useColumnIndexFilter(true);}
public Builder parquet-mr_f5566_0(boolean usePageChecksumVerification)
{    this.usePageChecksumVerification = usePageChecksumVerification;    return this;}
public Builder parquet-mr_f5567_0()
{    return usePageChecksumVerification(true);}
public Builder parquet-mr_f5568_0(FilterCompat.Filter rowGroupFilter)
{    this.recordFilter = rowGroupFilter;    return this;}
public Builder parquet-mr_f5569_0(long start, long end)
{    this.metadataFilter = ParquetMetadataConverter.range(start, end);    return this;}
public Builder parquet-mr_f5570_0(long... rowGroupOffsets)
{    this.metadataFilter = ParquetMetadataConverter.offsets(rowGroupOffsets);    return this;}
public Builder parquet-mr_f5571_0(ParquetMetadataConverter.MetadataFilter metadataFilter)
{    this.metadataFilter = metadataFilter;    return this;}
public Builder parquet-mr_f5572_0(CompressionCodecFactory codecFactory)
{    this.codecFactory = codecFactory;    return this;}
public Builder parquet-mr_f5573_0(ByteBufferAllocator allocator)
{    this.allocator = allocator;    return this;}
public Builder parquet-mr_f5574_0(int allocationSizeInBytes)
{    this.maxAllocationSize = allocationSizeInBytes;    return this;}
public Builder parquet-mr_f5575_0(boolean val)
{    this.usePageChecksumVerification = val;    return this;}
public Builder parquet-mr_f5576_0(String key, String value)
{    properties.put(key, value);    return this;}
public Builder parquet-mr_f5577_0(ParquetReadOptions options)
{    useSignedStringMinMax(options.useSignedStringMinMax);    useStatsFilter(options.useStatsFilter);    useDictionaryFilter(options.useDictionaryFilter);    useRecordFilter(options.useRecordFilter);    withRecordFilter(options.recordFilter);    withMetadataFilter(options.metadataFilter);    withCodecFactory(options.codecFactory);    withAllocator(options.allocator);    withPageChecksumVerification(options.usePageChecksumVerification);    for (Map.Entry<String, String> keyValue : options.properties.entrySet()) {        set(keyValue.getKey(), keyValue.getValue());    }    return this;}
public ParquetReadOptions parquet-mr_f5578_0()
{    return new ParquetReadOptions(useSignedStringMinMax, useStatsFilter, useDictionaryFilter, useRecordFilter, useColumnIndexFilter, usePageChecksumVerification, recordFilter, metadataFilter, codecFactory, allocator, maxAllocationSize, properties);}
protected Path parquet-mr_f5579_0(String type, DirectWriter writer) throws IOException
{    return writeDirect(MessageTypeParser.parseMessageType(type), writer);}
protected Path parquet-mr_f5580_0(String type, DirectWriter writer, Map<String, String> metadata) throws IOException
{    return writeDirect(MessageTypeParser.parseMessageType(type), writer, metadata);}
protected Path parquet-mr_f5581_0(MessageType type, DirectWriter writer) throws IOException
{    return writeDirect(type, writer, new HashMap<String, String>());}
protected Path parquet-mr_f5582_0(MessageType type, DirectWriter writer, Map<String, String> metadata) throws IOException
{    File temp = tempDir.newFile(UUID.randomUUID().toString());    temp.deleteOnExit();    temp.delete();    Path path = new Path(temp.getPath());    ParquetWriter<Void> parquetWriter = new ParquetWriter<Void>(path, new DirectWriteSupport(type, writer, metadata));    parquetWriter.write(null);    parquetWriter.close();    return path;}
public WriteContext parquet-mr_f5583_0(Configuration configuration)
{    return new WriteContext(type, metadata);}
public void parquet-mr_f5584_0(RecordConsumer recordConsumer)
{    this.recordConsumer = recordConsumer;}
public void parquet-mr_f5585_0(Void record)
{    writer.write(recordConsumer);}
public static Collection<Object[]> parquet-mr_f5586_0()
{    List<PrimitiveTypeName> types = Arrays.asList(PrimitiveTypeName.BOOLEAN, PrimitiveTypeName.INT32, PrimitiveTypeName.INT64, PrimitiveTypeName.INT96, PrimitiveTypeName.FLOAT, PrimitiveTypeName.DOUBLE, PrimitiveTypeName.BINARY, PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY);    List<CompressionCodecName> codecs;    String codecList = System.getenv("TEST_CODECS");    if (codecList != null) {        codecs = new ArrayList<CompressionCodecName>();        for (String codec : codecList.split(",")) {            codecs.add(CompressionCodecName.valueOf(codec.toUpperCase(Locale.ENGLISH)));        }    } else {                codecs = Arrays.asList(CompressionCodecName.UNCOMPRESSED);    }    System.err.println("Testing codecs: " + codecs);    List<Object[]> parameters = new ArrayList<Object[]>();    for (PrimitiveTypeName type : types) {        for (CompressionCodecName codec : codecs) {            parameters.add(new Object[] { type, codec });        }    }    return parameters;}
public static void parquet-mr_f5587_0() throws IOException
{    Random random = new Random(RANDOM_SEED);    intGenerator = new RandomValues.IntGenerator(random.nextLong());    longGenerator = new RandomValues.LongGenerator(random.nextLong());    int96Generator = new RandomValues.Int96Generator(random.nextLong());    floatGenerator = new RandomValues.FloatGenerator(random.nextLong());    doubleGenerator = new RandomValues.DoubleGenerator(random.nextLong());    binaryGenerator = new RandomValues.BinaryGenerator(random.nextLong());    fixedBinaryGenerator = new RandomValues.FixedGenerator(random.nextLong(), FIXED_LENGTH);}
public void parquet-mr_f5588_0() throws Exception
{    final boolean DISABLE_DICTIONARY = false;    List<?> randomValues;    randomValues = generateRandomValues(this.paramTypeName, RECORD_COUNT);    /* Run an encoding test per each writer version.     * This loop will make sure to test future writer versions added to WriterVersion enum.     */    for (WriterVersion writerVersion : WriterVersion.values()) {        System.out.println(String.format("Testing %s/%s/%s encodings using ROW_GROUP_SIZE=%d PAGE_SIZE=%d", writerVersion, this.paramTypeName, this.compression, TEST_ROW_GROUP_SIZE, TEST_PAGE_SIZE));        Path parquetFile = createTempFile();        writeValuesToFile(parquetFile, this.paramTypeName, randomValues, TEST_ROW_GROUP_SIZE, TEST_PAGE_SIZE, DISABLE_DICTIONARY, writerVersion);        PageGroupValidator.validatePages(parquetFile, randomValues);    }}
public void parquet-mr_f5589_0() throws Exception
{    final boolean ENABLE_DICTIONARY = true;    List<?> dictionaryValues = generateDictionaryValues(this.paramTypeName, RECORD_COUNT);    /* Run an encoding test per each writer version.     * This loop will make sure to test future writer versions added to WriterVersion enum.     */    for (WriterVersion writerVersion : WriterVersion.values()) {        System.out.println(String.format("Testing %s/%s/%s + DICTIONARY encodings using ROW_GROUP_SIZE=%d PAGE_SIZE=%d", writerVersion, this.paramTypeName, this.compression, TEST_ROW_GROUP_SIZE, TEST_PAGE_SIZE));        Path parquetFile = createTempFile();        writeValuesToFile(parquetFile, this.paramTypeName, dictionaryValues, TEST_ROW_GROUP_SIZE, TEST_PAGE_SIZE, ENABLE_DICTIONARY, writerVersion);        PageGroupValidator.validatePages(parquetFile, dictionaryValues);    }}
private Path parquet-mr_f5590_0() throws IOException
{    File tempFile = tempFolder.newFile();    tempFile.delete();    return new Path(tempFile.getAbsolutePath());}
private void parquet-mr_f5591_0(Path file, PrimitiveTypeName type, List<?> values, int rowGroupSize, int pageSize, boolean enableDictionary, WriterVersion version) throws IOException
{    MessageType schema;    if (type == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {        schema = Types.buildMessage().required(type).length(FIXED_LENGTH).named("field").named("test");    } else {        schema = Types.buildMessage().required(type).named("field").named("test");    }    SimpleGroupFactory message = new SimpleGroupFactory(schema);    GroupWriteSupport.setSchema(schema, configuration);    ParquetWriter<Group> writer = ExampleParquetWriter.builder(file).withCompressionCodec(compression).withRowGroupSize(rowGroupSize).withPageSize(pageSize).withDictionaryPageSize(TEST_DICT_PAGE_SIZE).withDictionaryEncoding(enableDictionary).withWriterVersion(version).withConf(configuration).build();    for (Object o : values) {        switch(type) {            case BOOLEAN:                writer.write(message.newGroup().append("field", (Boolean) o));                break;            case INT32:                writer.write(message.newGroup().append("field", (Integer) o));                break;            case INT64:                writer.write(message.newGroup().append("field", (Long) o));                break;            case FLOAT:                writer.write(message.newGroup().append("field", (Float) o));                break;            case DOUBLE:                writer.write(message.newGroup().append("field", (Double) o));                break;            case INT96:            case BINARY:            case FIXED_LEN_BYTE_ARRAY:                writer.write(message.newGroup().append("field", (Binary) o));                break;            default:                throw new IllegalArgumentException("Unknown type name: " + type);        }    }    writer.close();}
private List<?> parquet-mr_f5592_0(PrimitiveTypeName type, int count)
{    List<Object> values = new ArrayList<Object>();    for (int i = 0; i < count; i++) {        Object value;        switch(type) {            case BOOLEAN:                value = (intGenerator.nextValue() % 2 == 0) ? true : false;                break;            case INT32:                value = intGenerator.nextValue();                break;            case INT64:                value = longGenerator.nextValue();                break;            case FLOAT:                value = floatGenerator.nextValue();                break;            case DOUBLE:                value = doubleGenerator.nextValue();                break;            case INT96:                value = int96Generator.nextBinaryValue();                break;            case BINARY:                value = binaryGenerator.nextBinaryValue();                break;            case FIXED_LEN_BYTE_ARRAY:                value = fixedBinaryGenerator.nextBinaryValue();                break;            default:                throw new IllegalArgumentException("Unknown type name: " + type);        }        values.add(value);    }    return values;}
private List<?> parquet-mr_f5593_0(PrimitiveTypeName type, int count)
{    final int DICT_VALUES_SIZE = 100;    final List<?> DICT_BINARY_VALUES = generateRandomValues(PrimitiveTypeName.BINARY, DICT_VALUES_SIZE);    final List<?> DICT_INT96_VALUES = generateRandomValues(PrimitiveTypeName.INT96, DICT_VALUES_SIZE);    final List<?> DICT_FIXED_LEN_VALUES = generateRandomValues(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, DICT_VALUES_SIZE);    List<Object> values = new ArrayList<Object>();    for (int i = 0; i < count; i++) {        int dictValue = i % DICT_VALUES_SIZE;        Object value;        switch(type) {            case BOOLEAN:                value = (i % 2 == 0) ? true : false;                break;            case INT32:                value = dictValue;                break;            case INT64:                value = (long) dictValue;                break;            case FLOAT:                value = (float) dictValue;                break;            case DOUBLE:                value = (double) dictValue;                break;            case INT96:                value = DICT_INT96_VALUES.get(dictValue);                break;            case BINARY:                value = DICT_BINARY_VALUES.get(dictValue);                break;            case FIXED_LEN_BYTE_ARRAY:                value = DICT_FIXED_LEN_VALUES.get(dictValue);                break;            default:                throw new IllegalArgumentException("Unknown type name: " + type);        }        values.add(value);    }    return values;}
public static void parquet-mr_f5594_0(Path file, List<?> expectedValues) throws IOException
{    List<PageReadStore> blockReaders = readBlocksFromFile(file);    MessageType fileSchema = readSchemaFromFile(file);    int rowGroupID = 0;    int rowsRead = 0;    for (PageReadStore pageReadStore : blockReaders) {        for (ColumnDescriptor columnsDesc : fileSchema.getColumns()) {            List<DataPage> pageGroup = getPageGroupForColumn(pageReadStore, columnsDesc);            DictionaryPage dictPage = reusableCopy(getDictionaryPageForColumn(pageReadStore, columnsDesc));            List<?> expectedRowGroupValues = expectedValues.subList(rowsRead, (int) (rowsRead + pageReadStore.getRowCount()));            validateFirstToLast(rowGroupID, dictPage, pageGroup, columnsDesc, expectedRowGroupValues);            validateLastToFirst(rowGroupID, dictPage, pageGroup, columnsDesc, expectedRowGroupValues);        }        rowsRead += pageReadStore.getRowCount();        rowGroupID++;    }}
private static DictionaryPage parquet-mr_f5595_0(DictionaryPage dict)
{    if (dict == null) {        return null;    }    try {        return new DictionaryPage(BytesInput.from(dict.getBytes().toByteArray()), dict.getDictionarySize(), dict.getEncoding());    } catch (IOException e) {        throw new ParquetDecodingException("Cannot read dictionary", e);    }}
private static DataPage parquet-mr_f5596_0(DataPage page)
{    return page.accept(new DataPage.Visitor<DataPage>() {        @Override        public DataPage visit(DataPageV1 data) {            try {                return new DataPageV1(BytesInput.from(data.getBytes().toByteArray()), data.getValueCount(), data.getUncompressedSize(), data.getStatistics(), data.getRlEncoding(), data.getDlEncoding(), data.getValueEncoding());            } catch (IOException e) {                throw new ParquetDecodingException("Cannot read data", e);            }        }        @Override        public DataPage visit(DataPageV2 data) {            try {                return new DataPageV2(data.getRowCount(), data.getNullCount(), data.getValueCount(), BytesInput.from(data.getRepetitionLevels().toByteArray()), BytesInput.from(data.getDefinitionLevels().toByteArray()), data.getDataEncoding(), BytesInput.from(data.getData().toByteArray()), data.getUncompressedSize(), data.getStatistics(), data.isCompressed());            } catch (IOException e) {                throw new ParquetDecodingException("Cannot read data", e);            }        }    });}
public DataPage parquet-mr_f5597_0(DataPageV1 data)
{    try {        return new DataPageV1(BytesInput.from(data.getBytes().toByteArray()), data.getValueCount(), data.getUncompressedSize(), data.getStatistics(), data.getRlEncoding(), data.getDlEncoding(), data.getValueEncoding());    } catch (IOException e) {        throw new ParquetDecodingException("Cannot read data", e);    }}
public DataPage parquet-mr_f5598_0(DataPageV2 data)
{    try {        return new DataPageV2(data.getRowCount(), data.getNullCount(), data.getValueCount(), BytesInput.from(data.getRepetitionLevels().toByteArray()), BytesInput.from(data.getDefinitionLevels().toByteArray()), data.getDataEncoding(), BytesInput.from(data.getData().toByteArray()), data.getUncompressedSize(), data.getStatistics(), data.isCompressed());    } catch (IOException e) {        throw new ParquetDecodingException("Cannot read data", e);    }}
private static void parquet-mr_f5599_0(int rowGroupID, DictionaryPage dictPage, List<DataPage> pageGroup, ColumnDescriptor desc, List<?> expectedValues)
{    int rowsRead = 0, pageID = 0;    for (DataPage page : pageGroup) {        List<?> expectedPageValues = expectedValues.subList(rowsRead, rowsRead + page.getValueCount());        PageValuesValidator.validateValuesForPage(rowGroupID, pageID, dictPage, page, desc, expectedPageValues);        rowsRead += page.getValueCount();        pageID++;    }}
private static void parquet-mr_f5600_0(int rowGroupID, DictionaryPage dictPage, List<DataPage> pageGroup, ColumnDescriptor desc, List<?> expectedValues)
{    int rowsLeft = expectedValues.size();    for (int pageID = pageGroup.size() - 1; pageID >= 0; pageID--) {        DataPage page = pageGroup.get(pageID);        int offset = rowsLeft - page.getValueCount();        List<?> expectedPageValues = expectedValues.subList(offset, offset + page.getValueCount());        PageValuesValidator.validateValuesForPage(rowGroupID, pageID, dictPage, page, desc, expectedPageValues);        rowsLeft -= page.getValueCount();    }}
private static DictionaryPage parquet-mr_f5601_0(PageReadStore pageReadStore, ColumnDescriptor columnDescriptor)
{    PageReader pageReader = pageReadStore.getPageReader(columnDescriptor);    return pageReader.readDictionaryPage();}
private static List<DataPage> parquet-mr_f5602_0(PageReadStore pageReadStore, ColumnDescriptor columnDescriptor)
{    PageReader pageReader = pageReadStore.getPageReader(columnDescriptor);    List<DataPage> pageGroup = new ArrayList<DataPage>();    DataPage page;    while ((page = pageReader.readPage()) != null) {        pageGroup.add(reusableCopy(page));    }    return pageGroup;}
private static MessageType parquet-mr_f5603_0(Path file) throws IOException
{    ParquetMetadata metadata = ParquetFileReader.readFooter(configuration, file, ParquetMetadataConverter.NO_FILTER);    return metadata.getFileMetaData().getSchema();}
private static List<PageReadStore> parquet-mr_f5604_0(Path file) throws IOException
{    List<PageReadStore> rowGroups = new ArrayList<PageReadStore>();    ParquetMetadata metadata = ParquetFileReader.readFooter(configuration, file, ParquetMetadataConverter.NO_FILTER);    ParquetFileReader fileReader = new ParquetFileReader(configuration, metadata.getFileMetaData(), file, metadata.getBlocks(), metadata.getFileMetaData().getSchema().getColumns());    PageReadStore group;    while ((group = fileReader.readNextRowGroup()) != null) {        rowGroups.add(group);    }    return rowGroups;}
public void parquet-mr_f5605_0(Object value)
{    assertEquals(String.format("Value from page is different than expected, ROW_GROUP_ID=%d PAGE_ID=%d VALUE_POS=%d", rowGroupID, pageID, currentPos), expectedValues.get(currentPos++), value);}
public static void parquet-mr_f5606_0(int rowGroupID, int pageID, DictionaryPage dictPage, DataPage page, ColumnDescriptor columnDesc, List<?> expectedValues)
{    TestStatistics.SingletonPageReader pageReader = new TestStatistics.SingletonPageReader(dictPage, page);    PrimitiveConverter converter = getConverter(rowGroupID, pageID, columnDesc.getType(), expectedValues);    ColumnReaderImpl column = new ColumnReaderImpl(columnDesc, pageReader, converter, null);    for (int i = 0; i < pageReader.getTotalValueCount(); i += 1) {        column.writeCurrentValueToConverter();        column.consume();    }}
private static PrimitiveConverter parquet-mr_f5607_0(final int rowGroupID, final int pageID, PrimitiveTypeName type, final List<?> expectedValues)
{    return type.convert(new PrimitiveType.PrimitiveTypeNameConverter<PrimitiveConverter, RuntimeException>() {        @Override        public PrimitiveConverter convertFLOAT(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);            return new PrimitiveConverter() {                @Override                public void addFloat(float value) {                    validator.validateNextValue(value);                }            };        }        @Override        public PrimitiveConverter convertDOUBLE(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);            return new PrimitiveConverter() {                @Override                public void addDouble(double value) {                    validator.validateNextValue(value);                }            };        }        @Override        public PrimitiveConverter convertINT32(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);            return new PrimitiveConverter() {                @Override                public void addInt(int value) {                    validator.validateNextValue(value);                }            };        }        @Override        public PrimitiveConverter convertINT64(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);            return new PrimitiveConverter() {                @Override                public void addLong(long value) {                    validator.validateNextValue(value);                }            };        }        @Override        public PrimitiveConverter convertINT96(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return convertBINARY(primitiveTypeName);        }        @Override        public PrimitiveConverter convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return convertBINARY(primitiveTypeName);        }        @Override        public PrimitiveConverter convertBOOLEAN(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);            return new PrimitiveConverter() {                @Override                public void addBoolean(boolean value) {                    validator.validateNextValue(value);                }            };        }        @Override        public PrimitiveConverter convertBINARY(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);            return new PrimitiveConverter() {                @Override                public void addBinary(Binary value) {                    validator.validateNextValue(value);                }            };        }    });}
public PrimitiveConverter parquet-mr_f5608_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);    return new PrimitiveConverter() {        @Override        public void addFloat(float value) {            validator.validateNextValue(value);        }    };}
public void parquet-mr_f5609_0(float value)
{    validator.validateNextValue(value);}
public PrimitiveConverter parquet-mr_f5610_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);    return new PrimitiveConverter() {        @Override        public void addDouble(double value) {            validator.validateNextValue(value);        }    };}
public void parquet-mr_f5611_0(double value)
{    validator.validateNextValue(value);}
public PrimitiveConverter parquet-mr_f5612_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);    return new PrimitiveConverter() {        @Override        public void addInt(int value) {            validator.validateNextValue(value);        }    };}
public void parquet-mr_f5613_0(int value)
{    validator.validateNextValue(value);}
public PrimitiveConverter parquet-mr_f5614_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);    return new PrimitiveConverter() {        @Override        public void addLong(long value) {            validator.validateNextValue(value);        }    };}
public void parquet-mr_f5615_0(long value)
{    validator.validateNextValue(value);}
public PrimitiveConverter parquet-mr_f5616_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return convertBINARY(primitiveTypeName);}
public PrimitiveConverter parquet-mr_f5617_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return convertBINARY(primitiveTypeName);}
public PrimitiveConverter parquet-mr_f5618_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);    return new PrimitiveConverter() {        @Override        public void addBoolean(boolean value) {            validator.validateNextValue(value);        }    };}
public void parquet-mr_f5619_0(boolean value)
{    validator.validateNextValue(value);}
public PrimitiveConverter parquet-mr_f5620_0(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);    return new PrimitiveConverter() {        @Override        public void addBinary(Binary value) {            validator.validateNextValue(value);        }    };}
public void parquet-mr_f5621_0(Binary value)
{    validator.validateNextValue(value);}
public void parquet-mr_f5622_0()
{    List<BlockMetaData> blocks = new ArrayList<BlockMetaData>();    IntStatistics stats1 = new IntStatistics();    stats1.setMinMax(10, 100);    stats1.setNumNulls(4);    BlockMetaData b1 = makeBlockFromStats(stats1, 301);    blocks.add(b1);    IntStatistics stats2 = new IntStatistics();    stats2.setMinMax(8, 102);    stats2.setNumNulls(0);    BlockMetaData b2 = makeBlockFromStats(stats2, 302);    blocks.add(b2);    IntStatistics stats3 = new IntStatistics();    stats3.setMinMax(100, 102);    stats3.setNumNulls(12);    BlockMetaData b3 = makeBlockFromStats(stats3, 303);    blocks.add(b3);    IntStatistics stats4 = new IntStatistics();    stats4.setMinMax(0, 0);    stats4.setNumNulls(304);    BlockMetaData b4 = makeBlockFromStats(stats4, 304);    blocks.add(b4);    IntStatistics stats5 = new IntStatistics();    stats5.setMinMax(50, 50);    stats5.setNumNulls(7);    BlockMetaData b5 = makeBlockFromStats(stats5, 305);    blocks.add(b5);    IntStatistics stats6 = new IntStatistics();    stats6.setMinMax(0, 0);    stats6.setNumNulls(12);    BlockMetaData b6 = makeBlockFromStats(stats6, 306);    blocks.add(b6);    MessageType schema = MessageTypeParser.parseMessageType("message Document { optional int32 foo; }");    IntColumn foo = intColumn("foo");    List<BlockMetaData> filtered = RowGroupFilter.filterRowGroups(FilterCompat.get(eq(foo, 50)), blocks, schema);    assertEquals(Arrays.asList(b1, b2, b5), filtered);    filtered = RowGroupFilter.filterRowGroups(FilterCompat.get(notEq(foo, 50)), blocks, schema);    assertEquals(Arrays.asList(b1, b2, b3, b4, b5, b6), filtered);    filtered = RowGroupFilter.filterRowGroups(FilterCompat.get(eq(foo, null)), blocks, schema);    assertEquals(Arrays.asList(b1, b3, b4, b5, b6), filtered);    filtered = RowGroupFilter.filterRowGroups(FilterCompat.get(notEq(foo, null)), blocks, schema);    assertEquals(Arrays.asList(b1, b2, b3, b5, b6), filtered);    filtered = RowGroupFilter.filterRowGroups(FilterCompat.get(eq(foo, 0)), blocks, schema);    assertEquals(Arrays.asList(b6), filtered);}
private static Binary parquet-mr_f5623_0(String decimalWithoutScale, int byteCount)
{    return toBinary(new BigInteger(decimalWithoutScale), byteCount);}
private static Binary parquet-mr_f5624_0(BigInteger decimalWithoutScale, int byteCount)
{    byte[] src = decimalWithoutScale.toByteArray();    if (src.length > byteCount) {        throw new IllegalArgumentException("Too large decimal value for byte count " + byteCount);    }    byte[] dest = new byte[byteCount];    System.arraycopy(src, 0, dest, dest.length - src.length, src.length);    return Binary.fromConstantByteArray(dest);}
private static void parquet-mr_f5625_0(SimpleGroupFactory f, ParquetWriter<Group> writer) throws IOException
{    for (int i = 0; i < nElements; i++) {        int index = i % ALPHABET.length();        Group group = f.newGroup().append("binary_field", ALPHABET.substring(index, index + 1)).append("single_value_field", "sharp").append("fixed_field", DECIMAL_VALUES[i % DECIMAL_VALUES.length]).append("int32_field", intValues[i % intValues.length]).append("int64_field", longValues[i % longValues.length]).append("double_field", toDouble(intValues[i % intValues.length])).append("float_field", toFloat(intValues[i % intValues.length])).append("plain_int32_field", i).append("fallback_binary_field", i < (nElements / 2) ? ALPHABET.substring(index, index + 1) : UUID.randomUUID().toString()).append("int96_field", INT96_VALUES[i % INT96_VALUES.length]);                if (index % 10 > 0) {            group.append("optional_single_value_field", "sharp");        }        writer.write(group);    }    writer.close();}
public static void parquet-mr_f5626_0() throws IOException
{    cleanup();    prepareFile(PARQUET_1_0, FILE_V1);    prepareFile(PARQUET_2_0, FILE_V2);}
private static void parquet-mr_f5627_0(WriterVersion version, Path file) throws IOException
{    GroupWriteSupport.setSchema(schema, conf);    SimpleGroupFactory f = new SimpleGroupFactory(schema);    ParquetWriter<Group> writer = ExampleParquetWriter.builder(file).withWriterVersion(version).withCompressionCodec(GZIP).withRowGroupSize(1024 * 1024).withPageSize(1024).enableDictionaryEncoding().withDictionaryPageSize(2 * 1024).withConf(conf).build();    writeData(f, writer);}
public static void parquet-mr_f5628_0() throws IOException
{    deleteFile(FILE_V1);    deleteFile(FILE_V2);}
private static void parquet-mr_f5629_0(Path file) throws IOException
{    FileSystem fs = file.getFileSystem(conf);    if (fs.exists(file)) {        fs.delete(file, true);    }}
public static Object[] parquet-mr_f5630_0()
{    return new Object[] { PARQUET_1_0, PARQUET_2_0 };}
public void parquet-mr_f5631_0() throws Exception
{    reader = ParquetFileReader.open(conf, file);    ParquetMetadata meta = reader.getFooter();    ccmd = meta.getBlocks().get(0).getColumns();    dictionaries = reader.getDictionaryReader(meta.getBlocks().get(0));}
public void parquet-mr_f5632_0() throws Exception
{    reader.close();}
public void parquet-mr_f5633_0() throws Exception
{    switch(version) {        case PARQUET_1_0:            testDictionaryEncodedColumnsV1();            break;        case PARQUET_2_0:            testDictionaryEncodedColumnsV2();            break;    }}
private void parquet-mr_f5634_0() throws Exception
{    Set<String> dictionaryEncodedColumns = new HashSet<String>(Arrays.asList("binary_field", "single_value_field", "optional_single_value_field", "int32_field", "int64_field", "double_field", "float_field", "int96_field"));    for (ColumnChunkMetaData column : ccmd) {        String name = column.getPath().toDotString();        if (dictionaryEncodedColumns.contains(name)) {            assertTrue("Column should be dictionary encoded: " + name, column.getEncodings().contains(Encoding.PLAIN_DICTIONARY));            assertFalse("Column should not have plain data pages" + name, column.getEncodings().contains(Encoding.PLAIN));        } else {            assertTrue("Column should have plain encoding: " + name, column.getEncodings().contains(Encoding.PLAIN));            if (name.startsWith("fallback")) {                assertTrue("Column should have some dictionary encoding: " + name, column.getEncodings().contains(Encoding.PLAIN_DICTIONARY));            } else {                assertFalse("Column should have no dictionary encoding: " + name, column.getEncodings().contains(Encoding.PLAIN_DICTIONARY));            }        }    }}
private void parquet-mr_f5635_0() throws Exception
{    Set<String> dictionaryEncodedColumns = new HashSet<String>(Arrays.asList("binary_field", "single_value_field", "optional_single_value_field", "fixed_field", "int32_field", "int64_field", "double_field", "float_field", "int96_field"));    for (ColumnChunkMetaData column : ccmd) {        EncodingStats encStats = column.getEncodingStats();        String name = column.getPath().toDotString();        if (dictionaryEncodedColumns.contains(name)) {            assertTrue("Column should have dictionary pages: " + name, encStats.hasDictionaryPages());            assertTrue("Column should have dictionary encoded pages: " + name, encStats.hasDictionaryEncodedPages());            assertFalse("Column should not have non-dictionary encoded pages: " + name, encStats.hasNonDictionaryEncodedPages());        } else {            assertTrue("Column should have non-dictionary encoded pages: " + name, encStats.hasNonDictionaryEncodedPages());            if (name.startsWith("fallback")) {                assertTrue("Column should have dictionary pages: " + name, encStats.hasDictionaryPages());                assertTrue("Column should have dictionary encoded pages: " + name, encStats.hasDictionaryEncodedPages());            } else {                assertFalse("Column should not have dictionary pages: " + name, encStats.hasDictionaryPages());                assertFalse("Column should not have dictionary encoded pages: " + name, encStats.hasDictionaryEncodedPages());            }        }    }}
public void parquet-mr_f5636_0() throws Exception
{    BinaryColumn b = binaryColumn("binary_field");    FilterPredicate pred = eq(b, Binary.fromString("c"));    assertFalse("Should not drop block for lower case letters", canDrop(pred, ccmd, dictionaries));    assertTrue("Should drop block for upper case letters", canDrop(eq(b, Binary.fromString("A")), ccmd, dictionaries));    assertFalse("Should not drop block for null", canDrop(eq(b, null), ccmd, dictionaries));}
public void parquet-mr_f5637_0() throws Exception
{    BinaryColumn b = binaryColumn("fixed_field");        if (version == PARQUET_2_0) {        assertTrue("Should drop block for -2", canDrop(eq(b, toBinary("-2", 17)), ccmd, dictionaries));    }    assertFalse("Should not drop block for -1", canDrop(eq(b, toBinary("-1", 17)), ccmd, dictionaries));    assertFalse("Should not drop block for null", canDrop(eq(b, null), ccmd, dictionaries));}
public void parquet-mr_f5638_0() throws Exception
{    BinaryColumn b = binaryColumn("int96_field");        assertFalse("Should not drop block for -2", canDrop(eq(b, toBinary("-2", 12)), ccmd, dictionaries));    assertFalse("Should not drop block for -1", canDrop(eq(b, toBinary("-1", 12)), ccmd, dictionaries));    assertFalse("Should not drop block for null", canDrop(eq(b, null), ccmd, dictionaries));}
public void parquet-mr_f5639_0() throws Exception
{    BinaryColumn sharp = binaryColumn("single_value_field");    BinaryColumn sharpAndNull = binaryColumn("optional_single_value_field");    BinaryColumn b = binaryColumn("binary_field");    assertTrue("Should drop block with only the excluded value", canDrop(notEq(sharp, Binary.fromString("sharp")), ccmd, dictionaries));    assertFalse("Should not drop block with any other value", canDrop(notEq(sharp, Binary.fromString("applause")), ccmd, dictionaries));    assertFalse("Should not drop block with only the excluded value and null", canDrop(notEq(sharpAndNull, Binary.fromString("sharp")), ccmd, dictionaries));    assertFalse("Should not drop block with any other value", canDrop(notEq(sharpAndNull, Binary.fromString("applause")), ccmd, dictionaries));    assertFalse("Should not drop block with a known value", canDrop(notEq(b, Binary.fromString("x")), ccmd, dictionaries));    assertFalse("Should not drop block with a known value", canDrop(notEq(b, Binary.fromString("B")), ccmd, dictionaries));    assertFalse("Should not drop block for null", canDrop(notEq(b, null), ccmd, dictionaries));}
public void parquet-mr_f5640_0() throws Exception
{    IntColumn i32 = intColumn("int32_field");    int lowest = Integer.MAX_VALUE;    for (int value : intValues) {        lowest = Math.min(lowest, value);    }    assertTrue("Should drop: < lowest value", canDrop(lt(i32, lowest), ccmd, dictionaries));    assertFalse("Should not drop: < (lowest value + 1)", canDrop(lt(i32, lowest + 1), ccmd, dictionaries));    assertFalse("Should not drop: contains matching values", canDrop(lt(i32, Integer.MAX_VALUE), ccmd, dictionaries));}
public void parquet-mr_f5641_0() throws Exception
{    BinaryColumn fixed = binaryColumn("fixed_field");        if (version == PARQUET_2_0) {        assertTrue("Should drop: < lowest value", canDrop(lt(fixed, DECIMAL_VALUES[0]), ccmd, dictionaries));    }    assertFalse("Should not drop: < 2nd lowest value", canDrop(lt(fixed, DECIMAL_VALUES[1]), ccmd, dictionaries));}
public void parquet-mr_f5642_0() throws Exception
{    LongColumn i64 = longColumn("int64_field");    long lowest = Long.MAX_VALUE;    for (long value : longValues) {        lowest = Math.min(lowest, value);    }    assertTrue("Should drop: <= lowest - 1", canDrop(ltEq(i64, lowest - 1), ccmd, dictionaries));    assertFalse("Should not drop: <= lowest", canDrop(ltEq(i64, lowest), ccmd, dictionaries));    assertFalse("Should not drop: contains matching values", canDrop(ltEq(i64, Long.MAX_VALUE), ccmd, dictionaries));}
public void parquet-mr_f5643_0() throws Exception
{    FloatColumn f = floatColumn("float_field");    float highest = Float.MIN_VALUE;    for (int value : intValues) {        highest = Math.max(highest, toFloat(value));    }    assertTrue("Should drop: > highest value", canDrop(gt(f, highest), ccmd, dictionaries));    assertFalse("Should not drop: > (highest value - 1.0)", canDrop(gt(f, highest - 1.0f), ccmd, dictionaries));    assertFalse("Should not drop: contains matching values", canDrop(gt(f, Float.MIN_VALUE), ccmd, dictionaries));}
public void parquet-mr_f5644_0() throws Exception
{    DoubleColumn d = doubleColumn("double_field");    double highest = Double.MIN_VALUE;    for (int value : intValues) {        highest = Math.max(highest, toDouble(value));    }    assertTrue("Should drop: >= highest + 0.00000001", canDrop(gtEq(d, highest + 0.00000001), ccmd, dictionaries));    assertFalse("Should not drop: >= highest", canDrop(gtEq(d, highest), ccmd, dictionaries));    assertFalse("Should not drop: contains matching values", canDrop(gtEq(d, Double.MIN_VALUE), ccmd, dictionaries));}
public void parquet-mr_f5645_0() throws Exception
{    BinaryColumn col = binaryColumn("binary_field");        FilterPredicate B = eq(col, Binary.fromString("B"));    FilterPredicate C = eq(col, Binary.fromString("C"));        FilterPredicate x = eq(col, Binary.fromString("x"));    FilterPredicate y = eq(col, Binary.fromString("y"));    assertTrue("Should drop when either predicate must be false", canDrop(and(B, y), ccmd, dictionaries));    assertTrue("Should drop when either predicate must be false", canDrop(and(x, C), ccmd, dictionaries));    assertTrue("Should drop when either predicate must be false", canDrop(and(B, C), ccmd, dictionaries));    assertFalse("Should not drop when either predicate could be true", canDrop(and(x, y), ccmd, dictionaries));}
public void parquet-mr_f5646_0() throws Exception
{    BinaryColumn col = binaryColumn("binary_field");        FilterPredicate B = eq(col, Binary.fromString("B"));    FilterPredicate C = eq(col, Binary.fromString("C"));        FilterPredicate x = eq(col, Binary.fromString("x"));    FilterPredicate y = eq(col, Binary.fromString("y"));    assertFalse("Should not drop when one predicate could be true", canDrop(or(B, y), ccmd, dictionaries));    assertFalse("Should not drop when one predicate could be true", canDrop(or(x, C), ccmd, dictionaries));    assertTrue("Should drop when both predicates must be false", canDrop(or(B, C), ccmd, dictionaries));    assertFalse("Should not drop when one predicate could be true", canDrop(or(x, y), ccmd, dictionaries));}
public void parquet-mr_f5647_0() throws Exception
{    InInt32UDP dropabble = new InInt32UDP(ImmutableSet.of(42));    InInt32UDP undroppable = new InInt32UDP(ImmutableSet.of(205));    assertTrue("Should drop block for non-matching UDP", canDrop(userDefined(intColumn("int32_field"), dropabble), ccmd, dictionaries));    assertFalse("Should not drop block for matching UDP", canDrop(userDefined(intColumn("int32_field"), undroppable), ccmd, dictionaries));}
public void parquet-mr_f5648_0() throws Exception
{    InInt32UDP droppable = new InInt32UDP(ImmutableSet.of(42));    InInt32UDP undroppable = new InInt32UDP(ImmutableSet.of(205));    Set<Integer> allValues = ImmutableSet.copyOf(Arrays.asList(ArrayUtils.toObject(intValues)));    InInt32UDP completeMatch = new InInt32UDP(allValues);    FilterPredicate inverse = LogicalInverseRewriter.rewrite(not(userDefined(intColumn("int32_field"), droppable)));    FilterPredicate inverse1 = LogicalInverseRewriter.rewrite(not(userDefined(intColumn("int32_field"), undroppable)));    FilterPredicate inverse2 = LogicalInverseRewriter.rewrite(not(userDefined(intColumn("int32_field"), completeMatch)));    assertFalse("Should not drop block for inverse of non-matching UDP", canDrop(inverse, ccmd, dictionaries));    assertFalse("Should not drop block for inverse of UDP with some matches", canDrop(inverse1, ccmd, dictionaries));    assertTrue("Should drop block for inverse of UDP with all matches", canDrop(inverse2, ccmd, dictionaries));}
public void parquet-mr_f5649_0() throws Exception
{    IntColumn plain = intColumn("plain_int32_field");    DictionaryPageReadStore dictionaryStore = mock(DictionaryPageReadStore.class);    assertFalse("Should never drop block using plain encoding", canDrop(eq(plain, -10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(lt(plain, -10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(ltEq(plain, -10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(gt(plain, nElements + 10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(gtEq(plain, nElements + 10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(notEq(plain, nElements + 10), ccmd, dictionaryStore));    verifyZeroInteractions(dictionaryStore);}
public void parquet-mr_f5650_0() throws Exception
{    IntColumn plain = intColumn("fallback_binary_field");    DictionaryPageReadStore dictionaryStore = mock(DictionaryPageReadStore.class);    assertFalse("Should never drop block using plain encoding", canDrop(eq(plain, -10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(lt(plain, -10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(ltEq(plain, -10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(gt(plain, nElements + 10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(gtEq(plain, nElements + 10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(notEq(plain, nElements + 10), ccmd, dictionaryStore));    verifyZeroInteractions(dictionaryStore);}
public void parquet-mr_f5651_0() throws Exception
{    BinaryColumn b = binaryColumn("missing_column");    assertTrue("Should drop block for non-null query", canDrop(eq(b, Binary.fromString("any")), ccmd, dictionaries));    assertFalse("Should not drop block null query", canDrop(eq(b, null), ccmd, dictionaries));}
public void parquet-mr_f5652_0() throws Exception
{    BinaryColumn b = binaryColumn("missing_column");    assertFalse("Should not drop block for non-null query", canDrop(notEq(b, Binary.fromString("any")), ccmd, dictionaries));    assertTrue("Should not drop block null query", canDrop(notEq(b, null), ccmd, dictionaries));}
public void parquet-mr_f5653_0() throws Exception
{    BinaryColumn b = binaryColumn("missing_column");    assertTrue("Should drop block for any non-null query", canDrop(lt(b, Binary.fromString("any")), ccmd, dictionaries));}
public void parquet-mr_f5654_0() throws Exception
{    BinaryColumn b = binaryColumn("missing_column");    assertTrue("Should drop block for any non-null query", canDrop(ltEq(b, Binary.fromString("any")), ccmd, dictionaries));}
public void parquet-mr_f5655_0() throws Exception
{    BinaryColumn b = binaryColumn("missing_column");    assertTrue("Should drop block for any non-null query", canDrop(gt(b, Binary.fromString("any")), ccmd, dictionaries));}
public void parquet-mr_f5656_0() throws Exception
{    BinaryColumn b = binaryColumn("missing_column");    assertTrue("Should drop block for any non-null query", canDrop(gtEq(b, Binary.fromString("any")), ccmd, dictionaries));}
public void parquet-mr_f5657_0() throws Exception
{    InInt32UDP nullRejecting = new InInt32UDP(ImmutableSet.of(42));    InInt32UDP nullAccepting = new InInt32UDP(Sets.newHashSet((Integer) null));    IntColumn fake = intColumn("missing_column");    assertTrue("Should drop block for null rejecting udp", canDrop(userDefined(fake, nullRejecting), ccmd, dictionaries));    assertFalse("Should not drop block for null accepting udp", canDrop(userDefined(fake, nullAccepting), ccmd, dictionaries));}
public void parquet-mr_f5658_0() throws Exception
{    InInt32UDP nullRejecting = new InInt32UDP(ImmutableSet.of(42));    InInt32UDP nullAccepting = new InInt32UDP(Sets.newHashSet((Integer) null));    IntColumn fake = intColumn("missing_column");    assertTrue("Should drop block for null accepting udp", canDrop(LogicalInverseRewriter.rewrite(not(userDefined(fake, nullAccepting))), ccmd, dictionaries));    assertFalse("Should not drop block for null rejecting udp", canDrop(LogicalInverseRewriter.rewrite(not(userDefined(fake, nullRejecting))), ccmd, dictionaries));}
public boolean parquet-mr_f5659_0(Integer value)
{    return ints.contains(value);}
public boolean parquet-mr_f5660_0(Statistics<Integer> statistics)
{    return false;}
public boolean parquet-mr_f5661_0(Statistics<Integer> statistics)
{    return false;}
private static double parquet-mr_f5662_0(int value)
{    return (value * 1.0);}
private static float parquet-mr_f5663_0(int value)
{    return (float) (value * 2.0);}
public Double parquet-mr_f5664_0()
{    return lon;}
public Double parquet-mr_f5665_0()
{    return lat;}
public boolean parquet-mr_f5666_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Location location = (Location) o;    if (lat != null ? !lat.equals(location.lat) : location.lat != null)        return false;    if (lon != null ? !lon.equals(location.lon) : location.lon != null)        return false;    return true;}
public int parquet-mr_f5667_0()
{    int result = lon != null ? lon.hashCode() : 0;    result = 31 * result + (lat != null ? lat.hashCode() : 0);    return result;}
public String parquet-mr_f5668_0()
{    return "Location [lon=" + lon + ", lat=" + lat + "]";}
public long parquet-mr_f5669_0()
{    return number;}
public String parquet-mr_f5670_0()
{    return kind;}
public boolean parquet-mr_f5671_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    PhoneNumber that = (PhoneNumber) o;    if (number != that.number)        return false;    if (kind != null ? !kind.equals(that.kind) : that.kind != null)        return false;    return true;}
public int parquet-mr_f5672_0()
{    int result = (int) (number ^ (number >>> 32));    result = 31 * result + (kind != null ? kind.hashCode() : 0);    return result;}
public String parquet-mr_f5673_0()
{    return "PhoneNumber [number=" + number + ", kind=" + kind + "]";}
public long parquet-mr_f5674_0()
{    return id;}
public String parquet-mr_f5675_0()
{    return name;}
public List<PhoneNumber> parquet-mr_f5676_0()
{    return phoneNumbers;}
public Location parquet-mr_f5677_0()
{    return location;}
public boolean parquet-mr_f5678_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    User user = (User) o;    if (id != user.id)        return false;    if (location != null ? !location.equals(user.location) : user.location != null)        return false;    if (name != null ? !name.equals(user.name) : user.name != null)        return false;    if (phoneNumbers != null ? !phoneNumbers.equals(user.phoneNumbers) : user.phoneNumbers != null)        return false;    return true;}
public int parquet-mr_f5679_0()
{    int result = (int) (id ^ (id >>> 32));    result = 31 * result + (name != null ? name.hashCode() : 0);    result = 31 * result + (phoneNumbers != null ? phoneNumbers.hashCode() : 0);    result = 31 * result + (location != null ? location.hashCode() : 0);    return result;}
public String parquet-mr_f5680_0()
{    return "User [id=" + id + ", name=" + name + ", phoneNumbers=" + phoneNumbers + ", location=" + location + "]";}
public static SimpleGroup parquet-mr_f5681_0(User user)
{    SimpleGroup root = new SimpleGroup(schema);    root.append("id", user.getId());    if (user.getName() != null) {        root.append("name", user.getName());    }    if (user.getPhoneNumbers() != null) {        Group phoneNumbers = root.addGroup("phoneNumbers");        for (PhoneNumber number : user.getPhoneNumbers()) {            Group phone = phoneNumbers.addGroup("phone");            phone.append("number", number.getNumber());            if (number.getKind() != null) {                phone.append("kind", number.getKind());            }        }    }    if (user.getLocation() != null) {        Group location = root.addGroup("location");        if (user.getLocation().getLon() != null) {            location.append("lon", user.getLocation().getLon());        }        if (user.getLocation().getLat() != null) {            location.append("lat", user.getLocation().getLat());        }    }    return root;}
private static User parquet-mr_f5682_0(Group root)
{    return new User(getLong(root, "id"), getString(root, "name"), getPhoneNumbers(getGroup(root, "phoneNumbers")), getLocation(getGroup(root, "location")));}
private static List<PhoneNumber> parquet-mr_f5683_0(Group phoneNumbers)
{    if (phoneNumbers == null) {        return null;    }    List<PhoneNumber> list = new ArrayList<>();    for (int i = 0, n = phoneNumbers.getFieldRepetitionCount("phone"); i < n; ++i) {        Group phone = phoneNumbers.getGroup("phone", i);        list.add(new PhoneNumber(getLong(phone, "number"), getString(phone, "kind")));    }    return list;}
private static Location parquet-mr_f5684_0(Group location)
{    if (location == null) {        return null;    }    return new Location(getDouble(location, "lon"), getDouble(location, "lat"));}
private static boolean parquet-mr_f5685_0(Group group, String field)
{    int repetition = group.getFieldRepetitionCount(field);    if (repetition == 0) {        return true;    } else if (repetition == 1) {        return false;    }    throw new AssertionError("Invalid repetitionCount " + repetition + " for field " + field + " in group " + group);}
private static Long parquet-mr_f5686_0(Group group, String field)
{    return isNull(group, field) ? null : group.getLong(field, 0);}
private static String parquet-mr_f5687_0(Group group, String field)
{    return isNull(group, field) ? null : group.getString(field, 0);}
private static Double parquet-mr_f5688_0(Group group, String field)
{    return isNull(group, field) ? null : group.getDouble(field, 0);}
private static Group parquet-mr_f5689_0(Group group, String field)
{    return isNull(group, field) ? null : group.getGroup(field, 0);}
public static File parquet-mr_f5690_0(List<User> users) throws IOException
{    File f = File.createTempFile("phonebook", ".parquet");    f.deleteOnExit();    if (!f.delete()) {        throw new IOException("couldn't delete tmp file" + f);    }    writeToFile(f, users);    return f;}
public static void parquet-mr_f5691_0(File f, List<User> users) throws IOException
{    write(ExampleParquetWriter.builder(new Path(f.getAbsolutePath())), users);}
public static void parquet-mr_f5692_0(ParquetWriter.Builder<Group, ?> builder, List<User> users) throws IOException
{    builder.config(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString());    try (ParquetWriter<Group> writer = builder.build()) {        for (User u : users) {            writer.write(groupFromUser(u));        }    }}
private static ParquetReader<Group> parquet-mr_f5693_0(Path file, Filter filter) throws IOException
{    Configuration conf = new Configuration();    GroupWriteSupport.setSchema(schema, conf);    return ParquetReader.builder(new GroupReadSupport(), file).withConf(conf).withFilter(filter).build();}
public static List<Group> parquet-mr_f5694_0(File f, Filter filter) throws IOException
{    ParquetReader<Group> reader = createReader(new Path(f.getAbsolutePath()), filter);    Group current;    List<Group> users = new ArrayList<Group>();    current = reader.read();    while (current != null) {        users.add(current);        current = reader.read();    }    return users;}
public static List<User> parquet-mr_f5695_0(ParquetReader.Builder<Group> builder) throws IOException
{    ParquetReader<Group> reader = builder.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString()).build();    List<User> users = new ArrayList<>();    for (Group group = reader.read(); group != null; group = reader.read()) {        users.add(userFromGroup(group));    }    return users;}
public static void parquet-mr_f5696_0(String[] args) throws IOException
{    File f = new File(args[0]);    writeToFile(f, TestRecordLevelFilters.makeUsers());}
public static List<User> parquet-mr_f5697_0()
{    List<User> users = new ArrayList<User>();    users.add(new User(17, null, null, null));    users.add(new User(18, "bob", null, null));    users.add(new User(19, "alice", new ArrayList<PhoneNumber>(), null));    users.add(new User(20, "thing1", Arrays.asList(new PhoneNumber(5555555555L, null)), null));    users.add(new User(27, "thing2", Arrays.asList(new PhoneNumber(1111111111L, "home")), null));    users.add(new User(28, "popular", Arrays.asList(new PhoneNumber(1111111111L, "home"), new PhoneNumber(2222222222L, null), new PhoneNumber(3333333333L, "mobile")), null));    users.add(new User(30, null, Arrays.asList(new PhoneNumber(1111111111L, "home")), null));    for (int i = 100; i < 200; i++) {        Location location = null;        if (i % 3 == 1) {            location = new Location((double) i, (double) i * 2);        }        if (i % 3 == 2) {            location = new Location((double) i, null);        }        users.add(new User(i, "p" + i, Arrays.asList(new PhoneNumber(i, "cell")), location));    }    return users;}
public static void parquet-mr_f5698_0() throws IOException
{    users = makeUsers();    phonebookFile = PhoneBookWriter.writeToFile(users);}
private static List<Group> parquet-mr_f5699_0(UserFilter f)
{    List<Group> expected = new ArrayList<Group>();    for (User u : users) {        if (f.keep(u)) {            expected.add(PhoneBookWriter.groupFromUser(u));        }    }    return expected;}
private static void parquet-mr_f5700_0(List<Group> found, UserFilter f)
{    List<Group> expected = getExpected(f);    assertEquals(expected.size(), found.size());    Iterator<Group> expectedIter = expected.iterator();    Iterator<Group> foundIter = found.iterator();    while (expectedIter.hasNext()) {        assertEquals(expectedIter.next().toString(), foundIter.next().toString());    }}
public void parquet-mr_f5701_0() throws Exception
{    List<Group> found = PhoneBookWriter.readFile(phonebookFile, FilterCompat.NOOP);    assertFilter(found, new UserFilter() {        @Override        public boolean keep(User u) {            return true;        }    });}
public boolean parquet-mr_f5702_0(User u)
{    return true;}
public void parquet-mr_f5703_0() throws Exception
{    BinaryColumn name = binaryColumn("name");    FilterPredicate pred = eq(name, Binary.fromString("no matches"));    List<Group> found = PhoneBookWriter.readFile(phonebookFile, FilterCompat.get(pred));    assertEquals(new ArrayList<Group>(), found);}
public void parquet-mr_f5704_0() throws Exception
{    BinaryColumn name = binaryColumn("name");    FilterPredicate pred = notEq(name, null);    List<Group> found = PhoneBookWriter.readFile(phonebookFile, FilterCompat.get(pred));    assertFilter(found, new UserFilter() {        @Override        public boolean keep(User u) {            return u.getName() != null;        }    });}
public boolean parquet-mr_f5705_0(User u)
{    return u.getName() != null;}
public boolean parquet-mr_f5706_0(Binary value)
{    if (value == null) {        return false;    }    return value.toStringUsingUTF8().startsWith("p");}
public boolean parquet-mr_f5707_0(Statistics<Binary> statistics)
{    return false;}
public boolean parquet-mr_f5708_0(Statistics<Binary> statistics)
{    return false;}
public boolean parquet-mr_f5709_0(Long value)
{    if (value == null) {        return false;    }    return hSet.contains(value);}
public boolean parquet-mr_f5710_0(Statistics<Long> statistics)
{    return false;}
public boolean parquet-mr_f5711_0(Statistics<Long> statistics)
{    return false;}
public void parquet-mr_f5712_0() throws Exception
{    BinaryColumn name = binaryColumn("name");    FilterPredicate pred = not(userDefined(name, StartWithP.class));    List<Group> found = PhoneBookWriter.readFile(phonebookFile, FilterCompat.get(pred));    assertFilter(found, new UserFilter() {        @Override        public boolean keep(User u) {            return u.getName() == null || !u.getName().startsWith("p");        }    });}
public boolean parquet-mr_f5713_0(User u)
{    return u.getName() == null || !u.getName().startsWith("p");}
public void parquet-mr_f5714_0() throws Exception
{    LongColumn name = longColumn("id");    final HashSet<Long> h = new HashSet<Long>();    h.add(20L);    h.add(27L);    h.add(28L);    FilterPredicate pred = userDefined(name, new SetInFilter(h));    List<Group> found = PhoneBookWriter.readFile(phonebookFile, FilterCompat.get(pred));    assertFilter(found, new UserFilter() {        @Override        public boolean keep(User u) {            return u != null && h.contains(u.getId());        }    });}
public boolean parquet-mr_f5715_0(User u)
{    return u != null && h.contains(u.getId());}
public void parquet-mr_f5716_0() throws Exception
{    BinaryColumn name = binaryColumn("name");    DoubleColumn lon = doubleColumn("location.lon");    DoubleColumn lat = doubleColumn("location.lat");    FilterPredicate pred = or(and(gt(lon, 150.0), notEq(lat, null)), eq(name, Binary.fromString("alice")));    List<Group> found = PhoneBookWriter.readFile(phonebookFile, FilterCompat.get(pred));    assertFilter(found, new UserFilter() {        @Override        public boolean keep(User u) {            String name = u.getName();            Double lat = null;            Double lon = null;            if (u.getLocation() != null) {                lat = u.getLocation().getLat();                lon = u.getLocation().getLon();            }            return (lon != null && lon > 150.0 && lat != null) || "alice".equals(name);        }    });}
public boolean parquet-mr_f5717_0(User u)
{    String name = u.getName();    Double lat = null;    Double lon = null;    if (u.getLocation() != null) {        lat = u.getLocation().getLat();        lon = u.getLocation().getLon();    }    return (lon != null && lon > 150.0 && lat != null) || "alice".equals(name);}
private static ColumnChunkMetaData parquet-mr_f5718_0(org.apache.parquet.column.statistics.Statistics<?> stats, long valueCount)
{    return ColumnChunkMetaData.get(ColumnPath.get("int", "column"), PrimitiveTypeName.INT32, CompressionCodecName.GZIP, new HashSet<Encoding>(Arrays.asList(Encoding.PLAIN)), stats, 0L, 0L, valueCount, 0L, 0L);}
private static ColumnChunkMetaData parquet-mr_f5719_0(org.apache.parquet.column.statistics.Statistics<?> stats, long valueCount)
{    return ColumnChunkMetaData.get(ColumnPath.get("double", "column"), PrimitiveTypeName.DOUBLE, CompressionCodecName.GZIP, new HashSet<Encoding>(Arrays.asList(Encoding.PLAIN)), stats, 0L, 0L, valueCount, 0L, 0L);}
public void parquet-mr_f5720_0()
{    assertTrue(canDrop(eq(intColumn, 9), columnMetas));    assertFalse(canDrop(eq(intColumn, 10), columnMetas));    assertFalse(canDrop(eq(intColumn, 100), columnMetas));    assertTrue(canDrop(eq(intColumn, 101), columnMetas));        assertTrue(canDrop(eq(intColumn, 0), nullColumnMetas));    assertTrue(canDrop(eq(missingColumn, fromString("any")), columnMetas));    assertFalse(canDrop(eq(intColumn, 50), missingMinMaxColumnMetas));    assertFalse(canDrop(eq(doubleColumn, 50.0), missingMinMaxColumnMetas));}
public void parquet-mr_f5721_0()
{    IntStatistics statsNoNulls = new IntStatistics();    statsNoNulls.setMinMax(10, 100);    statsNoNulls.setNumNulls(0);    IntStatistics statsSomeNulls = new IntStatistics();    statsSomeNulls.setMinMax(10, 100);    statsSomeNulls.setNumNulls(3);    assertTrue(canDrop(eq(intColumn, null), Arrays.asList(getIntColumnMeta(statsNoNulls, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(eq(intColumn, null), Arrays.asList(getIntColumnMeta(statsSomeNulls, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(eq(missingColumn, null), columnMetas));    assertFalse(canDrop(eq(intColumn, null), missingMinMaxColumnMetas));    assertFalse(canDrop(eq(doubleColumn, null), missingMinMaxColumnMetas));}
public void parquet-mr_f5722_0()
{    assertFalse(canDrop(notEq(intColumn, 9), columnMetas));    assertFalse(canDrop(notEq(intColumn, 10), columnMetas));    assertFalse(canDrop(notEq(intColumn, 100), columnMetas));    assertFalse(canDrop(notEq(intColumn, 101), columnMetas));    IntStatistics allSevens = new IntStatistics();    allSevens.setMinMax(7, 7);    assertTrue(canDrop(notEq(intColumn, 7), Arrays.asList(getIntColumnMeta(allSevens, 177L), getDoubleColumnMeta(doubleStats, 177L))));    allSevens.setNumNulls(100L);    assertFalse(canDrop(notEq(intColumn, 7), Arrays.asList(getIntColumnMeta(allSevens, 177L), getDoubleColumnMeta(doubleStats, 177L))));    allSevens.setNumNulls(177L);    assertFalse(canDrop(notEq(intColumn, 7), Arrays.asList(getIntColumnMeta(allSevens, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(notEq(missingColumn, fromString("any")), columnMetas));    assertFalse(canDrop(notEq(intColumn, 50), missingMinMaxColumnMetas));    assertFalse(canDrop(notEq(doubleColumn, 50.0), missingMinMaxColumnMetas));}
public void parquet-mr_f5723_0()
{    IntStatistics statsNoNulls = new IntStatistics();    statsNoNulls.setMinMax(10, 100);    statsNoNulls.setNumNulls(0);    IntStatistics statsSomeNulls = new IntStatistics();    statsSomeNulls.setMinMax(10, 100);    statsSomeNulls.setNumNulls(3);    IntStatistics statsAllNulls = new IntStatistics();    statsAllNulls.setMinMax(0, 0);    statsAllNulls.setNumNulls(177);    assertFalse(canDrop(notEq(intColumn, null), Arrays.asList(getIntColumnMeta(statsNoNulls, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(notEq(intColumn, null), Arrays.asList(getIntColumnMeta(statsSomeNulls, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertTrue(canDrop(notEq(intColumn, null), Arrays.asList(getIntColumnMeta(statsAllNulls, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertTrue(canDrop(notEq(missingColumn, null), columnMetas));    assertFalse(canDrop(notEq(intColumn, null), missingMinMaxColumnMetas));    assertFalse(canDrop(notEq(doubleColumn, null), missingMinMaxColumnMetas));}
public void parquet-mr_f5724_0()
{    assertTrue(canDrop(lt(intColumn, 9), columnMetas));    assertTrue(canDrop(lt(intColumn, 10), columnMetas));    assertFalse(canDrop(lt(intColumn, 100), columnMetas));    assertFalse(canDrop(lt(intColumn, 101), columnMetas));    assertTrue(canDrop(lt(intColumn, 0), nullColumnMetas));    assertTrue(canDrop(lt(intColumn, 7), nullColumnMetas));    assertTrue(canDrop(lt(missingColumn, fromString("any")), columnMetas));    assertFalse(canDrop(lt(intColumn, 0), missingMinMaxColumnMetas));    assertFalse(canDrop(lt(doubleColumn, 0.0), missingMinMaxColumnMetas));}
public void parquet-mr_f5725_0()
{    assertTrue(canDrop(ltEq(intColumn, 9), columnMetas));    assertFalse(canDrop(ltEq(intColumn, 10), columnMetas));    assertFalse(canDrop(ltEq(intColumn, 100), columnMetas));    assertFalse(canDrop(ltEq(intColumn, 101), columnMetas));    assertTrue(canDrop(ltEq(intColumn, 0), nullColumnMetas));    assertTrue(canDrop(ltEq(intColumn, 7), nullColumnMetas));    assertTrue(canDrop(ltEq(missingColumn, fromString("any")), columnMetas));    assertFalse(canDrop(ltEq(intColumn, -1), missingMinMaxColumnMetas));    assertFalse(canDrop(ltEq(doubleColumn, -0.1), missingMinMaxColumnMetas));}
public void parquet-mr_f5726_0()
{    assertFalse(canDrop(gt(intColumn, 9), columnMetas));    assertFalse(canDrop(gt(intColumn, 10), columnMetas));    assertTrue(canDrop(gt(intColumn, 100), columnMetas));    assertTrue(canDrop(gt(intColumn, 101), columnMetas));    assertTrue(canDrop(gt(intColumn, 0), nullColumnMetas));    assertTrue(canDrop(gt(intColumn, 7), nullColumnMetas));    assertTrue(canDrop(gt(missingColumn, fromString("any")), columnMetas));    assertFalse(canDrop(gt(intColumn, 0), missingMinMaxColumnMetas));    assertFalse(canDrop(gt(doubleColumn, 0.0), missingMinMaxColumnMetas));}
public void parquet-mr_f5727_0()
{    assertFalse(canDrop(gtEq(intColumn, 9), columnMetas));    assertFalse(canDrop(gtEq(intColumn, 10), columnMetas));    assertFalse(canDrop(gtEq(intColumn, 100), columnMetas));    assertTrue(canDrop(gtEq(intColumn, 101), columnMetas));    assertTrue(canDrop(gtEq(intColumn, 0), nullColumnMetas));    assertTrue(canDrop(gtEq(intColumn, 7), nullColumnMetas));    assertTrue(canDrop(gtEq(missingColumn, fromString("any")), columnMetas));    assertFalse(canDrop(gtEq(intColumn, 1), missingMinMaxColumnMetas));    assertFalse(canDrop(gtEq(doubleColumn, 0.1), missingMinMaxColumnMetas));}
public void parquet-mr_f5728_0()
{    FilterPredicate yes = eq(intColumn, 9);    FilterPredicate no = eq(doubleColumn, 50D);    assertTrue(canDrop(and(yes, yes), columnMetas));    assertTrue(canDrop(and(yes, no), columnMetas));    assertTrue(canDrop(and(no, yes), columnMetas));    assertFalse(canDrop(and(no, no), columnMetas));}
public void parquet-mr_f5729_0()
{    FilterPredicate yes = eq(intColumn, 9);    FilterPredicate no = eq(doubleColumn, 50D);    assertTrue(canDrop(or(yes, yes), columnMetas));    assertFalse(canDrop(or(yes, no), columnMetas));    assertFalse(canDrop(or(no, yes), columnMetas));    assertFalse(canDrop(or(no, no), columnMetas));}
public boolean parquet-mr_f5730_0(Integer value)
{    if (value == null) {        return true;    }    throw new RuntimeException("this method should not be called with value != null");}
public boolean parquet-mr_f5731_0(Statistics<Integer> statistics)
{    return statistics.getMin() == 7 && statistics.getMax() == 7;}
public boolean parquet-mr_f5732_0(Statistics<Integer> statistics)
{    return statistics.getMin() == 8 && statistics.getMax() == 8;}
public boolean parquet-mr_f5733_0(Integer value)
{    if (value == null) {        return false;    }    throw new RuntimeException("this method should not be called with value != null");}
public boolean parquet-mr_f5734_0(Double value)
{    if (value == null) {        return true;    }    throw new RuntimeException("this method should not be called with value != null");}
public boolean parquet-mr_f5735_0(Statistics<Double> statistics)
{    return statistics.getMin() <= 0.0;}
public boolean parquet-mr_f5736_0(Statistics<Double> statistics)
{    return statistics.getMin() > 0.0;}
public void parquet-mr_f5737_0()
{    FilterPredicate pred = userDefined(intColumn, SevensAndEightsUdp.class);    FilterPredicate invPred = LogicalInverseRewriter.rewrite(not(userDefined(intColumn, SevensAndEightsUdp.class)));    FilterPredicate udpDropMissingColumn = userDefined(missingColumn2, DropNullUdp.class);    FilterPredicate invUdpDropMissingColumn = LogicalInverseRewriter.rewrite(not(userDefined(missingColumn2, DropNullUdp.class)));    FilterPredicate udpKeepMissingColumn = userDefined(missingColumn2, SevensAndEightsUdp.class);    FilterPredicate invUdpKeepMissingColumn = LogicalInverseRewriter.rewrite(not(userDefined(missingColumn2, SevensAndEightsUdp.class)));    FilterPredicate allPositivePred = userDefined(doubleColumn, AllPositiveUdp.class);    IntStatistics seven = new IntStatistics();    seven.setMinMax(7, 7);    IntStatistics eight = new IntStatistics();    eight.setMinMax(8, 8);    IntStatistics neither = new IntStatistics();    neither.setMinMax(1, 2);    assertTrue(canDrop(pred, Arrays.asList(getIntColumnMeta(seven, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(pred, Arrays.asList(getIntColumnMeta(eight, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(pred, Arrays.asList(getIntColumnMeta(neither, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(invPred, Arrays.asList(getIntColumnMeta(seven, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertTrue(canDrop(invPred, Arrays.asList(getIntColumnMeta(eight, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(invPred, Arrays.asList(getIntColumnMeta(neither, 177L), getDoubleColumnMeta(doubleStats, 177L))));        assertTrue(canDrop(udpDropMissingColumn, Arrays.asList(getIntColumnMeta(seven, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertTrue(canDrop(udpDropMissingColumn, Arrays.asList(getIntColumnMeta(eight, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertTrue(canDrop(udpDropMissingColumn, Arrays.asList(getIntColumnMeta(neither, 177L), getDoubleColumnMeta(doubleStats, 177L))));        assertFalse(canDrop(invUdpDropMissingColumn, Arrays.asList(getIntColumnMeta(seven, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(invUdpDropMissingColumn, Arrays.asList(getIntColumnMeta(eight, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(invUdpDropMissingColumn, Arrays.asList(getIntColumnMeta(neither, 177L), getDoubleColumnMeta(doubleStats, 177L))));        assertFalse(canDrop(udpKeepMissingColumn, Arrays.asList(getIntColumnMeta(seven, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(udpKeepMissingColumn, Arrays.asList(getIntColumnMeta(eight, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(udpKeepMissingColumn, Arrays.asList(getIntColumnMeta(neither, 177L), getDoubleColumnMeta(doubleStats, 177L))));        assertTrue(canDrop(invUdpKeepMissingColumn, Arrays.asList(getIntColumnMeta(seven, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertTrue(canDrop(invUdpKeepMissingColumn, Arrays.asList(getIntColumnMeta(eight, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertTrue(canDrop(invUdpKeepMissingColumn, Arrays.asList(getIntColumnMeta(neither, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(allPositivePred, missingMinMaxColumnMetas));}
public void parquet-mr_f5738_0()
{    List<ColumnChunkMetaData> columnMetas = Arrays.asList(getDoubleColumnMeta(new DoubleStatistics(), 0L), getIntColumnMeta(new IntStatistics(), 0L));    FilterPredicate pred = and(not(eq(doubleColumn, 12.0)), eq(intColumn, 17));    try {        canDrop(pred, columnMetas);        fail("This should throw");    } catch (IllegalArgumentException e) {        assertEquals("This predicate contains a not! Did you forget to run this predicate through LogicalInverseRewriter?" + " not(eq(double.column, 12.0))", e.getMessage());    }}
public void parquet-mr_f5739_0() throws Exception
{    File file = temp.newFile("test.parquet");    this.path = new Path(file.toString());    MessageType type = Types.buildMessage().required(INT64).named("id").required(BINARY).as(UTF8).named("data").named("test");    SimpleGroupFactory factory = new SimpleGroupFactory(type);    ParquetWriter<Group> writer = ExampleParquetWriter.builder(path).withWriteMode(ParquetFileWriter.Mode.OVERWRITE).withType(type).build();    try {        for (long i = 0; i < 1000; i += 1) {            Group g = factory.newGroup();            g.add(0, i);            g.add(1, "data-" + i);            writer.write(g);        }    } finally {        writer.close();    }}
public void parquet-mr_f5740_0() throws Exception
{    assertEquals(500, countFilteredRecords(path, lt(longColumn("id"), 500L)));}
public void parquet-mr_f5741_0() throws Exception
{    assertEquals(0, countFilteredRecords(path, lt(longColumn("missing"), 500L)));}
public void parquet-mr_f5742_0() throws Exception
{        assertEquals(500, countFilteredRecords(path, and(lt(longColumn("id"), 500L), eq(binaryColumn("missing"), null))));    assertEquals(500, countFilteredRecords(path, and(lt(longColumn("id"), 500L), notEq(binaryColumn("missing"), fromString("any")))));    assertEquals(500, countFilteredRecords(path, and(eq(binaryColumn("missing"), null), lt(longColumn("id"), 500L))));    assertEquals(500, countFilteredRecords(path, and(notEq(binaryColumn("missing"), fromString("any")), lt(longColumn("id"), 500L))));        assertEquals(0, countFilteredRecords(path, and(lt(longColumn("id"), 500L), eq(binaryColumn("missing"), fromString("any")))));    assertEquals(0, countFilteredRecords(path, and(lt(longColumn("id"), 500L), notEq(binaryColumn("missing"), null))));    assertEquals(0, countFilteredRecords(path, and(lt(longColumn("id"), 500L), lt(doubleColumn("missing"), 33.33))));    assertEquals(0, countFilteredRecords(path, and(lt(longColumn("id"), 500L), ltEq(doubleColumn("missing"), 33.33))));    assertEquals(0, countFilteredRecords(path, and(lt(longColumn("id"), 500L), gt(doubleColumn("missing"), 33.33))));    assertEquals(0, countFilteredRecords(path, and(lt(longColumn("id"), 500L), gtEq(doubleColumn("missing"), 33.33))));    assertEquals(0, countFilteredRecords(path, and(eq(binaryColumn("missing"), fromString("any")), lt(longColumn("id"), 500L))));    assertEquals(0, countFilteredRecords(path, and(notEq(binaryColumn("missing"), null), lt(longColumn("id"), 500L))));    assertEquals(0, countFilteredRecords(path, and(lt(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));    assertEquals(0, countFilteredRecords(path, and(ltEq(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));    assertEquals(0, countFilteredRecords(path, and(gt(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));    assertEquals(0, countFilteredRecords(path, and(gtEq(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));}
public void parquet-mr_f5743_0() throws Exception
{        assertEquals(500, countFilteredRecords(path, or(lt(longColumn("id"), 500L), eq(binaryColumn("missing"), fromString("any")))));    assertEquals(500, countFilteredRecords(path, or(lt(longColumn("id"), 500L), notEq(binaryColumn("missing"), null))));    assertEquals(500, countFilteredRecords(path, or(lt(longColumn("id"), 500L), lt(doubleColumn("missing"), 33.33))));    assertEquals(500, countFilteredRecords(path, or(lt(longColumn("id"), 500L), ltEq(doubleColumn("missing"), 33.33))));    assertEquals(500, countFilteredRecords(path, or(lt(longColumn("id"), 500L), gt(doubleColumn("missing"), 33.33))));    assertEquals(500, countFilteredRecords(path, or(lt(longColumn("id"), 500L), gtEq(doubleColumn("missing"), 33.33))));    assertEquals(500, countFilteredRecords(path, or(eq(binaryColumn("missing"), fromString("any")), lt(longColumn("id"), 500L))));    assertEquals(500, countFilteredRecords(path, or(notEq(binaryColumn("missing"), null), lt(longColumn("id"), 500L))));    assertEquals(500, countFilteredRecords(path, or(lt(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));    assertEquals(500, countFilteredRecords(path, or(ltEq(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));    assertEquals(500, countFilteredRecords(path, or(gt(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));    assertEquals(500, countFilteredRecords(path, or(gtEq(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));        assertEquals(1000, countFilteredRecords(path, or(lt(longColumn("id"), 500L), eq(binaryColumn("missing"), null))));    assertEquals(1000, countFilteredRecords(path, or(lt(longColumn("id"), 500L), notEq(binaryColumn("missing"), fromString("any")))));    assertEquals(1000, countFilteredRecords(path, or(eq(binaryColumn("missing"), null), lt(longColumn("id"), 500L))));    assertEquals(1000, countFilteredRecords(path, or(notEq(binaryColumn("missing"), fromString("any")), lt(longColumn("id"), 500L))));}
public static long parquet-mr_f5744_0(Path path, FilterPredicate pred) throws IOException
{    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), path).withFilter(FilterCompat.get(pred)).build();    long count = 0;    try {        while (reader.read() != null) {            count += 1;        }    } finally {        reader.close();    }    return count;}
public void parquet-mr_f5745_0() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    PageType type = PageType.DATA_PAGE;    int compSize = 10;    int uncSize = 20;    PageHeader pageHeader = new PageHeader(type, uncSize, compSize);    writePageHeader(pageHeader, out);    PageHeader readPageHeader = readPageHeader(new ByteArrayInputStream(out.toByteArray()));    assertEquals(pageHeader, readPageHeader);}
public void parquet-mr_f5746_0()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    List<SchemaElement> parquetSchema = parquetMetadataConverter.toParquetSchema(Paper.schema);    MessageType schema = parquetMetadataConverter.fromParquetSchema(parquetSchema, null);    assertEquals(Paper.schema, schema);}
public void parquet-mr_f5747_0()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    List<SchemaElement> schemaElements = parquetMetadataConverter.toParquetSchema(Types.buildMessage().required(PrimitiveTypeName.BINARY).as(OriginalType.DECIMAL).precision(9).scale(2).named("aBinaryDecimal").optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY).length(4).as(OriginalType.DECIMAL).precision(9).scale(2).named("aFixedDecimal").named("Message"));    List<SchemaElement> expected = Lists.newArrayList(new SchemaElement("Message").setNum_children(2), new SchemaElement("aBinaryDecimal").setRepetition_type(FieldRepetitionType.REQUIRED).setType(Type.BYTE_ARRAY).setConverted_type(ConvertedType.DECIMAL).setLogicalType(LogicalType.DECIMAL(new DecimalType(2, 9))).setPrecision(9).setScale(2), new SchemaElement("aFixedDecimal").setRepetition_type(FieldRepetitionType.OPTIONAL).setType(Type.FIXED_LEN_BYTE_ARRAY).setType_length(4).setConverted_type(ConvertedType.DECIMAL).setLogicalType(LogicalType.DECIMAL(new DecimalType(2, 9))).setPrecision(9).setScale(2));    Assert.assertEquals(expected, schemaElements);}
public void parquet-mr_f5748_0()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    MessageType expected = Types.buildMessage().required(PrimitiveTypeName.BINARY).as(OriginalType.DECIMAL).precision(9).scale(2).named("aBinaryDecimal").named("Message");    List<SchemaElement> parquetSchema = parquetMetadataConverter.toParquetSchema(expected);            parquetSchema.get(1).setLogicalType(null);    MessageType schema = parquetMetadataConverter.fromParquetSchema(parquetSchema, null);    assertEquals(expected, schema);}
public void parquet-mr_f5749_0()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    MessageType schema = Types.buildMessage().required(PrimitiveTypeName.BINARY).as(OriginalType.DECIMAL).precision(9).scale(2).named("aBinary").named("Message");    MessageType expected = Types.buildMessage().required(PrimitiveTypeName.BINARY).as(LogicalTypeAnnotation.jsonType()).named("aBinary").named("Message");    List<SchemaElement> parquetSchema = parquetMetadataConverter.toParquetSchema(schema);        parquetSchema.get(1).setConverted_type(ConvertedType.JSON);    MessageType actual = parquetMetadataConverter.fromParquetSchema(parquetSchema, null);    assertEquals(expected, actual);}
public void parquet-mr_f5750_0()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    MessageType expected = Types.buildMessage().required(PrimitiveTypeName.INT64).as(timestampType(false, MILLIS)).named("aTimestampNonUtcMillis").required(PrimitiveTypeName.INT64).as(timestampType(true, MILLIS)).named("aTimestampUtcMillis").required(PrimitiveTypeName.INT64).as(timestampType(false, MICROS)).named("aTimestampNonUtcMicros").required(PrimitiveTypeName.INT64).as(timestampType(true, MICROS)).named("aTimestampUtcMicros").required(PrimitiveTypeName.INT64).as(timestampType(false, NANOS)).named("aTimestampNonUtcNanos").required(PrimitiveTypeName.INT64).as(timestampType(true, NANOS)).named("aTimestampUtcNanos").required(PrimitiveTypeName.INT32).as(timeType(false, MILLIS)).named("aTimeNonUtcMillis").required(PrimitiveTypeName.INT32).as(timeType(true, MILLIS)).named("aTimeUtcMillis").required(PrimitiveTypeName.INT64).as(timeType(false, MICROS)).named("aTimeNonUtcMicros").required(PrimitiveTypeName.INT64).as(timeType(true, MICROS)).named("aTimeUtcMicros").required(PrimitiveTypeName.INT64).as(timeType(false, NANOS)).named("aTimeNonUtcNanos").required(PrimitiveTypeName.INT64).as(timeType(true, NANOS)).named("aTimeUtcNanos").named("Message");    List<SchemaElement> parquetSchema = parquetMetadataConverter.toParquetSchema(expected);    MessageType schema = parquetMetadataConverter.fromParquetSchema(parquetSchema, null);    assertEquals(expected, schema);}
public void parquet-mr_f5751_0()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    assertEquals(ConvertedType.UTF8, parquetMetadataConverter.convertToConvertedType(stringType()));    assertEquals(ConvertedType.ENUM, parquetMetadataConverter.convertToConvertedType(enumType()));    assertEquals(ConvertedType.INT_8, parquetMetadataConverter.convertToConvertedType(intType(8, true)));    assertEquals(ConvertedType.INT_16, parquetMetadataConverter.convertToConvertedType(intType(16, true)));    assertEquals(ConvertedType.INT_32, parquetMetadataConverter.convertToConvertedType(intType(32, true)));    assertEquals(ConvertedType.INT_64, parquetMetadataConverter.convertToConvertedType(intType(64, true)));    assertEquals(ConvertedType.UINT_8, parquetMetadataConverter.convertToConvertedType(intType(8, false)));    assertEquals(ConvertedType.UINT_16, parquetMetadataConverter.convertToConvertedType(intType(16, false)));    assertEquals(ConvertedType.UINT_32, parquetMetadataConverter.convertToConvertedType(intType(32, false)));    assertEquals(ConvertedType.UINT_64, parquetMetadataConverter.convertToConvertedType(intType(64, false)));    assertEquals(ConvertedType.DECIMAL, parquetMetadataConverter.convertToConvertedType(decimalType(8, 16)));    assertEquals(ConvertedType.TIMESTAMP_MILLIS, parquetMetadataConverter.convertToConvertedType(timestampType(true, MILLIS)));    assertEquals(ConvertedType.TIMESTAMP_MICROS, parquetMetadataConverter.convertToConvertedType(timestampType(true, MICROS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timestampType(true, NANOS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timestampType(false, MILLIS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timestampType(false, MICROS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timestampType(false, NANOS)));    assertEquals(ConvertedType.TIME_MILLIS, parquetMetadataConverter.convertToConvertedType(timeType(true, MILLIS)));    assertEquals(ConvertedType.TIME_MICROS, parquetMetadataConverter.convertToConvertedType(timeType(true, MICROS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timeType(true, NANOS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timeType(false, MILLIS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timeType(false, MICROS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timeType(false, NANOS)));    assertEquals(ConvertedType.DATE, parquetMetadataConverter.convertToConvertedType(dateType()));    assertEquals(ConvertedType.INTERVAL, parquetMetadataConverter.convertToConvertedType(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation.getInstance()));    assertEquals(ConvertedType.JSON, parquetMetadataConverter.convertToConvertedType(jsonType()));    assertEquals(ConvertedType.BSON, parquetMetadataConverter.convertToConvertedType(bsonType()));    assertEquals(ConvertedType.LIST, parquetMetadataConverter.convertToConvertedType(listType()));    assertEquals(ConvertedType.MAP, parquetMetadataConverter.convertToConvertedType(mapType()));    assertEquals(ConvertedType.MAP_KEY_VALUE, parquetMetadataConverter.convertToConvertedType(LogicalTypeAnnotation.MapKeyValueTypeAnnotation.getInstance()));}
public void parquet-mr_f5752_0()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    for (org.apache.parquet.column.Encoding encoding : org.apache.parquet.column.Encoding.values()) {        assertEquals(encoding, parquetMetadataConverter.getEncoding(parquetMetadataConverter.getEncoding(encoding)));    }    for (org.apache.parquet.format.Encoding encoding : org.apache.parquet.format.Encoding.values()) {        assertEquals(encoding, parquetMetadataConverter.getEncoding(parquetMetadataConverter.getEncoding(encoding)));    }    for (Repetition repetition : Repetition.values()) {        assertEquals(repetition, parquetMetadataConverter.fromParquetRepetition(parquetMetadataConverter.toParquetRepetition(repetition)));    }    for (FieldRepetitionType repetition : FieldRepetitionType.values()) {        assertEquals(repetition, parquetMetadataConverter.toParquetRepetition(parquetMetadataConverter.fromParquetRepetition(repetition)));    }    for (PrimitiveTypeName primitiveTypeName : PrimitiveTypeName.values()) {        assertEquals(primitiveTypeName, parquetMetadataConverter.getPrimitive(parquetMetadataConverter.getType(primitiveTypeName)));    }    for (Type type : Type.values()) {        assertEquals(type, parquetMetadataConverter.getType(parquetMetadataConverter.getPrimitive(type)));    }    for (OriginalType original : OriginalType.values()) {        assertEquals(original, parquetMetadataConverter.getLogicalTypeAnnotation(parquetMetadataConverter.convertToConvertedType(LogicalTypeAnnotation.fromOriginalType(original, null)), null).toOriginalType());    }    for (ConvertedType converted : ConvertedType.values()) {        assertEquals(converted, parquetMetadataConverter.convertToConvertedType(parquetMetadataConverter.getLogicalTypeAnnotation(converted, null)));    }}
private FileMetaData parquet-mr_f5753_0(long... sizes)
{    List<SchemaElement> schema = emptyList();    List<RowGroup> rowGroups = new ArrayList<RowGroup>();    long offset = 0;    for (long size : sizes) {        ColumnChunk columnChunk = new ColumnChunk(offset);        columnChunk.setMeta_data(new ColumnMetaData(INT32, Collections.<org.apache.parquet.format.Encoding>emptyList(), Collections.<String>emptyList(), UNCOMPRESSED, 10l, size * 2, size, offset));        rowGroups.add(new RowGroup(Arrays.asList(columnChunk), size, 1));        offset += size;    }    return new FileMetaData(1, schema, sizes.length, rowGroups);}
private FileMetaData parquet-mr_f5754_0(FileMetaData md, long start, long end)
{    return filterFileMetaDataByMidpoint(new FileMetaData(md), new ParquetMetadataConverter.RangeMetadataFilter(start, end));}
private FileMetaData parquet-mr_f5755_0(FileMetaData md, Long... blockStart)
{    return filterFileMetaDataByStart(new FileMetaData(md), new ParquetMetadataConverter.OffsetMetadataFilter(Sets.newHashSet((Long[]) blockStart)));}
private FileMetaData parquet-mr_f5756_0(FileMetaData md, long blockStart)
{    return filterFileMetaDataByStart(new FileMetaData(md), new ParquetMetadataConverter.OffsetMetadataFilter(Sets.newHashSet(blockStart)));}
private void parquet-mr_f5757_0(FileMetaData md, long... offsets)
{    assertEquals(offsets.length, md.row_groups.size());    for (int i = 0; i < offsets.length; i++) {        long offset = offsets[i];        RowGroup rowGroup = md.getRow_groups().get(i);        assertEquals(offset, getOffset(rowGroup));    }}
private void parquet-mr_f5758_0(FileMetaData md, long splitWidth)
{    Set<Long> offsetsFound = new TreeSet<Long>();    for (long start = 0; start < fileSize(md); start += splitWidth) {        FileMetaData filtered = filter(md, start, start + splitWidth);        for (RowGroup rg : filtered.getRow_groups()) {            long o = getOffset(rg);            if (offsetsFound.contains(o)) {                fail("found the offset twice: " + o);            } else {                offsetsFound.add(o);            }        }    }    if (offsetsFound.size() != md.row_groups.size()) {        fail("missing row groups, " + "found: " + offsetsFound + "\nexpected " + md.getRow_groups());    }}
private long parquet-mr_f5759_0(FileMetaData md)
{    long size = 0;    for (RowGroup rg : md.getRow_groups()) {        size += rg.total_byte_size;    }    return size;}
public void parquet-mr_f5760_0()
{    verifyMD(filter(metadata(50, 50, 50), 0, 50), 0);    verifyMD(filter(metadata(50, 50, 50), 50, 100), 50);    verifyMD(filter(metadata(50, 50, 50), 100, 150), 100);        verifyMD(filter(metadata(50, 50, 50), 25, 75), 0);        verifyMD(filter(metadata(50, 50, 50), 26, 75));        verifyMD(filter(metadata(50, 50, 50), 26, 76), 50);    verifyAllFilters(metadata(50, 50, 50), 10);    verifyAllFilters(metadata(50, 50, 50), 51);        verifyAllFilters(metadata(50, 50, 50), 25);    verifyAllFilters(metadata(50, 50, 50), 24);    verifyAllFilters(metadata(50, 50, 50), 26);    verifyAllFilters(metadata(50, 50, 50), 110);    verifyAllFilters(metadata(10, 50, 500), 110);    verifyAllFilters(metadata(10, 50, 500), 10);    verifyAllFilters(metadata(10, 50, 500), 600);    verifyAllFilters(metadata(11, 9, 10), 10);    verifyAllFilters(metadata(11, 9, 10), 9);    verifyAllFilters(metadata(11, 9, 10), 8);}
public void parquet-mr_f5761_0()
{    verifyMD(find(metadata(50, 50, 50), 0), 0);    verifyMD(find(metadata(50, 50, 50), 50), 50);    verifyMD(find(metadata(50, 50, 50), 100), 100);    verifyMD(find(metadata(50, 50, 50), 0L, 50L), 0, 50);    verifyMD(find(metadata(50, 50, 50), 0L, 50L, 100L), 0, 50, 100);    verifyMD(find(metadata(50, 50, 50), 50L, 100L), 50, 100);        verifyMD(find(metadata(50, 50, 50), 10));}
public void parquet-mr_f5762_0()
{            Random random = new Random(42);    for (int j = 0; j < 100; j++) {        long[] rgs = new long[random.nextInt(50)];        for (int i = 0; i < rgs.length; i++) {                        rgs[i] = random.nextInt(10000) + 1;        }                int splitSize = random.nextInt(10000) + 1;        try {            verifyAllFilters(metadata(rgs), splitSize);        } catch (AssertionError e) {            throw (AssertionError) new AssertionError("fail verifyAllFilters(metadata(" + Arrays.toString(rgs) + "), " + splitSize + ")").initCause(e);        }    }}
public void parquet-mr_f5763_0()
{    MessageType schema = parseMessageType("message test { optional binary some_null_field; }");    org.apache.parquet.hadoop.metadata.FileMetaData fileMetaData = new org.apache.parquet.hadoop.metadata.FileMetaData(schema, new HashMap<String, String>(), null);    List<BlockMetaData> blockMetaDataList = new ArrayList<BlockMetaData>();    BlockMetaData blockMetaData = new BlockMetaData();    blockMetaData.addColumn(createColumnChunkMetaData());    blockMetaDataList.add(blockMetaData);    ParquetMetadata metadata = new ParquetMetadata(fileMetaData, blockMetaDataList);    ParquetMetadata.toJSON(metadata);}
public void parquet-mr_f5764_0()
{    ParquetMetadata metadata = new ParquetMetadata(null, null);    assertEquals("{\"fileMetaData\":null,\"blocks\":null}", ParquetMetadata.toJSON(metadata));    assertEquals("{\n" + "  \"fileMetaData\" : null,\n" + "  \"blocks\" : null\n" + "}", ParquetMetadata.toPrettyJSON(metadata));}
private ColumnChunkMetaData parquet-mr_f5765_0()
{    Set<org.apache.parquet.column.Encoding> e = new HashSet<org.apache.parquet.column.Encoding>();    PrimitiveTypeName t = PrimitiveTypeName.BINARY;    ColumnPath p = ColumnPath.get("foo");    CompressionCodecName c = CompressionCodecName.GZIP;    BinaryStatistics s = new BinaryStatistics();    ColumnChunkMetaData md = ColumnChunkMetaData.get(p, t, c, e, s, 0, 0, 0, 0, 0);    return md;}
public void parquet-mr_f5766_0()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    List<org.apache.parquet.format.Encoding> formatEncodingsCopy1 = Arrays.asList(org.apache.parquet.format.Encoding.BIT_PACKED, org.apache.parquet.format.Encoding.RLE_DICTIONARY, org.apache.parquet.format.Encoding.DELTA_LENGTH_BYTE_ARRAY);    List<org.apache.parquet.format.Encoding> formatEncodingsCopy2 = Arrays.asList(org.apache.parquet.format.Encoding.BIT_PACKED, org.apache.parquet.format.Encoding.RLE_DICTIONARY, org.apache.parquet.format.Encoding.DELTA_LENGTH_BYTE_ARRAY);    Set<org.apache.parquet.column.Encoding> expected = new HashSet<org.apache.parquet.column.Encoding>();    expected.add(org.apache.parquet.column.Encoding.BIT_PACKED);    expected.add(org.apache.parquet.column.Encoding.RLE_DICTIONARY);    expected.add(org.apache.parquet.column.Encoding.DELTA_LENGTH_BYTE_ARRAY);    Set<org.apache.parquet.column.Encoding> res1 = parquetMetadataConverter.fromFormatEncodings(formatEncodingsCopy1);    Set<org.apache.parquet.column.Encoding> res2 = parquetMetadataConverter.fromFormatEncodings(formatEncodingsCopy1);    Set<org.apache.parquet.column.Encoding> res3 = parquetMetadataConverter.fromFormatEncodings(formatEncodingsCopy2);        assertEquals(expected, res1);    assertEquals(expected, res2);    assertEquals(expected, res3);        assertSame(res1, res2);    assertSame(res1, res3);        assertEquals("java.util.Collections$UnmodifiableSet", res1.getClass().getName());    assertEquals("java.util.Collections$UnmodifiableSet", res2.getClass().getName());    assertEquals("java.util.Collections$UnmodifiableSet", res3.getClass().getName());}
public void parquet-mr_f5767_0()
{    testBinaryStats(StatsHelper.V1);}
public void parquet-mr_f5768_0()
{    testBinaryStats(StatsHelper.V2);}
private void parquet-mr_f5769_0(StatsHelper helper)
{        BinaryStatistics stats = new BinaryStatistics();    stats.incrementNumNulls(3004);    byte[] min = new byte[904];    byte[] max = new byte[2388];    stats.updateStats(Binary.fromConstantByteArray(min));    stats.updateStats(Binary.fromConstantByteArray(max));    long totalLen = min.length + max.length;    Assert.assertFalse("Should not be smaller than min + max size", stats.isSmallerThan(totalLen));    Assert.assertTrue("Should be smaller than min + max size + 1", stats.isSmallerThan(totalLen + 1));    org.apache.parquet.format.Statistics formatStats = helper.toParquetStatistics(stats);    assertFalse("Min should not be set", formatStats.isSetMin());    assertFalse("Max should not be set", formatStats.isSetMax());    if (helper == StatsHelper.V2) {        Assert.assertArrayEquals("Min_value should match", min, formatStats.getMin_value());        Assert.assertArrayEquals("Max_value should match", max, formatStats.getMax_value());    }    Assert.assertEquals("Num nulls should match", 3004, formatStats.getNull_count());        stats.setMinMaxFromBytes(max, max);    formatStats = helper.toParquetStatistics(stats);    Assert.assertFalse("Min should not be set", formatStats.isSetMin());    Assert.assertFalse("Max should not be set", formatStats.isSetMax());    Assert.assertFalse("Min_value should not be set", formatStats.isSetMin_value());    Assert.assertFalse("Max_value should not be set", formatStats.isSetMax_value());    Assert.assertFalse("Num nulls should not be set", formatStats.isSetNull_count());    Statistics roundTripStats = ParquetMetadataConverter.fromParquetStatisticsInternal(Version.FULL_VERSION, formatStats, new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, ""), ParquetMetadataConverter.SortOrder.SIGNED);    Assert.assertTrue(roundTripStats.isEmpty());}
public void parquet-mr_f5770_0()
{    testIntegerStats(StatsHelper.V1);}
public void parquet-mr_f5771_0()
{    testIntegerStats(StatsHelper.V2);}
private void parquet-mr_f5772_0(StatsHelper helper)
{        IntStatistics stats = new IntStatistics();    stats.incrementNumNulls(3004);    int min = Integer.MIN_VALUE;    int max = Integer.MAX_VALUE;    stats.updateStats(min);    stats.updateStats(max);    org.apache.parquet.format.Statistics formatStats = helper.toParquetStatistics(stats);    Assert.assertEquals("Min should match", min, BytesUtils.bytesToInt(formatStats.getMin()));    Assert.assertEquals("Max should match", max, BytesUtils.bytesToInt(formatStats.getMax()));    Assert.assertEquals("Num nulls should match", 3004, formatStats.getNull_count());}
public void parquet-mr_f5773_0()
{    testLongStats(StatsHelper.V1);}
public void parquet-mr_f5774_0()
{    testLongStats(StatsHelper.V2);}
private void parquet-mr_f5775_0(StatsHelper helper)
{        LongStatistics stats = new LongStatistics();    stats.incrementNumNulls(3004);    long min = Long.MIN_VALUE;    long max = Long.MAX_VALUE;    stats.updateStats(min);    stats.updateStats(max);    org.apache.parquet.format.Statistics formatStats = helper.toParquetStatistics(stats);    Assert.assertEquals("Min should match", min, BytesUtils.bytesToLong(formatStats.getMin()));    Assert.assertEquals("Max should match", max, BytesUtils.bytesToLong(formatStats.getMax()));    Assert.assertEquals("Num nulls should match", 3004, formatStats.getNull_count());}
public void parquet-mr_f5776_0()
{    testFloatStats(StatsHelper.V1);}
public void parquet-mr_f5777_0()
{    testFloatStats(StatsHelper.V2);}
private void parquet-mr_f5778_0(StatsHelper helper)
{        FloatStatistics stats = new FloatStatistics();    stats.incrementNumNulls(3004);    float min = Float.MIN_VALUE;    float max = Float.MAX_VALUE;    stats.updateStats(min);    stats.updateStats(max);    org.apache.parquet.format.Statistics formatStats = helper.toParquetStatistics(stats);    Assert.assertEquals("Min should match", min, Float.intBitsToFloat(BytesUtils.bytesToInt(formatStats.getMin())), 0.000001);    Assert.assertEquals("Max should match", max, Float.intBitsToFloat(BytesUtils.bytesToInt(formatStats.getMax())), 0.000001);    Assert.assertEquals("Num nulls should match", 3004, formatStats.getNull_count());}
public void parquet-mr_f5779_0()
{    testDoubleStats(StatsHelper.V1);}
public void parquet-mr_f5780_0()
{    testDoubleStats(StatsHelper.V2);}
private void parquet-mr_f5781_0(StatsHelper helper)
{        DoubleStatistics stats = new DoubleStatistics();    stats.incrementNumNulls(3004);    double min = Double.MIN_VALUE;    double max = Double.MAX_VALUE;    stats.updateStats(min);    stats.updateStats(max);    org.apache.parquet.format.Statistics formatStats = helper.toParquetStatistics(stats);    Assert.assertEquals("Min should match", min, Double.longBitsToDouble(BytesUtils.bytesToLong(formatStats.getMin())), 0.000001);    Assert.assertEquals("Max should match", max, Double.longBitsToDouble(BytesUtils.bytesToLong(formatStats.getMax())), 0.000001);    Assert.assertEquals("Num nulls should match", 3004, formatStats.getNull_count());}
public void parquet-mr_f5782_0()
{    testBooleanStats(StatsHelper.V1);}
public void parquet-mr_f5783_0()
{    testBooleanStats(StatsHelper.V2);}
private void parquet-mr_f5784_0(StatsHelper helper)
{        BooleanStatistics stats = new BooleanStatistics();    stats.incrementNumNulls(3004);    boolean min = Boolean.FALSE;    boolean max = Boolean.TRUE;    stats.updateStats(min);    stats.updateStats(max);    org.apache.parquet.format.Statistics formatStats = helper.toParquetStatistics(stats);    Assert.assertEquals("Min should match", min, BytesUtils.bytesToBool(formatStats.getMin()));    Assert.assertEquals("Max should match", max, BytesUtils.bytesToBool(formatStats.getMax()));    Assert.assertEquals("Num nulls should match", 3004, formatStats.getNull_count());}
public void parquet-mr_f5785_0()
{    ParquetMetadataConverter converter = new ParquetMetadataConverter();    BinaryStatistics stats = new BinaryStatistics();    stats.incrementNumNulls();    stats.updateStats(Binary.fromString("A"));    stats.incrementNumNulls();    stats.updateStats(Binary.fromString("z"));    stats.incrementNumNulls();    PrimitiveType binaryType = Types.required(PrimitiveTypeName.BINARY).as(OriginalType.UTF8).named("b");    Statistics convertedStats = converter.fromParquetStatistics(Version.FULL_VERSION, StatsHelper.V1.toParquetStatistics(stats), binaryType);    Assert.assertFalse("Stats should not include min/max: " + convertedStats, convertedStats.hasNonNullValue());    Assert.assertTrue("Stats should have null count: " + convertedStats, convertedStats.isNumNullsSet());    Assert.assertEquals("Stats should have 3 nulls: " + convertedStats, 3L, convertedStats.getNumNulls());}
public void parquet-mr_f5786_0()
{    testStillUseStatsWithSignedSortOrderIfSingleValue(StatsHelper.V1);}
public void parquet-mr_f5787_0()
{    testStillUseStatsWithSignedSortOrderIfSingleValue(StatsHelper.V2);}
private void parquet-mr_f5788_0(StatsHelper helper)
{    ParquetMetadataConverter converter = new ParquetMetadataConverter();    BinaryStatistics stats = new BinaryStatistics();    stats.incrementNumNulls();    stats.updateStats(Binary.fromString("A"));    stats.incrementNumNulls();    stats.updateStats(Binary.fromString("A"));    stats.incrementNumNulls();    PrimitiveType binaryType = Types.required(PrimitiveTypeName.BINARY).as(OriginalType.UTF8).named("b");    Statistics convertedStats = converter.fromParquetStatistics(Version.FULL_VERSION, ParquetMetadataConverter.toParquetStatistics(stats), binaryType);    Assert.assertFalse("Stats should not be empty: " + convertedStats, convertedStats.isEmpty());    Assert.assertArrayEquals("min == max: " + convertedStats, convertedStats.getMaxBytes(), convertedStats.getMinBytes());}
public void parquet-mr_f5789_0()
{    testUseStatsWithSignedSortOrder(StatsHelper.V1);}
public void parquet-mr_f5790_0()
{    testUseStatsWithSignedSortOrder(StatsHelper.V2);}
private void parquet-mr_f5791_0(StatsHelper helper)
{        Configuration conf = new Configuration();    conf.setBoolean("parquet.strings.signed-min-max.enabled", true);    ParquetMetadataConverter converter = new ParquetMetadataConverter(conf);    BinaryStatistics stats = new BinaryStatistics();    stats.incrementNumNulls();    stats.updateStats(Binary.fromString("A"));    stats.incrementNumNulls();    stats.updateStats(Binary.fromString("z"));    stats.incrementNumNulls();    PrimitiveType binaryType = Types.required(PrimitiveTypeName.BINARY).as(OriginalType.UTF8).named("b");    Statistics convertedStats = converter.fromParquetStatistics(Version.FULL_VERSION, helper.toParquetStatistics(stats), binaryType);    Assert.assertFalse("Stats should not be empty", convertedStats.isEmpty());    Assert.assertTrue(convertedStats.isNumNullsSet());    Assert.assertEquals("Should have 3 nulls", 3, convertedStats.getNumNulls());    if (helper == StatsHelper.V1) {        assertFalse("Min-max should be null for V1 stats", convertedStats.hasNonNullValue());    } else {        Assert.assertEquals("Should have correct min (unsigned sort)", Binary.fromString("A"), convertedStats.genericGetMin());        Assert.assertEquals("Should have correct max (unsigned sort)", Binary.fromString("z"), convertedStats.genericGetMax());    }}
public void parquet-mr_f5792_0()
{    ParquetMetadataConverter converter = new ParquetMetadataConverter();    PrimitiveType type = Types.required(PrimitiveTypeName.INT32).named("test_int32");    org.apache.parquet.format.Statistics formatStats = new org.apache.parquet.format.Statistics();    Statistics<?> stats = converter.fromParquetStatistics(Version.FULL_VERSION, formatStats, type);    assertFalse(stats.isNumNullsSet());    assertFalse(stats.hasNonNullValue());    assertTrue(stats.isEmpty());    assertEquals(-1, stats.getNumNulls());    formatStats.clear();    formatStats.setMin(BytesUtils.intToBytes(-100));    formatStats.setMax(BytesUtils.intToBytes(100));    stats = converter.fromParquetStatistics(Version.FULL_VERSION, formatStats, type);    assertFalse(stats.isNumNullsSet());    assertTrue(stats.hasNonNullValue());    assertFalse(stats.isEmpty());    assertEquals(-1, stats.getNumNulls());    assertEquals(-100, stats.genericGetMin());    assertEquals(100, stats.genericGetMax());    formatStats.clear();    formatStats.setNull_count(2000);    stats = converter.fromParquetStatistics(Version.FULL_VERSION, formatStats, type);    assertTrue(stats.isNumNullsSet());    assertFalse(stats.hasNonNullValue());    assertFalse(stats.isEmpty());    assertEquals(2000, stats.getNumNulls());}
public void parquet-mr_f5793_0()
{    testSkippedV2Stats(Types.optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY).length(12).as(OriginalType.INTERVAL).named(""), new BigInteger("12345678"), new BigInteger("12345679"));    testSkippedV2Stats(Types.optional(PrimitiveTypeName.INT96).named(""), new BigInteger("-75687987"), new BigInteger("45367657"));}
private void parquet-mr_f5794_0(PrimitiveType type, Object min, Object max)
{    Statistics<?> stats = createStats(type, min, max);    org.apache.parquet.format.Statistics statistics = ParquetMetadataConverter.toParquetStatistics(stats);    assertFalse(statistics.isSetMin());    assertFalse(statistics.isSetMax());    assertFalse(statistics.isSetMin_value());    assertFalse(statistics.isSetMax_value());}
public void parquet-mr_f5795_0()
{    testV2OnlyStats(Types.optional(PrimitiveTypeName.INT32).as(OriginalType.UINT_8).named(""), 0x7F, 0x80);    testV2OnlyStats(Types.optional(PrimitiveTypeName.INT32).as(OriginalType.UINT_16).named(""), 0x7FFF, 0x8000);    testV2OnlyStats(Types.optional(PrimitiveTypeName.INT32).as(OriginalType.UINT_32).named(""), 0x7FFFFFFF, 0x80000000);    testV2OnlyStats(Types.optional(PrimitiveTypeName.INT64).as(OriginalType.UINT_64).named(""), 0x7FFFFFFFFFFFFFFFL, 0x8000000000000000L);    testV2OnlyStats(Types.optional(PrimitiveTypeName.BINARY).as(OriginalType.DECIMAL).precision(6).named(""), new BigInteger("-765875"), new BigInteger("876856"));    testV2OnlyStats(Types.optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY).length(14).as(OriginalType.DECIMAL).precision(7).named(""), new BigInteger("-6769643"), new BigInteger("9864675"));}
private void parquet-mr_f5796_0(PrimitiveType type, Object min, Object max)
{    Statistics<?> stats = createStats(type, min, max);    org.apache.parquet.format.Statistics statistics = ParquetMetadataConverter.toParquetStatistics(stats);    assertFalse(statistics.isSetMin());    assertFalse(statistics.isSetMax());    assertEquals(ByteBuffer.wrap(stats.getMinBytes()), statistics.min_value);    assertEquals(ByteBuffer.wrap(stats.getMaxBytes()), statistics.max_value);}
public void parquet-mr_f5797_0()
{    testV2StatsEqualMinMax(Types.optional(PrimitiveTypeName.INT32).as(OriginalType.UINT_8).named(""), 93, 93);    testV2StatsEqualMinMax(Types.optional(PrimitiveTypeName.INT32).as(OriginalType.UINT_16).named(""), -5892, -5892);    testV2StatsEqualMinMax(Types.optional(PrimitiveTypeName.INT32).as(OriginalType.UINT_32).named(""), 234998934, 234998934);    testV2StatsEqualMinMax(Types.optional(PrimitiveTypeName.INT64).as(OriginalType.UINT_64).named(""), -2389943895984985L, -2389943895984985L);    testV2StatsEqualMinMax(Types.optional(PrimitiveTypeName.BINARY).as(OriginalType.DECIMAL).precision(6).named(""), new BigInteger("823749"), new BigInteger("823749"));    testV2StatsEqualMinMax(Types.optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY).length(14).as(OriginalType.DECIMAL).precision(7).named(""), new BigInteger("-8752832"), new BigInteger("-8752832"));    testV2StatsEqualMinMax(Types.optional(PrimitiveTypeName.INT96).named(""), new BigInteger("81032984"), new BigInteger("81032984"));}
private void parquet-mr_f5798_0(PrimitiveType type, Object min, Object max)
{    Statistics<?> stats = createStats(type, min, max);    org.apache.parquet.format.Statistics statistics = ParquetMetadataConverter.toParquetStatistics(stats);    assertEquals(ByteBuffer.wrap(stats.getMinBytes()), statistics.min);    assertEquals(ByteBuffer.wrap(stats.getMaxBytes()), statistics.max);    assertEquals(ByteBuffer.wrap(stats.getMinBytes()), statistics.min_value);    assertEquals(ByteBuffer.wrap(stats.getMaxBytes()), statistics.max_value);}
private static Statistics<?> parquet-mr_f5799_0(PrimitiveType type, T min, T max)
{    Class<?> c = min.getClass();    if (c == Integer.class) {        return createStatsTyped(type, (Integer) min, (Integer) max);    } else if (c == Long.class) {        return createStatsTyped(type, (Long) min, (Long) max);    } else if (c == BigInteger.class) {        return createStatsTyped(type, (BigInteger) min, (BigInteger) max);    }    fail("Not implemented");    return null;}
private static Statistics<?> parquet-mr_f5800_0(PrimitiveType type, int min, int max)
{    Statistics<?> stats = Statistics.createStats(type);    stats.updateStats(max);    stats.updateStats(min);    assertEquals(min, stats.genericGetMin());    assertEquals(max, stats.genericGetMax());    return stats;}
private static Statistics<?> parquet-mr_f5801_0(PrimitiveType type, long min, long max)
{    Statistics<?> stats = Statistics.createStats(type);    stats.updateStats(max);    stats.updateStats(min);    assertEquals(min, stats.genericGetMin());    assertEquals(max, stats.genericGetMax());    return stats;}
private static Statistics<?> parquet-mr_f5802_0(PrimitiveType type, BigInteger min, BigInteger max)
{    Statistics<?> stats = Statistics.createStats(type);    Binary minBinary = Binary.fromConstantByteArray(min.toByteArray());    Binary maxBinary = Binary.fromConstantByteArray(max.toByteArray());    stats.updateStats(maxBinary);    stats.updateStats(minBinary);    assertEquals(minBinary, stats.genericGetMin());    assertEquals(maxBinary, stats.genericGetMax());    return stats;}
public org.apache.parquet.format.Statistics parquet-mr_f5803_0(Statistics<?> stats)
{    org.apache.parquet.format.Statistics statistics = ParquetMetadataConverter.toParquetStatistics(stats);    statistics.unsetMin_value();    statistics.unsetMax_value();    return statistics;}
public org.apache.parquet.format.Statistics parquet-mr_f5804_0(Statistics<?> stats)
{    return ParquetMetadataConverter.toParquetStatistics(stats);}
public void parquet-mr_f5805_0() throws IOException
{    MessageType schema = parseMessageType("message test {" +     "  optional binary binary_col;" + "  optional group map_col (MAP) {" + "    repeated group map (MAP_KEY_VALUE) {" +     "        required binary key (UTF8);" + "        optional group list_col (LIST) {" + "          repeated group list {" +     "            optional int96 array_element;" + "          }" + "        }" + "    }" + "  }" + "}");    org.apache.parquet.hadoop.metadata.FileMetaData fileMetaData = new org.apache.parquet.hadoop.metadata.FileMetaData(schema, new HashMap<String, String>(), null);    ParquetMetadata metadata = new ParquetMetadata(fileMetaData, new ArrayList<BlockMetaData>());    ParquetMetadataConverter converter = new ParquetMetadataConverter();    FileMetaData formatMetadata = converter.toParquetMetadata(1, metadata);    List<org.apache.parquet.format.ColumnOrder> columnOrders = formatMetadata.getColumn_orders();    assertEquals(3, columnOrders.size());    for (org.apache.parquet.format.ColumnOrder columnOrder : columnOrders) {        assertTrue(columnOrder.isSetTYPE_ORDER());    }            columnOrders.get(1).clear();    MessageType resultSchema = converter.fromParquetMetadata(formatMetadata).getFileMetaData().getSchema();    List<ColumnDescriptor> columns = resultSchema.getColumns();    assertEquals(3, columns.size());    assertEquals(ColumnOrder.typeDefined(), columns.get(0).getPrimitiveType().columnOrder());    assertEquals(ColumnOrder.undefined(), columns.get(1).getPrimitiveType().columnOrder());    assertEquals(ColumnOrder.undefined(), columns.get(2).getPrimitiveType().columnOrder());}
public void parquet-mr_f5806_0()
{    OffsetIndexBuilder builder = OffsetIndexBuilder.getBuilder();    builder.add(1000, 10000, 0);    builder.add(22000, 12000, 100);    OffsetIndex offsetIndex = ParquetMetadataConverter.fromParquetOffsetIndex(ParquetMetadataConverter.toParquetOffsetIndex(builder.build(100000)));    assertEquals(2, offsetIndex.getPageCount());    assertEquals(101000, offsetIndex.getOffset(0));    assertEquals(10000, offsetIndex.getCompressedPageSize(0));    assertEquals(0, offsetIndex.getFirstRowIndex(0));    assertEquals(122000, offsetIndex.getOffset(1));    assertEquals(12000, offsetIndex.getCompressedPageSize(1));    assertEquals(100, offsetIndex.getFirstRowIndex(1));}
public void parquet-mr_f5807_0()
{    PrimitiveType type = Types.required(PrimitiveTypeName.INT64).named("test_int64");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    Statistics<?> stats = Statistics.createStats(type);    stats.incrementNumNulls(16);    stats.updateStats(-100l);    stats.updateStats(100l);    builder.add(stats);    stats = Statistics.createStats(type);    stats.incrementNumNulls(111);    builder.add(stats);    stats = Statistics.createStats(type);    stats.updateStats(200l);    stats.updateStats(500l);    builder.add(stats);    org.apache.parquet.format.ColumnIndex parquetColumnIndex = ParquetMetadataConverter.toParquetColumnIndex(type, builder.build());    ColumnIndex columnIndex = ParquetMetadataConverter.fromParquetColumnIndex(type, parquetColumnIndex);    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertTrue(Arrays.asList(false, true, false).equals(columnIndex.getNullPages()));    assertTrue(Arrays.asList(16l, 111l, 0l).equals(columnIndex.getNullCounts()));    assertTrue(Arrays.asList(ByteBuffer.wrap(BytesUtils.longToBytes(-100l)), ByteBuffer.allocate(0), ByteBuffer.wrap(BytesUtils.longToBytes(200l))).equals(columnIndex.getMinValues()));    assertTrue(Arrays.asList(ByteBuffer.wrap(BytesUtils.longToBytes(100l)), ByteBuffer.allocate(0), ByteBuffer.wrap(BytesUtils.longToBytes(500l))).equals(columnIndex.getMaxValues()));    assertNull("Should handle null column index", ParquetMetadataConverter.toParquetColumnIndex(Types.required(PrimitiveTypeName.INT32).named("test_int32"), null));    assertNull("Should ignore unsupported types", ParquetMetadataConverter.toParquetColumnIndex(Types.required(PrimitiveTypeName.INT96).named("test_int96"), columnIndex));    assertNull("Should ignore unsupported types", ParquetMetadataConverter.fromParquetColumnIndex(Types.required(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY).length(12).as(OriginalType.INTERVAL).named("test_interval"), parquetColumnIndex));}
public void parquet-mr_f5808_0() throws IOException
{    shouldUseParquetFlagToSetCodec("gzip", CompressionCodecName.GZIP);    shouldUseHadoopFlagToSetCodec(CompressionCodecName.GZIP.getHadoopCompressionCodecClassName(), CompressionCodecName.GZIP);    shouldUseParquetFlagToSetCodec("snappy", CompressionCodecName.SNAPPY);    shouldUseHadoopFlagToSetCodec(CompressionCodecName.SNAPPY.getHadoopCompressionCodecClassName(), CompressionCodecName.SNAPPY);        shouldUseHadoopFlagToSetCodec("unexistedCodec", CompressionCodecName.UNCOMPRESSED);        shouldUseHadoopFlagToSetCodec("org.apache.hadoop.io.compress.DefaultCodec", CompressionCodecName.UNCOMPRESSED);}
public void parquet-mr_f5809_0(String codecNameStr, CompressionCodecName expectedCodec) throws IOException
{        Job job = new Job();    Configuration conf = job.getConfiguration();    conf.set(ParquetOutputFormat.COMPRESSION, codecNameStr);    TaskAttemptContext task = ContextUtil.newTaskAttemptContext(conf, new TaskAttemptID(new TaskID(new JobID("test", 1), false, 1), 1));    Assert.assertEquals(CodecConfig.from(task).getCodec(), expectedCodec);        JobConf jobConf = new JobConf();    jobConf.set(ParquetOutputFormat.COMPRESSION, codecNameStr);    Assert.assertEquals(CodecConfig.from(jobConf).getCodec(), expectedCodec);}
public void parquet-mr_f5810_0(String codecClassStr, CompressionCodecName expectedCodec) throws IOException
{        Job job = new Job();    Configuration conf = job.getConfiguration();    conf.setBoolean("mapred.output.compress", true);    conf.set("mapred.output.compression.codec", codecClassStr);    TaskAttemptContext task = ContextUtil.newTaskAttemptContext(conf, new TaskAttemptID(new TaskID(new JobID("test", 1), false, 1), 1));    Assert.assertEquals(expectedCodec, CodecConfig.from(task).getCodec());        JobConf jobConf = new JobConf();    jobConf.setBoolean("mapred.output.compress", true);    jobConf.set("mapred.output.compression.codec", codecClassStr);    Assert.assertEquals(CodecConfig.from(jobConf).getCodec(), expectedCodec);}
public void parquet-mr_f5811_0()
{    conf = new Configuration();    jobConf = new JobConf();    writeSchema = "message example {\n" + "required int32 line;\n" + "required binary content;\n" + "}";    readSchema = "message example {\n" + "required int32 line;\n" + "required binary content;\n" + "}";}
private void parquet-mr_f5812_0(CompressionCodecName codec) throws IOException, ClassNotFoundException, InterruptedException
{    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        writeJob = new Job(conf, "write");        TextInputFormat.addInputPath(writeJob, inputPath);        writeJob.setInputFormatClass(TextInputFormat.class);        writeJob.setNumReduceTasks(0);        ExampleOutputFormat.setCompression(writeJob, codec);        ExampleOutputFormat.setOutputPath(writeJob, parquetPath);        writeJob.setOutputFormatClass(ExampleOutputFormat.class);        writeJob.setMapperClass(ReadMapper.class);        ExampleOutputFormat.setSchema(writeJob, MessageTypeParser.parseMessageType(writeSchema));        writeJob.submit();        waitForJob(writeJob);    }    {        jobConf.set(ReadSupport.PARQUET_READ_SCHEMA, readSchema);        jobConf.set(ParquetInputFormat.READ_SUPPORT_CLASS, GroupReadSupport.class.getCanonicalName());        jobConf.setInputFormat(MyDeprecatedInputFormat.class);        MyDeprecatedInputFormat.setInputPaths(jobConf, parquetPath);        jobConf.setOutputFormat(org.apache.hadoop.mapred.TextOutputFormat.class);        org.apache.hadoop.mapred.TextOutputFormat.setOutputPath(jobConf, outputPath);        jobConf.setMapperClass(DeprecatedWriteMapper.class);        jobConf.setNumReduceTasks(0);        mapRedJob = JobClient.runJob(jobConf);    }}
public void parquet-mr_f5813_0() throws Exception
{    runMapReduceJob(CompressionCodecName.GZIP);    assertTrue(mapRedJob.getCounters().getGroup("parquet").getCounterForName("bytesread").getValue() > 0L);    assertTrue(mapRedJob.getCounters().getGroup("parquet").getCounterForName("bytestotal").getValue() > 0L);    assertTrue(mapRedJob.getCounters().getGroup("parquet").getCounterForName("bytesread").getValue() == mapRedJob.getCounters().getGroup("parquet").getCounterForName("bytestotal").getValue());}
public void parquet-mr_f5814_0() throws Exception
{    jobConf.set("parquet.benchmark.time.read", "false");    jobConf.set("parquet.benchmark.bytes.total", "false");    jobConf.set("parquet.benchmark.bytes.read", "false");    runMapReduceJob(CompressionCodecName.GZIP);    assertEquals(mapRedJob.getCounters().getGroup("parquet").getCounterForName("bytesread").getValue(), 0L);    assertEquals(mapRedJob.getCounters().getGroup("parquet").getCounterForName("bytestotal").getValue(), 0L);    assertEquals(mapRedJob.getCounters().getGroup("parquet").getCounterForName("timeread").getValue(), 0L);}
private void parquet-mr_f5815_0(Job job) throws InterruptedException, IOException
{    while (!job.isComplete()) {        System.out.println("waiting for job " + job.getJobName());        sleep(100);    }    System.out.println("status for job " + job.getJobName() + ": " + (job.isSuccessful() ? "SUCCESS" : "FAILURE"));    if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
protected void parquet-mr_f5816_0(Context context) throws IOException, InterruptedException
{    factory = new SimpleGroupFactory(GroupWriteSupport.getSchema(ContextUtil.getConfiguration(context)));}
protected void parquet-mr_f5817_0(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{    Group group = factory.newGroup().append("line", (int) key.get()).append("content", value.toString());    context.write(null, group);}
public void parquet-mr_f5818_0(Void aVoid, Container<Group> valueContainer, OutputCollector<LongWritable, Text> longWritableTextOutputCollector, Reporter reporter) throws IOException
{    Group value = valueContainer.get();    longWritableTextOutputCollector.collect(new LongWritable(value.getInteger("line", 0)), new Text(value.getString("content", 0)));}
public void parquet-mr_f5819_0() throws IOException
{}
public void parquet-mr_f5820_0(JobConf entries)
{}
public void parquet-mr_f5821_0()
{    conf = new Configuration();    jobConf = new JobConf();    writeSchema = "message example {\n" + "required int32 line;\n" + "required binary content;\n" + "}";}
private void parquet-mr_f5822_0(CompressionCodecName codec) throws IOException, ClassNotFoundException, InterruptedException
{    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        jobConf.setInputFormat(TextInputFormat.class);        TextInputFormat.addInputPath(jobConf, inputPath);        jobConf.setNumReduceTasks(0);        jobConf.setOutputFormat(DeprecatedParquetOutputFormat.class);        DeprecatedParquetOutputFormat.setCompression(jobConf, codec);        DeprecatedParquetOutputFormat.setOutputPath(jobConf, parquetPath);        DeprecatedParquetOutputFormat.setWriteSupportClass(jobConf, GroupWriteSupport.class);        GroupWriteSupport.setSchema(MessageTypeParser.parseMessageType(writeSchema), jobConf);        jobConf.setMapperClass(DeprecatedMapper.class);        mapRedJob = JobClient.runJob(jobConf);    }}
public void parquet-mr_f5823_0() throws Exception
{    runMapReduceJob(CompressionCodecName.GZIP);    assert (mapRedJob.isSuccessful());}
public void parquet-mr_f5824_0(JobConf job)
{    factory = new SimpleGroupFactory(GroupWriteSupport.getSchema(job));}
public void parquet-mr_f5825_0(LongWritable key, Text value, OutputCollector<Void, Group> outputCollector, Reporter reporter) throws IOException
{    Group group = factory.newGroup().append("line", (int) key.get()).append("content", value.toString());    outputCollector.collect(null, group);}
public void parquet-mr_f5826_0()
{}
public void parquet-mr_f5827_0() throws Exception
{    GroupReadSupport s = new GroupReadSupport();    Configuration configuration = new Configuration();    Map<String, String> keyValueMetaData = new HashMap<String, String>();    MessageType fileSchema = MessageTypeParser.parseMessageType(fullSchemaStr);    ReadSupport.ReadContext context = s.init(configuration, keyValueMetaData, fileSchema);    assertEquals(context.getRequestedSchema(), fileSchema);}
public void parquet-mr_f5828_0()
{    GroupReadSupport s = new GroupReadSupport();    Configuration configuration = new Configuration();    Map<String, String> keyValueMetaData = new HashMap<String, String>();    MessageType fileSchema = MessageTypeParser.parseMessageType(fullSchemaStr);    MessageType partialSchema = MessageTypeParser.parseMessageType(partialSchemaStr);    configuration.set(ReadSupport.PARQUET_READ_SCHEMA, partialSchemaStr);    ReadSupport.ReadContext context = s.init(configuration, keyValueMetaData, fileSchema);    assertEquals(context.getRequestedSchema(), partialSchema);}
public void parquet-mr_f5829_0()
{    conf = new Configuration();    writeSchema = "message example {\n" + "required int32 line;\n" + "required binary content;\n" + "}";    readSchema = "message example {\n" + "required int32 line;\n" + "required binary content;\n" + "}";    partialSchema = "message example {\n" + "required int32 line;\n" + "}";    readMapperClass = ReadMapper.class;    writeMapperClass = WriteMapper.class;}
public void parquet-mr_f5830_0(Group record)
{    super.write(record);    ++count;}
public org.apache.parquet.hadoop.api.WriteSupport.FinalizedWriteContext parquet-mr_f5831_0()
{    Map<String, String> extraMetadata = new HashMap<String, String>();    extraMetadata.put("my.count", String.valueOf(count));    return new FinalizedWriteContext(extraMetadata);}
public org.apache.parquet.hadoop.api.ReadSupport.ReadContext parquet-mr_f5832_0(InitContext context)
{    Set<String> counts = context.getKeyValueMetadata().get("my.count");    assertTrue("counts: " + counts, counts.size() > 0);    return super.init(context);}
protected void parquet-mr_f5833_0(org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Void, Group>.Context context) throws java.io.IOException, InterruptedException
{    factory = new SimpleGroupFactory(GroupWriteSupport.getSchema(ContextUtil.getConfiguration(context)));}
protected void parquet-mr_f5834_0(LongWritable key, Text value, Mapper<LongWritable, Text, Void, Group>.Context context) throws java.io.IOException, InterruptedException
{    Group group = factory.newGroup().append("line", (int) key.get()).append("content", value.toString());    context.write(null, group);}
protected void parquet-mr_f5835_0(Void key, Group value, Mapper<Void, Group, LongWritable, Text>.Context context) throws IOException, InterruptedException
{    context.write(new LongWritable(value.getInteger("line", 0)), new Text(value.getString("content", 0)));}
protected void parquet-mr_f5836_0(Void key, Group value, Mapper<Void, Group, LongWritable, Text>.Context context) throws IOException, InterruptedException
{    context.write(new LongWritable(value.getInteger("line", 0)), new Text("dummy"));}
private void parquet-mr_f5837_0(CompressionCodecName codec) throws IOException, ClassNotFoundException, InterruptedException
{    runMapReduceJob(codec, Collections.<String, String>emptyMap());}
private void parquet-mr_f5838_0(CompressionCodecName codec, Map<String, String> extraConf) throws IOException, ClassNotFoundException, InterruptedException
{    Configuration conf = new Configuration(this.conf);    for (Map.Entry<String, String> entry : extraConf.entrySet()) {        conf.set(entry.getKey(), entry.getValue());    }    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        writeJob = new Job(conf, "write");        TextInputFormat.addInputPath(writeJob, inputPath);        writeJob.setInputFormatClass(TextInputFormat.class);        writeJob.setNumReduceTasks(0);        ParquetOutputFormat.setCompression(writeJob, codec);        ParquetOutputFormat.setOutputPath(writeJob, parquetPath);        writeJob.setOutputFormatClass(ParquetOutputFormat.class);        writeJob.setMapperClass(readMapperClass);        ParquetOutputFormat.setWriteSupportClass(writeJob, MyWriteSupport.class);        GroupWriteSupport.setSchema(MessageTypeParser.parseMessageType(writeSchema), writeJob.getConfiguration());        writeJob.submit();        waitForJob(writeJob);    }    {        conf.set(ReadSupport.PARQUET_READ_SCHEMA, readSchema);        readJob = new Job(conf, "read");        readJob.setInputFormatClass(ParquetInputFormat.class);        ParquetInputFormat.setReadSupportClass(readJob, MyReadSupport.class);        ParquetInputFormat.setInputPaths(readJob, parquetPath);        readJob.setOutputFormatClass(TextOutputFormat.class);        TextOutputFormat.setOutputPath(readJob, outputPath);        readJob.setMapperClass(writeMapperClass);        readJob.setNumReduceTasks(0);        readJob.submit();        waitForJob(readJob);    }}
private void parquet-mr_f5839_0(CompressionCodecName codec) throws IOException, ClassNotFoundException, InterruptedException
{    testReadWrite(codec, Collections.<String, String>emptyMap());}
private void parquet-mr_f5840_0(CompressionCodecName codec, Map<String, String> conf) throws IOException, ClassNotFoundException, InterruptedException
{    runMapReduceJob(codec, conf);    final BufferedReader in = new BufferedReader(new FileReader(new File(inputPath.toString())));    final BufferedReader out = new BufferedReader(new FileReader(new File(outputPath.toString(), "part-m-00000")));    String lineIn;    String lineOut = null;    int lineNumber = 0;    while ((lineIn = in.readLine()) != null && (lineOut = out.readLine()) != null) {        ++lineNumber;        lineOut = lineOut.substring(lineOut.indexOf("\t") + 1);        assertEquals("line " + lineNumber, lineIn, lineOut);    }    assertNull("line " + lineNumber, out.readLine());    assertNull("line " + lineNumber, lineIn);    in.close();    out.close();}
public void parquet-mr_f5841_0() throws IOException, ClassNotFoundException, InterruptedException
{        testReadWrite(CompressionCodecName.GZIP);    testReadWrite(CompressionCodecName.UNCOMPRESSED);    testReadWrite(CompressionCodecName.SNAPPY);}
public void parquet-mr_f5842_0() throws IOException, ClassNotFoundException, InterruptedException
{    testReadWrite(CompressionCodecName.UNCOMPRESSED, new HashMap<String, String>() {        {            put("parquet.task.side.metadata", "true");        }    });}
public void parquet-mr_f5843_0() throws IOException, ClassNotFoundException, InterruptedException
{    Configuration conf = new Configuration();        ParquetInputFormat.setFilterPredicate(conf, FilterApi.eq(FilterApi.intColumn("line"), -1000));    final String fpString = conf.get(ParquetInputFormat.FILTER_PREDICATE);    runMapReduceJob(CompressionCodecName.UNCOMPRESSED, new HashMap<String, String>() {        {            put("parquet.task.side.metadata", "true");            put(ParquetInputFormat.FILTER_PREDICATE, fpString);        }    });    File file = new File(outputPath.toString(), "part-m-00000");    List<String> lines = Files.readAllLines(file.toPath(), StandardCharsets.UTF_8);    assertTrue(lines.isEmpty());}
public void parquet-mr_f5844_0() throws IOException, ClassNotFoundException, InterruptedException
{    Configuration conf = new Configuration();            ParquetInputFormat.setFilterPredicate(conf, FilterApi.lt(FilterApi.intColumn("line"), 500));    final String fpString = conf.get(ParquetInputFormat.FILTER_PREDICATE);    runMapReduceJob(CompressionCodecName.UNCOMPRESSED, new HashMap<String, String>() {        {            put("parquet.task.side.metadata", "true");            put(ParquetInputFormat.FILTER_PREDICATE, fpString);        }    });    File file = new File(inputPath.toString());    List<String> expected = Files.readAllLines(file.toPath(), StandardCharsets.UTF_8);        int size = 0;    Iterator<String> iter = expected.iterator();    while (iter.hasNext()) {        String next = iter.next();        if (size < 500) {            size += next.length();            continue;        }        iter.remove();    }        File file2 = new File(outputPath.toString(), "part-m-00000");    List<String> found = Files.readAllLines(file2.toPath(), StandardCharsets.UTF_8);    StringBuilder sbFound = new StringBuilder();    for (String line : found) {        sbFound.append(line.split("\t", -1)[1]);        sbFound.append("\n");    }    sbFound.deleteCharAt(sbFound.length() - 1);    assertEquals(Strings.join(expected, "\n"), sbFound.toString());}
public void parquet-mr_f5845_0() throws Exception
{    readSchema = partialSchema;    writeMapperClass = PartialWriteMapper.class;    runMapReduceJob(CompressionCodecName.GZIP);}
private static long parquet-mr_f5846_0(Job job, String groupName, String name) throws Exception
{        Method getGroup = org.apache.hadoop.mapreduce.Counters.class.getMethod("getGroup", String.class);        Method findCounter = org.apache.hadoop.mapreduce.CounterGroup.class.getMethod("findCounter", String.class);        Method getValue = org.apache.hadoop.mapreduce.Counter.class.getMethod("getValue");    CounterGroup group = (CounterGroup) getGroup.invoke(job.getCounters(), groupName);    Counter counter = (Counter) findCounter.invoke(group, name);    return (Long) getValue.invoke(counter);}
public void parquet-mr_f5847_0() throws Exception
{    runMapReduceJob(CompressionCodecName.GZIP);    assertTrue(value(readJob, "parquet", "bytesread") > 0L);    assertTrue(value(readJob, "parquet", "bytestotal") > 0L);    assertTrue(value(readJob, "parquet", "bytesread") == value(readJob, "parquet", "bytestotal"));}
public void parquet-mr_f5848_0() throws Exception
{    conf.set("parquet.benchmark.time.read", "false");    conf.set("parquet.benchmark.bytes.total", "false");    conf.set("parquet.benchmark.bytes.read", "false");    runMapReduceJob(CompressionCodecName.GZIP);    assertTrue(value(readJob, "parquet", "bytesread") == 0L);    assertTrue(value(readJob, "parquet", "bytestotal") == 0L);    assertTrue(value(readJob, "parquet", "timeread") == 0L);}
private void parquet-mr_f5849_1(Job job) throws InterruptedException, IOException
{    while (!job.isComplete()) {                sleep(100);    }        if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
public void parquet-mr_f5850_0()
{    long big = (long) Integer.MAX_VALUE + 1;    ColumnChunkMetaData md = newMD(big);    assertTrue(md instanceof IntColumnChunkMetaData);    assertEquals(big, md.getFirstDataPageOffset());}
public void parquet-mr_f5851_0()
{    long small = 1;    ColumnChunkMetaData md = newMD(small);    assertTrue(md instanceof IntColumnChunkMetaData);    assertEquals(small, md.getFirstDataPageOffset());}
public void parquet-mr_f5852_0()
{    long veryBig = (long) Integer.MAX_VALUE * 3;    ColumnChunkMetaData md = newMD(veryBig);    assertTrue(md instanceof LongColumnChunkMetaData);    assertEquals(veryBig, md.getFirstDataPageOffset());}
public void parquet-mr_f5853_0()
{    long neg = -1;    ColumnChunkMetaData md = newMD(neg);    assertTrue(md instanceof LongColumnChunkMetaData);    assertEquals(neg, md.getFirstDataPageOffset());}
private ColumnChunkMetaData parquet-mr_f5854_0(long big)
{    Set<Encoding> e = new HashSet<Encoding>();    PrimitiveTypeName t = BINARY;    ColumnPath p = ColumnPath.get("foo");    CompressionCodecName c = CompressionCodecName.GZIP;    BinaryStatistics s = new BinaryStatistics();    ColumnChunkMetaData md = ColumnChunkMetaData.get(p, t, c, e, s, big, 0, 0, 0, 0);    return md;}
 PositionOutputStream parquet-mr_f5855_0()
{    return out;}
public PositionOutputStream parquet-mr_f5856_0(long blockSizeHint) throws IOException
{    return out = file.create(blockSizeHint);}
public PositionOutputStream parquet-mr_f5857_0(long blockSizeHint) throws IOException
{    return out = file.createOrOverwrite(blockSizeHint);}
public boolean parquet-mr_f5858_0()
{    return file.supportsBlockSize();}
public long parquet-mr_f5859_0()
{    return file.defaultBlockSize();}
public void parquet-mr_f5860_0()
{    this.conf = new Configuration();}
public void parquet-mr_f5861_0() throws Exception
{    Path file = new Path("target/test/TestColumnChunkPageWriteStore/test.parquet");    Path root = file.getParent();    FileSystem fs = file.getFileSystem(conf);    if (fs.exists(root)) {        fs.delete(root, true);    }    fs.mkdirs(root);    MessageType schema = MessageTypeParser.parseMessageType("message test { repeated binary bar; }");    ColumnDescriptor col = schema.getColumns().get(0);    Encoding dataEncoding = PLAIN;    int valueCount = 10;    int d = 1;    int r = 2;    int v = 3;    BytesInput definitionLevels = BytesInput.fromInt(d);    BytesInput repetitionLevels = BytesInput.fromInt(r);    Statistics<?> statistics = Statistics.getBuilderForReading(Types.required(PrimitiveTypeName.BINARY).named("test_binary")).build();    BytesInput data = BytesInput.fromInt(v);    int rowCount = 5;    int nullCount = 1;    statistics.incrementNumNulls(nullCount);    statistics.setMinMaxFromBytes(new byte[] { 0, 1, 2 }, new byte[] { 0, 1, 2, 3 });    long pageOffset;    long pageSize;    {        OutputFileForTesting outputFile = new OutputFileForTesting(file, conf);        ParquetFileWriter writer = new ParquetFileWriter(outputFile, schema, Mode.CREATE, ParquetWriter.DEFAULT_BLOCK_SIZE, ParquetWriter.MAX_PADDING_SIZE_DEFAULT);        writer.start();        writer.startBlock(rowCount);        pageOffset = outputFile.out().getPos();        {            ColumnChunkPageWriteStore store = new ColumnChunkPageWriteStore(compressor(GZIP), schema, new HeapByteBufferAllocator(), Integer.MAX_VALUE);            PageWriter pageWriter = store.getPageWriter(col);            pageWriter.writePageV2(rowCount, nullCount, valueCount, repetitionLevels, definitionLevels, dataEncoding, data, statistics);            store.flushToFileWriter(writer);            pageSize = outputFile.out().getPos() - pageOffset;        }        writer.endBlock();        writer.end(new HashMap<String, String>());    }    {        ParquetMetadata footer = ParquetFileReader.readFooter(conf, file, NO_FILTER);        ParquetFileReader reader = new ParquetFileReader(conf, footer.getFileMetaData(), file, footer.getBlocks(), schema.getColumns());        PageReadStore rowGroup = reader.readNextRowGroup();        PageReader pageReader = rowGroup.getPageReader(col);        DataPageV2 page = (DataPageV2) pageReader.readPage();        assertEquals(rowCount, page.getRowCount());        assertEquals(nullCount, page.getNullCount());        assertEquals(valueCount, page.getValueCount());        assertEquals(d, intValue(page.getDefinitionLevels()));        assertEquals(r, intValue(page.getRepetitionLevels()));        assertEquals(dataEncoding, page.getDataEncoding());        assertEquals(v, intValue(page.getData()));                ColumnChunkMetaData column = footer.getBlocks().get(0).getColumns().get(0);        ColumnIndex columnIndex = reader.readColumnIndex(column);        assertArrayEquals(statistics.getMinBytes(), columnIndex.getMinValues().get(0).array());        assertArrayEquals(statistics.getMaxBytes(), columnIndex.getMaxValues().get(0).array());        assertEquals(statistics.getNumNulls(), columnIndex.getNullCounts().get(0).longValue());        assertFalse(columnIndex.getNullPages().get(0));        OffsetIndex offsetIndex = reader.readOffsetIndex(column);        assertEquals(1, offsetIndex.getPageCount());        assertEquals(pageSize, offsetIndex.getCompressedPageSize(0));        assertEquals(0, offsetIndex.getFirstRowIndex(0));        assertEquals(pageOffset, offsetIndex.getOffset(0));        reader.close();    }}
private int parquet-mr_f5862_0(BytesInput in) throws IOException
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    in.writeAllTo(baos);    LittleEndianDataInputStream os = new LittleEndianDataInputStream(new ByteArrayInputStream(baos.toByteArray()));    int i = os.readInt();    os.close();    return i;}
public void parquet-mr_f5863_0() throws IOException
{    ParquetFileWriter mockFileWriter = Mockito.mock(ParquetFileWriter.class);    InOrder inOrder = inOrder(mockFileWriter);    MessageType schema = Types.buildMessage().required(BINARY).as(UTF8).named("a_string").required(INT32).named("an_int").required(INT64).named("a_long").required(FLOAT).named("a_float").required(DOUBLE).named("a_double").named("order_test");    BytesInput fakeData = BytesInput.fromInt(34);    int fakeCount = 3;    BinaryStatistics fakeStats = new BinaryStatistics();            ColumnChunkPageWriteStore store = new ColumnChunkPageWriteStore(compressor(UNCOMPRESSED), schema, new HeapByteBufferAllocator(), Integer.MAX_VALUE);    for (ColumnDescriptor col : schema.getColumns()) {        PageWriter pageWriter = store.getPageWriter(col);        pageWriter.writePage(fakeData, fakeCount, fakeStats, RLE, RLE, PLAIN);    }        store.flushToFileWriter(mockFileWriter);    for (ColumnDescriptor col : schema.getColumns()) {        inOrder.verify(mockFileWriter).writeColumnChunk(eq(col), eq((long) fakeCount), eq(UNCOMPRESSED), isNull(DictionaryPage.class), any(), eq(fakeData.size()), eq(fakeData.size()), eq(fakeStats),         same(ColumnIndexBuilder.getNoOpBuilder()),         same(OffsetIndexBuilder.getNoOpBuilder()), any(), any(), any());    }}
private CodecFactory.BytesCompressor parquet-mr_f5864_0(CompressionCodecName codec)
{    return new CodecFactory(conf, pageSize).getCompressor(codec);}
public static Collection<Object[]> parquet-mr_f5865_0()
{    return Arrays.asList(new Object[] { FILE_V1 }, new Object[] { FILE_V2 });}
private static List<User> parquet-mr_f5866_0(int rowCount)
{    List<User> users = new ArrayList<>();    List<String> names = generateNames(rowCount);    for (int i = 0; i < rowCount; ++i) {        users.add(new User(i, names.get(i), generatePhoneNumbers(), generateLocation(i, rowCount)));    }    return users;}
private static List<String> parquet-mr_f5867_0(int rowCount)
{    List<String> list = new ArrayList<>();        list.add("anderson");    list.add("anderson");    list.add("miller");    list.add("miller");    list.add("miller");    list.add("thomas");    list.add("thomas");    list.add("williams");    int nullCount = rowCount / 100;    String alphabet = "aabcdeefghiijklmnoopqrstuuvwxyz";    int maxLength = 8;    for (int i = rowCount - list.size() - nullCount; i >= 0; --i) {        int l = RANDOM.nextInt(maxLength);        StringBuilder builder = new StringBuilder(l);        for (int j = 0; j < l; ++j) {            builder.append(alphabet.charAt(RANDOM.nextInt(alphabet.length())));        }        list.add(builder.toString());    }    Collections.sort(list, (str1, str2) -> -str1.compareTo(str2));        for (int i = 0; i < nullCount; ++i) {        list.add(RANDOM.nextInt(list.size()), null);    }    return list;}
private static List<PhoneNumber> parquet-mr_f5868_0()
{    int length = RANDOM.nextInt(5) - 1;    if (length < 0) {        return null;    }    List<PhoneNumber> phoneNumbers = new ArrayList<>(length);    for (int i = 0; i < length; ++i) {                long number = Math.abs(RANDOM.nextLong() % 900000) + 100000;        phoneNumbers.add(new PhoneNumber(number, PHONE_KINDS[RANDOM.nextInt(PHONE_KINDS.length)]));    }    return phoneNumbers;}
private static Location parquet-mr_f5869_0(int id, int rowCount)
{    if (RANDOM.nextDouble() < 0.01) {        return null;    }    double lat = RANDOM.nextDouble() * 90.0 - (id < rowCount / 2 ? 90.0 : 0.0);    double lon = RANDOM.nextDouble() * 90.0 - (id < rowCount / 4 || id >= 3 * rowCount / 4 ? 90.0 : 0.0);    return new Location(RANDOM.nextDouble() < 0.01 ? null : lat, RANDOM.nextDouble() < 0.01 ? null : lon);}
private static Path parquet-mr_f5870_0()
{    try {        return new Path(Files.createTempFile("test-ci_", ".parquet").toAbsolutePath().toString());    } catch (IOException e) {        throw new AssertionError("Unable to create temporary file", e);    }}
private List<User> parquet-mr_f5871_0(FilterPredicate filter, boolean useOtherFiltering) throws IOException
{    return readUsers(FilterCompat.get(filter), useOtherFiltering, true);}
private List<User> parquet-mr_f5872_0(FilterPredicate filter, boolean useOtherFiltering, boolean useColumnIndexFilter) throws IOException
{    return readUsers(FilterCompat.get(filter), useOtherFiltering, useColumnIndexFilter);}
private List<User> parquet-mr_f5873_0(Filter filter, boolean useOtherFiltering) throws IOException
{    return readUsers(filter, useOtherFiltering, true);}
private List<User> parquet-mr_f5874_0(Filter filter, boolean useOtherFiltering, boolean useColumnIndexFilter) throws IOException
{    return PhoneBookWriter.readUsers(ParquetReader.builder(new GroupReadSupport(), file).withFilter(filter).useDictionaryFilter(useOtherFiltering).useStatsFilter(useOtherFiltering).useRecordFilter(useOtherFiltering).useColumnIndexFilter(useColumnIndexFilter));}
private static void parquet-mr_f5875_0(Stream<User> expected, List<User> actual)
{    Iterator<User> expIt = expected.iterator();    if (!expIt.hasNext()) {        return;    }    User exp = expIt.next();    for (User act : actual) {        if (act.equals(exp)) {            if (!expIt.hasNext()) {                break;            }            exp = expIt.next();        }    }    assertFalse("Not all expected elements are in the actual list. E.g.: " + exp, expIt.hasNext());}
public static void parquet-mr_f5877_0() throws IOException
{        int pageSize = DATA.size() / 10;        int rowGroupSize = pageSize * 6 * 5;    PhoneBookWriter.write(ExampleParquetWriter.builder(FILE_V1).withWriteMode(OVERWRITE).withRowGroupSize(rowGroupSize).withPageSize(pageSize).withWriterVersion(WriterVersion.PARQUET_1_0), DATA);    PhoneBookWriter.write(ExampleParquetWriter.builder(FILE_V2).withWriteMode(OVERWRITE).withRowGroupSize(rowGroupSize).withPageSize(pageSize).withWriterVersion(WriterVersion.PARQUET_2_0), DATA);}
public static void parquet-mr_f5878_0() throws IOException
{    FILE_V1.getFileSystem(new Configuration()).delete(FILE_V1, false);    FILE_V2.getFileSystem(new Configuration()).delete(FILE_V2, false);}
public void parquet-mr_f5879_0() throws IOException
{    assertCorrectFiltering(record -> record.getId() == 1234, eq(longColumn("id"), 1234l));    assertCorrectFiltering(record -> "miller".equals(record.getName()), eq(binaryColumn("name"), Binary.fromString("miller")));    assertCorrectFiltering(record -> record.getName() == null, eq(binaryColumn("name"), null));}
public void parquet-mr_f5880_0() throws IOException
{        assertEquals(DATA, readUsers(FilterCompat.NOOP, false));    assertEquals(DATA, readUsers(FilterCompat.NOOP, true));        assertEquals(DATA.stream().filter(user -> user.getId() == 1234).collect(Collectors.toList()), readUsers(eq(longColumn("id"), 1234l), true, false));    assertEquals(DATA.stream().filter(user -> "miller".equals(user.getName())).collect(Collectors.toList()), readUsers(eq(binaryColumn("name"), Binary.fromString("miller")), true, false));    assertEquals(DATA.stream().filter(user -> user.getName() == null).collect(Collectors.toList()), readUsers(eq(binaryColumn("name"), null), true, false));        assertEquals(DATA, readUsers(eq(longColumn("id"), 1234l), false, false));    assertEquals(DATA, readUsers(eq(binaryColumn("name"), Binary.fromString("miller")), false, false));    assertEquals(DATA, readUsers(eq(binaryColumn("name"), null), false, false));}
public void parquet-mr_f5881_0() throws IOException
{    assertCorrectFiltering(record -> {        Location loc = record.getLocation();        Double lat = loc == null ? null : loc.getLat();        Double lon = loc == null ? null : loc.getLon();        return lat != null && lon != null && 37 <= lat && lat <= 70 && -21 <= lon && lon <= 35;    }, and(and(gtEq(doubleColumn("location.lat"), 37.0), ltEq(doubleColumn("location.lat"), 70.0)), and(gtEq(doubleColumn("location.lon"), -21.0), ltEq(doubleColumn("location.lon"), 35.0))));    assertCorrectFiltering(record -> {        Location loc = record.getLocation();        return loc == null || (loc.getLat() == null && loc.getLon() == null);    }, and(eq(doubleColumn("location.lat"), null), eq(doubleColumn("location.lon"), null)));    assertCorrectFiltering(record -> {        String name = record.getName();        return name != null && name.compareTo("thomas") < 0 && record.getId() <= 3 * DATA.size() / 4;    }, and(lt(binaryColumn("name"), Binary.fromString("thomas")), ltEq(longColumn("id"), 3l * DATA.size() / 4)));}
private static boolean parquet-mr_f5882_0(String str)
{    if (str == null || str.isEmpty()) {        return false;    }    switch(str.charAt(0)) {        case 'a':        case 'e':        case 'i':        case 'o':        case 'u':            return true;        default:            return false;    }}
public boolean parquet-mr_f5883_0(Binary value)
{    return value != null && isStartingWithVowel(value.toStringUsingUTF8());}
public boolean parquet-mr_f5884_0(Statistics<Binary> statistics)
{    Comparator<Binary> cmp = statistics.getComparator();    Binary min = statistics.getMin();    Binary max = statistics.getMax();    return cmp.compare(max, A) < 0 || (cmp.compare(min, B) >= 0 && cmp.compare(max, E) < 0) || (cmp.compare(min, F) >= 0 && cmp.compare(max, I) < 0) || (cmp.compare(min, J) >= 0 && cmp.compare(max, O) < 0) || (cmp.compare(min, P) >= 0 && cmp.compare(max, U) < 0) || cmp.compare(min, V) >= 0;}
public boolean parquet-mr_f5885_0(Statistics<Binary> statistics)
{    Comparator<Binary> cmp = statistics.getComparator();    Binary min = statistics.getMin();    Binary max = statistics.getMax();    return (cmp.compare(min, A) >= 0 && cmp.compare(max, B) < 0) || (cmp.compare(min, E) >= 0 && cmp.compare(max, F) < 0) || (cmp.compare(min, I) >= 0 && cmp.compare(max, J) < 0) || (cmp.compare(min, O) >= 0 && cmp.compare(max, P) < 0) || (cmp.compare(min, U) >= 0 && cmp.compare(max, V) < 0);}
public boolean parquet-mr_f5886_0(Long value)
{        return value % divisor == 0;}
public boolean parquet-mr_f5887_0(Statistics<Long> statistics)
{    long min = statistics.getMin();    long max = statistics.getMax();    return min % divisor != 0 && max % divisor != 0 && min / divisor == max / divisor;}
public boolean parquet-mr_f5888_0(Statistics<Long> statistics)
{    long min = statistics.getMin();    long max = statistics.getMax();    return min == max && min % divisor == 0;}
public void parquet-mr_f5889_0() throws IOException
{    assertCorrectFiltering(record -> NameStartsWithVowel.isStartingWithVowel(record.getName()) || record.getId() % 234 == 0, or(userDefined(binaryColumn("name"), NameStartsWithVowel.class), userDefined(longColumn("id"), new IsDivisibleBy(234))));    assertCorrectFiltering(record -> !(NameStartsWithVowel.isStartingWithVowel(record.getName()) || record.getId() % 234 == 0), not(or(userDefined(binaryColumn("name"), NameStartsWithVowel.class), userDefined(longColumn("id"), new IsDivisibleBy(234)))));}
public void parquet-mr_f5890_0() throws IOException
{        assertEquals(DATA, readUsers(notEq(binaryColumn("not-existing-binary"), Binary.EMPTY), true));    assertCorrectFiltering(record -> record.getId() == 1234, and(eq(longColumn("id"), 1234l), eq(longColumn("not-existing-long"), null)));    assertCorrectFiltering(record -> "miller".equals(record.getName()), and(eq(binaryColumn("name"), Binary.fromString("miller")), invert(userDefined(binaryColumn("not-existing-binary"), NameStartsWithVowel.class))));        assertEquals(emptyList(), readUsers(lt(longColumn("not-existing-long"), 0l), true));    assertCorrectFiltering(record -> "miller".equals(record.getName()), or(eq(binaryColumn("name"), Binary.fromString("miller")), gtEq(binaryColumn("not-existing-binary"), Binary.EMPTY)));    assertCorrectFiltering(record -> record.getId() == 1234, or(eq(longColumn("id"), 1234l), userDefined(longColumn("not-existing-long"), new IsDivisibleBy(1))));}
private Path parquet-mr_f5891_0(Configuration conf, CompressionCodecName compression) throws IOException
{    File file = tempFolder.newFile();    file.delete();    Path path = new Path(file.toURI());    for (int i = 0; i < PAGE_SIZE; i++) {        colAPage1Bytes[i] = (byte) i;        colAPage2Bytes[i] = (byte) -i;        colBPage1Bytes[i] = (byte) (i + 100);        colBPage2Bytes[i] = (byte) (i - 100);    }    ParquetFileWriter writer = new ParquetFileWriter(conf, schemaSimple, path, ParquetWriter.DEFAULT_BLOCK_SIZE, ParquetWriter.MAX_PADDING_SIZE_DEFAULT);    writer.start();    writer.startBlock(numRecordsLargeFile);    CodecFactory codecFactory = new CodecFactory(conf, PAGE_SIZE);    CodecFactory.BytesCompressor compressor = codecFactory.getCompressor(compression);    ColumnChunkPageWriteStore writeStore = new ColumnChunkPageWriteStore(compressor, schemaSimple, new HeapByteBufferAllocator(), Integer.MAX_VALUE, ParquetOutputFormat.getPageWriteChecksumEnabled(conf));    PageWriter pageWriter = writeStore.getPageWriter(colADesc);    pageWriter.writePage(BytesInput.from(colAPage1Bytes), numRecordsLargeFile / 2, numRecordsLargeFile / 2, EMPTY_STATS_INT32, Encoding.RLE, Encoding.RLE, Encoding.PLAIN);    pageWriter.writePage(BytesInput.from(colAPage2Bytes), numRecordsLargeFile / 2, numRecordsLargeFile / 2, EMPTY_STATS_INT32, Encoding.RLE, Encoding.RLE, Encoding.PLAIN);    pageWriter = writeStore.getPageWriter(colBDesc);    pageWriter.writePage(BytesInput.from(colBPage1Bytes), numRecordsLargeFile / 2, numRecordsLargeFile / 2, EMPTY_STATS_INT32, Encoding.RLE, Encoding.RLE, Encoding.PLAIN);    pageWriter.writePage(BytesInput.from(colBPage2Bytes), numRecordsLargeFile / 2, numRecordsLargeFile / 2, EMPTY_STATS_INT32, Encoding.RLE, Encoding.RLE, Encoding.PLAIN);    writeStore.flushToFileWriter(writer);    writer.endBlock();    writer.end(new HashMap<>());    codecFactory.release();    return path;}
private Path parquet-mr_f5892_0(Configuration conf, boolean dictionaryEncoding, CompressionCodecName compression) throws IOException
{    File file = tempFolder.newFile();    file.delete();    Path path = new Path(file.toURI());    try (ParquetWriter<Group> writer = ExampleParquetWriter.builder(path).withConf(conf).withWriteMode(ParquetFileWriter.Mode.OVERWRITE).withCompressionCodec(compression).withDictionaryEncoding(dictionaryEncoding).withType(schemaNestedWithNulls).withPageWriteChecksumEnabled(ParquetOutputFormat.getPageWriteChecksumEnabled(conf)).build()) {        GroupFactory groupFactory = new SimpleGroupFactory(schemaNestedWithNulls);        Random rand = new Random(42);        for (int i = 0; i < numRecordsNestedWithNullsFile; i++) {            Group group = groupFactory.newGroup();            if (rand.nextDouble() > nullRatio) {                                if (rand.nextDouble() > 0.5) {                    group.addGroup("c").append("id", (long) i).addGroup("d").append("val", rand.nextInt() % 10);                } else {                    group.addGroup("c").append("id", (long) i).addGroup("d").append("val", rand.nextInt() % 10).append("val", rand.nextInt() % 10).append("val", rand.nextInt() % 10);                }            }            writer.write(group);        }    }    return path;}
public void parquet-mr_f5893_0() throws IOException
{    Configuration conf = new Configuration();    conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, true);    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, false);    Path path = writeSimpleParquetFile(conf, CompressionCodecName.UNCOMPRESSED);    try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colADesc, colBDesc))) {        PageReadStore pageReadStore = reader.readNextRowGroup();        DataPageV1 colAPage1 = readNextPage(colADesc, pageReadStore);        assertCrcSetAndCorrect(colAPage1, colAPage1Bytes);        assertCorrectContent(colAPage1.getBytes().toByteArray(), colAPage1Bytes);        DataPageV1 colAPage2 = readNextPage(colADesc, pageReadStore);        assertCrcSetAndCorrect(colAPage2, colAPage2Bytes);        assertCorrectContent(colAPage2.getBytes().toByteArray(), colAPage2Bytes);        DataPageV1 colBPage1 = readNextPage(colBDesc, pageReadStore);        assertCrcSetAndCorrect(colBPage1, colBPage1Bytes);        assertCorrectContent(colBPage1.getBytes().toByteArray(), colBPage1Bytes);        DataPageV1 colBPage2 = readNextPage(colBDesc, pageReadStore);        assertCrcSetAndCorrect(colBPage2, colBPage2Bytes);        assertCorrectContent(colBPage2.getBytes().toByteArray(), colBPage2Bytes);    }}
public void parquet-mr_f5894_0() throws IOException
{    Configuration conf = new Configuration();    conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, false);    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, false);    Path path = writeSimpleParquetFile(conf, CompressionCodecName.UNCOMPRESSED);    try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colADesc, colBDesc))) {        PageReadStore pageReadStore = reader.readNextRowGroup();        assertCrcNotSet(readNextPage(colADesc, pageReadStore));        assertCrcNotSet(readNextPage(colADesc, pageReadStore));        assertCrcNotSet(readNextPage(colBDesc, pageReadStore));        assertCrcNotSet(readNextPage(colBDesc, pageReadStore));    }}
public void parquet-mr_f5895_0() throws IOException
{    Configuration conf = new Configuration();    conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, false);    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, true);    Path path = writeSimpleParquetFile(conf, CompressionCodecName.UNCOMPRESSED);    try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colADesc, colBDesc))) {        PageReadStore pageReadStore = reader.readNextRowGroup();        assertCorrectContent(readNextPage(colADesc, pageReadStore).getBytes().toByteArray(), colAPage1Bytes);        assertCorrectContent(readNextPage(colADesc, pageReadStore).getBytes().toByteArray(), colAPage2Bytes);        assertCorrectContent(readNextPage(colBDesc, pageReadStore).getBytes().toByteArray(), colBPage1Bytes);        assertCorrectContent(readNextPage(colBDesc, pageReadStore).getBytes().toByteArray(), colBPage2Bytes);    }}
public void parquet-mr_f5896_0() throws IOException
{    Configuration conf = new Configuration();    conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, true);    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, true);    Path path = writeSimpleParquetFile(conf, CompressionCodecName.UNCOMPRESSED);    try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colADesc, colBDesc))) {        PageReadStore pageReadStore = reader.readNextRowGroup();        DataPageV1 colAPage1 = readNextPage(colADesc, pageReadStore);        assertCrcSetAndCorrect(colAPage1, colAPage1Bytes);        assertCorrectContent(colAPage1.getBytes().toByteArray(), colAPage1Bytes);        DataPageV1 colAPage2 = readNextPage(colADesc, pageReadStore);        assertCrcSetAndCorrect(colAPage2, colAPage2Bytes);        assertCorrectContent(colAPage2.getBytes().toByteArray(), colAPage2Bytes);        DataPageV1 colBPage1 = readNextPage(colBDesc, pageReadStore);        assertCrcSetAndCorrect(colBPage1, colBPage1Bytes);        assertCorrectContent(colBPage1.getBytes().toByteArray(), colBPage1Bytes);        DataPageV1 colBPage2 = readNextPage(colBDesc, pageReadStore);        assertCrcSetAndCorrect(colBPage2, colBPage2Bytes);        assertCorrectContent(colBPage2.getBytes().toByteArray(), colBPage2Bytes);    }}
public void parquet-mr_f5897_0() throws IOException
{    Configuration conf = new Configuration();    conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, true);    Path path = writeSimpleParquetFile(conf, CompressionCodecName.UNCOMPRESSED);    InputFile inputFile = HadoopInputFile.fromPath(path, conf);    try (SeekableInputStream inputStream = inputFile.newStream()) {        int fileLen = (int) inputFile.getLength();        byte[] fileBytes = new byte[fileLen];        inputStream.readFully(fileBytes);        inputStream.close();                                fileBytes[fileLen / 8]++;        fileBytes[fileLen / 8 + ((fileLen / 4) * 3)]++;        OutputFile outputFile = HadoopOutputFile.fromPath(path, conf);        try (PositionOutputStream outputStream = outputFile.createOrOverwrite(1024 * 1024)) {            outputStream.write(fileBytes);            outputStream.close();                                    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, false);            try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colADesc, colBDesc))) {                PageReadStore pageReadStore = reader.readNextRowGroup();                DataPageV1 colAPage1 = readNextPage(colADesc, pageReadStore);                assertFalse("Data in page was not corrupted", Arrays.equals(colAPage1.getBytes().toByteArray(), colAPage1Bytes));                readNextPage(colADesc, pageReadStore);                readNextPage(colBDesc, pageReadStore);                DataPageV1 colBPage2 = readNextPage(colBDesc, pageReadStore);                assertFalse("Data in page was not corrupted", Arrays.equals(colBPage2.getBytes().toByteArray(), colBPage2Bytes));            }                        conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, true);            try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colADesc, colBDesc))) {                                assertVerificationFailed(reader);            }        }    }}
public void parquet-mr_f5898_0() throws IOException
{    Configuration conf = new Configuration();    conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, true);    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, true);    Path path = writeSimpleParquetFile(conf, CompressionCodecName.SNAPPY);    try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colADesc, colBDesc))) {        PageReadStore pageReadStore = reader.readNextRowGroup();        DataPageV1 colAPage1 = readNextPage(colADesc, pageReadStore);        assertCrcSetAndCorrect(colAPage1, snappy(colAPage1Bytes));        assertCorrectContent(colAPage1.getBytes().toByteArray(), colAPage1Bytes);        DataPageV1 colAPage2 = readNextPage(colADesc, pageReadStore);        assertCrcSetAndCorrect(colAPage2, snappy(colAPage2Bytes));        assertCorrectContent(colAPage2.getBytes().toByteArray(), colAPage2Bytes);        DataPageV1 colBPage1 = readNextPage(colBDesc, pageReadStore);        assertCrcSetAndCorrect(colBPage1, snappy(colBPage1Bytes));        assertCorrectContent(colBPage1.getBytes().toByteArray(), colBPage1Bytes);        DataPageV1 colBPage2 = readNextPage(colBDesc, pageReadStore);        assertCrcSetAndCorrect(colBPage2, snappy(colBPage2Bytes));        assertCorrectContent(colBPage2.getBytes().toByteArray(), colBPage2Bytes);    }}
public void parquet-mr_f5899_0() throws IOException
{    Configuration conf = new Configuration();            conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, false);    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, false);    Path refPath = writeNestedWithNullsSampleParquetFile(conf, false, CompressionCodecName.SNAPPY);    try (ParquetFileReader refReader = getParquetFileReader(refPath, conf, Arrays.asList(colCIdDesc, colDValDesc))) {        PageReadStore refPageReadStore = refReader.readNextRowGroup();        byte[] colCIdPageBytes = readNextPage(colCIdDesc, refPageReadStore).getBytes().toByteArray();        byte[] colDValPageBytes = readNextPage(colDValDesc, refPageReadStore).getBytes().toByteArray();                conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, true);        conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, true);        Path path = writeNestedWithNullsSampleParquetFile(conf, false, CompressionCodecName.SNAPPY);        try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colCIdDesc, colDValDesc))) {            PageReadStore pageReadStore = reader.readNextRowGroup();            DataPageV1 colCIdPage = readNextPage(colCIdDesc, pageReadStore);            assertCrcSetAndCorrect(colCIdPage, snappy(colCIdPageBytes));            assertCorrectContent(colCIdPage.getBytes().toByteArray(), colCIdPageBytes);            DataPageV1 colDValPage = readNextPage(colDValDesc, pageReadStore);            assertCrcSetAndCorrect(colDValPage, snappy(colDValPageBytes));            assertCorrectContent(colDValPage.getBytes().toByteArray(), colDValPageBytes);        }    }}
public void parquet-mr_f5900_0() throws IOException
{    Configuration conf = new Configuration();            conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, false);    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, false);    Path refPath = writeNestedWithNullsSampleParquetFile(conf, true, CompressionCodecName.SNAPPY);    try (ParquetFileReader refReader = getParquetFileReader(refPath, conf, Collections.singletonList(colDValDesc))) {        PageReadStore refPageReadStore = refReader.readNextRowGroup();                byte[] dictPageBytes = readDictPage(colDValDesc, refPageReadStore).getBytes().toByteArray();        byte[] colDValPageBytes = readNextPage(colDValDesc, refPageReadStore).getBytes().toByteArray();                conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, true);        conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, true);        Path path = writeNestedWithNullsSampleParquetFile(conf, true, CompressionCodecName.SNAPPY);        try (ParquetFileReader reader = getParquetFileReader(path, conf, Collections.singletonList(colDValDesc))) {            PageReadStore pageReadStore = reader.readNextRowGroup();            DictionaryPage dictPage = readDictPage(colDValDesc, pageReadStore);            assertCrcSetAndCorrect(dictPage, snappy(dictPageBytes));            assertCorrectContent(dictPage.getBytes().toByteArray(), dictPageBytes);            DataPageV1 colDValPage = readNextPage(colDValDesc, pageReadStore);            assertCrcSetAndCorrect(colDValPage, snappy(colDValPageBytes));            assertCorrectContent(colDValPage.getBytes().toByteArray(), colDValPageBytes);        }    }}
private byte[] parquet-mr_f5901_0(byte[] bytes) throws IOException
{    SnappyCompressor compressor = new SnappyCompressor();    compressor.reset();    compressor.setInput(bytes, 0, bytes.length);    compressor.finish();    byte[] buffer = new byte[bytes.length * 2];    int compressedSize = compressor.compress(buffer, 0, buffer.length);    return Arrays.copyOfRange(buffer, 0, compressedSize);}
private ParquetFileReader parquet-mr_f5902_0(Path path, Configuration conf, List<ColumnDescriptor> columns) throws IOException
{    ParquetMetadata footer = ParquetFileReader.readFooter(conf, path);    return new ParquetFileReader(conf, footer.getFileMetaData(), path, footer.getBlocks(), columns);}
private DictionaryPage parquet-mr_f5903_0(ColumnDescriptor colDesc, PageReadStore pageReadStore)
{    return pageReadStore.getPageReader(colDesc).readDictionaryPage();}
private DataPageV1 parquet-mr_f5904_0(ColumnDescriptor colDesc, PageReadStore pageReadStore)
{    return (DataPageV1) pageReadStore.getPageReader(colDesc).readPage();}
private void parquet-mr_f5905_0(byte[] pageBytes, byte[] referenceBytes)
{    assertArrayEquals("Read page content was different from expected page content", referenceBytes, pageBytes);}
private void parquet-mr_f5906_0(Page page, byte[] referenceBytes)
{    assertTrue("Checksum was not set in page", page.getCrc().isPresent());    int crcFromPage = page.getCrc().getAsInt();    crc.reset();    crc.update(referenceBytes);    assertEquals("Checksum found in page did not match calculated reference checksum", crc.getValue(), (long) crcFromPage & 0xffffffffL);}
private void parquet-mr_f5907_0(Page page)
{    assertFalse("Checksum was set in page", page.getCrc().isPresent());}
private void parquet-mr_f5908_0(ParquetFileReader reader)
{    try {        reader.readNextRowGroup();        fail("Expected checksum verification exception to be thrown");    } catch (Exception e) {        assertTrue("Thrown exception is of incorrect type", e instanceof ParquetDecodingException);        assertTrue("Did not catch checksum verification ParquetDecodingException", e.getMessage().contains("CRC checksum verification failed"));    }}
private void parquet-mr_f5909_0(int size, CompressionCodecName codec, boolean useOnHeapCompression, Decompression decomp)
{    ByteBuffer rawBuf = null;    ByteBuffer outBuf = null;    ByteBufferAllocator allocator = null;    try {        allocator = new DirectByteBufferAllocator();        final CodecFactory codecFactory = CodecFactory.createDirectCodecFactory(new Configuration(), allocator, pageSize);        rawBuf = allocator.allocate(size);        final byte[] rawArr = new byte[size];        outBuf = allocator.allocate(size * 2);        final Random r = new Random();        final byte[] random = new byte[1024];        int pos = 0;        while (pos < size) {            r.nextBytes(random);            rawBuf.put(random);            System.arraycopy(random, 0, rawArr, pos, random.length);            pos += random.length;        }        rawBuf.flip();        final DirectCodecFactory.BytesCompressor c = codecFactory.getCompressor(codec);        final CodecFactory.BytesDecompressor d = codecFactory.getDecompressor(codec);        final BytesInput compressed;        if (useOnHeapCompression) {            compressed = c.compress(BytesInput.from(rawArr));        } else {            compressed = c.compress(BytesInput.from(rawBuf));        }        switch(decomp) {            case OFF_HEAP:                {                    final ByteBuffer buf = compressed.toByteBuffer();                    final ByteBuffer b = allocator.allocate(buf.capacity());                    try {                        b.put(buf);                        b.flip();                        d.decompress(b, (int) compressed.size(), outBuf, size);                        for (int i = 0; i < size; i++) {                            Assert.assertTrue("Data didn't match at " + i, outBuf.get(i) == rawBuf.get(i));                        }                    } finally {                        allocator.release(b);                    }                    break;                }            case OFF_HEAP_BYTES_INPUT:                {                    final ByteBuffer buf = compressed.toByteBuffer();                    final ByteBuffer b = allocator.allocate(buf.limit());                    try {                        b.put(buf);                        b.flip();                        final BytesInput input = d.decompress(BytesInput.from(b), size);                        Assert.assertArrayEquals(String.format("While testing codec %s", codec), input.toByteArray(), rawArr);                    } finally {                        allocator.release(b);                    }                    break;                }            case ON_HEAP:                {                    final byte[] buf = compressed.toByteArray();                    final BytesInput input = d.decompress(BytesInput.from(buf), size);                    Assert.assertArrayEquals(input.toByteArray(), rawArr);                    break;                }        }    } catch (Exception e) {        final String msg = String.format("Failure while testing Codec: %s, OnHeapCompressionInput: %s, Decompression Mode: %s, Data Size: %d", codec.name(), useOnHeapCompression, decomp.name(), size);        System.out.println(msg);        throw new RuntimeException(msg, e);    } finally {        if (rawBuf != null) {            allocator.release(rawBuf);        }        if (outBuf != null) {            allocator.release(rawBuf);        }    }}
public void parquet-mr_f5910_0()
{    String errorMsg = "Test failed, creation of a direct codec factory should have failed when passed a non-direct allocator.";    try {        CodecFactory.createDirectCodecFactory(new Configuration(), new HeapByteBufferAllocator(), 0);        throw new RuntimeException(errorMsg);    } catch (IllegalStateException ex) {                Assert.assertTrue("Missing expected error message.", ex.getMessage().contains("A DirectCodecFactory requires a direct buffer allocator be provided."));    } catch (Exception ex) {        throw new RuntimeException(errorMsg + " Failed with the wrong error.");    }}
public void parquet-mr_f5911_0() throws Exception
{    final int[] sizes = { 4 * 1024, 1 * 1024 * 1024 };    final boolean[] comp = { true, false };    Set<CompressionCodecName> codecsToSkip = new HashSet<>();        codecsToSkip.add(LZO);        codecsToSkip.add(LZ4);        codecsToSkip.add(ZSTD);    for (final int size : sizes) {        for (final boolean useOnHeapComp : comp) {            for (final Decompression decomp : Decompression.values()) {                for (final CompressionCodecName codec : CompressionCodecName.values()) {                    if (codecsToSkip.contains(codec)) {                        continue;                    }                    test(size, codec, useOnHeapComp, decomp);                }            }        }    }}
public void parquet-mr_f5912_0()
{    blocks = new ArrayList<BlockMetaData>();    for (int i = 0; i < 10; i++) {        blocks.add(newBlock(i * 10, 10));    }    schema = MessageTypeParser.parseMessageType("message doc { required binary foo; }");    fileMetaData = new FileMetaData(schema, new HashMap<String, String>(), "parquet-mr");}
public void parquet-mr_f5913_0() throws IOException
{    try {        generateSplitByMinMaxSize(50, 49);        fail("should throw exception when max split size is smaller than the min split size");    } catch (ParquetDecodingException e) {        assertEquals("maxSplitSize and minSplitSize should be positive and max should be greater or equal to the minSplitSize: maxSplitSize = 49; minSplitSize is 50", e.getMessage());    }}
public void parquet-mr_f5914_0() throws IOException
{    try {        generateSplitByMinMaxSize(-100, -50);        fail("should throw exception when max split size is negative");    } catch (ParquetDecodingException e) {        assertEquals("maxSplitSize and minSplitSize should be positive and max should be greater or equal to the minSplitSize: maxSplitSize = -50; minSplitSize is -100", e.getMessage());    }}
public void parquet-mr_f5915_0() throws IOException
{    IntColumn intColumn = intColumn("foo");    FilterPredicate p = or(eq(intColumn, 7), eq(intColumn, 12));    Configuration conf = new Configuration();    ParquetInputFormat.setFilterPredicate(conf, p);    Filter read = ParquetInputFormat.getFilter(conf);    assertTrue(read instanceof FilterPredicateCompat);    assertEquals(p, ((FilterPredicateCompat) read).getFilterPredicate());    conf = new Configuration();    ParquetInputFormat.setFilterPredicate(conf, not(p));    read = ParquetInputFormat.getFilter(conf);    assertTrue(read instanceof FilterPredicateCompat);    assertEquals(and(notEq(intColumn, 7), notEq(intColumn, 12)), ((FilterPredicateCompat) read).getFilterPredicate());    assertEquals(FilterCompat.NOOP, ParquetInputFormat.getFilter(new Configuration()));}
public void parquet-mr_f5916_0() throws IOException
{    withHDFSBlockSize(50, 50);    List<ParquetInputSplit> splits = generateSplitByMinMaxSize(50, 50);    shouldSplitBlockSizeBe(splits, 5, 5);    shouldSplitLocationBe(splits, 0, 1);    shouldSplitLengthBe(splits, 50, 50);    splits = generateSplitByMinMaxSize(0, Long.MAX_VALUE);    shouldSplitBlockSizeBe(splits, 5, 5);    shouldSplitLocationBe(splits, 0, 1);    shouldSplitLengthBe(splits, 50, 50);}
public void parquet-mr_f5917_0() throws IOException
{        withHDFSBlockSize(51, 51);    List<ParquetInputSplit> splits = generateSplitByMinMaxSize(50, 50);    shouldSplitBlockSizeBe(splits, 5, 5);        shouldSplitLocationBe(splits, 0, 0);    shouldSplitLengthBe(splits, 50, 50);        withHDFSBlockSize(49, 49);    splits = generateSplitByMinMaxSize(50, 50);    shouldSplitBlockSizeBe(splits, 5, 5);    shouldSplitLocationBe(splits, 0, 1);    shouldSplitLengthBe(splits, 50, 50);    /*    aaaa bbbbb c    for the 5th row group, the midpoint is 45, but the end of first hdfsBlock is 44, therefore a new split(b) will be created    for 9th group, the mid point is 85, the end of second block is 88, so it's considered mainly in the 2nd hdfs block, and therefore inserted as    a row group of split b     */    withHDFSBlockSize(44, 44, 44);    splits = generateSplitByMinMaxSize(40, 50);    shouldSplitBlockSizeBe(splits, 4, 5, 1);    shouldSplitLocationBe(splits, 0, 0, 2);    shouldSplitLengthBe(splits, 40, 50, 10);}
public void parquet-mr_f5918_0() throws IOException, InterruptedException
{    withHDFSBlockSize(50, 50);    List<ParquetInputSplit> splits = generateSplitByMinMaxSize(55, 56);    shouldSplitBlockSizeBe(splits, 6, 4);    shouldSplitLocationBe(splits, 0, 1);    shouldSplitLengthBe(splits, 60, 40);    withHDFSBlockSize(51, 51);    splits = generateSplitByMinMaxSize(55, 56);    shouldSplitBlockSizeBe(splits, 6, 4);        shouldSplitLocationBe(splits, 0, 1);    shouldSplitLengthBe(splits, 60, 40);    withHDFSBlockSize(49, 49, 49);    splits = generateSplitByMinMaxSize(55, 56);    shouldSplitBlockSizeBe(splits, 6, 4);    shouldSplitLocationBe(splits, 0, 1);    shouldSplitLengthBe(splits, 60, 40);}
public void parquet-mr_f5919_0() throws Exception
{    withHDFSBlockSize(50, 50);    List<ParquetInputSplit> splits = generateSplitByMinMaxSize(18, 30);    shouldSplitBlockSizeBe(splits, 3, 2, 3, 2);    shouldSplitLocationBe(splits, 0, 0, 1, 1);    shouldSplitLengthBe(splits, 30, 20, 30, 20);    /*    aaabb cccdd         */    withHDFSBlockSize(51, 51);    splits = generateSplitByMinMaxSize(18, 30);    shouldSplitBlockSizeBe(splits, 3, 2, 3, 2);        shouldSplitLocationBe(splits, 0, 0, 0, 1);    shouldSplitLengthBe(splits, 30, 20, 30, 20);    /*    aaabb cccdd     */    withHDFSBlockSize(49, 49, 49);    splits = generateSplitByMinMaxSize(18, 30);    shouldSplitBlockSizeBe(splits, 3, 2, 3, 2);    shouldSplitLocationBe(splits, 0, 0, 1, 1);    shouldSplitLengthBe(splits, 30, 20, 30, 20);}
public void parquet-mr_f5920_0() throws Exception
{    withHDFSBlockSize(50, 50);    List<ParquetInputSplit> splits = generateSplitByMinMaxSize(25, 30);    shouldSplitBlockSizeBe(splits, 3, 3, 3, 1);    shouldSplitLocationBe(splits, 0, 0, 1, 1);    shouldSplitLengthBe(splits, 30, 30, 30, 10);}
public void parquet-mr_f5921_0() throws Exception
{    withHDFSBlockSize(50, 50);    List<ParquetInputSplit> splits = generateSplitByMinMaxSize(10, 18);    shouldSplitBlockSizeBe(splits, 2, 2, 1, 2, 2, 1);    shouldSplitLocationBe(splits, 0, 0, 0, 1, 1, 1);    shouldSplitLengthBe(splits, 20, 20, 10, 20, 20, 10);    /*    aabbc ddeef    notice the first byte of split d is in the first hdfs block:    when adding the 6th row group, although the first byte of it is in the first hdfs block    , but the mid point of the row group is in the second hdfs block, there for a new split(d) is created including that row group     */    withHDFSBlockSize(51, 51);    splits = generateSplitByMinMaxSize(10, 18);    shouldSplitBlockSizeBe(splits, 2, 2, 1, 2, 2, 1);        shouldSplitLocationBe(splits, 0, 0, 0, 0, 1, 1);    shouldSplitLengthBe(splits, 20, 20, 10, 20, 20, 10);    /*    aabbc ddeef    same as the case where block sizes are 50 50     */    withHDFSBlockSize(49, 49);    splits = generateSplitByMinMaxSize(10, 18);    shouldSplitBlockSizeBe(splits, 2, 2, 1, 2, 2, 1);    shouldSplitLocationBe(splits, 0, 0, 0, 1, 1, 1);    shouldSplitLengthBe(splits, 20, 20, 10, 20, 20, 10);}
public RecordFilter parquet-mr_f5922_0(Iterable<ColumnReader> readers)
{    return null;}
public void parquet-mr_f5923_0() throws Exception
{    IntColumn foo = intColumn("foo");    FilterPredicate p = or(eq(foo, 10), eq(foo, 11));    Job job = new Job();    Configuration conf = job.getConfiguration();    ParquetInputFormat.setUnboundRecordFilter(job, DummyUnboundRecordFilter.class);    try {        ParquetInputFormat.setFilterPredicate(conf, p);        fail("this should throw");    } catch (IllegalArgumentException e) {        assertEquals("You cannot provide a FilterPredicate after providing an UnboundRecordFilter", e.getMessage());    }    job = new Job();    conf = job.getConfiguration();    ParquetInputFormat.setFilterPredicate(conf, p);    try {        ParquetInputFormat.setUnboundRecordFilter(job, DummyUnboundRecordFilter.class);        fail("this should throw");    } catch (IllegalArgumentException e) {        assertEquals("You cannot provide an UnboundRecordFilter after providing a FilterPredicate", e.getMessage());    }}
public static BlockMetaData parquet-mr_f5924_0(IntStatistics stats, long valueCount)
{    BlockMetaData blockMetaData = new BlockMetaData();    ColumnChunkMetaData column = ColumnChunkMetaData.get(ColumnPath.get("foo"), PrimitiveTypeName.INT32, CompressionCodecName.GZIP, new HashSet<Encoding>(Arrays.asList(Encoding.PLAIN)), stats, 100l, 100l, valueCount, 100l, 100l);    blockMetaData.addColumn(column);    blockMetaData.setTotalByteSize(200l);    blockMetaData.setRowCount(valueCount);    return blockMetaData;}
public void parquet-mr_f5925_0() throws IOException, InterruptedException
{    File tempFile = getTempFile();    FileSystem fs = FileSystem.getLocal(new Configuration());    ParquetInputFormat.FootersCacheValue cacheValue = getDummyCacheValue(tempFile, fs);    assertTrue(tempFile.setLastModified(tempFile.lastModified() + 5000));    assertFalse(cacheValue.isCurrent(new ParquetInputFormat.FileStatusWrapper(fs.getFileStatus(new Path(tempFile.getAbsolutePath())))));}
public void parquet-mr_f5926_0() throws IOException
{    File tempFile = getTempFile();    FileSystem fs = FileSystem.getLocal(new Configuration());    ParquetInputFormat.FootersCacheValue cacheValue = getDummyCacheValue(tempFile, fs);    assertTrue(cacheValue.isNewerThan(null));    assertFalse(cacheValue.isNewerThan(cacheValue));    assertTrue(tempFile.setLastModified(tempFile.lastModified() + 5000));    ParquetInputFormat.FootersCacheValue newerCacheValue = getDummyCacheValue(tempFile, fs);    assertTrue(newerCacheValue.isNewerThan(cacheValue));    assertFalse(cacheValue.isNewerThan(newerCacheValue));}
public void parquet-mr_f5927_0() throws Exception
{    withHDFSBlockSize(50, 50);    List<ParquetInputSplit> splits = generateSplitByDeprecatedConstructor(50, 50);    shouldSplitBlockSizeBe(splits, 5, 5);    shouldOneSplitRowGroupOffsetBe(splits.get(0), 0, 10, 20, 30, 40);    shouldOneSplitRowGroupOffsetBe(splits.get(1), 50, 60, 70, 80, 90);    shouldSplitLengthBe(splits, 50, 50);    shouldSplitStartBe(splits, 0, 50);}
public void parquet-mr_f5928_0() throws IOException
{    File tempDir = Files.createTempDir();    tempDir.deleteOnExit();        int numFiles = 10;    String url = "";    for (int i = 0; i < numFiles; i++) {        File file = new File(tempDir, String.format("part-%05d.parquet", i));        createParquetFile(file);        if (i > 0) {            url += ",";        }        url += "file:" + file.getAbsolutePath();    }    Job job = new Job();    FileInputFormat.setInputPaths(job, url);    List<Footer> footers = new ParquetInputFormat<Object>().getFooters(job);    for (int i = 0; i < numFiles; i++) {        Footer footer = footers.get(i);        File file = new File(tempDir, String.format("part-%05d.parquet", i));        assertEquals("file:" + file.getAbsolutePath(), footer.getFile().toString());    }}
private void parquet-mr_f5929_0(File file) throws IOException
{    Path path = new Path(file.toURI());    Configuration configuration = new Configuration();    MessageType schema = MessageTypeParser.parseMessageType("message m { required group a {required binary b;}}");    String[] columnPath = { "a", "b" };    ColumnDescriptor c1 = schema.getColumnDescription(columnPath);    byte[] bytes1 = { 0, 1, 2, 3 };    byte[] bytes2 = { 2, 3, 4, 5 };    CompressionCodecName codec = CompressionCodecName.UNCOMPRESSED;    BinaryStatistics stats = new BinaryStatistics();    ParquetFileWriter w = new ParquetFileWriter(configuration, schema, path);    w.start();    w.startBlock(3);    w.startColumn(c1, 5, codec);    w.writeDataPage(2, 4, BytesInput.from(bytes1), stats, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(bytes1), stats, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.startBlock(4);    w.startColumn(c1, 7, codec);    w.writeDataPage(7, 4, BytesInput.from(bytes2), stats, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.end(new HashMap<String, String>());}
private File parquet-mr_f5930_0() throws IOException
{    File tempFile = File.createTempFile("footer_", ".txt");    tempFile.deleteOnExit();    return tempFile;}
private ParquetInputFormat.FootersCacheValue parquet-mr_f5931_0(File file, FileSystem fs) throws IOException
{    Path path = new Path(file.getPath());    FileStatus status = fs.getFileStatus(path);    ParquetInputFormat.FileStatusWrapper statusWrapper = new ParquetInputFormat.FileStatusWrapper(status);    ParquetMetadata mockMetadata = mock(ParquetMetadata.class);    ParquetInputFormat.FootersCacheValue cacheValue = new ParquetInputFormat.FootersCacheValue(statusWrapper, new Footer(path, mockMetadata));    assertTrue(cacheValue.isCurrent(statusWrapper));    return cacheValue;}
private List<ParquetInputSplit> parquet-mr_f5932_0(long min, long max) throws IOException
{    return ClientSideMetadataSplitStrategy.generateSplits(blocks, hdfsBlocks, fileStatus, schema.toString(), extramd, min, max);}
private List<ParquetInputSplit> parquet-mr_f5933_0(long min, long max) throws IOException
{    List<ParquetInputSplit> splits = new ArrayList<ParquetInputSplit>();    List<ClientSideMetadataSplitStrategy.SplitInfo> splitInfos = ClientSideMetadataSplitStrategy.generateSplitInfo(blocks, hdfsBlocks, min, max);    for (ClientSideMetadataSplitStrategy.SplitInfo splitInfo : splitInfos) {        ParquetInputSplit split = new ParquetInputSplit(fileStatus.getPath(), splitInfo.hdfsBlock.getOffset(), splitInfo.hdfsBlock.getLength(), splitInfo.hdfsBlock.getHosts(), splitInfo.rowGroups, schema.toString(), null, null, extramd);        splits.add(split);    }    return splits;}
private void parquet-mr_f5934_0(List<ParquetInputSplit> splits, long... offsets)
{    assertEquals(message(splits), offsets.length, splits.size());    for (int i = 0; i < offsets.length; i++) {        assertEquals(message(splits) + i, offsets[i], splits.get(i).getStart());    }}
private void parquet-mr_f5935_0(List<ParquetInputSplit> splits, int... sizes)
{    assertEquals(message(splits), sizes.length, splits.size());    for (int i = 0; i < sizes.length; i++) {        assertEquals(message(splits) + i, sizes[i], splits.get(i).getRowGroupOffsets().length);    }}
private void parquet-mr_f5936_0(List<ParquetInputSplit> splits, int... locations) throws IOException
{    assertEquals(message(splits), locations.length, splits.size());    for (int i = 0; i < locations.length; i++) {        int loc = locations[i];        ParquetInputSplit split = splits.get(i);        assertEquals(message(splits) + i, "[foo" + loc + ".datanode, bar" + loc + ".datanode]", Arrays.toString(split.getLocations()));    }}
private void parquet-mr_f5937_0(ParquetInputSplit split, int... rowGroupOffsets)
{    assertEquals(split.toString(), rowGroupOffsets.length, split.getRowGroupOffsets().length);    for (int i = 0; i < rowGroupOffsets.length; i++) {        assertEquals(split.toString(), rowGroupOffsets[i], split.getRowGroupOffsets()[i]);    }}
private String parquet-mr_f5938_0(List<ParquetInputSplit> splits)
{    return String.valueOf(splits) + " " + Arrays.toString(hdfsBlocks) + "\n";}
private void parquet-mr_f5939_0(List<ParquetInputSplit> splits, int... lengths)
{    assertEquals(message(splits), lengths.length, splits.size());    for (int i = 0; i < lengths.length; i++) {        assertEquals(message(splits) + i, lengths[i], splits.get(i).getLength());    }}
private void parquet-mr_f5940_0(long... blockSizes)
{    hdfsBlocks = new BlockLocation[blockSizes.length];    long offset = 0;    for (int i = 0; i < blockSizes.length; i++) {        long blockSize = blockSizes[i];        hdfsBlocks[i] = new BlockLocation(new String[0], new String[] { "foo" + i + ".datanode", "bar" + i + ".datanode" }, offset, blockSize);        offset += blockSize;    }    fileStatus = new FileStatus(offset, false, 2, 50, 0, new Path("hdfs://foo.namenode:1234/bar"));}
private BlockMetaData parquet-mr_f5941_0(long start, long compressedBlockSize)
{    BlockMetaData blockMetaData = new BlockMetaData();        long uncompressedSize = compressedBlockSize * 2;    ColumnChunkMetaData column = ColumnChunkMetaData.get(ColumnPath.get("foo"), PrimitiveTypeName.BINARY, CompressionCodecName.GZIP, new HashSet<Encoding>(Arrays.asList(Encoding.PLAIN)), new BinaryStatistics(), start, 0l, 0l, compressedBlockSize, uncompressedSize);    blockMetaData.addColumn(column);    blockMetaData.setTotalByteSize(uncompressedSize);    return blockMetaData;}
protected void parquet-mr_f5942_0(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{        String line = value.toString();    for (int i = 0; i < line.length(); i += 1) {        Group group = GROUP_FACTORY.newGroup();        group.add(0, Binary.fromString(UUID.randomUUID().toString()));        group.add(1, Binary.fromString(line.substring(i, i + 1)));        context.write(null, group);    }}
public static void parquet-mr_f5943_0(Counter bytesRead)
{    bytesReadCounter = bytesRead;}
protected void parquet-mr_f5944_0(Void key, Group value, Context context) throws IOException, InterruptedException
{        setBytesReadCounter(ContextUtil.getCounter(context, "parquet", "bytesread"));}
public void parquet-mr_f5945_0() throws Exception
{        Assume.assumeTrue(org.apache.hadoop.mapreduce.JobContext.class.isInterface());    File inputFile = temp.newFile();    FileOutputStream out = new FileOutputStream(inputFile);    out.write(FILE_CONTENT.getBytes("UTF-8"));    out.close();    File tempFolder = temp.newFolder();    tempFolder.delete();    Path tempPath = new Path(tempFolder.toURI());    File outputFolder = temp.newFile();    outputFolder.delete();    Configuration conf = new Configuration();        conf.set("parquet.read.schema", Types.buildMessage().required(BINARY).as(UTF8).named("char").named("FormatTestObject").toString());        conf.set("parquet.enable.summary-metadata", "false");    conf.set("parquet.example.schema", PARQUET_TYPE.toString());    {        Job writeJob = new Job(conf, "write");        writeJob.setInputFormatClass(TextInputFormat.class);        TextInputFormat.addInputPath(writeJob, new Path(inputFile.toString()));        writeJob.setOutputFormatClass(ExampleOutputFormat.class);        writeJob.setMapperClass(Writer.class);                writeJob.setNumReduceTasks(0);        ParquetOutputFormat.setBlockSize(writeJob, 10240);        ParquetOutputFormat.setPageSize(writeJob, 512);        ParquetOutputFormat.setDictionaryPageSize(writeJob, 1024);        ParquetOutputFormat.setEnableDictionary(writeJob, true);                ParquetOutputFormat.setMaxPaddingSize(writeJob, 1023);        ParquetOutputFormat.setOutputPath(writeJob, tempPath);        waitForJob(writeJob);    }    long bytesWritten = 0;    FileSystem fs = FileSystem.getLocal(conf);    for (FileStatus file : fs.listStatus(tempPath)) {        bytesWritten += file.getLen();    }    long bytesRead;    {        Job readJob = new Job(conf, "read");        readJob.setInputFormatClass(ExampleInputFormat.class);        TextInputFormat.addInputPath(readJob, tempPath);        readJob.setOutputFormatClass(TextOutputFormat.class);        readJob.setMapperClass(Reader.class);                readJob.setNumReduceTasks(0);        TextOutputFormat.setOutputPath(readJob, new Path(outputFolder.toString()));        waitForJob(readJob);        bytesRead = Reader.bytesReadCounter.getValue();    }    Assert.assertTrue("Should read less than 10% of the input file size", bytesRead < (bytesWritten / 10));}
private void parquet-mr_f5946_0(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {        sleep(100);    }    if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
protected boolean parquet-mr_f5947_0(JobContext context, Path filename)
{    return false;}
protected void parquet-mr_f5948_0(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{        String line = value.toString();    for (int i = 0; i < line.length(); i += 1) {        Group group = GROUP_FACTORY.newGroup();        group.add(0, Binary.fromString(UUID.randomUUID().toString()));        group.add(1, Binary.fromString(line.substring(i, i + 1)));        context.write(null, group);    }}
protected void parquet-mr_f5949_0(Void key, Group value, Context context) throws IOException, InterruptedException
{    context.write(null, new Text(value.getString("char", 0)));}
public void parquet-mr_f5950_0() throws Exception
{    HadoopOutputFile.getBlockFileSystems().add("file");    File inputFile = temp.newFile();    FileOutputStream out = new FileOutputStream(inputFile);    out.write(FILE_CONTENT.getBytes("UTF-8"));    out.close();    File tempFolder = temp.newFolder();    tempFolder.delete();    Path tempPath = new Path(tempFolder.toURI());    File outputFolder = temp.newFile();    outputFolder.delete();    Configuration conf = new Configuration();        conf.set("dfs.block.size", "1024");    conf.set("dfs.blocksize", "1024");    conf.set("dfs.blockSize", "1024");    conf.set("fs.local.block.size", "1024");        conf.set("fs.file.impl.disable.cache", "true");        conf.set("parquet.enable.summary-metadata", "false");    conf.set("parquet.example.schema", PARQUET_TYPE.toString());    {        Job writeJob = new Job(conf, "write");        writeJob.setInputFormatClass(TextInputFormat.class);        TextInputFormat.addInputPath(writeJob, new Path(inputFile.toString()));        writeJob.setOutputFormatClass(ParquetOutputFormat.class);        writeJob.setMapperClass(Writer.class);                writeJob.setNumReduceTasks(0);        ParquetOutputFormat.setWriteSupportClass(writeJob, GroupWriteSupport.class);        ParquetOutputFormat.setBlockSize(writeJob, 1024);        ParquetOutputFormat.setPageSize(writeJob, 512);        ParquetOutputFormat.setDictionaryPageSize(writeJob, 512);        ParquetOutputFormat.setEnableDictionary(writeJob, true);                ParquetOutputFormat.setMaxPaddingSize(writeJob, 1023);        ParquetOutputFormat.setOutputPath(writeJob, tempPath);        waitForJob(writeJob);    }        File parquetFile = getDataFile(tempFolder);    ParquetMetadata footer = ParquetFileReader.readFooter(conf, new Path(parquetFile.toString()), ParquetMetadataConverter.NO_FILTER);    for (BlockMetaData block : footer.getBlocks()) {        Assert.assertTrue("Block should start at a multiple of the block size", block.getStartingPos() % 1024 == 0);    }    {        Job readJob = new Job(conf, "read");        readJob.setInputFormatClass(NoSplits.class);        ParquetInputFormat.setReadSupportClass(readJob, GroupReadSupport.class);        TextInputFormat.addInputPath(readJob, tempPath);        readJob.setOutputFormatClass(TextOutputFormat.class);        readJob.setMapperClass(Reader.class);                readJob.setNumReduceTasks(0);        TextOutputFormat.setOutputPath(readJob, new Path(outputFolder.toString()));        waitForJob(readJob);    }    File dataFile = getDataFile(outputFolder);    Assert.assertNotNull("Should find a data file", dataFile);    StringBuilder contentBuilder = new StringBuilder();    for (String line : Files.readAllLines(dataFile.toPath(), StandardCharsets.UTF_8)) {        contentBuilder.append(line);    }    String reconstructed = contentBuilder.toString();    Assert.assertEquals("Should match written file content", FILE_CONTENT, reconstructed);    HadoopOutputFile.getBlockFileSystems().remove("file");}
private void parquet-mr_f5951_0(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {        sleep(100);    }    if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
private File parquet-mr_f5952_0(File location)
{    File[] files = location.listFiles();    File dataFile = null;    if (files != null) {        for (File file : files) {            if (file.getName().startsWith("part-")) {                dataFile = file;                break;            }        }    }    return dataFile;}
public boolean parquet-mr_f5953_0(String key)
{    return current;}
public void parquet-mr_f5954_0(boolean current)
{    this.current = current;}
public boolean parquet-mr_f5955_0(SimpleValue otherValue)
{    return newerThan;}
public void parquet-mr_f5956_0()
{    LruCache<String, SimpleValue> cache = new LruCache<String, SimpleValue>(1);    String oldKey = DEFAULT_KEY;    String newKey = oldKey + "_new";    SimpleValue oldValue = new SimpleValue(true, true);    cache.put(oldKey, oldValue);    assertEquals(oldValue, cache.getCurrentValue(oldKey));    assertEquals(1, cache.size());    SimpleValue newValue = new SimpleValue(true, true);    cache.put(newKey, newValue);    assertNull(cache.getCurrentValue(oldKey));    assertEquals(newValue, cache.getCurrentValue(newKey));    assertEquals(1, cache.size());}
public void parquet-mr_f5957_0()
{    LruCache<String, SimpleValue> cache = new LruCache<String, SimpleValue>(1);    SimpleValue currentValue = new SimpleValue(true, true);    SimpleValue notAsCurrentValue = new SimpleValue(true, false);    cache.put(DEFAULT_KEY, currentValue);    cache.put(DEFAULT_KEY, notAsCurrentValue);    assertEquals("The existing value in the cache was overwritten", currentValue, cache.getCurrentValue(DEFAULT_KEY));}
public void parquet-mr_f5958_0()
{    LruCache<String, SimpleValue> cache = new LruCache<String, SimpleValue>(1);    SimpleValue outdatedValue = new SimpleValue(false, true);    cache.put(DEFAULT_KEY, outdatedValue);    assertEquals(0, cache.size());    assertNull(cache.getCurrentValue(DEFAULT_KEY));}
public void parquet-mr_f5959_0()
{    LruCache<String, SimpleValue> cache = new LruCache<String, SimpleValue>(1);    SimpleValue currentValue = new SimpleValue(true, true);    SimpleValue notAsCurrentValue = new SimpleValue(true, false);    cache.put(DEFAULT_KEY, notAsCurrentValue);    assertEquals(1, cache.size());    cache.put(DEFAULT_KEY, currentValue);    assertEquals(1, cache.size());    assertEquals("The existing value in the cache was NOT overwritten", currentValue, cache.getCurrentValue(DEFAULT_KEY));}
public void parquet-mr_f5960_0()
{    LruCache<String, SimpleValue> cache = new LruCache<String, SimpleValue>(1);    SimpleValue value = new SimpleValue(true, true);    cache.put(DEFAULT_KEY, value);    assertEquals(1, cache.size());    assertEquals(value, cache.getCurrentValue(DEFAULT_KEY));    value.setCurrent(false);    assertNull("The value should not be current anymore", cache.getCurrentValue(DEFAULT_KEY));    assertEquals(0, cache.size());}
public void parquet-mr_f5961_0()
{    LruCache<String, SimpleValue> cache = new LruCache<String, SimpleValue>(1);    SimpleValue value = new SimpleValue(true, true);    cache.put(DEFAULT_KEY, value);    assertEquals(1, cache.size());    assertEquals(value, cache.getCurrentValue(DEFAULT_KEY));        assertEquals(value, cache.remove(DEFAULT_KEY));    assertNull(cache.getCurrentValue(DEFAULT_KEY));    assertEquals(0, cache.size());}
public void parquet-mr_f5962_0()
{    LruCache<String, SimpleValue> cache = new LruCache<String, SimpleValue>(2);    String key1 = DEFAULT_KEY + 1;    String key2 = DEFAULT_KEY + 2;    SimpleValue value = new SimpleValue(true, true);    cache.put(key1, value);    cache.put(key2, value);    assertEquals(value, cache.getCurrentValue(key1));    assertEquals(value, cache.getCurrentValue(key2));    assertEquals(2, cache.size());    cache.clear();    assertNull(cache.getCurrentValue(key1));    assertNull(cache.getCurrentValue(key2));    assertEquals(0, cache.size());}
public void parquet-mr_f5963_0() throws Exception
{    parquetOutputFormat = new ParquetOutputFormat(new GroupWriteSupport());    GroupWriteSupport.setSchema(MessageTypeParser.parseMessageType(writeSchema), conf);    expectedPoolSize = Math.round((double) ManagementFactory.getMemoryMXBean().getHeapMemoryUsage().getMax() * MemoryManager.DEFAULT_MEMORY_POOL_RATIO);    long rowGroupSize = expectedPoolSize / 2;    conf.setLong(ParquetOutputFormat.BLOCK_SIZE, rowGroupSize);        createWriter(0).close(null);}
public void parquet-mr_f5964_0()
{                long poolSize = ParquetOutputFormat.getMemoryManager().getTotalMemoryPool();    Assert.assertTrue("Pool size should be within 10% of the expected value" + " (expected = " + expectedPoolSize + " actual = " + poolSize + ")", Math.abs(expectedPoolSize - poolSize) < (long) (expectedPoolSize * 0.10));}
public void parquet-mr_f5965_0() throws Exception
{    long poolSize = ParquetOutputFormat.getMemoryManager().getTotalMemoryPool();    long rowGroupSize = poolSize / 2;    conf.setLong(ParquetOutputFormat.BLOCK_SIZE, rowGroupSize);    Assert.assertTrue("Pool should hold 2 full row groups", (2 * rowGroupSize) <= poolSize);    Assert.assertTrue("Pool should not hold 3 full row groups", poolSize < (3 * rowGroupSize));    Assert.assertEquals("Allocations should start out at 0", 0, getTotalAllocation());    RecordWriter writer1 = createWriter(1);    Assert.assertTrue("Allocations should never exceed pool size", getTotalAllocation() <= poolSize);    Assert.assertEquals("First writer should be limited by row group size", rowGroupSize, getTotalAllocation());    RecordWriter writer2 = createWriter(2);    Assert.assertTrue("Allocations should never exceed pool size", getTotalAllocation() <= poolSize);    Assert.assertEquals("Second writer should be limited by row group size", 2 * rowGroupSize, getTotalAllocation());    RecordWriter writer3 = createWriter(3);    Assert.assertTrue("Allocations should never exceed pool size", getTotalAllocation() <= poolSize);    writer1.close(null);    Assert.assertTrue("Allocations should never exceed pool size", getTotalAllocation() <= poolSize);    Assert.assertEquals("Allocations should be increased to the row group size", 2 * rowGroupSize, getTotalAllocation());    writer2.close(null);    Assert.assertTrue("Allocations should never exceed pool size", getTotalAllocation() <= poolSize);    Assert.assertEquals("Allocations should be increased to the row group size", rowGroupSize, getTotalAllocation());    writer3.close(null);    Assert.assertEquals("Allocations should be increased to the row group size", 0, getTotalAllocation());}
public void parquet-mr_f5966_0() throws Exception
{        long poolSize = ParquetOutputFormat.getMemoryManager().getTotalMemoryPool();    long rowGroupSize = poolSize / 2;    conf.setLong(ParquetOutputFormat.BLOCK_SIZE, rowGroupSize);    Assert.assertTrue("Pool should hold 2 full row groups", (2 * rowGroupSize) <= poolSize);    Assert.assertTrue("Pool should not hold 3 full row groups", poolSize < (3 * rowGroupSize));    Runnable callback = () -> counter++;        ParquetOutputFormat.getMemoryManager().registerScaleCallBack("increment-test-counter", callback);    try {        ParquetOutputFormat.getMemoryManager().registerScaleCallBack("increment-test-counter", callback);        Assert.fail("Duplicated registering callback should throw duplicates exception.");    } catch (IllegalArgumentException e) {        }        RecordWriter writer1 = createWriter(1);    RecordWriter writer2 = createWriter(2);    RecordWriter writer3 = createWriter(3);    writer1.close(null);    writer2.close(null);    writer3.close(null);        Assert.assertEquals("Allocations should be adjusted once", 1, counter);    Assert.assertEquals("Should not allow duplicate callbacks", 1, ParquetOutputFormat.getMemoryManager().getScaleCallBacks().size());}
private RecordWriter parquet-mr_f5967_0(int index) throws Exception
{    File file = temp.newFile(String.valueOf(index) + ".parquet");    if (!file.delete()) {        throw new RuntimeException("Could not delete file: " + file);    }    RecordWriter writer = parquetOutputFormat.getRecordWriter(conf, new Path(file.toString()), CompressionCodecName.UNCOMPRESSED);    return writer;}
private long parquet-mr_f5968_0()
{    Set<InternalParquetRecordWriter> writers = ParquetOutputFormat.getMemoryManager().getWriterList().keySet();    long total = 0;    for (InternalParquetRecordWriter writer : writers) {        total += writer.getRowGroupSizeThreshold();    }    return total;}
private static void parquet-mr_f5969_0(File out, Configuration conf, boolean useSchema2) throws IOException
{    if (!useSchema2) {        GroupWriteSupport.setSchema(schema, conf);    } else {        GroupWriteSupport.setSchema(schema2, conf);    }    SimpleGroupFactory f = new SimpleGroupFactory(schema);    Map<String, String> extraMetaData = new HashMap<String, String>();    extraMetaData.put("schema_num", useSchema2 ? "2" : "1");    ParquetWriter<Group> writer = ExampleParquetWriter.builder(new Path(out.getAbsolutePath())).withConf(conf).withExtraMetaData(extraMetaData).build();    for (int i = 0; i < 1000; i++) {        Group g = f.newGroup().append("binary_field", "test" + i).append("int32_field", i).append("int64_field", (long) i).append("boolean_field", i % 2 == 0).append("float_field", (float) i).append("double_field", (double) i).append("flba_field", "foo");        if (!useSchema2) {            g = g.append("int96_field", Binary.fromConstantByteArray(new byte[12]));        }        writer.write(g);    }    writer.close();}
private WrittenFileInfo parquet-mr_f5970_0(boolean mixedSchemas) throws Exception
{    WrittenFileInfo info = new WrittenFileInfo();    Configuration conf = new Configuration();    info.conf = conf;    File root1 = new File(temp.getRoot(), "out1");    File root2 = new File(temp.getRoot(), "out2");    Path rootPath1 = new Path(root1.getAbsolutePath());    Path rootPath2 = new Path(root2.getAbsolutePath());    for (int i = 0; i < 10; i++) {        writeFile(new File(root1, i + ".parquet"), conf, true);    }    List<Footer> footers = ParquetFileReader.readFooters(conf, rootPath1.getFileSystem(conf).getFileStatus(rootPath1), false);    ParquetFileWriter.writeMetadataFile(conf, rootPath1, footers, JobSummaryLevel.ALL);    for (int i = 0; i < 7; i++) {        writeFile(new File(root2, i + ".parquet"), conf, !mixedSchemas);    }    footers = ParquetFileReader.readFooters(conf, rootPath2.getFileSystem(conf).getFileStatus(rootPath2), false);    ParquetFileWriter.writeMetadataFile(conf, rootPath2, footers, JobSummaryLevel.ALL);    info.commonMetaPath1 = new Path(new File(root1, ParquetFileWriter.PARQUET_COMMON_METADATA_FILE).getAbsolutePath());    info.commonMetaPath2 = new Path(new File(root2, ParquetFileWriter.PARQUET_COMMON_METADATA_FILE).getAbsolutePath());    info.metaPath1 = new Path(new File(root1, ParquetFileWriter.PARQUET_METADATA_FILE).getAbsolutePath());    info.metaPath2 = new Path(new File(root2, ParquetFileWriter.PARQUET_METADATA_FILE).getAbsolutePath());    return info;}
public void parquet-mr_f5971_0() throws Exception
{    WrittenFileInfo info = writeFiles(false);    ParquetMetadata commonMeta1 = ParquetFileReader.readFooter(info.conf, info.commonMetaPath1, ParquetMetadataConverter.NO_FILTER);    ParquetMetadata commonMeta2 = ParquetFileReader.readFooter(info.conf, info.commonMetaPath2, ParquetMetadataConverter.NO_FILTER);    ParquetMetadata meta1 = ParquetFileReader.readFooter(info.conf, info.metaPath1, ParquetMetadataConverter.NO_FILTER);    ParquetMetadata meta2 = ParquetFileReader.readFooter(info.conf, info.metaPath2, ParquetMetadataConverter.NO_FILTER);    assertTrue(commonMeta1.getBlocks().isEmpty());    assertTrue(commonMeta2.getBlocks().isEmpty());    assertEquals(commonMeta1.getFileMetaData().getSchema(), commonMeta2.getFileMetaData().getSchema());    assertFalse(meta1.getBlocks().isEmpty());    assertFalse(meta2.getBlocks().isEmpty());    assertEquals(meta1.getFileMetaData().getSchema(), meta2.getFileMetaData().getSchema());    assertEquals(commonMeta1.getFileMetaData().getKeyValueMetaData(), commonMeta2.getFileMetaData().getKeyValueMetaData());    assertEquals(meta1.getFileMetaData().getKeyValueMetaData(), meta2.getFileMetaData().getKeyValueMetaData());        Path mergedOut = new Path(new File(temp.getRoot(), "merged_meta").getAbsolutePath());    Path mergedCommonOut = new Path(new File(temp.getRoot(), "merged_common_meta").getAbsolutePath());    ParquetFileWriter.writeMergedMetadataFile(Arrays.asList(info.metaPath1, info.metaPath2), mergedOut, info.conf);    ParquetFileWriter.writeMergedMetadataFile(Arrays.asList(info.commonMetaPath1, info.commonMetaPath2), mergedCommonOut, info.conf);    ParquetMetadata mergedMeta = ParquetFileReader.readFooter(info.conf, mergedOut, ParquetMetadataConverter.NO_FILTER);    ParquetMetadata mergedCommonMeta = ParquetFileReader.readFooter(info.conf, mergedCommonOut, ParquetMetadataConverter.NO_FILTER);        assertEquals(meta1.getBlocks().size() + meta2.getBlocks().size(), mergedMeta.getBlocks().size());    assertTrue(mergedCommonMeta.getBlocks().isEmpty());    assertEquals(meta1.getFileMetaData().getSchema(), mergedMeta.getFileMetaData().getSchema());    assertEquals(commonMeta1.getFileMetaData().getSchema(), mergedCommonMeta.getFileMetaData().getSchema());    assertEquals(meta1.getFileMetaData().getKeyValueMetaData(), mergedMeta.getFileMetaData().getKeyValueMetaData());    assertEquals(commonMeta1.getFileMetaData().getKeyValueMetaData(), mergedCommonMeta.getFileMetaData().getKeyValueMetaData());}
public void parquet-mr_f5972_0() throws Exception
{    WrittenFileInfo info = writeFiles(true);    Path mergedOut = new Path(new File(temp.getRoot(), "merged_meta").getAbsolutePath());    Path mergedCommonOut = new Path(new File(temp.getRoot(), "merged_common_meta").getAbsolutePath());    try {        ParquetFileWriter.writeMergedMetadataFile(Arrays.asList(info.metaPath1, info.metaPath2), mergedOut, info.conf);        fail("this should throw");    } catch (RuntimeException e) {        boolean eq1 = e.getMessage().equals("could not merge metadata: key schema_num has conflicting values: [2, 1]");        boolean eq2 = e.getMessage().equals("could not merge metadata: key schema_num has conflicting values: [1, 2]");        assertEquals(eq1 || eq2, true);    }    try {        ParquetFileWriter.writeMergedMetadataFile(Arrays.asList(info.commonMetaPath1, info.commonMetaPath2), mergedCommonOut, info.conf);        fail("this should throw");    } catch (RuntimeException e) {        boolean eq1 = e.getMessage().equals("could not merge metadata: key schema_num has conflicting values: [2, 1]");        boolean eq2 = e.getMessage().equals("could not merge metadata: key schema_num has conflicting values: [1, 2]");        assertEquals(eq1 || eq2, true);    }}
private String parquet-mr_f5973_0(int minSize, int maxSize)
{    int size = random.nextInt(maxSize - minSize) + minSize;    StringBuilder builder = new StringBuilder(size);    for (int i = 0; i < size; ++i) {        builder.append(ALPHABET.charAt(random.nextInt(ALPHABET.length())));    }    return builder.toString();}
public Group parquet-mr_f5974_0()
{    Group group = factory.newGroup();    group.add("id", random.nextInt());    group.add("name", getString(NAME_MIN_SIZE, NAME_MAX_SIZE));    Group phoneNumbers = group.addGroup("phone_numbers");    for (int i = 0, n = random.nextInt(PHONE_NUMBERS_MAX_SIZE); i < n; ++i) {        Group phoneNumber = phoneNumbers.addGroup(0);        phoneNumber.add(0, random.nextLong() % (MAX_PHONE_NUMBER - MIN_PHONE_NUMBER) + MIN_PHONE_NUMBER);    }    if (random.nextDouble() >= COMMENT_NULL_RATIO) {        group.add("comment", getString(0, COMMENT_MAX_SIZE));    }    return group;}
public static void parquet-mr_f5975_0()
{    tmpDir = new Path(Files.createTempDir().getAbsolutePath().toString());}
public static void parquet-mr_f5976_0() throws IOException
{    tmpDir.getFileSystem(new Configuration()).delete(tmpDir, true);}
private Path parquet-mr_f5977_0(Iterable<Group> data) throws IOException
{    Path file = new Path(tmpDir, "testMultipleReadWrite_" + UUID.randomUUID() + ".parquet");    try (ParquetWriter<Group> writer = ExampleParquetWriter.builder(file).config(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, SCHEMA.toString()).build()) {        for (Group group : data) {            writer.write(group);        }    }    return file;}
private void parquet-mr_f5978_0(Path file, List<Group> data) throws IOException
{    try (ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file).build()) {        for (Group group : data) {            assertEquals(group.toString(), reader.read().toString());        }    }}
private void parquet-mr_f5979_0(Path file, Filter filter, Stream<Group> data) throws IOException
{    try (ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file).withFilter(filter).build()) {        for (Iterator<Group> it = data.iterator(); it.hasNext(); ) {            assertEquals(it.next().toString(), reader.read().toString());        }    }}
private void parquet-mr_f5980_0(Path file, List<Group> data) throws IOException
{    validateFile(file, FilterCompat.get(eq(intColumn("id"), 0)), data.stream().filter(group -> group.getInteger("id", 0) == 0));}
private void parquet-mr_f5981_0(Path file, List<Group> data) throws IOException
{    validateFile(file, FilterCompat.get(eq(binaryColumn("comment"), null)), data.stream().filter(group -> group.getFieldRepetitionCount("comment") == 0));}
private void parquet-mr_f5982_0(Path file, List<Group> data) throws IOException
{    Binary binaryValueB = fromString("b");    Filter filter = FilterCompat.get(and(gtEq(intColumn("id"), 0), and(lt(binaryColumn("name"), binaryValueB), notEq(binaryColumn("comment"), null))));    Predicate<Group> predicate = group -> group.getInteger("id", 0) >= 0 && BINARY_COMPARATOR.compare(group.getBinary("name", 0), binaryValueB) < 0 && group.getFieldRepetitionCount("comment") > 0;    validateFile(file, filter, data.stream().filter(predicate));}
public void parquet-mr_f5983_0() throws Throwable
{        List<List<Group>> data = new ArrayList<>();    for (int i = 0; i < 10; ++i) {        data.add(Stream.generate(new DataGenerator(i)).limit(10000 - i * 1000).collect(Collectors.toList()));    }        List<Future<Path>> futureFiles = new ArrayList<>();    ExecutorService exec = Executors.newFixedThreadPool(6);    for (List<Group> d : data) {        futureFiles.add(exec.submit(() -> {            Path file = writeFile(d);            validateFile(file, d);            return file;        }));    }    List<Path> files = new ArrayList<>();    for (Future<Path> future : futureFiles) {        try {            files.add(future.get());        } catch (ExecutionException e) {            throw e.getCause();        }    }        List<Future<?>> futures = new ArrayList<>();    for (int i = 0; i < 10; ++i) {        Path file = files.get(i);        List<Group> d = data.get(i);        futures.add(exec.submit(() -> {            validateFileWithIdFilter(file, d);            return null;        }));        futures.add(exec.submit(() -> {            validateFileWithCommentFilter(file, d);            return null;        }));        futures.add(exec.submit(() -> {            validateFileWithComplexFilter(file, d);            return null;        }));    }    for (Future<?> future : futures) {        try {            future.get();        } catch (ExecutionException e) {            throw e.getCause();        }    }}
public void parquet-mr_f5984_0() throws Exception
{    File testFile = temp.newFile();    MessageType schema = MessageTypeParser.parseMessageType("message m { required group a {required binary b;} required group " + "c { required int64 d; }}");    Configuration conf = new Configuration();    ParquetFileWriter writer = null;    boolean exceptionThrown = false;    Path path = new Path(testFile.toURI());    try {        writer = new ParquetFileWriter(conf, schema, path, ParquetFileWriter.Mode.CREATE);    } catch (IOException ioe1) {        exceptionThrown = true;    }    assertTrue(exceptionThrown);    exceptionThrown = false;    try {        writer = new ParquetFileWriter(conf, schema, path, OVERWRITE);    } catch (IOException ioe2) {        exceptionThrown = true;    }    assertTrue(!exceptionThrown);    testFile.delete();}
public void parquet-mr_f5985_0() throws Exception
{    File testFile = temp.newFile();    testFile.delete();    Path path = new Path(testFile.toURI());    Configuration configuration = new Configuration();    ParquetFileWriter w = new ParquetFileWriter(configuration, SCHEMA, path);    w.start();    w.startBlock(3);    w.startColumn(C1, 5, CODEC);    long c1Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c1Ends = w.getPos();    w.startColumn(C2, 6, CODEC);    long c2Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(1, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c2Ends = w.getPos();    w.endBlock();    w.startBlock(4);    w.startColumn(C1, 7, CODEC);    w.writeDataPage(7, 4, BytesInput.from(BYTES3), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(C2, 8, CODEC);    w.writeDataPage(8, 4, BytesInput.from(BYTES4), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.end(new HashMap<String, String>());    ParquetMetadata readFooter = ParquetFileReader.readFooter(configuration, path);    assertEquals("footer: " + readFooter, 2, readFooter.getBlocks().size());    assertEquals(c1Ends - c1Starts, readFooter.getBlocks().get(0).getColumns().get(0).getTotalSize());    assertEquals(c2Ends - c2Starts, readFooter.getBlocks().get(0).getColumns().get(1).getTotalSize());    assertEquals(c2Ends - c1Starts, readFooter.getBlocks().get(0).getTotalByteSize());    HashSet<Encoding> expectedEncoding = new HashSet<Encoding>();    expectedEncoding.add(PLAIN);    expectedEncoding.add(BIT_PACKED);    assertEquals(expectedEncoding, readFooter.getBlocks().get(0).getColumns().get(0).getEncodings());    {                ParquetFileReader r = new ParquetFileReader(configuration, readFooter.getFileMetaData(), path, Arrays.asList(readFooter.getBlocks().get(0)), Arrays.asList(SCHEMA.getColumnDescription(PATH1)));        PageReadStore pages = r.readNextRowGroup();        assertEquals(3, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 2, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH1, 3, BytesInput.from(BYTES1));        assertNull(r.readNextRowGroup());    }    {                ParquetFileReader r = new ParquetFileReader(configuration, readFooter.getFileMetaData(), path, readFooter.getBlocks(), Arrays.asList(SCHEMA.getColumnDescription(PATH1), SCHEMA.getColumnDescription(PATH2)));        PageReadStore pages = r.readNextRowGroup();        assertEquals(3, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 2, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH1, 3, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH2, 2, BytesInput.from(BYTES2));        validateContains(SCHEMA, pages, PATH2, 3, BytesInput.from(BYTES2));        validateContains(SCHEMA, pages, PATH2, 1, BytesInput.from(BYTES2));        pages = r.readNextRowGroup();        assertEquals(4, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 7, BytesInput.from(BYTES3));        validateContains(SCHEMA, pages, PATH2, 8, BytesInput.from(BYTES4));        assertNull(r.readNextRowGroup());    }    PrintFooter.main(new String[] { path.toString() });}
public void parquet-mr_f5986_0() throws Exception
{    File testFile = temp.newFile();    Path path = new Path(testFile.toURI());    Configuration conf = new Configuration();        conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, false);        ParquetFileWriter w = new ParquetFileWriter(conf, SCHEMA, path, 120, 60);    w.start();    w.startBlock(3);    w.startColumn(C1, 5, CODEC);    long c1Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c1Ends = w.getPos();    w.startColumn(C2, 6, CODEC);    long c2Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(1, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c2Ends = w.getPos();    w.endBlock();        long firstRowGroupEnds = w.getPos();    w.startBlock(4);    w.startColumn(C1, 7, CODEC);    w.writeDataPage(7, 4, BytesInput.from(BYTES3), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(C2, 8, CODEC);    w.writeDataPage(8, 4, BytesInput.from(BYTES4), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    long secondRowGroupEnds = w.getPos();    w.end(new HashMap<String, String>());    FileSystem fs = path.getFileSystem(conf);    long fileLen = fs.getFileStatus(path).getLen();    FSDataInputStream data = fs.open(path);        data.seek(fileLen - 8);    long footerLen = BytesUtils.readIntLittleEndian(data);    long startFooter = fileLen - footerLen - 8;    assertEquals("Footer should start after second row group without padding", secondRowGroupEnds, startFooter);    ParquetMetadata readFooter = ParquetFileReader.readFooter(conf, path);    assertEquals("footer: " + readFooter, 2, readFooter.getBlocks().size());    assertEquals(c1Ends - c1Starts, readFooter.getBlocks().get(0).getColumns().get(0).getTotalSize());    assertEquals(c2Ends - c2Starts, readFooter.getBlocks().get(0).getColumns().get(1).getTotalSize());    assertEquals(c2Ends - c1Starts, readFooter.getBlocks().get(0).getTotalByteSize());    HashSet<Encoding> expectedEncoding = new HashSet<Encoding>();    expectedEncoding.add(PLAIN);    expectedEncoding.add(BIT_PACKED);    assertEquals(expectedEncoding, readFooter.getBlocks().get(0).getColumns().get(0).getEncodings());        assertEquals("First row group should start after magic", 4, readFooter.getBlocks().get(0).getStartingPos());    assertTrue("First row group should end before the block size (120)", firstRowGroupEnds < 120);    assertEquals("Second row group should start at the block size", 120, readFooter.getBlocks().get(1).getStartingPos());    {                ParquetFileReader r = new ParquetFileReader(conf, readFooter.getFileMetaData(), path, Arrays.asList(readFooter.getBlocks().get(0)), Arrays.asList(SCHEMA.getColumnDescription(PATH1)));        PageReadStore pages = r.readNextRowGroup();        assertEquals(3, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 2, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH1, 3, BytesInput.from(BYTES1));        assertNull(r.readNextRowGroup());    }    {                ParquetFileReader r = new ParquetFileReader(conf, readFooter.getFileMetaData(), path, readFooter.getBlocks(), Arrays.asList(SCHEMA.getColumnDescription(PATH1), SCHEMA.getColumnDescription(PATH2)));        PageReadStore pages = r.readNextRowGroup();        assertEquals(3, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 2, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH1, 3, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH2, 2, BytesInput.from(BYTES2));        validateContains(SCHEMA, pages, PATH2, 3, BytesInput.from(BYTES2));        validateContains(SCHEMA, pages, PATH2, 1, BytesInput.from(BYTES2));        pages = r.readNextRowGroup();        assertEquals(4, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 7, BytesInput.from(BYTES3));        validateContains(SCHEMA, pages, PATH2, 8, BytesInput.from(BYTES4));        assertNull(r.readNextRowGroup());    }    PrintFooter.main(new String[] { path.toString() });}
public void parquet-mr_f5987_0() throws Exception
{    File testFile = temp.newFile();    Path path = new Path(testFile.toURI());    Configuration conf = new Configuration();        conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, false);        ParquetFileWriter w = new ParquetFileWriter(conf, SCHEMA, path, 100, 50);    w.start();    w.startBlock(3);    w.startColumn(C1, 5, CODEC);    long c1Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c1Ends = w.getPos();    w.startColumn(C2, 6, CODEC);    long c2Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(1, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c2Ends = w.getPos();    w.endBlock();        long firstRowGroupEnds = w.getPos();    w.startBlock(4);    w.startColumn(C1, 7, CODEC);    w.writeDataPage(7, 4, BytesInput.from(BYTES3), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(C2, 8, CODEC);    w.writeDataPage(8, 4, BytesInput.from(BYTES4), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    long secondRowGroupEnds = w.getPos();    w.end(new HashMap<String, String>());    FileSystem fs = path.getFileSystem(conf);    long fileLen = fs.getFileStatus(path).getLen();    FSDataInputStream data = fs.open(path);        data.seek(fileLen - 8);    long footerLen = BytesUtils.readIntLittleEndian(data);    long startFooter = fileLen - footerLen - 8;    assertEquals("Footer should start after second row group without padding", secondRowGroupEnds, startFooter);    ParquetMetadata readFooter = ParquetFileReader.readFooter(conf, path);    assertEquals("footer: " + readFooter, 2, readFooter.getBlocks().size());    assertEquals(c1Ends - c1Starts, readFooter.getBlocks().get(0).getColumns().get(0).getTotalSize());    assertEquals(c2Ends - c2Starts, readFooter.getBlocks().get(0).getColumns().get(1).getTotalSize());    assertEquals(c2Ends - c1Starts, readFooter.getBlocks().get(0).getTotalByteSize());    HashSet<Encoding> expectedEncoding = new HashSet<Encoding>();    expectedEncoding.add(PLAIN);    expectedEncoding.add(BIT_PACKED);    assertEquals(expectedEncoding, readFooter.getBlocks().get(0).getColumns().get(0).getEncodings());        assertEquals("First row group should start after magic", 4, readFooter.getBlocks().get(0).getStartingPos());    assertTrue("First row group should end before the block size (120)", firstRowGroupEnds > 100);    assertEquals("Second row group should start after no padding", 109, readFooter.getBlocks().get(1).getStartingPos());    {                ParquetFileReader r = new ParquetFileReader(conf, readFooter.getFileMetaData(), path, Arrays.asList(readFooter.getBlocks().get(0)), Arrays.asList(SCHEMA.getColumnDescription(PATH1)));        PageReadStore pages = r.readNextRowGroup();        assertEquals(3, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 2, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH1, 3, BytesInput.from(BYTES1));        assertNull(r.readNextRowGroup());    }    {                ParquetFileReader r = new ParquetFileReader(conf, readFooter.getFileMetaData(), path, readFooter.getBlocks(), Arrays.asList(SCHEMA.getColumnDescription(PATH1), SCHEMA.getColumnDescription(PATH2)));        PageReadStore pages = r.readNextRowGroup();        assertEquals(3, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 2, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH1, 3, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH2, 2, BytesInput.from(BYTES2));        validateContains(SCHEMA, pages, PATH2, 3, BytesInput.from(BYTES2));        validateContains(SCHEMA, pages, PATH2, 1, BytesInput.from(BYTES2));        pages = r.readNextRowGroup();        assertEquals(4, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 7, BytesInput.from(BYTES3));        validateContains(SCHEMA, pages, PATH2, 8, BytesInput.from(BYTES4));        assertNull(r.readNextRowGroup());    }    PrintFooter.main(new String[] { path.toString() });}
public void parquet-mr_f5988_0() throws Exception
{    long[] longArray = new long[] { 39L, 99L, 12L, 1000L, 65L, 542L, 2533461316L, -253346131996L, Long.MAX_VALUE, Long.MIN_VALUE };    LongStatistics parquetMRstats = new LongStatistics();    for (long l : longArray) {        parquetMRstats.updateStats(l);    }    final String createdBy = "parquet-mr version 1.8.0 (build d4d5a07ec9bd262ca1e93c309f1d7d4a74ebda4c)";    Statistics thriftStats = org.apache.parquet.format.converter.ParquetMetadataConverter.toParquetStatistics(parquetMRstats);    LongStatistics convertedBackStats = (LongStatistics) org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetStatistics(createdBy, thriftStats, PrimitiveTypeName.INT64);    assertEquals(parquetMRstats.getMax(), convertedBackStats.getMax());    assertEquals(parquetMRstats.getMin(), convertedBackStats.getMin());    assertEquals(parquetMRstats.getNumNulls(), convertedBackStats.getNumNulls());}
public void parquet-mr_f5989_0() throws Exception
{        Assume.assumeTrue(!shouldIgnoreStatistics(Version.FULL_VERSION, BINARY));    File testFile = temp.newFile();    testFile.delete();    Path path = new Path(testFile.toURI());    Configuration configuration = new Configuration();    configuration.setBoolean("parquet.strings.signed-min-max.enabled", true);    MessageType schema = MessageTypeParser.parseMessageType("message m { required group a {required binary b (UTF8);} required group c { required int64 d; }}");    String[] path1 = { "a", "b" };    ColumnDescriptor c1 = schema.getColumnDescription(path1);    String[] path2 = { "c", "d" };    ColumnDescriptor c2 = schema.getColumnDescription(path2);    byte[] bytes1 = { 0, 1, 2, 3 };    byte[] bytes2 = { 1, 2, 3, 4 };    byte[] bytes3 = { 2, 3, 4, 5 };    byte[] bytes4 = { 3, 4, 5, 6 };    CompressionCodecName codec = CompressionCodecName.UNCOMPRESSED;    BinaryStatistics statsB1C1P1 = new BinaryStatistics();    BinaryStatistics statsB1C1P2 = new BinaryStatistics();    LongStatistics statsB1C2P1 = new LongStatistics();    LongStatistics statsB1C2P2 = new LongStatistics();    BinaryStatistics statsB2C1P1 = new BinaryStatistics();    LongStatistics statsB2C2P1 = new LongStatistics();    statsB1C1P1.setMinMax(Binary.fromString("s"), Binary.fromString("z"));    statsB1C1P2.setMinMax(Binary.fromString("a"), Binary.fromString("b"));    statsB1C2P1.setMinMax(2l, 10l);    statsB1C2P2.setMinMax(-6l, 4l);    statsB2C1P1.setMinMax(Binary.fromString("d"), Binary.fromString("e"));    statsB2C2P1.setMinMax(11l, 122l);    ParquetFileWriter w = new ParquetFileWriter(configuration, schema, path);    w.start();    w.startBlock(3);    w.startColumn(c1, 5, codec);    w.writeDataPage(2, 4, BytesInput.from(bytes1), statsB1C1P1, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(bytes1), statsB1C1P2, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(c2, 6, codec);    w.writeDataPage(3, 4, BytesInput.from(bytes2), statsB1C2P1, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(1, 4, BytesInput.from(bytes2), statsB1C2P2, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.startBlock(4);    w.startColumn(c1, 7, codec);    w.writeDataPage(7, 4, BytesInput.from(bytes3), statsB2C1P1, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(c2, 8, codec);    w.writeDataPage(8, 4, BytesInput.from(bytes4), statsB2C2P1, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.end(new HashMap<String, String>());    ParquetMetadata readFooter = ParquetFileReader.readFooter(configuration, path);    for (BlockMetaData block : readFooter.getBlocks()) {        for (ColumnChunkMetaData col : block.getColumns()) {            col.getPath();        }    }        BinaryStatistics bs1 = new BinaryStatistics();    bs1.setMinMax(Binary.fromString("a"), Binary.fromString("z"));    LongStatistics ls1 = new LongStatistics();    ls1.setMinMax(-6l, 10l);    BinaryStatistics bs2 = new BinaryStatistics();    bs2.setMinMax(Binary.fromString("d"), Binary.fromString("e"));    LongStatistics ls2 = new LongStatistics();    ls2.setMinMax(11l, 122l);    {                BinaryStatistics bsout = (BinaryStatistics) readFooter.getBlocks().get(0).getColumns().get(0).getStatistics();        String str = new String(bsout.getMaxBytes());        String str2 = new String(bsout.getMinBytes());        TestUtils.assertStatsValuesEqual(bs1, readFooter.getBlocks().get(0).getColumns().get(0).getStatistics());        TestUtils.assertStatsValuesEqual(ls1, readFooter.getBlocks().get(0).getColumns().get(1).getStatistics());    }    {                TestUtils.assertStatsValuesEqual(bs2, readFooter.getBlocks().get(1).getColumns().get(0).getStatistics());        TestUtils.assertStatsValuesEqual(ls2, readFooter.getBlocks().get(1).getColumns().get(1).getStatistics());    }}
public void parquet-mr_f5990_0() throws Exception
{    File testDir = temp.newFolder();    Path testDirPath = new Path(testDir.toURI());    Configuration configuration = new Configuration();    final FileSystem fs = testDirPath.getFileSystem(configuration);    enforceEmptyDir(configuration, testDirPath);    MessageType schema = MessageTypeParser.parseMessageType("message m { required group a {required binary b;} required group c { required int64 d; }}");    createFile(configuration, new Path(testDirPath, "part0"), schema);    createFile(configuration, new Path(testDirPath, "part1"), schema);    createFile(configuration, new Path(testDirPath, "part2"), schema);    FileStatus outputStatus = fs.getFileStatus(testDirPath);    List<Footer> footers = ParquetFileReader.readFooters(configuration, outputStatus, false);    validateFooters(footers);    ParquetFileWriter.writeMetadataFile(configuration, testDirPath, footers, JobSummaryLevel.ALL);    footers = ParquetFileReader.readFooters(configuration, outputStatus, false);    validateFooters(footers);    footers = ParquetFileReader.readFooters(configuration, fs.getFileStatus(new Path(testDirPath, "part0")), false);    assertEquals(1, footers.size());    final FileStatus metadataFile = fs.getFileStatus(new Path(testDirPath, ParquetFileWriter.PARQUET_METADATA_FILE));    final FileStatus metadataFileLight = fs.getFileStatus(new Path(testDirPath, ParquetFileWriter.PARQUET_COMMON_METADATA_FILE));    final List<Footer> metadata = ParquetFileReader.readSummaryFile(configuration, metadataFile);    validateFooters(metadata);    footers = ParquetFileReader.readAllFootersInParallelUsingSummaryFiles(configuration, Arrays.asList(fs.listStatus(testDirPath, HiddenFileFilter.INSTANCE)), false);    validateFooters(footers);    fs.delete(metadataFile.getPath(), false);    fs.delete(metadataFileLight.getPath(), false);    footers = ParquetFileReader.readAllFootersInParallelUsingSummaryFiles(configuration, Arrays.asList(fs.listStatus(testDirPath)), false);    validateFooters(footers);}
public void parquet-mr_f5991_0() throws Exception
{        Assume.assumeTrue(!shouldIgnoreStatistics(Version.FULL_VERSION, BINARY));    File testFile = temp.newFile();    testFile.delete();    writeSchema = "message example {\n" + "required binary content (UTF8);\n" + "}";    Path path = new Path(testFile.toURI());    MessageType schema = MessageTypeParser.parseMessageType(writeSchema);    Configuration configuration = new Configuration();    configuration.setBoolean("parquet.strings.signed-min-max.enabled", true);    GroupWriteSupport.setSchema(schema, configuration);    ParquetWriter<Group> writer = new ParquetWriter<Group>(path, configuration, new GroupWriteSupport());    Group r1 = new SimpleGroup(schema);    writer.write(r1);    writer.close();    ParquetMetadata readFooter = ParquetFileReader.readFooter(configuration, path);        org.apache.parquet.column.statistics.Statistics stats = readFooter.getBlocks().get(0).getColumns().get(0).getStatistics();    assertFalse("is empty: " + stats, stats.isEmpty());        assertEquals("nulls: " + stats, 1, stats.getNumNulls());}
private void parquet-mr_f5992_1(final List<Footer> metadata)
{        assertEquals(String.valueOf(metadata), 3, metadata.size());    for (Footer footer : metadata) {        final File file = new File(footer.getFile().toUri());        assertTrue(file.getName(), file.getName().startsWith("part"));        assertTrue(file.getPath(), file.exists());        final ParquetMetadata parquetMetadata = footer.getParquetMetadata();        assertEquals(2, parquetMetadata.getBlocks().size());        final Map<String, String> keyValueMetaData = parquetMetadata.getFileMetaData().getKeyValueMetaData();        assertEquals("bar", keyValueMetaData.get("foo"));        assertEquals(footer.getFile().getName(), keyValueMetaData.get(footer.getFile().getName()));    }}
private void parquet-mr_f5993_0(Configuration configuration, Path path, MessageType schema) throws IOException
{    String[] path1 = { "a", "b" };    ColumnDescriptor c1 = schema.getColumnDescription(path1);    String[] path2 = { "c", "d" };    ColumnDescriptor c2 = schema.getColumnDescription(path2);    byte[] bytes1 = { 0, 1, 2, 3 };    byte[] bytes2 = { 1, 2, 3, 4 };    byte[] bytes3 = { 2, 3, 4, 5 };    byte[] bytes4 = { 3, 4, 5, 6 };    CompressionCodecName codec = CompressionCodecName.UNCOMPRESSED;    BinaryStatistics stats1 = new BinaryStatistics();    BinaryStatistics stats2 = new BinaryStatistics();    ParquetFileWriter w = new ParquetFileWriter(configuration, schema, path);    w.start();    w.startBlock(3);    w.startColumn(c1, 5, codec);    w.writeDataPage(2, 4, BytesInput.from(bytes1), stats1, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(bytes1), stats1, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(c2, 6, codec);    w.writeDataPage(2, 4, BytesInput.from(bytes2), stats2, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(bytes2), stats2, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(1, 4, BytesInput.from(bytes2), stats2, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.startBlock(4);    w.startColumn(c1, 7, codec);    w.writeDataPage(7, 4, BytesInput.from(bytes3), stats1, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(c2, 8, codec);    w.writeDataPage(8, 4, BytesInput.from(bytes4), stats2, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    final HashMap<String, String> extraMetaData = new HashMap<String, String>();    extraMetaData.put("foo", "bar");    extraMetaData.put(path.getName(), path.getName());    w.end(extraMetaData);}
private void parquet-mr_f5994_0(MessageType schema, PageReadStore pages, String[] path, int values, BytesInput bytes) throws IOException
{    PageReader pageReader = pages.getPageReader(schema.getColumnDescription(path));    DataPage page = pageReader.readPage();    assertEquals(values, page.getValueCount());    assertArrayEquals(bytes.toByteArray(), ((DataPageV1) page).getBytes().toByteArray());}
public void parquet-mr_f5995_0()
{    FileMetaData md1 = new FileMetaData(new MessageType("root1", new PrimitiveType(REPEATED, BINARY, "a"), new PrimitiveType(OPTIONAL, BINARY, "b")), new HashMap<String, String>(), "test");    FileMetaData md2 = new FileMetaData(new MessageType("root2", new PrimitiveType(REQUIRED, BINARY, "c")), new HashMap<String, String>(), "test2");    GlobalMetaData merged = ParquetFileWriter.mergeInto(md2, ParquetFileWriter.mergeInto(md1, null));    assertEquals(merged.getSchema(), new MessageType("root1", new PrimitiveType(REPEATED, BINARY, "a"), new PrimitiveType(OPTIONAL, BINARY, "b"), new PrimitiveType(REQUIRED, BINARY, "c")));}
public void parquet-mr_f5996_0()
{    List<BlockMetaData> oneBlocks = new ArrayList<BlockMetaData>();    oneBlocks.add(new BlockMetaData());    oneBlocks.add(new BlockMetaData());    List<BlockMetaData> twoBlocks = new ArrayList<BlockMetaData>();    twoBlocks.add(new BlockMetaData());    List<BlockMetaData> expected = new ArrayList<BlockMetaData>();    expected.addAll(oneBlocks);    expected.addAll(twoBlocks);    Footer one = new Footer(new Path("file:/tmp/output/one.parquet"), new ParquetMetadata(new FileMetaData(new MessageType("root1", new PrimitiveType(REPEATED, BINARY, "a"), new PrimitiveType(OPTIONAL, BINARY, "b")), new HashMap<String, String>(), "test"), oneBlocks));    Footer two = new Footer(new Path("/tmp/output/two.parquet"), new ParquetMetadata(new FileMetaData(new MessageType("root2", new PrimitiveType(REQUIRED, BINARY, "c")), new HashMap<String, String>(), "test2"), twoBlocks));    List<Footer> footers = new ArrayList<Footer>();    footers.add(one);    footers.add(two);    ParquetMetadata merged = ParquetFileWriter.mergeFooters(new Path("/tmp"), footers);    assertEquals(new MessageType("root1", new PrimitiveType(REPEATED, BINARY, "a"), new PrimitiveType(OPTIONAL, BINARY, "b"), new PrimitiveType(REQUIRED, BINARY, "c")), merged.getFileMetaData().getSchema());    assertEquals("Should have all blocks", expected, merged.getBlocks());}
public void parquet-mr_f5997_0() throws IOException
{    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path relativeRoot = new Path("target/_test_relative");    Path qualifiedRoot = fs.makeQualified(relativeRoot);    ParquetMetadata mock = Mockito.mock(ParquetMetadata.class);    FileMetaData fileMetaData = new FileMetaData(new MessageType("root1", new PrimitiveType(REPEATED, BINARY, "a")), new HashMap<String, String>(), "test");    Mockito.when(mock.getFileMetaData()).thenReturn(fileMetaData);    List<Footer> footers = new ArrayList<Footer>();    Footer footer = new Footer(new Path(qualifiedRoot, "one"), mock);    footers.add(footer);        ParquetFileWriter.writeMetadataFile(conf, relativeRoot, footers, JobSummaryLevel.ALL);}
public void parquet-mr_f5998_0() throws Exception
{    File testFile = temp.newFile();    testFile.delete();    Path path = new Path(testFile.toURI());    Configuration configuration = new Configuration();    ParquetFileWriter w = new ParquetFileWriter(configuration, SCHEMA, path);    w.start();    w.startBlock(4);    w.startColumn(C1, 7, CODEC);    w.writeDataPage(7, 4, BytesInput.from(BYTES3), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(C2, 8, CODEC);    w.writeDataPage(8, 4, BytesInput.from(BYTES4), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.startBlock(4);    w.startColumn(C1, 5, CODEC);    long c1p1Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES1), statsC1(null, Binary.fromString("aaa")), 1, BIT_PACKED, BIT_PACKED, PLAIN);    long c1p2Starts = w.getPos();    w.writeDataPage(3, 4, BytesInput.from(BYTES1), statsC1(Binary.fromString("bbb"), Binary.fromString("ccc")), 3, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c1Ends = w.getPos();    w.startColumn(C2, 6, CODEC);    long c2p1Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES2), statsC2(117l, 100l), 1, BIT_PACKED, BIT_PACKED, PLAIN);    long c2p2Starts = w.getPos();    w.writeDataPage(3, 4, BytesInput.from(BYTES2), statsC2(null, null, null), 2, BIT_PACKED, BIT_PACKED, PLAIN);    long c2p3Starts = w.getPos();    w.writeDataPage(1, 4, BytesInput.from(BYTES2), statsC2(0l), 1, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c2Ends = w.getPos();    w.endBlock();    w.startBlock(4);    w.startColumn(C1, 7, CODEC);    w.writeDataPage(7, 4, BytesInput.from(BYTES3),     statsC1(Binary.fromConstantByteArray(new byte[(int) MAX_STATS_SIZE]), Binary.fromConstantByteArray(new byte[1])), 4, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(C2, 8, CODEC);    w.writeDataPage(8, 4, BytesInput.from(BYTES4), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.end(new HashMap<String, String>());    try (ParquetFileReader reader = new ParquetFileReader(HadoopInputFile.fromPath(path, configuration), ParquetReadOptions.builder().build())) {        ParquetMetadata footer = reader.getFooter();        assertEquals(3, footer.getBlocks().size());        BlockMetaData blockMeta = footer.getBlocks().get(1);        assertEquals(2, blockMeta.getColumns().size());        ColumnIndex columnIndex = reader.readColumnIndex(blockMeta.getColumns().get(0));        assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());        assertTrue(Arrays.asList(1l, 0l).equals(columnIndex.getNullCounts()));        assertTrue(Arrays.asList(false, false).equals(columnIndex.getNullPages()));        List<ByteBuffer> minValues = columnIndex.getMinValues();        assertEquals(2, minValues.size());        List<ByteBuffer> maxValues = columnIndex.getMaxValues();        assertEquals(2, maxValues.size());        assertEquals("aaa", new String(minValues.get(0).array(), StandardCharsets.UTF_8));        assertEquals("aaa", new String(maxValues.get(0).array(), StandardCharsets.UTF_8));        assertEquals("bbb", new String(minValues.get(1).array(), StandardCharsets.UTF_8));        assertEquals("ccc", new String(maxValues.get(1).array(), StandardCharsets.UTF_8));        columnIndex = reader.readColumnIndex(blockMeta.getColumns().get(1));        assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());        assertTrue(Arrays.asList(0l, 3l, 0l).equals(columnIndex.getNullCounts()));        assertTrue(Arrays.asList(false, true, false).equals(columnIndex.getNullPages()));        minValues = columnIndex.getMinValues();        assertEquals(3, minValues.size());        maxValues = columnIndex.getMaxValues();        assertEquals(3, maxValues.size());        assertEquals(100, BytesUtils.bytesToLong(minValues.get(0).array()));        assertEquals(117, BytesUtils.bytesToLong(maxValues.get(0).array()));        assertEquals(0, minValues.get(1).array().length);        assertEquals(0, maxValues.get(1).array().length);        assertEquals(0, BytesUtils.bytesToLong(minValues.get(2).array()));        assertEquals(0, BytesUtils.bytesToLong(maxValues.get(2).array()));        OffsetIndex offsetIndex = reader.readOffsetIndex(blockMeta.getColumns().get(0));        assertEquals(2, offsetIndex.getPageCount());        assertEquals(c1p1Starts, offsetIndex.getOffset(0));        assertEquals(c1p2Starts, offsetIndex.getOffset(1));        assertEquals(c1p2Starts - c1p1Starts, offsetIndex.getCompressedPageSize(0));        assertEquals(c1Ends - c1p2Starts, offsetIndex.getCompressedPageSize(1));        assertEquals(0, offsetIndex.getFirstRowIndex(0));        assertEquals(1, offsetIndex.getFirstRowIndex(1));        offsetIndex = reader.readOffsetIndex(blockMeta.getColumns().get(1));        assertEquals(3, offsetIndex.getPageCount());        assertEquals(c2p1Starts, offsetIndex.getOffset(0));        assertEquals(c2p2Starts, offsetIndex.getOffset(1));        assertEquals(c2p3Starts, offsetIndex.getOffset(2));        assertEquals(c2p2Starts - c2p1Starts, offsetIndex.getCompressedPageSize(0));        assertEquals(c2p3Starts - c2p2Starts, offsetIndex.getCompressedPageSize(1));        assertEquals(c2Ends - c2p3Starts, offsetIndex.getCompressedPageSize(2));        assertEquals(0, offsetIndex.getFirstRowIndex(0));        assertEquals(1, offsetIndex.getFirstRowIndex(1));        assertEquals(3, offsetIndex.getFirstRowIndex(2));        assertNull(reader.readColumnIndex(footer.getBlocks().get(2).getColumns().get(0)));    }}
private org.apache.parquet.column.statistics.Statistics<?> parquet-mr_f5999_0(Binary... values)
{    org.apache.parquet.column.statistics.Statistics<?> stats = org.apache.parquet.column.statistics.Statistics.createStats(C1.getPrimitiveType());    for (Binary value : values) {        if (value == null) {            stats.incrementNumNulls();        } else {            stats.updateStats(value);        }    }    return stats;}
private org.apache.parquet.column.statistics.Statistics<?> parquet-mr_f6000_0(Long... values)
{    org.apache.parquet.column.statistics.Statistics<?> stats = org.apache.parquet.column.statistics.Statistics.createStats(C2.getPrimitiveType());    for (Long value : values) {        if (value == null) {            stats.incrementNumNulls();        } else {            stats.updateStats(value);        }    }    return stats;}
public void parquet-mr_f6001_0() throws Exception
{    Configuration conf = new Configuration();        assertEquals(JobSummaryLevel.ALL, ParquetOutputFormat.getJobSummaryLevel(conf));}
public void parquet-mr_f6002_0() throws Exception
{    Configuration conf = new Configuration();    conf.set(ParquetOutputFormat.ENABLE_JOB_SUMMARY, "true");    assertEquals(JobSummaryLevel.ALL, ParquetOutputFormat.getJobSummaryLevel(conf));    conf.set(ParquetOutputFormat.ENABLE_JOB_SUMMARY, "false");    assertEquals(JobSummaryLevel.NONE, ParquetOutputFormat.getJobSummaryLevel(conf));}
public void parquet-mr_f6003_0() throws Exception
{    Configuration conf = new Configuration();    conf.set(ParquetOutputFormat.JOB_SUMMARY_LEVEL, "all");    assertEquals(JobSummaryLevel.ALL, ParquetOutputFormat.getJobSummaryLevel(conf));    conf.set(ParquetOutputFormat.JOB_SUMMARY_LEVEL, "common_only");    assertEquals(JobSummaryLevel.COMMON_ONLY, ParquetOutputFormat.getJobSummaryLevel(conf));    conf.set(ParquetOutputFormat.JOB_SUMMARY_LEVEL, "none");    assertEquals(JobSummaryLevel.NONE, ParquetOutputFormat.getJobSummaryLevel(conf));}
public void parquet-mr_f6004_0() throws Exception
{    Configuration conf = new Configuration();    conf.set(ParquetOutputFormat.JOB_SUMMARY_LEVEL, "common_only");    conf.set(ParquetOutputFormat.ENABLE_JOB_SUMMARY, "false");    assertEquals(JobSummaryLevel.COMMON_ONLY, ParquetOutputFormat.getJobSummaryLevel(conf));}
public void parquet-mr_f6005_0() throws Exception
{    Configuration conf = new Configuration();    Path root = new Path("target/tests/TestParquetWriter/");    enforceEmptyDir(conf, root);    MessageType schema = parseMessageType("message test { " + "required binary binary_field; " + "required int32 int32_field; " + "required int64 int64_field; " + "required boolean boolean_field; " + "required float float_field; " + "required double double_field; " + "required fixed_len_byte_array(3) flba_field; " + "required int96 int96_field; " + "} ");    GroupWriteSupport.setSchema(schema, conf);    SimpleGroupFactory f = new SimpleGroupFactory(schema);    Map<String, Encoding> expected = new HashMap<String, Encoding>();    expected.put("10-" + PARQUET_1_0, PLAIN_DICTIONARY);    expected.put("1000-" + PARQUET_1_0, PLAIN);    expected.put("10-" + PARQUET_2_0, RLE_DICTIONARY);    expected.put("1000-" + PARQUET_2_0, DELTA_BYTE_ARRAY);    for (int modulo : asList(10, 1000)) {        for (WriterVersion version : WriterVersion.values()) {            Path file = new Path(root, version.name() + "_" + modulo);            ParquetWriter<Group> writer = new ParquetWriter<Group>(file, new GroupWriteSupport(), UNCOMPRESSED, 1024, 1024, 512, true, false, version, conf);            for (int i = 0; i < 1000; i++) {                writer.write(f.newGroup().append("binary_field", "test" + (i % modulo)).append("int32_field", 32).append("int64_field", 64l).append("boolean_field", true).append("float_field", 1.0f).append("double_field", 2.0d).append("flba_field", "foo").append("int96_field", Binary.fromConstantByteArray(new byte[12])));            }            writer.close();            ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file).withConf(conf).build();            for (int i = 0; i < 1000; i++) {                Group group = reader.read();                assertEquals("test" + (i % modulo), group.getBinary("binary_field", 0).toStringUsingUTF8());                assertEquals(32, group.getInteger("int32_field", 0));                assertEquals(64l, group.getLong("int64_field", 0));                assertEquals(true, group.getBoolean("boolean_field", 0));                assertEquals(1.0f, group.getFloat("float_field", 0), 0.001);                assertEquals(2.0d, group.getDouble("double_field", 0), 0.001);                assertEquals("foo", group.getBinary("flba_field", 0).toStringUsingUTF8());                assertEquals(Binary.fromConstantByteArray(new byte[12]), group.getInt96("int96_field", 0));            }            reader.close();            ParquetMetadata footer = readFooter(conf, file, NO_FILTER);            for (BlockMetaData blockMetaData : footer.getBlocks()) {                for (ColumnChunkMetaData column : blockMetaData.getColumns()) {                    if (column.getPath().toDotString().equals("binary_field")) {                        String key = modulo + "-" + version;                        Encoding expectedEncoding = expected.get(key);                        assertTrue(key + ":" + column.getEncodings() + " should contain " + expectedEncoding, column.getEncodings().contains(expectedEncoding));                    }                }            }            assertEquals("Object model property should be example", "example", footer.getFileMetaData().getKeyValueMetaData().get(ParquetWriter.OBJECT_MODEL_NAME_PROP));        }    }}
public void parquet-mr_f6006_0() throws IOException
{    final File file = temp.newFile("test.parquet");    file.delete();    TestUtils.assertThrows("Should reject a schema with an empty group", InvalidSchemaException.class, (Callable<Void>) () -> {        ExampleParquetWriter.builder(new Path(file.toString())).withType(Types.buildMessage().addField(new GroupType(REQUIRED, "invalid_group")).named("invalid_message")).build();        return null;    });    Assert.assertFalse("Should not create a file when schema is rejected", file.exists());}
public void parquet-mr_f6007_0() throws IOException
{    MessageType schema = Types.buildMessage().optionalList().optionalElement(BINARY).as(stringType()).named("str_list").named("msg");    final int recordCount = 100;    Configuration conf = new Configuration();    GroupWriteSupport.setSchema(schema, conf);    GroupFactory factory = new SimpleGroupFactory(schema);    Group listNull = factory.newGroup();    File file = temp.newFile();    file.delete();    Path path = new Path(file.getAbsolutePath());    try (ParquetWriter<Group> writer = ExampleParquetWriter.builder(path).withPageRowCountLimit(10).withConf(conf).build()) {        for (int i = 0; i < recordCount; ++i) {            writer.write(listNull);        }    }    try (ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), path).build()) {        int readRecordCount = 0;        for (Group group = reader.read(); group != null; group = reader.read()) {            assertEquals(listNull.toString(), group.toString());            ++readRecordCount;        }        assertEquals("Number of written records should be equal to the read one", recordCount, readRecordCount);    }}
public void parquet-mr_f6008_0() throws IOException
{    this.file1 = newTemp();    this.file2 = newTemp();    ParquetWriter<Group> writer1 = ExampleParquetWriter.builder(file1).withType(FILE_SCHEMA).build();    ParquetWriter<Group> writer2 = ExampleParquetWriter.builder(file2).withType(FILE_SCHEMA).build();    for (int i = 0; i < FILE_SIZE; i += 1) {        Group group1 = GROUP_FACTORY.newGroup();        group1.add("id", i);        group1.add("string", UUID.randomUUID().toString());        writer1.write(group1);        file1content.add(group1);        Group group2 = GROUP_FACTORY.newGroup();        group2.add("id", FILE_SIZE + i);        group2.add("string", UUID.randomUUID().toString());        writer2.write(group2);        file2content.add(group2);    }    writer1.close();    writer2.close();}
public void parquet-mr_f6009_0() throws IOException
{    Path combinedFile = newTemp();    ParquetFileWriter writer = new ParquetFileWriter(CONF, FILE_SCHEMA, combinedFile);    writer.start();    writer.appendFile(CONF, file1);    writer.appendFile(CONF, file2);    writer.end(EMPTY_METADATA);    LinkedList<Group> expected = new LinkedList<Group>();    expected.addAll(file1content);    expected.addAll(file2content);    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), combinedFile).build();    Group next;    while ((next = reader.read()) != null) {        Group expectedNext = expected.removeFirst();                Assert.assertEquals("Each id should match", expectedNext.getInteger("id", 0), next.getInteger("id", 0));        Assert.assertEquals("Each string should match", expectedNext.getString("string", 0), next.getString("string", 0));    }    Assert.assertEquals("All records should be present", 0, expected.size());}
public void parquet-mr_f6010_0() throws IOException
{    Path combinedFile = newTemp();    ParquetFileWriter writer = new ParquetFileWriter(CONF, FILE_SCHEMA, combinedFile);    writer.start();    writer.appendFile(CONF, file1);    writer.appendFile(CONF, file2);    writer.end(EMPTY_METADATA);    ParquetMetadata combinedFooter = ParquetFileReader.readFooter(CONF, combinedFile, NO_FILTER);    ParquetMetadata f1Footer = ParquetFileReader.readFooter(CONF, file1, NO_FILTER);    ParquetMetadata f2Footer = ParquetFileReader.readFooter(CONF, file2, NO_FILTER);    LinkedList<BlockMetaData> expectedRowGroups = new LinkedList<BlockMetaData>();    expectedRowGroups.addAll(f1Footer.getBlocks());    expectedRowGroups.addAll(f2Footer.getBlocks());    Assert.assertEquals("Combined should have the right number of row groups", expectedRowGroups.size(), combinedFooter.getBlocks().size());    long nextStart = 4;    for (BlockMetaData rowGroup : combinedFooter.getBlocks()) {        BlockMetaData expected = expectedRowGroups.removeFirst();        Assert.assertEquals("Row count should match", expected.getRowCount(), rowGroup.getRowCount());        Assert.assertEquals("Compressed size should match", expected.getCompressedSize(), rowGroup.getCompressedSize());        Assert.assertEquals("Total size should match", expected.getTotalByteSize(), rowGroup.getTotalByteSize());        Assert.assertEquals("Start pos should be at the last row group's end", nextStart, rowGroup.getStartingPos());        assertColumnsEquivalent(expected.getColumns(), rowGroup.getColumns());        nextStart = rowGroup.getStartingPos() + rowGroup.getTotalByteSize();    }}
public void parquet-mr_f6011_0(List<ColumnChunkMetaData> expected, List<ColumnChunkMetaData> actual)
{    Assert.assertEquals("Should have the expected columns", expected.size(), actual.size());    for (int i = 0; i < actual.size(); i += 1) {        ColumnChunkMetaData current = actual.get(i);        if (i != 0) {            ColumnChunkMetaData previous = actual.get(i - 1);            long expectedStart = previous.getStartingPos() + previous.getTotalSize();            Assert.assertEquals("Should start after the previous column", expectedStart, current.getStartingPos());        }        assertColumnMetadataEquivalent(expected.get(i), current);    }}
public void parquet-mr_f6012_0(ColumnChunkMetaData expected, ColumnChunkMetaData actual)
{    Assert.assertEquals("Should be the expected column", expected.getPath(), expected.getPath());    Assert.assertEquals("Primitive type should not change", expected.getType(), actual.getType());    Assert.assertEquals("Compression codec should not change", expected.getCodec(), actual.getCodec());    Assert.assertEquals("Data encodings should not change", expected.getEncodings(), actual.getEncodings());    Assert.assertEquals("Statistics should not change", expected.getStatistics(), actual.getStatistics());    Assert.assertEquals("Uncompressed size should not change", expected.getTotalUncompressedSize(), actual.getTotalUncompressedSize());    Assert.assertEquals("Compressed size should not change", expected.getTotalSize(), actual.getTotalSize());    Assert.assertEquals("Number of values should not change", expected.getValueCount(), actual.getValueCount());}
public void parquet-mr_f6013_0() throws IOException
{    MessageType droppedColumnSchema = Types.buildMessage().required(BINARY).as(UTF8).named("string").named("AppendTest");    Path droppedColumnFile = newTemp();    ParquetFileWriter writer = new ParquetFileWriter(CONF, droppedColumnSchema, droppedColumnFile);    writer.start();    writer.appendFile(CONF, file1);    writer.appendFile(CONF, file2);    writer.end(EMPTY_METADATA);    LinkedList<Group> expected = new LinkedList<Group>();    expected.addAll(file1content);    expected.addAll(file2content);    ParquetMetadata footer = ParquetFileReader.readFooter(CONF, droppedColumnFile, NO_FILTER);    for (BlockMetaData rowGroup : footer.getBlocks()) {        Assert.assertEquals("Should have only the string column", 1, rowGroup.getColumns().size());    }    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), droppedColumnFile).build();    Group next;    while ((next = reader.read()) != null) {        Group expectedNext = expected.removeFirst();        Assert.assertEquals("Each string should match", expectedNext.getString("string", 0), next.getString("string", 0));    }    Assert.assertEquals("All records should be present", 0, expected.size());}
public void parquet-mr_f6014_0() throws IOException
{    MessageType droppedColumnSchema = Types.buildMessage().required(BINARY).as(UTF8).named("string").named("AppendTest");    final ParquetMetadata footer = ParquetFileReader.readFooter(CONF, file1, NO_FILTER);    final FSDataInputStream incoming = file1.getFileSystem(CONF).open(file1);    Path droppedColumnFile = newTemp();    final ParquetFileWriter writer = new ParquetFileWriter(CONF, droppedColumnSchema, droppedColumnFile);    writer.start();    TestUtils.assertThrows("Should complain that id column is dropped", IllegalArgumentException.class, (Callable<Void>) () -> {        writer.appendRowGroups(incoming, footer.getBlocks(), false);        return null;    });}
public void parquet-mr_f6015_0() throws IOException
{    MessageType fileSchema = Types.buildMessage().required(INT32).named("id").required(BINARY).as(UTF8).named("string").required(FLOAT).named("value").named("AppendTest");    Path missingColumnFile = newTemp();    final ParquetFileWriter writer = new ParquetFileWriter(CONF, fileSchema, missingColumnFile);    writer.start();    TestUtils.assertThrows("Should complain that value column is missing", IllegalArgumentException.class, (Callable<Void>) () -> {        writer.appendFile(CONF, file1);        return null;    });}
private Path parquet-mr_f6016_0() throws IOException
{    File file = temp.newFile();    Preconditions.checkArgument(file.delete(), "Could not remove temp file");    return new Path(file.toString());}
public void parquet-mr_f6017_0() throws Exception
{    Configuration conf = new Configuration();    Path root = new Path("target/tests/TestParquetWriter/");    FileSystem fs = root.getFileSystem(conf);    if (fs.exists(root)) {        fs.delete(root, true);    }    fs.mkdirs(root);    MessageType schema = parseMessageType("message test { " + "required binary binary_field; " + "required int32 int32_field; " + "required int64 int64_field; " + "required boolean boolean_field; " + "required float float_field; " + "required double double_field; " + "required fixed_len_byte_array(3) flba_field; " + "required int96 int96_field; " + "optional binary null_field; " + "} ");    GroupWriteSupport.setSchema(schema, conf);    SimpleGroupFactory f = new SimpleGroupFactory(schema);    Map<String, Encoding> expected = new HashMap<String, Encoding>();    expected.put("10-" + PARQUET_1_0, PLAIN_DICTIONARY);    expected.put("1000-" + PARQUET_1_0, PLAIN);    expected.put("10-" + PARQUET_2_0, RLE_DICTIONARY);    expected.put("1000-" + PARQUET_2_0, DELTA_BYTE_ARRAY);    for (int modulo : asList(10, 1000)) {        for (WriterVersion version : WriterVersion.values()) {            Path file = new Path(root, version.name() + "_" + modulo);            ParquetWriter<Group> writer = new ParquetWriter<Group>(file, new GroupWriteSupport(), UNCOMPRESSED, 1024, 1024, 512, true, false, version, conf);            for (int i = 0; i < 1000; i++) {                writer.write(f.newGroup().append("binary_field", "test" + (i % modulo)).append("int32_field", 32).append("int64_field", 64l).append("boolean_field", true).append("float_field", 1.0f).append("double_field", 2.0d).append("flba_field", "foo").append("int96_field", Binary.fromConstantByteArray(new byte[12])));            }            writer.close();            ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file).withConf(conf).build();            for (int i = 0; i < 1000; i++) {                Group group = reader.read();                assertEquals("test" + (i % modulo), group.getBinary("binary_field", 0).toStringUsingUTF8());                assertEquals(32, group.getInteger("int32_field", 0));                assertEquals(64l, group.getLong("int64_field", 0));                assertEquals(true, group.getBoolean("boolean_field", 0));                assertEquals(1.0f, group.getFloat("float_field", 0), 0.001);                assertEquals(2.0d, group.getDouble("double_field", 0), 0.001);                assertEquals("foo", group.getBinary("flba_field", 0).toStringUsingUTF8());                assertEquals(Binary.fromConstantByteArray(new byte[12]), group.getInt96("int96_field", 0));                assertEquals(0, group.getFieldRepetitionCount("null_field"));            }            reader.close();            ParquetMetadata footer = readFooter(conf, file, NO_FILTER);            for (BlockMetaData blockMetaData : footer.getBlocks()) {                for (ColumnChunkMetaData column : blockMetaData.getColumns()) {                    if (column.getPath().toDotString().equals("binary_field")) {                        String key = modulo + "-" + version;                        Encoding expectedEncoding = expected.get(key);                        assertTrue(key + ":" + column.getEncodings() + " should contain " + expectedEncoding, column.getEncodings().contains(expectedEncoding));                    }                }            }        }    }}
private static void parquet-mr_f6018_0(ParquetWriter<Group> writer) throws IOException
{    SimpleGroupFactory f = new SimpleGroupFactory(SCHEMA);    for (int i = 0; i < NUM_RECORDS; i += 1) {        int index = i % ALPHABET.length();        Group group = f.newGroup().append("dict_binary_field", ALPHABET.substring(index, index + 1)).append("plain_int32_field", i).append("fallback_binary_field", i < (NUM_RECORDS / 2) ? ALPHABET.substring(index, index + 1) : UUID.randomUUID().toString());        writer.write(group);    }}
public void parquet-mr_f6019_0() throws Exception
{    File file = temp.newFile("encoding-stats.parquet");    assertTrue(file.delete());    Path path = new Path(file.toString());    ParquetWriter<Group> writer = ExampleParquetWriter.builder(path).withWriterVersion(PARQUET_1_0).withPageSize(    1024).enableDictionaryEncoding().withDictionaryPageSize(2 * 1024).withConf(CONF).withType(SCHEMA).build();    writeData(writer);    writer.close();    ParquetFileReader reader = ParquetFileReader.open(CONF, path);    assertEquals("Should have one row group", 1, reader.getRowGroups().size());    BlockMetaData rowGroup = reader.getRowGroups().get(0);    ColumnChunkMetaData dictColumn = rowGroup.getColumns().get(0);    EncodingStats dictStats = dictColumn.getEncodingStats();    assertNotNull("Dict column should have non-null encoding stats", dictStats);    assertTrue("Dict column should have a dict page", dictStats.hasDictionaryPages());    assertTrue("Dict column should have dict-encoded pages", dictStats.hasDictionaryEncodedPages());    assertFalse("Dict column should not have non-dict pages", dictStats.hasNonDictionaryEncodedPages());    ColumnChunkMetaData plainColumn = rowGroup.getColumns().get(1);    EncodingStats plainStats = plainColumn.getEncodingStats();    assertNotNull("Plain column should have non-null encoding stats", plainStats);    assertFalse("Plain column should not have a dict page", plainStats.hasDictionaryPages());    assertFalse("Plain column should not have dict-encoded pages", plainStats.hasDictionaryEncodedPages());    assertTrue("Plain column should have non-dict pages", plainStats.hasNonDictionaryEncodedPages());    ColumnChunkMetaData fallbackColumn = rowGroup.getColumns().get(2);    EncodingStats fallbackStats = fallbackColumn.getEncodingStats();    assertNotNull("Fallback column should have non-null encoding stats", fallbackStats);    assertTrue("Fallback column should have a dict page", fallbackStats.hasDictionaryPages());    assertTrue("Fallback column should have dict-encoded pages", fallbackStats.hasDictionaryEncodedPages());    assertTrue("Fallback column should have non-dict pages", fallbackStats.hasNonDictionaryEncodedPages());}
public void parquet-mr_f6020_0() throws IOException
{        SnappyCompressor compressor = new SnappyCompressor();    SnappyDecompressor decompressor = new SnappyDecompressor();    TestSnappy(compressor, decompressor, "");    TestSnappy(compressor, decompressor, "FooBar");    TestSnappy(compressor, decompressor, "FooBar1", "FooBar2");    TestSnappy(compressor, decompressor, "FooBar");    TestSnappy(compressor, decompressor, "a", "blahblahblah", "abcdef");    TestSnappy(compressor, decompressor, "");    TestSnappy(compressor, decompressor, "FooBar");}
public void parquet-mr_f6021_0() throws IOException
{    SnappyCodec codec = new SnappyCodec();    codec.setConf(new Configuration());    int blockSize = 1024;    int inputSize = blockSize * 1024;    byte[] input = new byte[inputSize];    for (int i = 0; i < inputSize; ++i) {        input[i] = (byte) i;    }    ByteArrayOutputStream compressedStream = new ByteArrayOutputStream();    CompressionOutputStream compressor = codec.createOutputStream(compressedStream);    int bytesCompressed = 0;    while (bytesCompressed < inputSize) {        int len = Math.min(inputSize - bytesCompressed, blockSize);        compressor.write(input, bytesCompressed, len);        bytesCompressed += len;    }    compressor.finish();    byte[] rawCompressed = Snappy.compress(input);    byte[] codecCompressed = compressedStream.toByteArray();            assertArrayEquals(rawCompressed, codecCompressed);    ByteArrayInputStream inputStream = new ByteArrayInputStream(codecCompressed);    CompressionInputStream decompressor = codec.createInputStream(inputStream);    byte[] codecDecompressed = new byte[inputSize];    int bytesDecompressed = 0;    int numBytes;    while ((numBytes = decompressor.read(codecDecompressed, bytesDecompressed, blockSize)) != 0) {        bytesDecompressed += numBytes;        if (bytesDecompressed == inputSize)            break;    }    byte[] rawDecompressed = Snappy.uncompress(rawCompressed);    assertArrayEquals(input, rawDecompressed);    assertArrayEquals(input, codecDecompressed);}
private void parquet-mr_f6022_0(SnappyCompressor compressor, SnappyDecompressor decompressor, String... strings) throws IOException
{    compressor.reset();    decompressor.reset();    int uncompressedSize = 0;    for (String s : strings) {        uncompressedSize += s.length();    }    byte[] uncompressedData = new byte[uncompressedSize];    int len = 0;    for (String s : strings) {        byte[] tmp = s.getBytes();        System.arraycopy(tmp, 0, uncompressedData, len, s.length());        len += s.length();    }    assert (compressor.needsInput());    compressor.setInput(uncompressedData, 0, len);    assert (compressor.needsInput());    compressor.finish();    assert (!compressor.needsInput());    assert (!compressor.finished() || uncompressedSize == 0);    byte[] compressedData = new byte[1000];    int compressedSize = compressor.compress(compressedData, 0, 1000);    assert (compressor.finished());    assert (!decompressor.finished());    assert (decompressor.needsInput());    decompressor.setInput(compressedData, 0, compressedSize);    assert (!decompressor.finished());    byte[] decompressedData = new byte[uncompressedSize];    int decompressedSize = decompressor.decompress(decompressedData, 0, uncompressedSize);    assert (decompressor.finished());    assertEquals(uncompressedSize, decompressedSize);    assertArrayEquals(uncompressedData, decompressedData);}
public static void parquet-mr_f6023_0(Configuration conf, Path path) throws IOException
{    FileSystem fs = path.getFileSystem(conf);    if (fs.exists(path)) {        if (!fs.delete(path, true)) {            throw new IOException("can not delete path " + path);        }    }    if (!fs.mkdirs(path)) {        throw new IOException("can not create path " + path);    }}
public static void parquet-mr_f6024_0(String message, Class<? extends Exception> expected, Callable callable)
{    try {        callable.call();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        try {            Assert.assertEquals(message, expected, actual.getClass());        } catch (AssertionError e) {            e.addSuppressed(actual);            throw e;        }    }}
public static void parquet-mr_f6025_0(Statistics<?> stats1, Statistics<?> stats2)
{    assertStatsValuesEqual(null, stats1, stats2);}
public static void parquet-mr_f6026_0(String message, Statistics<?> expected, Statistics<?> actual)
{    if (expected == actual) {        return;    }    if (expected == null || actual == null) {        Assert.assertEquals(expected, actual);    }    Assert.assertThat(actual, CoreMatchers.instanceOf(expected.getClass()));    Assert.assertArrayEquals(message, expected.getMaxBytes(), actual.getMaxBytes());    Assert.assertArrayEquals(message, expected.getMinBytes(), actual.getMinBytes());    Assert.assertEquals(message, expected.getNumNulls(), actual.getNumNulls());}
public synchronized int parquet-mr_f6027_0(byte[] b, int off, int len)
{    if (current < lengths.length) {        if (len <= lengths[current]) {                        int bytesRead = super.read(b, off, len);            lengths[current] -= bytesRead;            return bytesRead;        } else {            int bytesRead = super.read(b, off, lengths[current]);            current += 1;            return bytesRead;        }    } else {        return super.read(b, off, len);    }}
public int parquet-mr_f6028_0(long position, byte[] buffer, int offset, int length) throws IOException
{    seek(position);    return read(buffer, offset, length);}
public void parquet-mr_f6029_0(long position, byte[] buffer, int offset, int length) throws IOException
{    throw new UnsupportedOperationException("Not actually supported.");}
public void parquet-mr_f6030_0(long position, byte[] buffer) throws IOException
{    throw new UnsupportedOperationException("Not actually supported.");}
public void parquet-mr_f6031_0(long pos) throws IOException
{    this.pos = (int) pos;}
public long parquet-mr_f6032_0() throws IOException
{    return this.pos;}
public boolean parquet-mr_f6033_0(long targetPos) throws IOException
{    seek(targetPos);    return true;}
public int parquet-mr_f6034_0(ByteBuffer buf) throws IOException
{            byte[] temp = new byte[buf.remaining()];    int bytesRead = stream.read(temp, 0, temp.length);    if (bytesRead > 0) {        buf.put(temp, 0, bytesRead);    }    return bytesRead;}
public void parquet-mr_f6035_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(8);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
public void parquet-mr_f6036_0() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocate(20);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());    final MockBufferReader reader = new MockBufferReader(hadoopStream);    TestUtils.assertThrows("Should throw EOFException", EOFException.class, () -> {        H2SeekableInputStream.readFully(reader, readBuffer);        return null;    });                            Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());}
public void parquet-mr_f6037_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(10);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());    MockBufferReader reader = new MockBufferReader(hadoopStream);        H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());        H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f6038_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(10);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f6039_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(10);    readBuffer.position(3);    readBuffer.mark();    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
public void parquet-mr_f6040_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(10);    readBuffer.limit(7);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f6041_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(10);    readBuffer.position(3);    readBuffer.limit(7);    readBuffer.mark();    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 4), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
public void parquet-mr_f6042_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(8);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
public void parquet-mr_f6043_0() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());    final MockBufferReader reader = new MockBufferReader(hadoopStream);    TestUtils.assertThrows("Should throw EOFException", EOFException.class, () -> {        H2SeekableInputStream.readFully(reader, readBuffer);        return null;    });                            Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());}
public void parquet-mr_f6044_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());    MockBufferReader reader = new MockBufferReader(hadoopStream);        H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());        H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f6045_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f6046_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    readBuffer.position(3);    readBuffer.mark();    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
public void parquet-mr_f6047_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    readBuffer.limit(7);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    H2SeekableInputStream.Reader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
public void parquet-mr_f6048_0() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    readBuffer.position(3);    readBuffer.limit(7);    readBuffer.mark();    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 4), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
public void parquet-mr_f6049_0() throws Exception
{    Map<Integer, String> anObject = new HashMap<Integer, String>();    anObject.put(7, "seven");    anObject.put(8, "eight");    Configuration conf = new Configuration();    SerializationUtil.writeObjectToConfAsBase64("anobject", anObject, conf);    Map<Integer, String> copy = SerializationUtil.readObjectFromConfAsBase64("anobject", conf);    assertEquals(anObject, copy);    try {        Set<String> bad = SerializationUtil.readObjectFromConfAsBase64("anobject", conf);        fail("This should throw a ClassCastException");    } catch (ClassCastException e) {    }    conf = new Configuration();    Object nullObj = null;    SerializationUtil.writeObjectToConfAsBase64("anobject", null, conf);    Object copyObj = SerializationUtil.readObjectFromConfAsBase64("anobject", conf);    assertEquals(nullObj, copyObj);}
public void parquet-mr_f6050_0() throws Exception
{    assertNull(SerializationUtil.readObjectFromConfAsBase64("non-existant-key", new Configuration()));}
public boolean parquet-mr_f6051_0()
{    return (random.nextInt(10) == 0);}
public int parquet-mr_f6052_0()
{    return random.nextInt();}
public int parquet-mr_f6053_0(int maximum)
{        return random.nextInt(Math.abs(maximum) + 1);}
public long parquet-mr_f6054_0()
{    return random.nextLong();}
public long parquet-mr_f6055_0(long maximum)
{    return randomLong() % maximum;}
public float parquet-mr_f6056_0()
{    return random.nextFloat();}
public float parquet-mr_f6057_0(float maximum)
{    return random.nextFloat() % maximum;}
public double parquet-mr_f6058_0()
{    return random.nextDouble();}
public double parquet-mr_f6059_0(double maximum)
{    return random.nextDouble() % maximum;}
public BigInteger parquet-mr_f6060_0()
{    return new BigInteger(95, random);}
public BigInteger parquet-mr_f6061_0(BigInteger maximum)
{    BigInteger result;    while ((result = randomInt96()).compareTo(maximum) > 0) ;    return result;}
public char parquet-mr_f6062_0()
{    return ALPHABET.charAt(randomPositiveInt(ALPHABET.length() - 1));}
public String parquet-mr_f6063_0(int maxLength)
{    return randomFixedLengthString(randomPositiveInt(maxLength));}
public String parquet-mr_f6064_0(int length)
{    StringBuilder builder = new StringBuilder();    for (int index = 0; index < length; index++) {        builder.append(randomLetter());    }    return builder.toString();}
public Binary parquet-mr_f6065_0(byte[] data)
{    int length = Math.min(data.length, bufferLength);    System.arraycopy(data, 0, buffer, 0, length);    return Binary.fromReusedByteArray(data, 0, length);}
public Integer parquet-mr_f6066_0()
{    return (minimum + randomPositiveInt(range));}
public Integer parquet-mr_f6067_0()
{    return super.nextValue() & mask;}
public Integer parquet-mr_f6068_0()
{    return randomInt();}
public Long parquet-mr_f6069_0()
{    return (minimum + randomLong(range));}
public Long parquet-mr_f6070_0()
{    return randomLong();}
public BigInteger parquet-mr_f6071_0()
{    return (minimum.add(randomInt96(range)));}
public Binary parquet-mr_f6072_0()
{    return asReusedBinary(nextValue().toByteArray());}
public Float parquet-mr_f6073_0()
{    return (minimum + randomFloat(range));}
public Float parquet-mr_f6074_0()
{    return randomFloat();}
public Double parquet-mr_f6075_0()
{    return (minimum + randomDouble(range));}
public Double parquet-mr_f6076_0()
{    return randomDouble();}
public String parquet-mr_f6077_0()
{    int stringLength = randomPositiveInt(15) + 1;    return randomString(stringLength);}
public Binary parquet-mr_f6078_0()
{    return asReusedBinary(nextValue().getBytes());}
public Binary parquet-mr_f6079_0()
{        int length = 5 + randomPositiveInt(buffer.length - 5);    for (int index = 0; index < length; index++) {        buffer[index] = (byte) randomInt();    }    return Binary.fromReusedByteArray(buffer, 0, length);}
public Binary parquet-mr_f6080_0()
{    return nextValue();}
public Binary parquet-mr_f6081_0()
{    for (int index = 0; index < buffer.length; index++) {        buffer[index] = (byte) randomInt();    }    return Binary.fromReusedByteArray(buffer);}
public Binary parquet-mr_f6082_0()
{    return nextValue();}
public T parquet-mr_f6083_0()
{    return this.minimum;}
public T parquet-mr_f6084_0()
{    return this.maximum;}
public static void parquet-mr_f6085_0(WriteContext context) throws IOException
{        Configuration configuration = new Configuration();    GroupWriteSupport.setSchema(context.schema, configuration);    GroupWriteSupport groupWriteSupport = new GroupWriteSupport();        final int blockSize = context.blockSize;    final int pageSize = context.pageSize;    final int dictionaryPageSize = pageSize;    final boolean enableDictionary = context.enableDictionary;    final boolean enableValidation = context.enableValidation;    ParquetProperties.WriterVersion writerVersion = context.version;    CompressionCodecName codec = CompressionCodecName.UNCOMPRESSED;    ParquetWriter<Group> writer = new ParquetWriter<Group>(context.fsPath, groupWriteSupport, codec, blockSize, pageSize, dictionaryPageSize, enableDictionary, enableValidation, writerVersion, configuration);    context.write(writer);    writer.close();    context.test();    context.path.delete();}
public DictionaryPage parquet-mr_f6086_0()
{    return dict;}
public long parquet-mr_f6087_0()
{    return data.getValueCount();}
public DataPage parquet-mr_f6088_0()
{    return data;}
private static Statistics<T> parquet-mr_f6089_0(DataPage page)
{    return page.accept(new DataPage.Visitor<Statistics<T>>() {        @Override        @SuppressWarnings("unchecked")        public Statistics<T> visit(DataPageV1 dataPageV1) {            return (Statistics<T>) dataPageV1.getStatistics();        }        @Override        @SuppressWarnings("unchecked")        public Statistics<T> visit(DataPageV2 dataPageV2) {            return (Statistics<T>) dataPageV2.getStatistics();        }    });}
public Statistics<T> parquet-mr_f6090_0(DataPageV1 dataPageV1)
{    return (Statistics<T>) dataPageV1.getStatistics();}
public Statistics<T> parquet-mr_f6091_0(DataPageV2 dataPageV2)
{    return (Statistics<T>) dataPageV2.getStatistics();}
public void parquet-mr_f6092_0(T value)
{    if (hasNonNull) {        assertTrue("min should be <= all values", comparator.compare(min, value) <= 0);        assertTrue("min should be >= all values", comparator.compare(max, value) >= 0);    }}
private static PrimitiveConverter parquet-mr_f6093_0(final DataPage page, PrimitiveTypeName type)
{    return type.convert(new PrimitiveType.PrimitiveTypeNameConverter<PrimitiveConverter, RuntimeException>() {        @Override        public PrimitiveConverter convertFLOAT(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Float> validator = new StatsValidator<Float>(page);            return new PrimitiveConverter() {                @Override                public void addFloat(float value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertDOUBLE(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Double> validator = new StatsValidator<Double>(page);            return new PrimitiveConverter() {                @Override                public void addDouble(double value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertINT32(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Integer> validator = new StatsValidator<Integer>(page);            return new PrimitiveConverter() {                @Override                public void addInt(int value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertINT64(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Long> validator = new StatsValidator<Long>(page);            return new PrimitiveConverter() {                @Override                public void addLong(long value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertBOOLEAN(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Boolean> validator = new StatsValidator<Boolean>(page);            return new PrimitiveConverter() {                @Override                public void addBoolean(boolean value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertINT96(PrimitiveTypeName primitiveTypeName) {            return convertBINARY(primitiveTypeName);        }        @Override        public PrimitiveConverter convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) {            return convertBINARY(primitiveTypeName);        }        @Override        public PrimitiveConverter convertBINARY(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Binary> validator = new StatsValidator<Binary>(page);            return new PrimitiveConverter() {                @Override                public void addBinary(Binary value) {                    validator.validate(value);                }            };        }    });}
public PrimitiveConverter parquet-mr_f6094_0(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Float> validator = new StatsValidator<Float>(page);    return new PrimitiveConverter() {        @Override        public void addFloat(float value) {            validator.validate(value);        }    };}
public void parquet-mr_f6095_0(float value)
{    validator.validate(value);}
public PrimitiveConverter parquet-mr_f6096_0(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Double> validator = new StatsValidator<Double>(page);    return new PrimitiveConverter() {        @Override        public void addDouble(double value) {            validator.validate(value);        }    };}
public void parquet-mr_f6097_0(double value)
{    validator.validate(value);}
public PrimitiveConverter parquet-mr_f6098_0(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Integer> validator = new StatsValidator<Integer>(page);    return new PrimitiveConverter() {        @Override        public void addInt(int value) {            validator.validate(value);        }    };}
public void parquet-mr_f6099_0(int value)
{    validator.validate(value);}
public PrimitiveConverter parquet-mr_f6100_0(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Long> validator = new StatsValidator<Long>(page);    return new PrimitiveConverter() {        @Override        public void addLong(long value) {            validator.validate(value);        }    };}
public void parquet-mr_f6101_0(long value)
{    validator.validate(value);}
public PrimitiveConverter parquet-mr_f6102_0(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Boolean> validator = new StatsValidator<Boolean>(page);    return new PrimitiveConverter() {        @Override        public void addBoolean(boolean value) {            validator.validate(value);        }    };}
public void parquet-mr_f6103_0(boolean value)
{    validator.validate(value);}
public PrimitiveConverter parquet-mr_f6104_0(PrimitiveTypeName primitiveTypeName)
{    return convertBINARY(primitiveTypeName);}
public PrimitiveConverter parquet-mr_f6105_0(PrimitiveTypeName primitiveTypeName)
{    return convertBINARY(primitiveTypeName);}
public PrimitiveConverter parquet-mr_f6106_0(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Binary> validator = new StatsValidator<Binary>(page);    return new PrimitiveConverter() {        @Override        public void addBinary(Binary value) {            validator.validate(value);        }    };}
public void parquet-mr_f6107_0(Binary value)
{    validator.validate(value);}
public void parquet-mr_f6108_0(MessageType schema, PageReadStore store)
{    for (ColumnDescriptor desc : schema.getColumns()) {        PageReader reader = store.getPageReader(desc);        DictionaryPage dict = reader.readDictionaryPage();        DataPage page;        while ((page = reader.readPage()) != null) {            validateStatsForPage(page, dict, desc);        }    }}
private void parquet-mr_f6109_0(DataPage page, DictionaryPage dict, ColumnDescriptor desc)
{    SingletonPageReader reader = new SingletonPageReader(dict, page);    PrimitiveConverter converter = getValidatingConverter(page, desc.getType());    Statistics<?> stats = getStatisticsFromPageHeader(page);    assertEquals("Statistics does not use the proper comparator", desc.getPrimitiveType().comparator().getClass(), stats.comparator().getClass());    if (stats.isEmpty()) {                        System.err.println(String.format("No stats written for page=%s col=%s", page, Arrays.toString(desc.getPath())));        return;    }    long numNulls = 0;    ColumnReaderImpl column = new ColumnReaderImpl(desc, reader, converter, null);    for (int i = 0; i < reader.getTotalValueCount(); i += 1) {        if (column.getCurrentDefinitionLevel() >= desc.getMaxDefinitionLevel()) {            column.writeCurrentValueToConverter();        } else {            numNulls += 1;        }        column.consume();    }    Assert.assertEquals(numNulls, stats.getNumNulls());    System.err.println(String.format("Validated stats min=%s max=%s nulls=%d for page=%s col=%s", stats.minAsString(), stats.maxAsString(), stats.getNumNulls(), page, Arrays.toString(desc.getPath())));}
private static MessageType parquet-mr_f6110_0(long seed)
{    Random random = new Random(seed);    int fixedBinaryLength = random.nextInt(21) + 1;    int fixedPrecision = calculatePrecision(fixedBinaryLength);    int fixedScale = fixedPrecision / 4;    int binaryPrecision = calculatePrecision(16);    int binaryScale = binaryPrecision / 4;    return new MessageType("schema", new PrimitiveType(OPTIONAL, INT32, "i32"), new PrimitiveType(OPTIONAL, INT64, "i64"), new PrimitiveType(OPTIONAL, INT96, "i96"), new PrimitiveType(OPTIONAL, FLOAT, "sngl"), new PrimitiveType(OPTIONAL, DOUBLE, "dbl"), new PrimitiveType(OPTIONAL, BINARY, "strings"), new PrimitiveType(OPTIONAL, BINARY, "binary"), new PrimitiveType(OPTIONAL, FIXED_LEN_BYTE_ARRAY, fixedBinaryLength, "fixed-binary"), new PrimitiveType(REQUIRED, INT32, "unconstrained-i32"), new PrimitiveType(REQUIRED, INT64, "unconstrained-i64"), new PrimitiveType(REQUIRED, FLOAT, "unconstrained-sngl"), new PrimitiveType(REQUIRED, DOUBLE, "unconstrained-dbl"), Types.optional(INT32).as(OriginalType.INT_8).named("int8"), Types.optional(INT32).as(OriginalType.UINT_8).named("uint8"), Types.optional(INT32).as(OriginalType.INT_16).named("int16"), Types.optional(INT32).as(OriginalType.UINT_16).named("uint16"), Types.optional(INT32).as(OriginalType.INT_32).named("int32"), Types.optional(INT32).as(OriginalType.UINT_32).named("uint32"), Types.optional(INT64).as(OriginalType.INT_64).named("int64"), Types.optional(INT64).as(OriginalType.UINT_64).named("uint64"), Types.optional(INT32).as(OriginalType.DECIMAL).precision(9).scale(2).named("decimal-int32"), Types.optional(INT64).as(OriginalType.DECIMAL).precision(18).scale(4).named("decimal-int64"), Types.optional(FIXED_LEN_BYTE_ARRAY).length(fixedBinaryLength).as(OriginalType.DECIMAL).precision(fixedPrecision).scale(fixedScale).named("decimal-fixed"), Types.optional(BINARY).as(OriginalType.DECIMAL).precision(binaryPrecision).scale(binaryScale).named("decimal-binary"), Types.optional(BINARY).as(OriginalType.UTF8).named("utf8"), Types.optional(BINARY).as(OriginalType.ENUM).named("enum"), Types.optional(BINARY).as(OriginalType.JSON).named("json"), Types.optional(BINARY).as(OriginalType.BSON).named("bson"), Types.optional(INT32).as(OriginalType.DATE).named("date"), Types.optional(INT32).as(OriginalType.TIME_MILLIS).named("time-millis"), Types.optional(INT64).as(OriginalType.TIME_MICROS).named("time-micros"), Types.optional(INT64).as(OriginalType.TIMESTAMP_MILLIS).named("timestamp-millis"), Types.optional(INT64).as(OriginalType.TIMESTAMP_MICROS).named("timestamp-micros"), Types.optional(FIXED_LEN_BYTE_ARRAY).length(12).as(OriginalType.INTERVAL).named("interval"));}
private static int parquet-mr_f6111_0(int byteCnt)
{    String maxValue = BigInteger.valueOf(2L).pow(8 * byteCnt - 1).toString();    return maxValue.length() - 1;}
public void parquet-mr_f6112_0(ParquetWriter<Group> writer) throws IOException
{    for (int index = 0; index < recordCount; index++) {        Group group = new SimpleGroup(super.schema);        for (int column = 0, columnCnt = schema.getFieldCount(); column < columnCnt; ++column) {            Type type = schema.getType(column);            RandomValueGenerator<?> generator = randomGenerators.get(column);            if (type.isRepetition(OPTIONAL) && generator.shouldGenerateNull()) {                continue;            }            switch(type.asPrimitiveType().getPrimitiveTypeName()) {                case BINARY:                case FIXED_LEN_BYTE_ARRAY:                case INT96:                    group.append(type.getName(), ((RandomBinaryBase<?>) generator).nextBinaryValue());                    break;                case INT32:                    group.append(type.getName(), (Integer) generator.nextValue());                    break;                case INT64:                    group.append(type.getName(), (Long) generator.nextValue());                    break;                case FLOAT:                    group.append(type.getName(), (Float) generator.nextValue());                    break;                case DOUBLE:                    group.append(type.getName(), (Double) generator.nextValue());                    break;                case BOOLEAN:                    group.append(type.getName(), (Boolean) generator.nextValue());                    break;            }        }        writer.write(group);    }}
public void parquet-mr_f6113_0() throws IOException
{    Configuration configuration = new Configuration();    ParquetMetadata metadata = ParquetFileReader.readFooter(configuration, super.fsPath, ParquetMetadataConverter.NO_FILTER);    ParquetFileReader reader = new ParquetFileReader(configuration, metadata.getFileMetaData(), super.fsPath, metadata.getBlocks(), metadata.getFileMetaData().getSchema().getColumns());    PageStatsValidator validator = new PageStatsValidator();    PageReadStore pageReadStore;    while ((pageReadStore = reader.readNextRowGroup()) != null) {        validator.validate(metadata.getFileMetaData().getSchema(), pageReadStore);    }}
public void parquet-mr_f6114_0() throws IOException
{    File file = folder.newFile("test_file.parquet");    file.delete();    System.out.println(String.format("RANDOM SEED: %s", RANDOM_SEED));    Random random = new Random(RANDOM_SEED);    int blockSize = (random.nextInt(54) + 10) * MEGABYTE;    int pageSize = (random.nextInt(10) + 1) * MEGABYTE;    List<DataContext> contexts = Arrays.asList(new DataContext(random.nextLong(), file, blockSize, pageSize, false, ParquetProperties.WriterVersion.PARQUET_1_0), new DataContext(random.nextLong(), file, blockSize, pageSize, true, ParquetProperties.WriterVersion.PARQUET_1_0), new DataContext(random.nextLong(), file, blockSize, pageSize, false, ParquetProperties.WriterVersion.PARQUET_2_0), new DataContext(random.nextLong(), file, blockSize, pageSize, true, ParquetProperties.WriterVersion.PARQUET_2_0));    for (DataContext test : contexts) {        DataGenerationContext.writeAndTest(test);    }}
private void parquet-mr_f6115_0(final JobConf job)
{    final String plan = HiveConf.getVar(job, HiveConf.ConfVars.PLAN);    if (mrwork == null && plan != null && plan.length() > 0) {        mrwork = Utilities.getMapRedWork(job);        pathToPartitionInfo.clear();        for (final Map.Entry<String, PartitionDesc> entry : mrwork.getPathToPartitionInfo().entrySet()) {            pathToPartitionInfo.put(new Path(entry.getKey()).toUri().getPath().toString(), entry.getValue());        }    }}
private void parquet-mr_f6116_1(final JobConf jobConf, final String splitPath, final String splitPathWithNoSchema)
{    if (mrwork == null) {                return;    } else if (mrwork.getPathToAliases() == null) {                return;    }    final ArrayList<String> aliases = new ArrayList<String>();    final Iterator<Entry<String, ArrayList<String>>> iterator = mrwork.getPathToAliases().entrySet().iterator();    while (iterator.hasNext()) {        final Entry<String, ArrayList<String>> entry = iterator.next();        final String key = new Path(entry.getKey()).toUri().getPath();        if (splitPath.equals(key) || splitPathWithNoSchema.equals(key)) {            final ArrayList<String> list = entry.getValue();            for (final String val : list) {                aliases.add(val);            }        }    }    for (final String alias : aliases) {        final Operator<? extends Serializable> op = mrwork.getAliasToWork().get(alias);        if (op != null && op instanceof TableScanOperator) {            final TableScanOperator tableScan = (TableScanOperator) op;                        final ArrayList<Integer> list = tableScan.getNeededColumnIDs();            if (list != null) {                ColumnProjectionUtils.appendReadColumnIDs(jobConf, list);            } else {                ColumnProjectionUtils.setFullyReadColumns(jobConf);            }            pushFilters(jobConf, tableScan);        }    }}
private void parquet-mr_f6117_1(final JobConf jobConf, final TableScanOperator tableScan)
{    final TableScanDesc scanDesc = tableScan.getConf();    if (scanDesc == null) {                return;    }        Utilities.setColumnNameList(jobConf, tableScan);        final ExprNodeDesc filterExpr = scanDesc.getFilterExpr();    if (filterExpr == null) {                return;    }    final String filterText = filterExpr.getExprString();    final String filterExprSerialized = Utilities.serializeExpression(filterExpr);    jobConf.set(TableScanDesc.FILTER_TEXT_CONF_STR, filterText);    jobConf.set(TableScanDesc.FILTER_EXPR_CONF_STR, filterExprSerialized);}
public JobConf parquet-mr_f6118_0(JobConf jobConf, Path path) throws IOException
{    init(jobConf);    final JobConf cloneJobConf = new JobConf(jobConf);    final PartitionDesc part = pathToPartitionInfo.get(path.toString());    if ((part != null) && (part.getTableDesc() != null)) {        Utilities.copyTableJobPropertiesToConf(part.getTableDesc(), cloneJobConf);    }    pushProjectionsAndFilters(cloneJobConf, path.toString(), path.toUri().toString());    return cloneJobConf;}
private void parquet-mr_f6119_0(final JobConf job)
{    final String plan = HiveConf.getVar(job, HiveConf.ConfVars.PLAN);    if (mapWork == null && plan != null && plan.length() > 0) {        mapWork = Utilities.getMapWork(job);        pathToPartitionInfo.clear();        for (final Map.Entry<String, PartitionDesc> entry : mapWork.getPathToPartitionInfo().entrySet()) {            pathToPartitionInfo.put(new Path(entry.getKey()).toUri().getPath().toString(), entry.getValue());        }    }}
private void parquet-mr_f6120_1(final JobConf jobConf, final String splitPath, final String splitPathWithNoSchema)
{    if (mapWork == null) {                return;    } else if (mapWork.getPathToAliases() == null) {                return;    }    final ArrayList<String> aliases = new ArrayList<String>();    final Iterator<Entry<String, ArrayList<String>>> iterator = mapWork.getPathToAliases().entrySet().iterator();    while (iterator.hasNext()) {        final Entry<String, ArrayList<String>> entry = iterator.next();        final String key = new Path(entry.getKey()).toUri().getPath();        if (splitPath.equals(key) || splitPathWithNoSchema.equals(key)) {            final ArrayList<String> list = entry.getValue();            for (final String val : list) {                aliases.add(val);            }        }    }    for (final String alias : aliases) {        final Operator<? extends Serializable> op = mapWork.getAliasToWork().get(alias);        if (op != null && op instanceof TableScanOperator) {            final TableScanOperator tableScan = (TableScanOperator) op;                        final ArrayList<Integer> list = tableScan.getNeededColumnIDs();            if (list != null) {                ColumnProjectionUtils.appendReadColumnIDs(jobConf, list);            } else {                ColumnProjectionUtils.setFullyReadColumns(jobConf);            }            pushFilters(jobConf, tableScan);        }    }}
private void parquet-mr_f6121_1(final JobConf jobConf, final TableScanOperator tableScan)
{    final TableScanDesc scanDesc = tableScan.getConf();    if (scanDesc == null) {                return;    }        Utilities.setColumnNameList(jobConf, tableScan);        final ExprNodeDesc filterExpr = scanDesc.getFilterExpr();    if (filterExpr == null) {                return;    }    final String filterText = filterExpr.getExprString();    final String filterExprSerialized = Utilities.serializeExpression(filterExpr);    jobConf.set(TableScanDesc.FILTER_TEXT_CONF_STR, filterText);    jobConf.set(TableScanDesc.FILTER_EXPR_CONF_STR, filterExprSerialized);}
public JobConf parquet-mr_f6122_0(JobConf jobConf, Path path) throws IOException
{    init(jobConf);    final JobConf cloneJobConf = new JobConf(jobConf);    final PartitionDesc part = pathToPartitionInfo.get(path.toString());    if ((part != null) && (part.getTableDesc() != null)) {        Utilities.copyTableJobPropertiesToConf(part.getTableDesc(), cloneJobConf);    }    pushProjectionsAndFilters(cloneJobConf, path.toString(), path.toUri().toString());    return cloneJobConf;}
public HiveBinding parquet-mr_f6123_0()
{    Class<? extends HiveBinding> bindingClazz = create(HiveBindingFactory.class.getClassLoader());    try {        return bindingClazz.newInstance();    } catch (Exception e) {        throw new HiveBindingInstantiationError("Unexpected error creating instance" + " of " + bindingClazz.getCanonicalName(), e);    }}
 Class<? extends HiveBinding> parquet-mr_f6124_1(ClassLoader classLoader)
{            Class hiveVersionInfo;    try {        hiveVersionInfo = Class.forName(HIVE_VERSION_CLASS_NAME, true, classLoader);    } catch (ClassNotFoundException e) {                return Hive010Binding.class;    }    return createInternal(hiveVersionInfo);}
 Class<? extends HiveBinding> parquet-mr_f6125_1(Class hiveVersionInfo)
{    String hiveVersion;    try {        Method getVersionMethod = hiveVersionInfo.getMethod(HIVE_VERSION_METHOD_NAME, (Class[]) null);        String rawVersion = (String) getVersionMethod.invoke(null, (Object[]) null);                hiveVersion = trimVersion(rawVersion);    } catch (Exception e) {        throw new UnexpectedHiveVersionProviderError("Unexpected error whilst " + "determining Hive version", e);    }    if (hiveVersion.equalsIgnoreCase(HIVE_VERSION_UNKNOWN)) {                return createBindingForUnknownVersion();    }    if (hiveVersion.startsWith(HIVE_VERSION_010)) {                return Hive010Binding.class;    } else if (hiveVersion.startsWith(HIVE_VERSION_011)) {                return Hive010Binding.class;    } else if (hiveVersion.startsWith(HIVE_VERSION_013)) {        throw new HiveBindingInstantiationError("Hive 0.13 contains native Parquet support " + "and the parquet-hive jars from the parquet project should not be included " + "in Hive's classpath.");    }            return Hive012Binding.class;}
private Class<? extends HiveBinding> parquet-mr_f6126_1()
{    try {        Class<?> utilitiesClass = Class.forName(HIVE_UTILITIES_CLASS_NAME);        for (Method method : utilitiesClass.getDeclaredMethods()) {            if (HIVE_012_INDICATOR_UTILITIES_GETMAPWORK.equals(method.getName())) {                                return Hive012Binding.class;            }        }                return Hive010Binding.class;    } catch (ClassNotFoundException e) {                return LATEST_BINDING;    }}
private static String parquet-mr_f6127_0(String s)
{    if (s == null) {        return HIVE_VERSION_NULL;    }    return s.trim();}
public void parquet-mr_f6128_0()
{    hiveBindingFactory = new HiveBindingFactory();}
public void parquet-mr_f6129_0()
{    Assert.assertEquals(Hive010Binding.class, hiveBindingFactory.create(new NoopClassLoader()));}
public void parquet-mr_f6130_0()
{    hiveBindingFactory.createInternal(NoHiveVersion.class);}
public void parquet-mr_f6131_0()
{    hiveBindingFactory.createInternal(BlankHiveVersion.class);    Assert.assertEquals(Hive012Binding.class, hiveBindingFactory.createInternal(BlankHiveVersion.class));}
public void parquet-mr_f6132_0()
{    hiveBindingFactory.createInternal(BlankHiveVersion.class);        Assert.assertEquals(Hive012Binding.class, hiveBindingFactory.createInternal(BlankHiveVersion.class));}
public void parquet-mr_f6133_0()
{    hiveBindingFactory.createInternal(NullHiveVersion.class);    Assert.assertEquals(Hive012Binding.class, hiveBindingFactory.createInternal(NullHiveVersion.class));}
public void parquet-mr_f6134_0()
{    Assert.assertEquals(Hive010Binding.class, hiveBindingFactory.createInternal(Hive010Version.class));}
public void parquet-mr_f6135_0()
{    Assert.assertEquals(Hive010Binding.class, hiveBindingFactory.createInternal(Hive010VersionWithSpaces.class));}
public void parquet-mr_f6136_0()
{    Assert.assertEquals(Hive010Binding.class, hiveBindingFactory.createInternal(Hive011Version.class));}
public void parquet-mr_f6137_0()
{    Assert.assertEquals(Hive012Binding.class, hiveBindingFactory.createInternal(Hive012Version.class));}
public void parquet-mr_f6138_0()
{    hiveBindingFactory.createInternal(Hive013Version.class);}
public Class<?> parquet-mr_f6139_0(String name) throws ClassNotFoundException
{    throw new ClassNotFoundException(name);}
public static String parquet-mr_f6140_0()
{    return "";}
public static String parquet-mr_f6141_0()
{    return HiveBindingFactory.HIVE_VERSION_UNKNOWN;}
public static String parquet-mr_f6142_0()
{    return null;}
public static String parquet-mr_f6143_0()
{    return HiveBindingFactory.HIVE_VERSION_010;}
public static String parquet-mr_f6144_0()
{    return " " + HiveBindingFactory.HIVE_VERSION_010 + " ";}
public static String parquet-mr_f6145_0()
{    return HiveBindingFactory.HIVE_VERSION_011;}
public static String parquet-mr_f6146_0()
{    return HiveBindingFactory.HIVE_VERSION_012;}
public static String parquet-mr_f6147_0()
{    return HiveBindingFactory.HIVE_VERSION_013;}
public List<String> parquet-mr_f6148_0(final String columns)
{    final List<String> result = (List<String>) StringUtils.getStringCollection(columns);    result.removeAll(virtualColumns);    return result;}
public Converter parquet-mr_f6149_0(final int fieldIndex)
{    return converters[fieldIndex];}
public void parquet-mr_f6150_0()
{    if (isMap) {        mapPairContainer = new Writable[2];    }}
public void parquet-mr_f6151_0()
{    if (isMap) {        currentValue = new ArrayWritable(Writable.class, mapPairContainer);    }    parent.add(index, currentValue);}
protected void parquet-mr_f6152_0(final int index, final Writable value)
{    if (index != 0 && mapPairContainer == null || index > 1) {        throw new ParquetDecodingException("Repeated group can only have one or two fields for maps." + " Not allowed to set for the index : " + index);    }    if (isMap) {        mapPairContainer[index] = value;    } else {        currentValue = value;    }}
protected void parquet-mr_f6153_0(final int index, final Writable value)
{    set(index, value);}
public final ArrayWritable parquet-mr_f6154_0()
{    final Writable[] writableArr;    if (this.rootMap != null) {                writableArr = this.rootMap;    } else {        writableArr = new Writable[currentArr.length];    }    for (int i = 0; i < currentArr.length; i++) {        final Object obj = currentArr[i];        if (obj instanceof List) {            final List<?> objList = (List<?>) obj;            final ArrayWritable arr = new ArrayWritable(Writable.class, objList.toArray(new Writable[objList.size()]));            writableArr[i] = arr;        } else {            writableArr[i] = (Writable) obj;        }    }    return new ArrayWritable(Writable.class, writableArr);}
protected final void parquet-mr_f6155_0(final int index, final Writable value)
{    currentArr[index] = value;}
public Converter parquet-mr_f6156_0(final int fieldIndex)
{    return converters[fieldIndex];}
public void parquet-mr_f6157_0()
{    for (int i = 0; i < currentArr.length; i++) {        currentArr[i] = null;    }}
public void parquet-mr_f6158_0()
{    if (parent != null) {        parent.set(index, getCurrentArray());    }}
protected void parquet-mr_f6159_0(final int index, final Writable value)
{    if (currentArr[index] != null) {        final Object obj = currentArr[index];        if (obj instanceof List) {            final List<Writable> list = (List<Writable>) obj;            list.add(value);        } else {            throw new IllegalStateException("This should be a List: " + obj);        }    } else {                                        final List<Writable> buffer = new ArrayList<Writable>();        buffer.add(value);        currentArr[index] = (Object) buffer;    }}
public ArrayWritable parquet-mr_f6160_0()
{    return root.getCurrentArray();}
public GroupConverter parquet-mr_f6161_0()
{    return root;}
private Class<?> parquet-mr_f6162_0()
{    return _type;}
public static Converter parquet-mr_f6163_0(final Class<?> type, final int index, final HiveGroupConverter parent)
{    for (final ETypeConverter eConverter : values()) {        if (eConverter.getType() == type) {            return eConverter.getConverter(type, index, parent);        }    }    throw new IllegalArgumentException("Converter not found ... for type : " + type);}
 Converter parquet-mr_f6164_0(final Class<?> type, final int index, final HiveGroupConverter parent)
{    return new PrimitiveConverter() {        @Override        public void addDouble(final double value) {            parent.set(index, new DoubleWritable(value));        }    };}
public void parquet-mr_f6165_0(final double value)
{    parent.set(index, new DoubleWritable(value));}
 Converter parquet-mr_f6166_0(final Class<?> type, final int index, final HiveGroupConverter parent)
{    return new PrimitiveConverter() {        @Override        public void addBoolean(final boolean value) {            parent.set(index, new BooleanWritable(value));        }    };}
public void parquet-mr_f6167_0(final boolean value)
{    parent.set(index, new BooleanWritable(value));}
 Converter parquet-mr_f6168_0(final Class<?> type, final int index, final HiveGroupConverter parent)
{    return new PrimitiveConverter() {        @Override        public void addFloat(final float value) {            parent.set(index, new FloatWritable(value));        }    };}
public void parquet-mr_f6169_0(final float value)
{    parent.set(index, new FloatWritable(value));}
 Converter parquet-mr_f6170_0(final Class<?> type, final int index, final HiveGroupConverter parent)
{    return new PrimitiveConverter() {        @Override        public void addInt(final int value) {            parent.set(index, new IntWritable(value));        }    };}
public void parquet-mr_f6171_0(final int value)
{    parent.set(index, new IntWritable(value));}
 Converter parquet-mr_f6172_0(final Class<?> type, final int index, final HiveGroupConverter parent)
{    return new PrimitiveConverter() {        @Override        public void addLong(final long value) {            parent.set(index, new LongWritable(value));        }    };}
public void parquet-mr_f6173_0(final long value)
{    parent.set(index, new LongWritable(value));}
 Converter parquet-mr_f6174_0(final Class<?> type, final int index, final HiveGroupConverter parent)
{    return new PrimitiveConverter() {                @Override        public void addDouble(final double value) {            parent.set(index, new DoubleWritable(value));        }    };}
public void parquet-mr_f6175_0(final double value)
{    parent.set(index, new DoubleWritable(value));}
 Converter parquet-mr_f6176_0(final Class<?> type, final int index, final HiveGroupConverter parent)
{    return new PrimitiveConverter() {        private Binary[] dictBinary;        private String[] dict;        @Override        public boolean hasDictionarySupport() {            return true;        }        @Override        public void setDictionary(Dictionary dictionary) {            dictBinary = new Binary[dictionary.getMaxId() + 1];            dict = new String[dictionary.getMaxId() + 1];            for (int i = 0; i <= dictionary.getMaxId(); i++) {                Binary binary = dictionary.decodeToBinary(i);                dictBinary[i] = binary;                dict[i] = binary.toStringUsingUTF8();            }        }        @Override        public void addValueFromDictionary(int dictionaryId) {            parent.set(index, new DicBinaryWritable(dictBinary[dictionaryId], dict[dictionaryId]));        }        @Override        public void addBinary(Binary value) {            parent.set(index, new BinaryWritable(value));        }    };}
public boolean parquet-mr_f6177_0()
{    return true;}
public void parquet-mr_f6178_0(Dictionary dictionary)
{    dictBinary = new Binary[dictionary.getMaxId() + 1];    dict = new String[dictionary.getMaxId() + 1];    for (int i = 0; i <= dictionary.getMaxId(); i++) {        Binary binary = dictionary.decodeToBinary(i);        dictBinary[i] = binary;        dict[i] = binary.toStringUsingUTF8();    }}
public void parquet-mr_f6179_0(int dictionaryId)
{    parent.set(index, new DicBinaryWritable(dictBinary[dictionaryId], dict[dictionaryId]));}
public void parquet-mr_f6180_0(Binary value)
{    parent.set(index, new BinaryWritable(value));}
protected static Converter parquet-mr_f6181_0(final Type type, final int index, final HiveGroupConverter parent)
{    if (type == null) {        return null;    }    if (type.isPrimitive()) {        return ETypeConverter.getNewConverter(type.asPrimitiveType().getPrimitiveTypeName().javaType, index, parent);    } else {        if (type.asGroupType().getRepetition() == Repetition.REPEATED) {            return new ArrayWritableGroupConverter(type.asGroupType(), parent, index);        } else {            return new DataWritableGroupConverter(type.asGroupType(), parent, index);        }    }}
public static MessageType parquet-mr_f6182_0(final List<String> columnNames, final List<TypeInfo> columnTypes)
{    final MessageType schema = new MessageType("hive_schema", convertTypes(columnNames, columnTypes));    return schema;}
private static Type[] parquet-mr_f6183_0(final List<String> columnNames, final List<TypeInfo> columnTypes)
{    if (columnNames.size() != columnTypes.size()) {        throw new IllegalStateException("Mismatched Hive columns and types. Hive columns names" + " found : " + columnNames + " . And Hive types found : " + columnTypes);    }    final Type[] types = new Type[columnNames.size()];    for (int i = 0; i < columnNames.size(); ++i) {        types[i] = convertType(columnNames.get(i), columnTypes.get(i));    }    return types;}
private static Type parquet-mr_f6184_0(final String name, final TypeInfo typeInfo)
{    return convertType(name, typeInfo, Repetition.OPTIONAL);}
private static Type parquet-mr_f6185_0(final String name, final TypeInfo typeInfo, final Repetition repetition)
{    if (typeInfo.getCategory().equals(Category.PRIMITIVE)) {        if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {            return new PrimitiveType(repetition, PrimitiveTypeName.BINARY, name);        } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo) || typeInfo.equals(TypeInfoFactory.shortTypeInfo) || typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {            return new PrimitiveType(repetition, PrimitiveTypeName.INT32, name);        } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {            return new PrimitiveType(repetition, PrimitiveTypeName.INT64, name);        } else if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {            return new PrimitiveType(repetition, PrimitiveTypeName.DOUBLE, name);        } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {            return new PrimitiveType(repetition, PrimitiveTypeName.FLOAT, name);        } else if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {            return new PrimitiveType(repetition, PrimitiveTypeName.BOOLEAN, name);        } else if (typeInfo.equals(TypeInfoFactory.binaryTypeInfo)) {                        throw new UnsupportedOperationException("Binary type not implemented");        } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {            throw new UnsupportedOperationException("Timestamp type not implemented");        } else if (typeInfo.equals(TypeInfoFactory.voidTypeInfo)) {            throw new UnsupportedOperationException("Void type not implemented");        } else if (typeInfo.equals(TypeInfoFactory.unknownTypeInfo)) {            throw new UnsupportedOperationException("Unknown type not implemented");        } else {            throw new IllegalArgumentException("Unknown type: " + typeInfo);        }    } else if (typeInfo.getCategory().equals(Category.LIST)) {        return convertArrayType(name, (ListTypeInfo) typeInfo);    } else if (typeInfo.getCategory().equals(Category.STRUCT)) {        return convertStructType(name, (StructTypeInfo) typeInfo);    } else if (typeInfo.getCategory().equals(Category.MAP)) {        return convertMapType(name, (MapTypeInfo) typeInfo);    } else if (typeInfo.getCategory().equals(Category.UNION)) {        throw new UnsupportedOperationException("Union type not implemented");    } else {        throw new IllegalArgumentException("Unknown type: " + typeInfo);    }}
private static GroupType parquet-mr_f6186_0(final String name, final ListTypeInfo typeInfo)
{    final TypeInfo subType = typeInfo.getListElementTypeInfo();    return listWrapper(name, listType(), new GroupType(Repetition.REPEATED, ParquetHiveSerDe.ARRAY.toString(), convertType("array_element", subType)));}
private static GroupType parquet-mr_f6187_0(final String name, final StructTypeInfo typeInfo)
{    final List<String> columnNames = typeInfo.getAllStructFieldNames();    final List<TypeInfo> columnTypes = typeInfo.getAllStructFieldTypeInfos();    return new GroupType(Repetition.OPTIONAL, name, convertTypes(columnNames, columnTypes));}
private static GroupType parquet-mr_f6188_0(final String name, final MapTypeInfo typeInfo)
{    final Type keyType = convertType(ParquetHiveSerDe.MAP_KEY.toString(), typeInfo.getMapKeyTypeInfo(), Repetition.REQUIRED);    final Type valueType = convertType(ParquetHiveSerDe.MAP_VALUE.toString(), typeInfo.getMapValueTypeInfo());    return ConversionPatterns.mapType(Repetition.OPTIONAL, name, keyType, valueType);}
private static GroupType parquet-mr_f6189_0(final String name, final LogicalTypeAnnotation logicalTypeAnnotation, final GroupType groupType)
{    return Types.optionalGroup().addField(groupType).as(logicalTypeAnnotation).named(name);}
public org.apache.hadoop.mapred.RecordReader<Void, ArrayWritable> parquet-mr_f6190_0(final org.apache.hadoop.mapred.InputSplit split, final org.apache.hadoop.mapred.JobConf job, final org.apache.hadoop.mapred.Reporter reporter) throws IOException
{    try {        return (RecordReader<Void, ArrayWritable>) new ParquetRecordReaderWrapper(realInput, split, job, reporter);    } catch (final InterruptedException e) {        throw new RuntimeException("Cannot create a RecordReaderWrapper", e);    }}
public void parquet-mr_f6191_0(final FileSystem ignored, final JobConf job) throws IOException
{    realOutputFormat.checkOutputSpecs(ShimLoader.getHadoopShims().getHCatShim().createJobContext(job, null));}
public RecordWriter<Void, ArrayWritable> parquet-mr_f6192_0(final FileSystem ignored, final JobConf job, final String name, final Progressable progress) throws IOException
{    throw new RuntimeException("Should never be used");}
public FileSinkOperator.RecordWriter parquet-mr_f6193_1(final JobConf jobConf, final Path finalOutPath, final Class<? extends Writable> valueClass, final boolean isCompressed, final Properties tableProperties, final Progressable progress) throws IOException
{        final String columnNameProperty = tableProperties.getProperty(IOConstants.COLUMNS);    final String columnTypeProperty = tableProperties.getProperty(IOConstants.COLUMNS_TYPES);    List<String> columnNames;    List<TypeInfo> columnTypes;    if (columnNameProperty.length() == 0) {        columnNames = new ArrayList<String>();    } else {        columnNames = Arrays.asList(columnNameProperty.split(","));    }    if (columnTypeProperty.length() == 0) {        columnTypes = new ArrayList<TypeInfo>();    } else {        columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);    }    DataWritableWriteSupport.setSchema(HiveSchemaConverter.convert(columnNames, columnTypes), jobConf);    return getParquerRecordWriterWrapper(realOutputFormat, jobConf, finalOutPath.toString(), progress);}
protected ParquetRecordWriterWrapper parquet-mr_f6194_0(ParquetOutputFormat<ArrayWritable> realOutputFormat, JobConf jobConf, String finalOutPath, Progressable progress) throws IOException
{    return new ParquetRecordWriterWrapper(realOutputFormat, jobConf, finalOutPath.toString(), progress);}
private static List<String> parquet-mr_f6195_0(final String columns)
{    return (new HiveBindingFactory()).create().getColumns(columns);}
public org.apache.parquet.hadoop.api.ReadSupport.ReadContext parquet-mr_f6196_0(final Configuration configuration, final Map<String, String> keyValueMetaData, final MessageType fileSchema)
{    final String columns = configuration.get(IOConstants.COLUMNS);    final Map<String, String> contextMetadata = new HashMap<String, String>();    if (columns != null) {        final List<String> listColumns = getColumns(columns);        final List<Type> typeListTable = new ArrayList<Type>();        for (final String col : listColumns) {                        if (fileSchema.containsField(col)) {                typeListTable.add(fileSchema.getType(col));            } else {                                typeListTable.add(new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, col));            }        }        MessageType tableSchema = new MessageType(TABLE_SCHEMA, typeListTable);        contextMetadata.put(HIVE_SCHEMA_KEY, tableSchema.toString());        MessageType requestedSchemaByUser = tableSchema;        final List<Integer> indexColumnsWanted = ColumnProjectionUtils.getReadColumnIDs(configuration);        final List<Type> typeListWanted = new ArrayList<Type>();        for (final Integer idx : indexColumnsWanted) {            typeListWanted.add(tableSchema.getType(listColumns.get(idx)));        }        requestedSchemaByUser = resolveSchemaAccess(new MessageType(fileSchema.getName(), typeListWanted), fileSchema, configuration);        return new ReadContext(requestedSchemaByUser, contextMetadata);    } else {        contextMetadata.put(HIVE_SCHEMA_KEY, fileSchema.toString());        return new ReadContext(fileSchema, contextMetadata);    }}
public RecordMaterializer<ArrayWritable> parquet-mr_f6197_0(final Configuration configuration, final Map<String, String> keyValueMetaData, final MessageType fileSchema, final org.apache.parquet.hadoop.api.ReadSupport.ReadContext readContext)
{    final Map<String, String> metadata = readContext.getReadSupportMetadata();    if (metadata == null) {        throw new IllegalStateException("ReadContext not initialized properly. " + "Don't know the Hive Schema.");    }    final MessageType tableSchema = resolveSchemaAccess(MessageTypeParser.parseMessageType(metadata.get(HIVE_SCHEMA_KEY)), fileSchema, configuration);    return new DataWritableRecordConverter(readContext.getRequestedSchema(), tableSchema);}
private MessageType parquet-mr_f6198_0(MessageType requestedSchema, MessageType fileSchema, Configuration configuration)
{    if (configuration.getBoolean(PARQUET_COLUMN_INDEX_ACCESS, false)) {        final List<String> listColumns = getColumns(configuration.get(IOConstants.COLUMNS));        List<Type> requestedTypes = new ArrayList<Type>();        for (Type t : requestedSchema.getFields()) {            int index = listColumns.indexOf(t.getName());            requestedTypes.add(fileSchema.getType(index));        }        requestedSchema = new MessageType(requestedSchema.getName(), requestedTypes);    }    return requestedSchema;}
public void parquet-mr_f6199_0() throws IOException
{    if (realReader != null) {        realReader.close();    }}
public Void parquet-mr_f6200_0()
{    return null;}
public ArrayWritable parquet-mr_f6201_0()
{    return valueObj;}
public long parquet-mr_f6202_0() throws IOException
{    return (long) (splitLen * getProgress());}
public float parquet-mr_f6203_0() throws IOException
{    if (realReader == null) {        return 1f;    } else {        try {            return realReader.getProgress();        } catch (final InterruptedException e) {            throw new IOException(e);        }    }}
public boolean parquet-mr_f6204_0(final Void key, final ArrayWritable value) throws IOException
{    if (eof) {        return false;    }    try {        if (firstRecord) {                        firstRecord = false;        } else if (!realReader.nextKeyValue()) {                        eof = true;            return false;        }        final ArrayWritable tmpCurValue = realReader.getCurrentValue();        if (value != tmpCurValue) {            final Writable[] arrValue = value.get();            final Writable[] arrCurrent = tmpCurValue.get();            if (value != null && arrValue.length == arrCurrent.length) {                System.arraycopy(arrCurrent, 0, arrValue, 0, arrCurrent.length);            } else {                if (arrValue.length != arrCurrent.length) {                    throw new IOException("DeprecatedParquetHiveInput : size of object differs. Value" + " size :  " + arrValue.length + ", Current Object size : " + arrCurrent.length);                } else {                    throw new IOException("DeprecatedParquetHiveInput can not support RecordReaders that" + " don't return same key & value & value is null");                }            }        }        return true;    } catch (final InterruptedException e) {        throw new IOException(e);    }}
protected ParquetInputSplit parquet-mr_f6205_0(final InputSplit oldSplit, final JobConf conf) throws IOException
{    if (oldSplit instanceof FileSplit) {        FileSplit fileSplit = (FileSplit) oldSplit;        final long splitStart = fileSplit.getStart();        final long splitLength = fileSplit.getLength();        final Path finalPath = fileSplit.getPath();        final JobConf cloneJob = hiveBinding.pushProjectionsAndFilters(conf, finalPath.getParent());        final ParquetMetadata parquetMetadata = ParquetFileReader.readFooter(cloneJob, finalPath, SKIP_ROW_GROUPS);        final FileMetaData fileMetaData = parquetMetadata.getFileMetaData();        final ReadContext readContext = new DataWritableReadSupport().init(cloneJob, fileMetaData.getKeyValueMetaData(), fileMetaData.getSchema());        schemaSize = MessageTypeParser.parseMessageType(readContext.getReadSupportMetadata().get(DataWritableReadSupport.HIVE_SCHEMA_KEY)).getFieldCount();        return new ParquetInputSplit(finalPath, splitStart, splitStart + splitLength, splitLength, fileSplit.getLocations(), null);    } else {        throw new IllegalArgumentException("Unknown split type: " + oldSplit);    }}
public String parquet-mr_f6206_0()
{    return "map<" + keyInspector.getTypeName() + "," + valueInspector.getTypeName() + ">";}
public Category parquet-mr_f6207_0()
{    return Category.MAP;}
public ObjectInspector parquet-mr_f6208_0()
{    return keyInspector;}
public ObjectInspector parquet-mr_f6209_0()
{    return valueInspector;}
public Map<?, ?> parquet-mr_f6210_0(final Object data)
{    if (data == null) {        return null;    }    if (data instanceof ArrayWritable) {        final Writable[] mapContainer = ((ArrayWritable) data).get();        if (mapContainer == null || mapContainer.length == 0) {            return null;        }        final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();        final Map<Writable, Writable> map = new HashMap<Writable, Writable>();        for (final Writable obj : mapArray) {            final ArrayWritable mapObj = (ArrayWritable) obj;            final Writable[] arr = mapObj.get();            map.put(arr[0], arr[1]);        }        return map;    }    if (data instanceof Map) {        return (Map) data;    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
public int parquet-mr_f6211_0(final Object data)
{    if (data == null) {        return -1;    }    if (data instanceof ArrayWritable) {        final Writable[] mapContainer = ((ArrayWritable) data).get();        if (mapContainer == null || mapContainer.length == 0) {            return -1;        } else {            return ((ArrayWritable) mapContainer[0]).get().length;        }    }    if (data instanceof Map) {        return ((Map) data).size();    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
public Object parquet-mr_f6212_0()
{    Map<Object, Object> m = new HashMap<Object, Object>();    return m;}
public Object parquet-mr_f6213_0(Object map, Object key, Object value)
{    Map<Object, Object> m = (HashMap<Object, Object>) map;    m.put(key, value);    return m;}
public Object parquet-mr_f6214_0(Object map, Object key)
{    Map<Object, Object> m = (HashMap<Object, Object>) map;    m.remove(key);    return m;}
public Object parquet-mr_f6215_0(Object map)
{    Map<Object, Object> m = (HashMap<Object, Object>) map;    m.clear();    return m;}
public int parquet-mr_f6216_0()
{    final int prime = 31;    int result = 1;    result = prime * result + ((keyInspector == null) ? 0 : keyInspector.hashCode());    result = prime * result + ((valueInspector == null) ? 0 : valueInspector.hashCode());    return result;}
public boolean parquet-mr_f6217_0(Object obj)
{    if (this == obj) {        return true;    }    if (obj == null) {        return false;    }    if (getClass() != obj.getClass()) {        return false;    }    final AbstractParquetMapInspector other = (AbstractParquetMapInspector) obj;    if (keyInspector == null) {        if (other.keyInspector != null) {            return false;        }    } else if (!keyInspector.equals(other.keyInspector)) {        return false;    }    if (valueInspector == null) {        if (other.valueInspector != null) {            return false;        }    } else if (!valueInspector.equals(other.valueInspector)) {        return false;    }    return true;}
private ObjectInspector parquet-mr_f6218_0(final TypeInfo typeInfo)
{    if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {        return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;    } else if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {        return PrimitiveObjectInspectorFactory.writableBooleanObjectInspector;    } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {        return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;    } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo)) {        return PrimitiveObjectInspectorFactory.writableIntObjectInspector;    } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {        return PrimitiveObjectInspectorFactory.writableLongObjectInspector;    } else if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {        return ParquetPrimitiveInspectorFactory.parquetStringInspector;    } else if (typeInfo.getCategory().equals(Category.STRUCT)) {        return new ArrayWritableObjectInspector((StructTypeInfo) typeInfo);    } else if (typeInfo.getCategory().equals(Category.LIST)) {        final TypeInfo subTypeInfo = ((ListTypeInfo) typeInfo).getListElementTypeInfo();        return new ParquetHiveArrayInspector(getObjectInspector(subTypeInfo));    } else if (typeInfo.getCategory().equals(Category.MAP)) {        final TypeInfo keyTypeInfo = ((MapTypeInfo) typeInfo).getMapKeyTypeInfo();        final TypeInfo valueTypeInfo = ((MapTypeInfo) typeInfo).getMapValueTypeInfo();        if (keyTypeInfo.equals(TypeInfoFactory.stringTypeInfo) || keyTypeInfo.equals(TypeInfoFactory.byteTypeInfo) || keyTypeInfo.equals(TypeInfoFactory.shortTypeInfo)) {            return new DeepParquetHiveMapInspector(getObjectInspector(keyTypeInfo), getObjectInspector(valueTypeInfo));        } else {            return new StandardParquetHiveMapInspector(getObjectInspector(keyTypeInfo), getObjectInspector(valueTypeInfo));        }    } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {        throw new UnsupportedOperationException("timestamp not implemented yet");    } else if (typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {        return ParquetPrimitiveInspectorFactory.parquetByteInspector;    } else if (typeInfo.equals(TypeInfoFactory.shortTypeInfo)) {        return ParquetPrimitiveInspectorFactory.parquetShortInspector;    } else {        throw new IllegalArgumentException("Unknown field info: " + typeInfo);    }}
public Category parquet-mr_f6219_0()
{    return Category.STRUCT;}
public String parquet-mr_f6220_0()
{    return typeInfo.getTypeName();}
public List<? extends StructField> parquet-mr_f6221_0()
{    return fields;}
public Object parquet-mr_f6222_0(final Object data, final StructField fieldRef)
{    if (data == null) {        return null;    }    if (data instanceof ArrayWritable) {        final ArrayWritable arr = (ArrayWritable) data;        return arr.get()[((StructFieldImpl) fieldRef).getIndex()];    }        if (data instanceof List) {        return ((List) data).get(((StructFieldImpl) fieldRef).getIndex());    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
public StructField parquet-mr_f6223_0(final String name)
{    return fieldsByName.get(name);}
public List<Object> parquet-mr_f6224_0(final Object data)
{    if (data == null) {        return null;    }    if (data instanceof ArrayWritable) {        final ArrayWritable arr = (ArrayWritable) data;        final Object[] arrWritable = arr.get();        return new ArrayList<Object>(Arrays.asList(arrWritable));    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
public Object parquet-mr_f6225_0()
{    final ArrayList<Object> list = new ArrayList<Object>(fields.size());    for (int i = 0; i < fields.size(); ++i) {        list.add(null);    }    return list;}
public Object parquet-mr_f6226_0(Object struct, StructField field, Object fieldValue)
{    final ArrayList<Object> list = (ArrayList<Object>) struct;    list.set(((StructFieldImpl) field).getIndex(), fieldValue);    return list;}
public boolean parquet-mr_f6227_0(Object obj)
{    if (obj == null) {        return false;    }    if (getClass() != obj.getClass()) {        return false;    }    final ArrayWritableObjectInspector other = (ArrayWritableObjectInspector) obj;    if (this.typeInfo != other.typeInfo && (this.typeInfo == null || !this.typeInfo.equals(other.typeInfo))) {        return false;    }    return true;}
public int parquet-mr_f6228_0()
{    int hash = 5;    hash = 29 * hash + (this.typeInfo != null ? this.typeInfo.hashCode() : 0);    return hash;}
public String parquet-mr_f6229_0()
{    return "";}
public String parquet-mr_f6230_0()
{    return name;}
public int parquet-mr_f6231_0()
{    return index;}
public ObjectInspector parquet-mr_f6232_0()
{    return inspector;}
public Object parquet-mr_f6233_0(final Object data, final Object key)
{    if (data == null || key == null) {        return null;    }    if (data instanceof ArrayWritable) {        final Writable[] mapContainer = ((ArrayWritable) data).get();        if (mapContainer == null || mapContainer.length == 0) {            return null;        }        final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();        for (final Writable obj : mapArray) {            final ArrayWritable mapObj = (ArrayWritable) obj;            final Writable[] arr = mapObj.get();            if (key.equals(arr[0]) || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveJavaObject(arr[0])) || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveWritableObject(arr[0]))) {                return arr[1];            }        }        return null;    }    if (data instanceof Map) {        final Map<?, ?> map = (Map<?, ?>) data;        if (map.containsKey(key)) {            return map.get(key);        }        for (final Map.Entry<?, ?> entry : map.entrySet()) {            if (key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveJavaObject(entry.getKey())) || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveWritableObject(entry.getKey()))) {                return entry.getValue();            }        }        return null;    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
public String parquet-mr_f6234_0()
{    return "array<" + arrayElementInspector.getTypeName() + ">";}
public Category parquet-mr_f6235_0()
{    return Category.LIST;}
public ObjectInspector parquet-mr_f6236_0()
{    return arrayElementInspector;}
public Object parquet-mr_f6237_0(final Object data, final int index)
{    if (data == null) {        return null;    }    if (data instanceof ArrayWritable) {        final Writable[] listContainer = ((ArrayWritable) data).get();        if (listContainer == null || listContainer.length == 0) {            return null;        }        final Writable subObj = listContainer[0];        if (subObj == null) {            return null;        }        if (index >= 0 && index < ((ArrayWritable) subObj).get().length) {            return ((ArrayWritable) subObj).get()[index];        } else {            return null;        }    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
public int parquet-mr_f6238_0(final Object data)
{    if (data == null) {        return -1;    }    if (data instanceof ArrayWritable) {        final Writable[] listContainer = ((ArrayWritable) data).get();        if (listContainer == null || listContainer.length == 0) {            return -1;        }        final Writable subObj = listContainer[0];        if (subObj == null) {            return 0;        }        return ((ArrayWritable) subObj).get().length;    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
public List<?> parquet-mr_f6239_0(final Object data)
{    if (data == null) {        return null;    }    if (data instanceof ArrayWritable) {        final Writable[] listContainer = ((ArrayWritable) data).get();        if (listContainer == null || listContainer.length == 0) {            return null;        }        final Writable subObj = listContainer[0];        if (subObj == null) {            return null;        }        final Writable[] array = ((ArrayWritable) subObj).get();        final List<Writable> list = new ArrayList<Writable>();        for (final Writable obj : array) {            list.add(obj);        }        return list;    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
public Object parquet-mr_f6240_0(final int size)
{    final ArrayList<Object> result = new ArrayList<Object>(size);    for (int i = 0; i < size; ++i) {        result.add(null);    }    return result;}
public Object parquet-mr_f6241_0(final Object list, final int index, final Object element)
{    final ArrayList l = (ArrayList) list;    l.set(index, element);    return list;}
public Object parquet-mr_f6242_0(final Object list, final int newSize)
{    final ArrayList l = (ArrayList) list;    l.ensureCapacity(newSize);    while (l.size() < newSize) {        l.add(null);    }    while (l.size() > newSize) {        l.remove(l.size() - 1);    }    return list;}
public boolean parquet-mr_f6243_0(final Object o)
{    if (o == null || o.getClass() != getClass()) {        return false;    } else if (o == this) {        return true;    } else {        final ObjectInspector other = ((ParquetHiveArrayInspector) o).arrayElementInspector;        return other.equals(arrayElementInspector);    }}
public int parquet-mr_f6244_0()
{    int hash = 3;    hash = 29 * hash + (this.arrayElementInspector != null ? this.arrayElementInspector.hashCode() : 0);    return hash;}
public final void parquet-mr_f6245_0(final Configuration conf, final Properties tbl) throws SerDeException
{    final TypeInfo rowTypeInfo;    final List<String> columnNames;    final List<TypeInfo> columnTypes;        final String columnNameProperty = tbl.getProperty(IOConstants.COLUMNS);    final String columnTypeProperty = tbl.getProperty(IOConstants.COLUMNS_TYPES);    if (columnNameProperty.length() == 0) {        columnNames = new ArrayList<String>();    } else {        columnNames = Arrays.asList(columnNameProperty.split(","));    }    if (columnTypeProperty.length() == 0) {        columnTypes = new ArrayList<TypeInfo>();    } else {        columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);    }    if (columnNames.size() != columnTypes.size()) {        throw new IllegalArgumentException("ParquetHiveSerde initialization failed. Number of column " + "name and column type differs. columnNames = " + columnNames + ", columnTypes = " + columnTypes);    }        rowTypeInfo = TypeInfoFactory.getStructTypeInfo(columnNames, columnTypes);    this.objInspector = new ArrayWritableObjectInspector((StructTypeInfo) rowTypeInfo);        stats = new SerDeStats();    serializedSize = 0;    deserializedSize = 0;    status = LAST_OPERATION.UNKNOWN;}
public Object parquet-mr_f6246_0(final Writable blob) throws SerDeException
{    status = LAST_OPERATION.DESERIALIZE;    deserializedSize = 0;    if (blob instanceof ArrayWritable) {        deserializedSize = ((ArrayWritable) blob).get().length;        return blob;    } else {        return null;    }}
public ObjectInspector parquet-mr_f6247_0() throws SerDeException
{    return objInspector;}
public Class<? extends Writable> parquet-mr_f6248_0()
{    return ArrayWritable.class;}
public Writable parquet-mr_f6249_0(final Object obj, final ObjectInspector objInspector) throws SerDeException
{    if (!objInspector.getCategory().equals(Category.STRUCT)) {        throw new SerDeException("Cannot serialize " + objInspector.getCategory() + ". Can only serialize a struct");    }    final ArrayWritable serializeData = createStruct(obj, (StructObjectInspector) objInspector);    serializedSize = serializeData.get().length;    status = LAST_OPERATION.SERIALIZE;    return serializeData;}
private ArrayWritable parquet-mr_f6250_0(final Object obj, final StructObjectInspector inspector) throws SerDeException
{    final List<? extends StructField> fields = inspector.getAllStructFieldRefs();    final Writable[] arr = new Writable[fields.size()];    for (int i = 0; i < fields.size(); i++) {        final StructField field = fields.get(i);        final Object subObj = inspector.getStructFieldData(obj, field);        final ObjectInspector subInspector = field.getFieldObjectInspector();        arr[i] = createObject(subObj, subInspector);    }    return new ArrayWritable(Writable.class, arr);}
private Writable parquet-mr_f6251_0(final Object obj, final MapObjectInspector inspector) throws SerDeException
{    final Map<?, ?> sourceMap = inspector.getMap(obj);    final ObjectInspector keyInspector = inspector.getMapKeyObjectInspector();    final ObjectInspector valueInspector = inspector.getMapValueObjectInspector();    final List<ArrayWritable> array = new ArrayList<ArrayWritable>();    if (sourceMap != null) {        for (final Entry<?, ?> keyValue : sourceMap.entrySet()) {            final Writable key = createObject(keyValue.getKey(), keyInspector);            final Writable value = createObject(keyValue.getValue(), valueInspector);            if (key != null) {                Writable[] arr = new Writable[2];                arr[0] = key;                arr[1] = value;                array.add(new ArrayWritable(Writable.class, arr));            }        }    }    if (array.size() > 0) {        final ArrayWritable subArray = new ArrayWritable(ArrayWritable.class, array.toArray(new ArrayWritable[array.size()]));        return new ArrayWritable(Writable.class, new Writable[] { subArray });    } else {        return null;    }}
private ArrayWritable parquet-mr_f6252_0(final Object obj, final ListObjectInspector inspector) throws SerDeException
{    final List<?> sourceArray = inspector.getList(obj);    final ObjectInspector subInspector = inspector.getListElementObjectInspector();    final List<Writable> array = new ArrayList<Writable>();    if (sourceArray != null) {        for (final Object curObj : sourceArray) {            final Writable newObj = createObject(curObj, subInspector);            if (newObj != null) {                array.add(newObj);            }        }    }    if (array.size() > 0) {        final ArrayWritable subArray = new ArrayWritable(array.get(0).getClass(), array.toArray(new Writable[array.size()]));        return new ArrayWritable(Writable.class, new Writable[] { subArray });    } else {        return null;    }}
private Writable parquet-mr_f6253_0(final Object obj, final PrimitiveObjectInspector inspector) throws SerDeException
{    if (obj == null) {        return null;    }    switch(inspector.getPrimitiveCategory()) {        case VOID:            return null;        case BOOLEAN:            return new BooleanWritable(((BooleanObjectInspector) inspector).get(obj) ? Boolean.TRUE : Boolean.FALSE);        case BYTE:            return new ByteWritable((byte) ((ByteObjectInspector) inspector).get(obj));        case DOUBLE:            return new DoubleWritable(((DoubleObjectInspector) inspector).get(obj));        case FLOAT:            return new FloatWritable(((FloatObjectInspector) inspector).get(obj));        case INT:            return new IntWritable(((IntObjectInspector) inspector).get(obj));        case LONG:            return new LongWritable(((LongObjectInspector) inspector).get(obj));        case SHORT:            return new ShortWritable((short) ((ShortObjectInspector) inspector).get(obj));        case STRING:            return new BinaryWritable(Binary.fromString(((StringObjectInspector) inspector).getPrimitiveJavaObject(obj)));        default:            throw new SerDeException("Unknown primitive : " + inspector.getPrimitiveCategory());    }}
private Writable parquet-mr_f6254_0(final Object obj, final ObjectInspector inspector) throws SerDeException
{    switch(inspector.getCategory()) {        case STRUCT:            return createStruct(obj, (StructObjectInspector) inspector);        case LIST:            return createArray(obj, (ListObjectInspector) inspector);        case MAP:            return createMap(obj, (MapObjectInspector) inspector);        case PRIMITIVE:            return createPrimitive(obj, (PrimitiveObjectInspector) inspector);        default:            throw new SerDeException("Unknown data type" + inspector.getCategory());    }}
public SerDeStats parquet-mr_f6255_0()
{        assert (status != LAST_OPERATION.UNKNOWN);    if (status == LAST_OPERATION.SERIALIZE) {        stats.setRawDataSize(serializedSize);    } else {        stats.setRawDataSize(deserializedSize);    }    return stats;}
public Object parquet-mr_f6256_0(final Object o)
{    return o == null ? null : new ByteWritable(get(o));}
public Object parquet-mr_f6257_0(final byte val)
{    return new ByteWritable(val);}
public Object parquet-mr_f6258_0(final Object o, final byte val)
{    ((ByteWritable) o).set(val);    return o;}
public byte parquet-mr_f6259_0(Object o)
{        if (o instanceof IntWritable) {        return (byte) ((IntWritable) o).get();    }    return ((ByteWritable) o).get();}
public Object parquet-mr_f6260_0(final Object o)
{    return o == null ? null : new ShortWritable(get(o));}
public Object parquet-mr_f6261_0(final short val)
{    return new ShortWritable(val);}
public Object parquet-mr_f6262_0(final Object o, final short val)
{    ((ShortWritable) o).set(val);    return o;}
public short parquet-mr_f6263_0(Object o)
{        if (o instanceof IntWritable) {        return (short) ((IntWritable) o).get();    }    return ((ShortWritable) o).get();}
public Object parquet-mr_f6264_0(final Object data, final Object key)
{    if (data == null || key == null) {        return null;    }    if (data instanceof ArrayWritable) {        final Writable[] mapContainer = ((ArrayWritable) data).get();        if (mapContainer == null || mapContainer.length == 0) {            return null;        }        final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();        for (final Writable obj : mapArray) {            final ArrayWritable mapObj = (ArrayWritable) obj;            final Writable[] arr = mapObj.get();            if (key.equals(arr[0])) {                return arr[1];            }        }        return null;    }    if (data instanceof Map) {        return ((Map) data).get(key);    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
public void parquet-mr_f6265_0(BigDecimal value)
{    value = value.stripTrailingZeros();    if (value.compareTo(BigDecimal.ZERO) == 0) {                        value = BigDecimal.ZERO;    }    set(value.unscaledValue().toByteArray(), value.scale());}
public void parquet-mr_f6266_0(final BigDecimalWritable writable)
{    set(writable.getBigDecimal());}
public void parquet-mr_f6267_0(final byte[] bytes, final int scale)
{    this.internalStorage = bytes;    this.scale = scale;}
public void parquet-mr_f6268_0(final byte[] bytes, int offset, final int length)
{    LazyBinaryUtils.readVInt(bytes, offset, vInt);    scale = vInt.value;    offset += vInt.length;    LazyBinaryUtils.readVInt(bytes, offset, vInt);    offset += vInt.length;    if (internalStorage.length != vInt.value) {        internalStorage = new byte[vInt.value];    }    System.arraycopy(bytes, offset, internalStorage, 0, vInt.value);}
public BigDecimal parquet-mr_f6269_0()
{    return new BigDecimal(new BigInteger(internalStorage), scale);}
public void parquet-mr_f6270_0(final DataInput in) throws IOException
{    scale = WritableUtils.readVInt(in);    final int byteArrayLen = WritableUtils.readVInt(in);    if (internalStorage.length != byteArrayLen) {        internalStorage = new byte[byteArrayLen];    }    in.readFully(internalStorage);}
public void parquet-mr_f6271_0(final DataOutput out) throws IOException
{    WritableUtils.writeVInt(out, scale);    WritableUtils.writeVInt(out, internalStorage.length);    out.write(internalStorage);}
public int parquet-mr_f6272_0(final BigDecimalWritable that)
{    return getBigDecimal().compareTo(that.getBigDecimal());}
public void parquet-mr_f6273_0(final Output byteStream)
{    LazyBinaryUtils.writeVInt(byteStream, scale);    LazyBinaryUtils.writeVInt(byteStream, internalStorage.length);    byteStream.write(internalStorage, 0, internalStorage.length);}
public String parquet-mr_f6274_0()
{    return getBigDecimal().toString();}
public boolean parquet-mr_f6275_0(final Object other)
{    if (other == null || !(other instanceof BigDecimalWritable)) {        return false;    }    final BigDecimalWritable bdw = (BigDecimalWritable) other;        return getBigDecimal().compareTo(bdw.getBigDecimal()) == 0;}
public int parquet-mr_f6276_0()
{    return getBigDecimal().hashCode();}
public Binary parquet-mr_f6277_0()
{    return binary;}
public byte[] parquet-mr_f6278_0()
{    return binary.getBytes();}
public String parquet-mr_f6279_0()
{    return binary.toStringUsingUTF8();}
public void parquet-mr_f6280_0(DataInput input) throws IOException
{    byte[] bytes = new byte[input.readInt()];    input.readFully(bytes);    binary = Binary.fromConstantByteArray(bytes);}
public void parquet-mr_f6281_0(DataOutput output) throws IOException
{    output.writeInt(binary.length());    binary.writeTo(output);}
public int parquet-mr_f6282_0()
{    return binary == null ? 0 : binary.hashCode();}
public boolean parquet-mr_f6283_0(Object obj)
{    if (obj instanceof BinaryWritable) {        final BinaryWritable other = (BinaryWritable) obj;        return binary.equals(other.binary);    }    return false;}
public String parquet-mr_f6284_0()
{    return string;}
public void parquet-mr_f6285_0(final ArrayWritable arr)
{    if (arr == null) {        return;    }    recordConsumer.startMessage();    writeData(arr, schema);    recordConsumer.endMessage();}
private void parquet-mr_f6286_0(final ArrayWritable arr, final GroupType type)
{    if (arr == null) {        return;    }    final int fieldCount = type.getFieldCount();    Writable[] values = arr.get();    for (int field = 0; field < fieldCount; ++field) {        final Type fieldType = type.getType(field);        final String fieldName = fieldType.getName();        final Writable value = values[field];        if (value == null) {            continue;        }        recordConsumer.startField(fieldName, field);        if (fieldType.isPrimitive()) {            writePrimitive(value);        } else {            recordConsumer.startGroup();            if (value instanceof ArrayWritable) {                if (fieldType.asGroupType().getRepetition().equals(Type.Repetition.REPEATED)) {                    writeArray((ArrayWritable) value, fieldType.asGroupType());                } else {                    writeData((ArrayWritable) value, fieldType.asGroupType());                }            } else if (value != null) {                throw new ParquetEncodingException("This should be an ArrayWritable or MapWritable: " + value);            }            recordConsumer.endGroup();        }        recordConsumer.endField(fieldName, field);    }}
private void parquet-mr_f6287_0(final ArrayWritable array, final GroupType type)
{    if (array == null) {        return;    }    final Writable[] subValues = array.get();    final int fieldCount = type.getFieldCount();    for (int field = 0; field < fieldCount; ++field) {        final Type subType = type.getType(field);        recordConsumer.startField(subType.getName(), field);        for (int i = 0; i < subValues.length; ++i) {            final Writable subValue = subValues[i];            if (subValue != null) {                if (subType.isPrimitive()) {                    if (subValue instanceof ArrayWritable) {                                                writePrimitive(((ArrayWritable) subValue).get()[field]);                    } else {                        writePrimitive(subValue);                    }                } else {                    if (!(subValue instanceof ArrayWritable)) {                        throw new RuntimeException("This should be a ArrayWritable: " + subValue);                    } else {                        recordConsumer.startGroup();                        writeData((ArrayWritable) subValue, subType.asGroupType());                        recordConsumer.endGroup();                    }                }            }        }        recordConsumer.endField(subType.getName(), field);    }}
private void parquet-mr_f6288_0(final Writable value)
{    if (value == null) {        return;    }    if (value instanceof DoubleWritable) {        recordConsumer.addDouble(((DoubleWritable) value).get());    } else if (value instanceof BooleanWritable) {        recordConsumer.addBoolean(((BooleanWritable) value).get());    } else if (value instanceof FloatWritable) {        recordConsumer.addFloat(((FloatWritable) value).get());    } else if (value instanceof IntWritable) {        recordConsumer.addInteger(((IntWritable) value).get());    } else if (value instanceof LongWritable) {        recordConsumer.addLong(((LongWritable) value).get());    } else if (value instanceof ShortWritable) {        recordConsumer.addInteger(((ShortWritable) value).get());    } else if (value instanceof ByteWritable) {        recordConsumer.addInteger(((ByteWritable) value).get());    } else if (value instanceof BigDecimalWritable) {        throw new UnsupportedOperationException("BigDecimal writing not implemented");    } else if (value instanceof BinaryWritable) {        recordConsumer.addBinary(((BinaryWritable) value).getBinary());    } else {        throw new IllegalArgumentException("Unknown value type: " + value + " " + value.getClass());    }}
public static void parquet-mr_f6289_0(final MessageType schema, final Configuration configuration)
{    configuration.set(PARQUET_HIVE_SCHEMA, schema.toString());}
public static MessageType parquet-mr_f6290_0(final Configuration configuration)
{    return MessageTypeParser.parseMessageType(configuration.get(PARQUET_HIVE_SCHEMA));}
public WriteContext parquet-mr_f6291_0(final Configuration configuration)
{    schema = getSchema(configuration);    return new WriteContext(schema, new HashMap<String, String>());}
public void parquet-mr_f6292_0(final RecordConsumer recordConsumer)
{    writer = new DataWritableWriter(recordConsumer, schema);}
public void parquet-mr_f6293_0(final ArrayWritable record)
{    writer.write(record);}
public void parquet-mr_f6294_0(final Reporter reporter) throws IOException
{    try {        realWriter.close(taskContext);    } catch (final InterruptedException e) {        throw new IOException(e);    }}
public void parquet-mr_f6295_0(final Void key, final ArrayWritable value) throws IOException
{    try {        realWriter.write(key, value);    } catch (final InterruptedException e) {        throw new IOException(e);    }}
public void parquet-mr_f6296_0(final boolean abort) throws IOException
{    close(null);}
public void parquet-mr_f6297_0(final Writable w) throws IOException
{    write(null, (ArrayWritable) w);}
public Text parquet-mr_f6298_0(final Object o)
{    if (o == null) {        return null;    }    if (o instanceof BinaryWritable) {        return new Text(((BinaryWritable) o).getBytes());    }    if (o instanceof Text) {        return (Text) o;    }    if (o instanceof String) {        return new Text((String) o);    }    throw new UnsupportedOperationException("Cannot inspect " + o.getClass().getCanonicalName());}
public String parquet-mr_f6299_0(final Object o)
{    if (o == null) {        return null;    }    if (o instanceof BinaryWritable) {        return ((BinaryWritable) o).getString();    }    if (o instanceof Text) {        return ((Text) o).toString();    }    if (o instanceof String) {        return (String) o;    }    throw new UnsupportedOperationException("Cannot inspect " + o.getClass().getCanonicalName());}
public Object parquet-mr_f6300_0(final Object o, final Text text)
{    return new BinaryWritable(text == null ? null : Binary.fromReusedByteArray(text.getBytes()));}
public Object parquet-mr_f6301_0(final Object o, final String string)
{    return new BinaryWritable(string == null ? null : Binary.fromString(string));}
public Object parquet-mr_f6302_0(final Text text)
{    if (text == null) {        return null;    }    return text.toString();}
public Object parquet-mr_f6303_0(final String string)
{    return string;}
public Object parquet-mr_f6304_0(Object o, Object o1)
{    throw new UnsupportedOperationException("Should not be called");}
public void parquet-mr_f6305_0()
{    inspector = new TestableAbstractParquetMapInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector, PrimitiveObjectInspectorFactory.javaIntObjectInspector);}
public void parquet-mr_f6306_0()
{    assertEquals("Wrong size", -1, inspector.getMapSize(null));    assertNull("Should be null", inspector.getMap(null));}
public void parquet-mr_f6307_0()
{    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);    assertEquals("Wrong size", -1, inspector.getMapSize(map));    assertNull("Should be null", inspector.getMap(map));}
public void parquet-mr_f6308_0()
{    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);    assertEquals("Wrong size", -1, inspector.getMapSize(map));    assertNull("Should be null", inspector.getMap(map));}
public void parquet-mr_f6309_0()
{    final Writable[] entry1 = new Writable[] { new IntWritable(0), new IntWritable(1) };    final Writable[] entry2 = new Writable[] { new IntWritable(2), new IntWritable(3) };    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[] { new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2) });    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[] { internalMap });    final Map<Writable, Writable> expected = new HashMap<Writable, Writable>();    expected.put(new IntWritable(0), new IntWritable(1));    expected.put(new IntWritable(2), new IntWritable(3));    assertEquals("Wrong size", 2, inspector.getMapSize(map));    assertEquals("Wrong result of inspection", expected, inspector.getMap(map));}
public void parquet-mr_f6310_0()
{    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();    map.put(new IntWritable(0), new IntWritable(1));    map.put(new IntWritable(2), new IntWritable(3));    map.put(new IntWritable(4), new IntWritable(5));    map.put(new IntWritable(6), new IntWritable(7));    assertEquals("Wrong size", 4, inspector.getMapSize(map));    assertEquals("Wrong result of inspection", map, inspector.getMap(map));}
public void parquet-mr_f6311_0()
{    inspector = new DeepParquetHiveMapInspector(ParquetPrimitiveInspectorFactory.parquetShortInspector, PrimitiveObjectInspectorFactory.javaIntObjectInspector);}
public void parquet-mr_f6312_0()
{    assertNull("Should be null", inspector.getMapValueElement(null, new ShortWritable((short) 0)));}
public void parquet-mr_f6313_0()
{    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);    assertNull("Should be null", inspector.getMapValueElement(map, new ShortWritable((short) 0)));}
public void parquet-mr_f6314_0()
{    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);    assertNull("Should be null", inspector.getMapValueElement(map, new ShortWritable((short) 0)));}
public void parquet-mr_f6315_0()
{    final Writable[] entry1 = new Writable[] { new IntWritable(0), new IntWritable(1) };    final Writable[] entry2 = new Writable[] { new IntWritable(2), new IntWritable(3) };    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[] { new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2) });    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[] { internalMap });    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new ShortWritable((short) 0)));    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new ShortWritable((short) 2)));}
public void parquet-mr_f6316_0()
{    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();    map.put(new IntWritable(0), new IntWritable(1));    map.put(new IntWritable(2), new IntWritable(3));    map.put(new IntWritable(4), new IntWritable(5));    map.put(new IntWritable(6), new IntWritable(7));    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new IntWritable(4)));    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new IntWritable(6)));    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new ShortWritable((short) 0)));    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new ShortWritable((short) 2)));    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new ShortWritable((short) 4)));    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new ShortWritable((short) 6)));}
public void parquet-mr_f6317_0()
{    inspector = new ParquetHiveArrayInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector);}
public void parquet-mr_f6318_0()
{    assertEquals("Wrong size", -1, inspector.getListLength(null));    assertNull("Should be null", inspector.getList(null));    assertNull("Should be null", inspector.getListElement(null, 0));}
public void parquet-mr_f6319_0()
{    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, null);    assertEquals("Wrong size", -1, inspector.getListLength(list));    assertNull("Should be null", inspector.getList(list));    assertNull("Should be null", inspector.getListElement(list, 0));}
public void parquet-mr_f6320_0()
{    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);    assertEquals("Wrong size", -1, inspector.getListLength(list));    assertNull("Should be null", inspector.getList(list));    assertNull("Should be null", inspector.getListElement(list, 0));}
public void parquet-mr_f6321_0()
{    final ArrayWritable internalList = new ArrayWritable(Writable.class, new Writable[] { new IntWritable(3), new IntWritable(5), new IntWritable(1) });    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, new ArrayWritable[] { internalList });    final List<Writable> expected = new ArrayList<Writable>();    expected.add(new IntWritable(3));    expected.add(new IntWritable(5));    expected.add(new IntWritable(1));    assertEquals("Wrong size", 3, inspector.getListLength(list));    assertEquals("Wrong result of inspection", expected, inspector.getList(list));    for (int i = 0; i < expected.size(); ++i) {        assertEquals("Wrong result of inspection", expected.get(i), inspector.getListElement(list, i));    }    assertNull("Should be null", inspector.getListElement(list, 3));}
public void parquet-mr_f6322_0()
{    inspector = new StandardParquetHiveMapInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector, PrimitiveObjectInspectorFactory.javaIntObjectInspector);}
public void parquet-mr_f6323_0()
{    assertNull("Should be null", inspector.getMapValueElement(null, new IntWritable(0)));}
public void parquet-mr_f6324_0()
{    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);    assertNull("Should be null", inspector.getMapValueElement(map, new IntWritable(0)));}
public void parquet-mr_f6325_0()
{    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);    assertNull("Should be null", inspector.getMapValueElement(map, new IntWritable(0)));}
public void parquet-mr_f6326_0()
{    final Writable[] entry1 = new Writable[] { new IntWritable(0), new IntWritable(1) };    final Writable[] entry2 = new Writable[] { new IntWritable(2), new IntWritable(3) };    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[] { new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2) });    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[] { internalMap });    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 0)));    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 2)));}
public void parquet-mr_f6327_0()
{    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();    map.put(new IntWritable(0), new IntWritable(1));    map.put(new IntWritable(2), new IntWritable(3));    map.put(new IntWritable(4), new IntWritable(5));    map.put(new IntWritable(6), new IntWritable(7));    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new IntWritable(4)));    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new IntWritable(6)));    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 0)));    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 2)));    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 4)));    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 6)));}
private List<String> parquet-mr_f6328_0(final String columnNamesStr)
{    List<String> columnNames;    if (columnNamesStr.length() == 0) {        columnNames = new ArrayList<String>();    } else {        columnNames = Arrays.asList(columnNamesStr.split(","));    }    return columnNames;}
private List<TypeInfo> parquet-mr_f6329_0(final String columnsTypeStr)
{    List<TypeInfo> columnTypes;    if (columnsTypeStr.length() == 0) {        columnTypes = new ArrayList<TypeInfo>();    } else {        columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnsTypeStr);    }    return columnTypes;}
private void parquet-mr_f6330_0(final String columnNamesStr, final String columnsTypeStr, final String expectedSchema) throws Exception
{    final List<String> columnNames = createHiveColumnsFrom(columnNamesStr);    final List<TypeInfo> columnTypes = createHiveTypeInfoFrom(columnsTypeStr);    final MessageType messageTypeFound = HiveSchemaConverter.convert(columnNames, columnTypes);    final MessageType expectedMT = MessageTypeParser.parseMessageType(expectedSchema);    assertEquals("converting " + columnNamesStr + ": " + columnsTypeStr + " to " + expectedSchema, expectedMT, messageTypeFound);}
public void parquet-mr_f6331_0() throws Exception
{    testConversion("a,b,c", "int,double,boolean", "message hive_schema {\n" + "  optional int32 a;\n" + "  optional double b;\n" + "  optional boolean c;\n" + "}\n");}
public void parquet-mr_f6332_0() throws Exception
{    testConversion("arrayCol", "array<int>", "message hive_schema {\n" + "  optional group arrayCol (LIST) {\n" + "    repeated group bag {\n" + "      optional int32 array_element;\n" + "    }\n" + "  }\n" + "}\n");}
public void parquet-mr_f6333_0() throws Exception
{    testConversion("structCol", "struct<a:int,b:double,c:boolean>", "message hive_schema {\n" + "  optional group structCol {\n" + "    optional int32 a;\n" + "    optional double b;\n" + "    optional boolean c;\n" + "  }\n" + "}\n");}
public void parquet-mr_f6334_0() throws Exception
{    testConversion("mapCol", "map<string,string>", "message hive_schema {\n" + "  optional group mapCol (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key;\n" + "      optional binary value;\n" + "    }\n" + "  }\n" + "}\n");}
public void parquet-mr_f6335_0() throws Exception
{    final String hiveColumnTypes = "map<string,string>";    final String hiveColumnNames = "mapCol";    final List<String> columnNames = createHiveColumnsFrom(hiveColumnNames);    final List<TypeInfo> columnTypes = createHiveTypeInfoFrom(hiveColumnTypes);    final MessageType messageTypeFound = HiveSchemaConverter.convert(columnNames, columnTypes);        assertEquals(1, messageTypeFound.getFieldCount());    org.apache.parquet.schema.Type topLevel = messageTypeFound.getFields().get(0);    assertEquals("mapCol", topLevel.getName());    assertEquals(OriginalType.MAP, topLevel.getOriginalType());    assertEquals(Repetition.OPTIONAL, topLevel.getRepetition());    assertEquals(1, topLevel.asGroupType().getFieldCount());    org.apache.parquet.schema.Type secondLevel = topLevel.asGroupType().getFields().get(0);        assertEquals("map", secondLevel.getName());    assertEquals(OriginalType.MAP_KEY_VALUE, secondLevel.getOriginalType());    assertEquals(Repetition.REPEATED, secondLevel.getRepetition());}
public void parquet-mr_f6336_0()
{    new MapredParquetInputFormat();}
public void parquet-mr_f6337_0()
{    new MapredParquetInputFormat((ParquetInputFormat<ArrayWritable>) mock(ParquetInputFormat.class));}
public void parquet-mr_f6338_0()
{    new MapredParquetOutputFormat();}
public void parquet-mr_f6339_0()
{    new MapredParquetOutputFormat((ParquetOutputFormat<ArrayWritable>) mock(ParquetOutputFormat.class));}
public void parquet-mr_f6340_0()
{    try {        new MapredParquetOutputFormat().getRecordWriter(null, null, null, null);        fail("should throw runtime exception.");    } catch (Exception e) {        assertEquals("Should never be used", e.getMessage());    }}
public void parquet-mr_f6341_0() throws IOException
{    Properties tableProps = new Properties();    tableProps.setProperty("columns", "foo,bar");    tableProps.setProperty("columns.types", "int:int");    final Progressable mockProgress = mock(Progressable.class);    final ParquetOutputFormat<ArrayWritable> outputFormat = (ParquetOutputFormat<ArrayWritable>) mock(ParquetOutputFormat.class);    JobConf jobConf = new JobConf();    try {        new MapredParquetOutputFormat(outputFormat) {            @Override            protected ParquetRecordWriterWrapper getParquerRecordWriterWrapper(ParquetOutputFormat<ArrayWritable> realOutputFormat, JobConf jobConf, String finalOutPath, Progressable progress) throws IOException {                assertEquals(outputFormat, realOutputFormat);                assertNotNull(jobConf.get(DataWritableWriteSupport.PARQUET_HIVE_SCHEMA));                assertEquals("/foo", finalOutPath.toString());                assertEquals(mockProgress, progress);                throw new RuntimeException("passed tests");            }        }.getHiveRecordWriter(jobConf, new Path("/foo"), null, false, tableProps, mockProgress);        fail("should throw runtime exception.");    } catch (RuntimeException e) {        assertEquals("passed tests", e.getMessage());    }}
protected ParquetRecordWriterWrapper parquet-mr_f6342_0(ParquetOutputFormat<ArrayWritable> realOutputFormat, JobConf jobConf, String finalOutPath, Progressable progress) throws IOException
{    assertEquals(outputFormat, realOutputFormat);    assertNotNull(jobConf.get(DataWritableWriteSupport.PARQUET_HIVE_SCHEMA));    assertEquals("/foo", finalOutPath.toString());    assertEquals(mockProgress, progress);    throw new RuntimeException("passed tests");}
public void parquet-mr_f6343_0() throws Throwable
{    try {                System.out.println("test: testParquetHiveSerDe");        final ParquetHiveSerDe serDe = new ParquetHiveSerDe();        final Configuration conf = new Configuration();        final Properties tbl = createProperties();        serDe.initialize(conf, tbl);                final Writable[] arr = new Writable[8];        arr[0] = new ByteWritable((byte) 123);        arr[1] = new ShortWritable((short) 456);        arr[2] = new IntWritable(789);        arr[3] = new LongWritable(1000l);        arr[4] = new DoubleWritable((double) 5.3);        arr[5] = new BinaryWritable(Binary.fromString("hive and hadoop and parquet. Big family."));        final Writable[] mapContainer = new Writable[1];        final Writable[] map = new Writable[3];        for (int i = 0; i < 3; ++i) {            final Writable[] pair = new Writable[2];            pair[0] = new BinaryWritable(Binary.fromString("key_" + i));            pair[1] = new IntWritable(i);            map[i] = new ArrayWritable(Writable.class, pair);        }        mapContainer[0] = new ArrayWritable(Writable.class, map);        arr[6] = new ArrayWritable(Writable.class, mapContainer);        final Writable[] arrayContainer = new Writable[1];        final Writable[] array = new Writable[5];        for (int i = 0; i < 5; ++i) {            array[i] = new BinaryWritable(Binary.fromString("elem_" + i));        }        arrayContainer[0] = new ArrayWritable(Writable.class, array);        arr[7] = new ArrayWritable(Writable.class, arrayContainer);        final ArrayWritable arrWritable = new ArrayWritable(Writable.class, arr);                deserializeAndSerializeLazySimple(serDe, arrWritable);        System.out.println("test: testParquetHiveSerDe - OK");    } catch (final Throwable e) {        e.printStackTrace();        throw e;    }}
private void parquet-mr_f6344_0(final ParquetHiveSerDe serDe, final ArrayWritable t) throws SerDeException
{        final StructObjectInspector oi = (StructObjectInspector) serDe.getObjectInspector();        final Object row = serDe.deserialize(t);    assertEquals("deserialization gives the wrong object class", row.getClass(), ArrayWritable.class);    assertEquals("size correct after deserialization", serDe.getSerDeStats().getRawDataSize(), t.get().length);    assertEquals("deserialization gives the wrong object", t, row);        final ArrayWritable serializedArr = (ArrayWritable) serDe.serialize(row, oi);    assertEquals("size correct after serialization", serDe.getSerDeStats().getRawDataSize(), serializedArr.get().length);    assertTrue("serialized object should be equal to starting object", arrayWritableEquals(t, serializedArr));}
private Properties parquet-mr_f6345_0()
{    final Properties tbl = new Properties();        tbl.setProperty("columns", "abyte,ashort,aint,along,adouble,astring,amap,alist");    tbl.setProperty("columns.types", "tinyint:smallint:int:bigint:double:string:map<string,int>:array<string>");    tbl.setProperty(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_NULL_FORMAT, "NULL");    return tbl;}
public static boolean parquet-mr_f6346_0(final ArrayWritable a1, final ArrayWritable a2)
{    final Writable[] a1Arr = a1.get();    final Writable[] a2Arr = a2.get();    if (a1Arr.length != a2Arr.length) {        return false;    }    for (int i = 0; i < a1Arr.length; ++i) {        if (a1Arr[i] instanceof ArrayWritable) {            if (!(a2Arr[i] instanceof ArrayWritable)) {                return false;            }            if (!arrayWritableEquals((ArrayWritable) a1Arr[i], (ArrayWritable) a2Arr[i])) {                return false;            }        } else {            if (!a1Arr[i].equals(a2Arr[i])) {                return false;            }        }    }    return true;}
public static BigDecimal parquet-mr_f6347_0(Binary value, int precision, int scale)
{    /*     * Precision <= 18 checks for the max number of digits for an unscaled long,     * else treat with big integer conversion     */    if (precision <= 18) {        ByteBuffer buffer = value.toByteBuffer();        byte[] bytes = buffer.array();        int start = buffer.arrayOffset() + buffer.position();        int end = buffer.arrayOffset() + buffer.limit();        long unscaled = 0L;        int i = start;        while (i < end) {            unscaled = (unscaled << 8 | bytes[i] & 0xff);            i++;        }        int bits = 8 * (end - start);        long unscaledNew = (unscaled << (64 - bits)) >> (64 - bits);        if (unscaledNew <= -pow(10, 18) || unscaledNew >= pow(10, 18)) {            return new BigDecimal(unscaledNew);        } else {            return BigDecimal.valueOf(unscaledNew / pow(10, scale));        }    } else {        return new BigDecimal(new BigInteger(value.getBytes()), scale);    }}
public Converter parquet-mr_f6348_0(int fieldIndex)
{    if (fieldIndex != 0) {        throw new IllegalArgumentException("maps have only one field. can't reach " + fieldIndex);    }    return keyValue;}
public final void parquet-mr_f6349_0()
{    buffer.clear();}
public void parquet-mr_f6350_0()
{    parent.add(new LinkedHashMap<String, Object>(buffer));}
public Iterator<java.util.Map.Entry<String, Object>> parquet-mr_f6351_0()
{    return entries.iterator();}
public int parquet-mr_f6352_0()
{    return entries.size();}
public Tuple parquet-mr_f6353_0(String key, Object value)
{    entries.add(new SimpleImmutableEntry<String, Object>(key, value));    return null;}
public void parquet-mr_f6354_0()
{    entries.clear();}
public Set<java.util.Map.Entry<String, Object>> parquet-mr_f6355_0()
{    return entrySet;}
 void parquet-mr_f6356_0(Object value)
{    currentKey = value;}
 void parquet-mr_f6357_0(Object value)
{    currentValue = value;}
public Converter parquet-mr_f6358_0(int fieldIndex)
{    if (fieldIndex == 0) {        return keyConverter;    } else if (fieldIndex == 1) {        return valueConverter;    }    throw new IllegalArgumentException("only the key (0) and value (1) fields expected: " + fieldIndex);}
public final void parquet-mr_f6359_0()
{    currentKey = null;    currentValue = null;}
public void parquet-mr_f6360_0()
{    buffer.put(currentKey.toString(), currentValue);    currentKey = null;    currentValue = null;}
public final void parquet-mr_f6361_0(Binary value)
{    currentKey = value.toStringUsingUTF8();}
 void parquet-mr_f6362_0(Object value)
{    TupleConverter.this.set(index, value);}
private Type parquet-mr_f6363_0(boolean columnIndexAccess, String alias, int index)
{    if (columnIndexAccess) {        if (index < parquetSchema.getFieldCount()) {            return parquetSchema.getType(index);        }    } else {        return parquetSchema.getType(parquetSchema.getFieldIndex(alias));    }    return null;}
 static Converter parquet-mr_f6364_0(FieldSchema pigField, Type type, final ParentValueContainer parent, boolean elephantBirdCompatible, boolean columnIndexAccess)
{    try {        switch(pigField.type) {            case DataType.BAG:                return new BagConverter(type.asGroupType(), pigField, parent, elephantBirdCompatible, columnIndexAccess);            case DataType.MAP:                return new MapConverter(type.asGroupType(), pigField, parent, elephantBirdCompatible, columnIndexAccess);            case DataType.TUPLE:                return new TupleConverter(type.asGroupType(), pigField.schema, elephantBirdCompatible, columnIndexAccess) {                    @Override                    public void end() {                        super.end();                        parent.add(this.currentTuple);                    }                };            case DataType.CHARARRAY:                                return new FieldStringConverter(parent, type.getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);            case DataType.BYTEARRAY:                return new FieldByteArrayConverter(parent);            case DataType.INTEGER:                return new FieldIntegerConverter(parent);            case DataType.BOOLEAN:                if (elephantBirdCompatible) {                    return new FieldIntegerConverter(parent);                } else {                    return new FieldBooleanConverter(parent);                }            case DataType.FLOAT:                return new FieldFloatConverter(parent);            case DataType.DOUBLE:                return new FieldDoubleConverter(parent);            case DataType.LONG:                return new FieldLongConverter(parent);            case DataType.BIGDECIMAL:                return new FieldBigDecimalConverter(type, parent);            default:                throw new TupleConversionException("unsupported pig type: " + pigField);        }    } catch (FrontendException e) {        throw new TupleConversionException("error while preparing converter for:\n" + pigField + "\n" + type, e);    } catch (RuntimeException e) {        throw new TupleConversionException("error while preparing converter for:\n" + pigField + "\n" + type, e);    }}
public void parquet-mr_f6365_0()
{    super.end();    parent.add(this.currentTuple);}
public Converter parquet-mr_f6366_0(int fieldIndex)
{    return converters[fieldIndex];}
public final void parquet-mr_f6367_0()
{    currentTuple = TF.newTuple(schemaSize);    if (elephantBirdCompatible) {        try {            int i = 0;            for (Type field : parquetSchema.getFields()) {                if (field.isPrimitive() && field.isRepetition(Repetition.OPTIONAL)) {                    PrimitiveType primitiveType = field.asPrimitiveType();                    switch(primitiveType.getPrimitiveTypeName()) {                        case INT32:                            currentTuple.set(i, I32_ZERO);                            break;                        case INT64:                            currentTuple.set(i, I64_ZERO);                            break;                        case FLOAT:                            currentTuple.set(i, FLOAT_ZERO);                            break;                        case DOUBLE:                            currentTuple.set(i, DOUBLE_ZERO);                            break;                        case BOOLEAN:                            currentTuple.set(i, I32_ZERO);                            break;                    }                }                ++i;            }        } catch (ExecException e) {            throw new RuntimeException(e);        }    }}
 final void parquet-mr_f6368_0(int fieldIndex, Object value)
{    try {        currentTuple.set(fieldIndex, value);    } catch (ExecException e) {        throw new TupleConversionException("Could not set " + value + " to current tuple " + currentTuple + " at " + fieldIndex, e);    }}
public void parquet-mr_f6369_0()
{}
public final Tuple parquet-mr_f6370_0()
{    return currentTuple;}
public final void parquet-mr_f6371_0(Binary value)
{    parent.add(value.toStringUsingUTF8());}
public boolean parquet-mr_f6372_0()
{    return dictionarySupport;}
public void parquet-mr_f6373_0(Dictionary dictionary)
{    dict = new String[dictionary.getMaxId() + 1];    for (int i = 0; i <= dictionary.getMaxId(); i++) {        dict[i] = dictionary.decodeToBinary(i).toStringUsingUTF8();    }}
public void parquet-mr_f6374_0(int dictionaryId)
{    parent.add(dict[dictionaryId]);}
public void parquet-mr_f6375_0(long value)
{    parent.add(Long.toString(value));}
public void parquet-mr_f6376_0(int value)
{    parent.add(Integer.toString(value));}
public void parquet-mr_f6377_0(float value)
{    parent.add(Float.toString(value));}
public void parquet-mr_f6378_0(double value)
{    parent.add(Double.toString(value));}
public void parquet-mr_f6379_0(boolean value)
{    parent.add(Boolean.toString(value));}
public final void parquet-mr_f6380_0(Binary value)
{    parent.add(new DataByteArray(value.getBytes()));}
public final void parquet-mr_f6381_0(double value)
{    parent.add(value);}
public void parquet-mr_f6382_0(long value)
{    parent.add((double) value);}
public void parquet-mr_f6383_0(int value)
{    parent.add((double) value);}
public void parquet-mr_f6384_0(float value)
{    parent.add((double) value);}
public void parquet-mr_f6385_0(boolean value)
{    parent.add(value ? 1.0d : 0.0d);}
public void parquet-mr_f6386_0(Binary value)
{    parent.add(Double.parseDouble(value.toStringUsingUTF8()));}
public final void parquet-mr_f6387_0(float value)
{    parent.add(value);}
public void parquet-mr_f6388_0(long value)
{    parent.add((float) value);}
public void parquet-mr_f6389_0(int value)
{    parent.add((float) value);}
public void parquet-mr_f6390_0(double value)
{    parent.add((float) value);}
public void parquet-mr_f6391_0(boolean value)
{    parent.add(value ? 1.0f : 0.0f);}
public void parquet-mr_f6392_0(Binary value)
{    parent.add(Float.parseFloat(value.toStringUsingUTF8()));}
public final void parquet-mr_f6393_0(long value)
{    parent.add(value);}
public void parquet-mr_f6394_0(int value)
{    parent.add((long) value);}
public void parquet-mr_f6395_0(float value)
{    parent.add((long) value);}
public void parquet-mr_f6396_0(double value)
{    parent.add((long) value);}
public void parquet-mr_f6397_0(boolean value)
{    parent.add(value ? 1L : 0L);}
public void parquet-mr_f6398_0(Binary value)
{    parent.add(Long.parseLong(value.toStringUsingUTF8()));}
public final void parquet-mr_f6399_0(boolean value)
{    parent.add(value ? 1 : 0);}
public final void parquet-mr_f6400_0(int value)
{    parent.add(value);}
public void parquet-mr_f6401_0(long value)
{    parent.add((int) value);}
public void parquet-mr_f6402_0(float value)
{    parent.add((int) value);}
public void parquet-mr_f6403_0(double value)
{    parent.add((int) value);}
public void parquet-mr_f6404_0(Binary value)
{    parent.add(Integer.parseInt(value.toStringUsingUTF8()));}
public final void parquet-mr_f6405_0(boolean value)
{    parent.add(value);}
public final void parquet-mr_f6406_0(int value)
{    parent.add(value != 0);}
public void parquet-mr_f6407_0(long value)
{    parent.add(value != 0);}
public void parquet-mr_f6408_0(float value)
{    parent.add(value != 0);}
public void parquet-mr_f6409_0(double value)
{    parent.add(value != 0);}
public void parquet-mr_f6410_0(Binary value)
{    parent.add(Boolean.parseBoolean(value.toStringUsingUTF8()));}
public final void parquet-mr_f6411_0(Binary value)
{    int precision = primitiveType.asPrimitiveType().getDecimalMetadata().getPrecision();    int scale = primitiveType.asPrimitiveType().getDecimalMetadata().getScale();    BigDecimal finaldecimal = DecimalUtils.binaryToDecimal(value, precision, scale);    parent.add(finaldecimal);}
 void parquet-mr_f6412_0(Object value)
{    buffer.add(TF.newTuple(value));}
 void parquet-mr_f6413_0(Object value)
{    buffer.add((Tuple) value);}
public Converter parquet-mr_f6414_0(int fieldIndex)
{    if (fieldIndex != 0) {        throw new IllegalArgumentException("bags have only one field. can't reach " + fieldIndex);    }    return child;}
public final void parquet-mr_f6415_0()
{    buffer.clear();}
public void parquet-mr_f6416_0()
{    parent.add(new NonSpillableDataBag(new ArrayList<Tuple>(buffer)));}
public Tuple parquet-mr_f6417_0()
{    return root.getCurrentTuple();}
public GroupConverter parquet-mr_f6418_0()
{    return root;}
public void parquet-mr_f6419_1(String location, Job job) throws IOException
{    if (LOG.isDebugEnabled()) {        String jobToString = String.format("job[id=%s, name=%s]", job.getJobID(), job.getJobName());            }    setInput(location, job);}
private void parquet-mr_f6420_0(String location, Job job) throws IOException
{    this.setLocationHasBeenCalled = true;    this.location = location;    setInputPaths(job, location);        if (UDFContext.getUDFContext().isFrontend()) {        storeInUDFContext(PARQUET_COLUMN_INDEX_ACCESS, Boolean.toString(columnIndexAccess));    }    schema = PigSchemaConverter.parsePigSchema(getPropertyFromUDFContext(PARQUET_PIG_SCHEMA));    requiredFieldList = PigSchemaConverter.deserializeRequiredFieldList(getPropertyFromUDFContext(PARQUET_PIG_REQUIRED_FIELDS));    columnIndexAccess = Boolean.parseBoolean(getPropertyFromUDFContext(PARQUET_COLUMN_INDEX_ACCESS));    initSchema(job);    if (UDFContext.getUDFContext().isFrontend()) {                storeInUDFContext(PARQUET_PIG_SCHEMA, pigSchemaToString(schema));        storeInUDFContext(PARQUET_PIG_REQUIRED_FIELDS, serializeRequiredFieldList(requiredFieldList));    }        getConfiguration(job).set(PARQUET_PIG_SCHEMA, pigSchemaToString(schema));    getConfiguration(job).set(PARQUET_PIG_REQUIRED_FIELDS, serializeRequiredFieldList(requiredFieldList));    getConfiguration(job).set(PARQUET_COLUMN_INDEX_ACCESS, Boolean.toString(columnIndexAccess));    FilterPredicate filterPredicate = (FilterPredicate) getFromUDFContext(ParquetInputFormat.FILTER_PREDICATE);    if (filterPredicate != null) {        ParquetInputFormat.setFilterPredicate(getConfiguration(job), filterPredicate);    }}
public InputFormat<Void, Tuple> parquet-mr_f6421_1() throws IOException
{        return getParquetInputFormat();}
private void parquet-mr_f6422_0()
{    if (!setLocationHasBeenCalled) {        throw new IllegalStateException("setLocation() must be called first");    }}
public RecordReader<Void, Tuple> parquet-mr_f6423_0(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException
{        inputFormatCache.remove(location);    return super.createRecordReader(inputSplit, taskAttemptContext);}
private ParquetInputFormat<Tuple> parquet-mr_f6424_0() throws ParserException
{    checkSetLocationHasBeenCalled();    if (parquetInputFormat == null) {                Reference<ParquetInputFormat<Tuple>> ref = inputFormatCache.get(location);        parquetInputFormat = ref == null ? null : ref.get();        if (parquetInputFormat == null) {            parquetInputFormat = new UnregisteringParquetInputFormat(location);            inputFormatCache.put(location, new SoftReference<ParquetInputFormat<Tuple>>(parquetInputFormat));        }    }    return parquetInputFormat;}
public void parquet-mr_f6425_1(@SuppressWarnings("rawtypes") RecordReader reader, PigSplit split) throws IOException
{        this.reader = reader;}
public Tuple parquet-mr_f6426_0() throws IOException
{    try {        if (reader.nextKeyValue()) {            return (Tuple) reader.getCurrentValue();        } else {            return null;        }    } catch (InterruptedException e) {        Thread.interrupted();        throw new ParquetDecodingException("Interrupted", e);    }}
public String[] parquet-mr_f6427_1(String location, Job job) throws IOException
{    if (LOG.isDebugEnabled()) {        String jobToString = String.format("job[id=%s, name=%s]", job.getJobID(), job.getJobName());            }    setInput(location, job);    return null;}
public ResourceSchema parquet-mr_f6428_1(String location, Job job) throws IOException
{    if (LOG.isDebugEnabled()) {        String jobToString = String.format("job[id=%s, name=%s]", job.getJobID(), job.getJobName());            }    setInput(location, job);    return new ResourceSchema(schema);}
private void parquet-mr_f6429_0(Job job) throws IOException
{    if (schema != null) {        return;    }    if (schema == null && requestedSchema != null) {                schema = requestedSchema;    }    if (schema == null) {                final GlobalMetaData globalMetaData = getParquetInputFormat().getGlobalMetaData(job);        schema = getPigSchemaFromMultipleFiles(globalMetaData.getSchema(), globalMetaData.getKeyValueMetaData());    }    if (isElephantBirdCompatible(job)) {        convertToElephantBirdCompatibleSchema(schema);    }}
private void parquet-mr_f6430_0(Schema schema)
{    if (schema == null) {        return;    }    for (FieldSchema fieldSchema : schema.getFields()) {        if (fieldSchema.type == DataType.BOOLEAN) {            fieldSchema.type = DataType.INTEGER;        }        convertToElephantBirdCompatibleSchema(fieldSchema.schema);    }}
private boolean parquet-mr_f6431_0(Job job)
{    return getConfiguration(job).getBoolean(TupleReadSupport.PARQUET_PIG_ELEPHANT_BIRD_COMPATIBLE, false);}
public ResourceStatistics parquet-mr_f6432_1(String location, Job job) throws IOException
{    if (LOG.isDebugEnabled()) {        String jobToString = String.format("job[id=%s, name=%s]", job.getJobID(), job.getJobName());            }    /* We need to call setInput since setLocation is not       guaranteed to be called before this */    setInput(location, job);    long length = 0;    try {        for (InputSplit split : getParquetInputFormat().getSplits(job)) {            length += split.getLength();        }    } catch (InterruptedException e) {                return null;    }    ResourceStatistics stats = new ResourceStatistics();        stats.setmBytes(length / 1024 / 1024);    return stats;}
public void parquet-mr_f6433_1(Expression expression) throws IOException
{    }
public List<OperatorSet> parquet-mr_f6434_0()
{    return asList(LoadPushDown.OperatorSet.PROJECTION);}
protected String parquet-mr_f6435_0(String key)
{    UDFContext udfContext = UDFContext.getUDFContext();    return udfContext.getUDFProperties(this.getClass(), new String[] { signature }).getProperty(key);}
protected Object parquet-mr_f6436_0(String key)
{    UDFContext udfContext = UDFContext.getUDFContext();    return udfContext.getUDFProperties(this.getClass(), new String[] { signature }).get(key);}
protected void parquet-mr_f6437_0(String key, Object value)
{    UDFContext udfContext = UDFContext.getUDFContext();    java.util.Properties props = udfContext.getUDFProperties(this.getClass(), new String[] { signature });    props.put(key, value);}
public RequiredFieldResponse parquet-mr_f6438_0(RequiredFieldList requiredFieldList) throws FrontendException
{    this.requiredFieldList = requiredFieldList;    if (requiredFieldList == null)        return null;    schema = getSchemaFromRequiredFieldList(schema, requiredFieldList.getFields());    storeInUDFContext(PARQUET_PIG_SCHEMA, pigSchemaToString(schema));    storeInUDFContext(PARQUET_PIG_REQUIRED_FIELDS, serializeRequiredFieldList(requiredFieldList));    return new RequiredFieldResponse(true);}
public void parquet-mr_f6439_0(String signature)
{    this.signature = signature;}
private Schema parquet-mr_f6440_0(Schema schema, List<RequiredField> fieldList) throws FrontendException
{    Schema s = new Schema();    for (RequiredField rf : fieldList) {        FieldSchema f;        try {            f = schema.getField(rf.getAlias()).clone();        } catch (CloneNotSupportedException e) {            throw new FrontendException("Clone not supported for the fieldschema", e);        }        if (rf.getSubFields() == null) {            s.add(f);        } else {            Schema innerSchema = getSchemaFromRequiredFieldList(f.schema, rf.getSubFields());            if (innerSchema == null) {                return null;            } else {                f.schema = innerSchema;                s.add(f);            }        }    }    return s;}
public List<String> parquet-mr_f6441_0(String s, Job job) throws IOException
{    if (!job.getConfiguration().getBoolean(ENABLE_PREDICATE_FILTER_PUSHDOWN, DEFAULT_PREDICATE_PUSHDOWN_ENABLED)) {        return null;    }    List<String> fields = new ArrayList<String>();    for (FieldSchema field : schema.getFields()) {        switch(field.type) {            case DataType.BOOLEAN:            case DataType.INTEGER:            case DataType.LONG:            case DataType.FLOAT:            case DataType.DOUBLE:            case DataType.CHARARRAY:                fields.add(field.alias);                break;            default:                                break;        }    }    return fields;}
public List<Expression.OpType> parquet-mr_f6442_0()
{    OpType[] supportedTypes = { OpType.OP_EQ, OpType.OP_NE, OpType.OP_GT, OpType.OP_GE, OpType.OP_LT, OpType.OP_LE, OpType.OP_AND, OpType.OP_OR,     OpType.OP_NOT };    return Arrays.asList(supportedTypes);}
public void parquet-mr_f6443_1(Expression e) throws IOException
{        FilterPredicate pred = buildFilter(e);        storeInUDFContext(ParquetInputFormat.FILTER_PREDICATE, pred);}
private FilterPredicate parquet-mr_f6444_0(Expression e)
{    OpType op = e.getOpType();    if (e instanceof BinaryExpression) {        Expression lhs = ((BinaryExpression) e).getLhs();        Expression rhs = ((BinaryExpression) e).getRhs();        switch(op) {            case OP_AND:                return and(buildFilter(lhs), buildFilter(rhs));            case OP_OR:                return or(buildFilter(lhs), buildFilter(rhs));            case OP_BETWEEN:                BetweenExpression between = (BetweenExpression) rhs;                return and(buildFilter(OpType.OP_GE, (Column) lhs, (Const) between.getLower()), buildFilter(OpType.OP_LE, (Column) lhs, (Const) between.getUpper()));            case OP_IN:                FilterPredicate current = null;                for (Object value : ((InExpression) rhs).getValues()) {                    FilterPredicate next = buildFilter(OpType.OP_EQ, (Column) lhs, (Const) value);                    if (current != null) {                        current = or(current, next);                    } else {                        current = next;                    }                }                return current;        }        if (lhs instanceof Column && rhs instanceof Const) {            return buildFilter(op, (Column) lhs, (Const) rhs);        } else if (lhs instanceof Const && rhs instanceof Column) {            return buildFilter(op, (Column) rhs, (Const) lhs);        }    } else if (e instanceof UnaryExpression && op == OpType.OP_NOT) {        return LogicalInverseRewriter.rewrite(not(buildFilter(((UnaryExpression) e).getExpression())));    }    throw new RuntimeException("Could not build filter for expression: " + e);}
private FilterPredicate parquet-mr_f6445_0(OpType op, Column col, Const value)
{    String name = col.getName();    try {        FieldSchema f = schema.getField(name);        switch(f.type) {            case DataType.BOOLEAN:                Operators.BooleanColumn boolCol = booleanColumn(name);                switch(op) {                    case OP_EQ:                        return eq(boolCol, getValue(value, boolCol.getColumnType()));                    case OP_NE:                        return notEq(boolCol, getValue(value, boolCol.getColumnType()));                    default:                        throw new RuntimeException("Operation " + op + " not supported for boolean column: " + name);                }            case DataType.INTEGER:                Operators.IntColumn intCol = intColumn(name);                return op(op, intCol, value);            case DataType.LONG:                Operators.LongColumn longCol = longColumn(name);                return op(op, longCol, value);            case DataType.FLOAT:                Operators.FloatColumn floatCol = floatColumn(name);                return op(op, floatCol, value);            case DataType.DOUBLE:                Operators.DoubleColumn doubleCol = doubleColumn(name);                return op(op, doubleCol, value);            case DataType.CHARARRAY:                Operators.BinaryColumn binaryCol = binaryColumn(name);                return op(op, binaryCol, value);            default:                throw new RuntimeException("Unsupported type " + f.type + " for field: " + name);        }    } catch (FrontendException e) {        throw new RuntimeException("Error processing pushdown for column:" + col, e);    }}
private static FilterPredicate parquet-mr_f6446_0(Expression.OpType op, COL col, Const valueExpr)
{    C value = getValue(valueExpr, col.getColumnType());    switch(op) {        case OP_EQ:            return eq(col, value);        case OP_NE:            return notEq(col, value);        case OP_GT:            return gt(col, value);        case OP_GE:            return gtEq(col, value);        case OP_LT:            return lt(col, value);        case OP_LE:            return ltEq(col, value);    }    return null;}
private static C parquet-mr_f6447_0(Const valueExpr, Class<C> type)
{    Object value = valueExpr.getValue();    if (value instanceof String) {        value = Binary.fromString((String) value);    }    return type.cast(value);}
private Properties parquet-mr_f6448_0()
{    UDFContext udfc = UDFContext.getUDFContext();    Properties p = udfc.getUDFProperties(this.getClass(), new String[] { signature });    return p;}
private Schema parquet-mr_f6449_0()
{    try {        final String schemaString = getProperties().getProperty(SCHEMA);        if (schemaString == null) {            throw new ParquetEncodingException("Can not store relation in Parquet as the schema is unknown");        }        return Utils.getSchemaFromString(schemaString);    } catch (ParserException e) {        throw new ParquetEncodingException("can not get schema from context", e);    }}
public void parquet-mr_f6450_0(String signature)
{    super.setStoreFuncUDFContextSignature(signature);    this.signature = signature;}
public void parquet-mr_f6451_0(ResourceSchema s) throws IOException
{    getProperties().setProperty(SCHEMA, s.toString());}
public OutputFormat<Void, Tuple> parquet-mr_f6452_0() throws IOException
{    Schema pigSchema = getSchema();    return new ParquetOutputFormat<Tuple>(new TupleWriteSupport(pigSchema));}
public void parquet-mr_f6453_0(RecordWriter recordWriter) throws IOException
{    this.recordWriter = recordWriter;}
public void parquet-mr_f6454_0(Tuple tuple) throws IOException
{    try {        this.recordWriter.write(null, tuple);    } catch (InterruptedException e) {        Thread.interrupted();        throw new ParquetEncodingException("Interrupted while writing", e);    }}
public void parquet-mr_f6455_0(String location, Job job) throws IOException
{    FileOutputFormat.setOutputPath(job, new Path(location));}
public void parquet-mr_f6456_0(ResourceSchema schema, String location, Job job) throws IOException
{}
public void parquet-mr_f6457_0(ResourceStatistics resourceStatistics, String location, Job job) throws IOException
{}
public static PigMetaData parquet-mr_f6458_0(Map<String, String> keyValueMetaData)
{    if (keyValueMetaData.containsKey(PIG_SCHEMA)) {        return new PigMetaData(keyValueMetaData.get(PIG_SCHEMA));    }    return null;}
public static Set<String> parquet-mr_f6459_0(Map<String, Set<String>> keyValueMetaData)
{    return keyValueMetaData.get(PIG_SCHEMA);}
public void parquet-mr_f6460_0(String pigSchema)
{    this.pigSchema = pigSchema;}
public String parquet-mr_f6461_0()
{    return pigSchema;}
public void parquet-mr_f6462_0(Map<String, String> map)
{    map.put(PIG_SCHEMA, pigSchema);}
public static Schema parquet-mr_f6463_0(String pigSchemaString)
{    try {        return pigSchemaString == null ? null : Utils.getSchemaFromString(pigSchemaString);    } catch (ParserException e) {        throw new SchemaConversionException("could not parse Pig schema: " + pigSchemaString, e);    }}
public List<Type> parquet-mr_f6464_0(GroupType schemaToFilter, Schema pigSchema, RequiredFieldList requiredFieldsList)
{    List<Type> newFields = new ArrayList<Type>();    List<Pair<FieldSchema, Integer>> indexedFields = new ArrayList<Pair<FieldSchema, Integer>>();    try {        if (requiredFieldsList == null) {            int index = 0;            for (FieldSchema fs : pigSchema.getFields()) {                indexedFields.add(new Pair<FieldSchema, Integer>(fs, index++));            }        } else {            for (RequiredField rf : requiredFieldsList.getFields()) {                indexedFields.add(new Pair<FieldSchema, Integer>(pigSchema.getField(rf.getAlias()), rf.getIndex()));            }        }        for (Pair<FieldSchema, Integer> p : indexedFields) {            FieldSchema fieldSchema = pigSchema.getField(p.first.alias);            if (p.second < schemaToFilter.getFieldCount()) {                Type type = schemaToFilter.getFields().get(p.second);                newFields.add(filter(type, fieldSchema));            }        }    } catch (FrontendException e) {        throw new RuntimeException("Failed to filter requested fields", e);    }    return newFields;}
public List<Type> parquet-mr_f6465_0(GroupType schemaToFilter, Schema requestedPigSchema, RequiredFieldList requiredFieldsList)
{    List<FieldSchema> fields = requestedPigSchema.getFields();    List<Type> newFields = new ArrayList<Type>();    for (int i = 0; i < fields.size(); i++) {        FieldSchema fieldSchema = fields.get(i);        String name = name(fieldSchema.alias, "field_" + i);        if (schemaToFilter.containsField(name)) {            newFields.add(filter(schemaToFilter.getType(name), fieldSchema));        }    }    return newFields;}
 static String parquet-mr_f6466_0(Schema pigSchema)
{    final String pigSchemaString = pigSchema.toString();    return pigSchemaString.substring(1, pigSchemaString.length() - 1);}
public static RequiredFieldList parquet-mr_f6467_0(String requiredFieldString)
{    if (requiredFieldString == null) {        return null;    }    try {        return (RequiredFieldList) ObjectSerializer.deserialize(requiredFieldString);    } catch (IOException e) {        throw new RuntimeException("Failed to deserialize pushProjection", e);    }}
 static String parquet-mr_f6468_0(RequiredFieldList requiredFieldList)
{    try {        return ObjectSerializer.serialize(requiredFieldList);    } catch (IOException e) {        throw new RuntimeException("Failed to searlize required fields.", e);    }}
public Schema parquet-mr_f6469_0(MessageType parquetSchema)
{    return convertFields(parquetSchema.getFields());}
public Schema parquet-mr_f6470_0(Type parquetType)
{    return convertFields(Arrays.asList(parquetType));}
private Schema parquet-mr_f6471_0(List<Type> parquetFields)
{    List<FieldSchema> fields = new ArrayList<Schema.FieldSchema>();    for (Type parquetType : parquetFields) {        try {            FieldSchema innerfieldSchema = getFieldSchema(parquetType);            if (parquetType.isRepetition(Repetition.REPEATED)) {                Schema bagSchema = new Schema(Arrays.asList(innerfieldSchema));                fields.add(new FieldSchema(null, bagSchema, DataType.BAG));            } else {                fields.add(innerfieldSchema);            }        } catch (FrontendException fe) {            throw new SchemaConversionException("can't convert " + parquetType, fe);        }    }    return new Schema(fields);}
private FieldSchema parquet-mr_f6472_1(final String fieldName, Type parquetType) throws FrontendException
{    final PrimitiveTypeName parquetPrimitiveTypeName = parquetType.asPrimitiveType().getPrimitiveTypeName();    final LogicalTypeAnnotation logicalTypeAnnotation = parquetType.getLogicalTypeAnnotation();    return parquetPrimitiveTypeName.convert(new PrimitiveTypeNameConverter<Schema.FieldSchema, FrontendException>() {        @Override        public FieldSchema convertFLOAT(PrimitiveTypeName primitiveTypeName) throws FrontendException {            return new FieldSchema(fieldName, null, DataType.FLOAT);        }        @Override        public FieldSchema convertDOUBLE(PrimitiveTypeName primitiveTypeName) throws FrontendException {            return new FieldSchema(fieldName, null, DataType.DOUBLE);        }        @Override        public FieldSchema convertINT32(PrimitiveTypeName primitiveTypeName) throws FrontendException {            return new FieldSchema(fieldName, null, DataType.INTEGER);        }        @Override        public FieldSchema convertINT64(PrimitiveTypeName primitiveTypeName) throws FrontendException {            return new FieldSchema(fieldName, null, DataType.LONG);        }        @Override        public FieldSchema convertINT96(PrimitiveTypeName primitiveTypeName) throws FrontendException {                        return new FieldSchema(fieldName, null, DataType.BYTEARRAY);        }        @Override        public FieldSchema convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) throws FrontendException {            if (logicalTypeAnnotation instanceof LogicalTypeAnnotation.DecimalLogicalTypeAnnotation) {                return new FieldSchema(fieldName, null, DataType.BIGDECIMAL);            } else {                return new FieldSchema(fieldName, null, DataType.BYTEARRAY);            }        }        @Override        public FieldSchema convertBOOLEAN(PrimitiveTypeName primitiveTypeName) throws FrontendException {            return new FieldSchema(fieldName, null, DataType.BOOLEAN);        }        @Override        public FieldSchema convertBINARY(PrimitiveTypeName primitiveTypeName) throws FrontendException {            if (logicalTypeAnnotation instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation) {                return new FieldSchema(fieldName, null, DataType.CHARARRAY);            } else {                return new FieldSchema(fieldName, null, DataType.BYTEARRAY);            }        }    });}
public FieldSchema parquet-mr_f6473_0(PrimitiveTypeName primitiveTypeName) throws FrontendException
{    return new FieldSchema(fieldName, null, DataType.FLOAT);}
public FieldSchema parquet-mr_f6474_0(PrimitiveTypeName primitiveTypeName) throws FrontendException
{    return new FieldSchema(fieldName, null, DataType.DOUBLE);}
public FieldSchema parquet-mr_f6475_0(PrimitiveTypeName primitiveTypeName) throws FrontendException
{    return new FieldSchema(fieldName, null, DataType.INTEGER);}
public FieldSchema parquet-mr_f6476_0(PrimitiveTypeName primitiveTypeName) throws FrontendException
{    return new FieldSchema(fieldName, null, DataType.LONG);}
public FieldSchema parquet-mr_f6477_1(PrimitiveTypeName primitiveTypeName) throws FrontendException
{        return new FieldSchema(fieldName, null, DataType.BYTEARRAY);}
public FieldSchema parquet-mr_f6478_0(PrimitiveTypeName primitiveTypeName) throws FrontendException
{    if (logicalTypeAnnotation instanceof LogicalTypeAnnotation.DecimalLogicalTypeAnnotation) {        return new FieldSchema(fieldName, null, DataType.BIGDECIMAL);    } else {        return new FieldSchema(fieldName, null, DataType.BYTEARRAY);    }}
public FieldSchema parquet-mr_f6479_0(PrimitiveTypeName primitiveTypeName) throws FrontendException
{    return new FieldSchema(fieldName, null, DataType.BOOLEAN);}
public FieldSchema parquet-mr_f6480_0(PrimitiveTypeName primitiveTypeName) throws FrontendException
{    if (logicalTypeAnnotation instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation) {        return new FieldSchema(fieldName, null, DataType.CHARARRAY);    } else {        return new FieldSchema(fieldName, null, DataType.BYTEARRAY);    }}
private FieldSchema parquet-mr_f6481_0(String fieldName, Type parquetType) throws FrontendException
{    GroupType parquetGroupType = parquetType.asGroupType();    LogicalTypeAnnotation logicalTypeAnnotation = parquetGroupType.getLogicalTypeAnnotation();    if (logicalTypeAnnotation != null) {        try {            return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<FieldSchema>() {                @Override                public Optional<FieldSchema> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType) {                    try {                                                if (parquetGroupType.getFieldCount() != 1 || parquetGroupType.getType(0).isPrimitive()) {                            throw new SchemaConversionException("Invalid map type " + parquetGroupType);                        }                        GroupType mapKeyValType = parquetGroupType.getType(0).asGroupType();                        if (!mapKeyValType.isRepetition(Repetition.REPEATED) || (mapKeyValType.getLogicalTypeAnnotation() != null && !mapKeyValType.getLogicalTypeAnnotation().equals(LogicalTypeAnnotation.MapKeyValueTypeAnnotation.getInstance())) || mapKeyValType.getFieldCount() != 2) {                            throw new SchemaConversionException("Invalid map type " + parquetGroupType);                        }                                                Type valueType = mapKeyValType.getType(1);                        Schema s = convertField(valueType);                        s.getField(0).alias = null;                        return of(new FieldSchema(fieldName, s, DataType.MAP));                    } catch (FrontendException e) {                        throw new FrontendExceptionWrapper(e);                    }                }                @Override                public Optional<FieldSchema> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType) {                    try {                        Type type = parquetGroupType.getType(0);                        if (parquetGroupType.getFieldCount() != 1 || type.isPrimitive()) {                                                        Schema primitiveSchema = new Schema(getSimpleFieldSchema(parquetGroupType.getFieldName(0), type));                            Schema tupleSchema = new Schema(new FieldSchema(ARRAY_VALUE_NAME, primitiveSchema, DataType.TUPLE));                            return of(new FieldSchema(fieldName, tupleSchema, DataType.BAG));                        }                        GroupType tupleType = parquetGroupType.getType(0).asGroupType();                        if (!tupleType.isRepetition(Repetition.REPEATED)) {                            throw new SchemaConversionException("Invalid list type " + parquetGroupType);                        }                        Schema tupleSchema = new Schema(new FieldSchema(tupleType.getName(), convertFields(tupleType.getFields()), DataType.TUPLE));                        return of(new FieldSchema(fieldName, tupleSchema, DataType.BAG));                    } catch (FrontendException e) {                        throw new FrontendExceptionWrapper(e);                    }                }            }).orElseThrow(() -> new SchemaConversionException("Unexpected original type for " + parquetType + ": " + logicalTypeAnnotation));        } catch (FrontendExceptionWrapper e) {            throw e.frontendException;        }    } else {                return new FieldSchema(fieldName, convertFields(parquetGroupType.getFields()), DataType.TUPLE);    }}
public Optional<FieldSchema> parquet-mr_f6482_0(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    try {                if (parquetGroupType.getFieldCount() != 1 || parquetGroupType.getType(0).isPrimitive()) {            throw new SchemaConversionException("Invalid map type " + parquetGroupType);        }        GroupType mapKeyValType = parquetGroupType.getType(0).asGroupType();        if (!mapKeyValType.isRepetition(Repetition.REPEATED) || (mapKeyValType.getLogicalTypeAnnotation() != null && !mapKeyValType.getLogicalTypeAnnotation().equals(LogicalTypeAnnotation.MapKeyValueTypeAnnotation.getInstance())) || mapKeyValType.getFieldCount() != 2) {            throw new SchemaConversionException("Invalid map type " + parquetGroupType);        }                Type valueType = mapKeyValType.getType(1);        Schema s = convertField(valueType);        s.getField(0).alias = null;        return of(new FieldSchema(fieldName, s, DataType.MAP));    } catch (FrontendException e) {        throw new FrontendExceptionWrapper(e);    }}
public Optional<FieldSchema> parquet-mr_f6483_0(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    try {        Type type = parquetGroupType.getType(0);        if (parquetGroupType.getFieldCount() != 1 || type.isPrimitive()) {                        Schema primitiveSchema = new Schema(getSimpleFieldSchema(parquetGroupType.getFieldName(0), type));            Schema tupleSchema = new Schema(new FieldSchema(ARRAY_VALUE_NAME, primitiveSchema, DataType.TUPLE));            return of(new FieldSchema(fieldName, tupleSchema, DataType.BAG));        }        GroupType tupleType = parquetGroupType.getType(0).asGroupType();        if (!tupleType.isRepetition(Repetition.REPEATED)) {            throw new SchemaConversionException("Invalid list type " + parquetGroupType);        }        Schema tupleSchema = new Schema(new FieldSchema(tupleType.getName(), convertFields(tupleType.getFields()), DataType.TUPLE));        return of(new FieldSchema(fieldName, tupleSchema, DataType.BAG));    } catch (FrontendException e) {        throw new FrontendExceptionWrapper(e);    }}
private FieldSchema parquet-mr_f6484_0(Type parquetType) throws FrontendException
{    final String fieldName = parquetType.getName();    if (parquetType.isPrimitive()) {        return getSimpleFieldSchema(fieldName, parquetType);    } else {        return getComplexFieldSchema(fieldName, parquetType);    }}
public MessageType parquet-mr_f6485_0(Schema pigSchema)
{    return new MessageType("pig_schema", convertTypes(pigSchema));}
private Type[] parquet-mr_f6486_0(Schema pigSchema)
{    List<FieldSchema> fields = pigSchema.getFields();    Type[] types = new Type[fields.size()];    for (int i = 0; i < types.length; i++) {        types[i] = convert(fields.get(i), i);    }    return types;}
private Type parquet-mr_f6487_0(FieldSchema fieldSchema, String defaultAlias)
{    String name = name(fieldSchema.alias, defaultAlias);    return convertWithName(fieldSchema, name);}
private Type parquet-mr_f6488_0(FieldSchema fieldSchema, String name)
{    try {        switch(fieldSchema.type) {            case DataType.BAG:                return convertBag(name, fieldSchema);            case DataType.TUPLE:                return convertTuple(name, fieldSchema, Repetition.OPTIONAL);            case DataType.MAP:                return convertMap(name, fieldSchema);            case DataType.BOOLEAN:                return primitive(name, PrimitiveTypeName.BOOLEAN);            case DataType.CHARARRAY:                return primitive(name, PrimitiveTypeName.BINARY, stringType());            case DataType.INTEGER:                return primitive(name, PrimitiveTypeName.INT32);            case DataType.LONG:                return primitive(name, PrimitiveTypeName.INT64);            case DataType.FLOAT:                return primitive(name, PrimitiveTypeName.FLOAT);            case DataType.DOUBLE:                return primitive(name, PrimitiveTypeName.DOUBLE);            case DataType.DATETIME:                throw new UnsupportedOperationException();            case DataType.BYTEARRAY:                return primitive(name, PrimitiveTypeName.BINARY);            default:                throw new SchemaConversionException("Unknown type " + fieldSchema.type + " " + DataType.findTypeName(fieldSchema.type));        }    } catch (FrontendException e) {        throw new SchemaConversionException("can't convert " + fieldSchema, e);    }}
private Type parquet-mr_f6489_0(FieldSchema fieldSchema, int index)
{    return convert(fieldSchema, "field_" + index);}
private GroupType parquet-mr_f6490_0(String name, FieldSchema fieldSchema) throws FrontendException
{    FieldSchema innerField = fieldSchema.schema.getField(0);    return ConversionPatterns.listType(Repetition.OPTIONAL, name, convertTuple(name(innerField.alias, "bag"), innerField, Repetition.REPEATED));}
private String parquet-mr_f6491_0(String fieldAlias, String defaultName)
{    return fieldAlias == null ? defaultName : fieldAlias;}
private Type parquet-mr_f6492_0(String name, PrimitiveTypeName primitive, LogicalTypeAnnotation logicalTypeAnnotation)
{    return Types.primitive(primitive, Repetition.OPTIONAL).as(logicalTypeAnnotation).named(name);}
private PrimitiveType parquet-mr_f6493_0(String name, PrimitiveTypeName primitive)
{    return Types.primitive(primitive, Repetition.OPTIONAL).named(name);}
private GroupType parquet-mr_f6494_0(String alias, FieldSchema fieldSchema)
{    Schema innerSchema = fieldSchema.schema;    if (innerSchema == null || innerSchema.size() != 1) {        throw new SchemaConversionException("Invalid map Schema, schema should contain exactly one field: " + fieldSchema);    }    FieldSchema innerField = null;    try {        innerField = innerSchema.getField(0);    } catch (FrontendException fe) {        throw new SchemaConversionException("Invalid map schema, cannot infer innerschema: ", fe);    }    Type convertedValue = convertWithName(innerField, "value");    return ConversionPatterns.stringKeyMapType(Repetition.OPTIONAL, alias, name(innerField.alias, "map"), convertedValue);}
private GroupType parquet-mr_f6495_0(String alias, FieldSchema field, Repetition repetition)
{    return new GroupType(repetition, alias, convertTypes(field.schema));}
public MessageType parquet-mr_f6496_0(MessageType schemaToFilter, Schema requestedPigSchema)
{    return filter(schemaToFilter, requestedPigSchema, null);}
private Type parquet-mr_f6498_1(Type type, FieldSchema fieldSchema)
{    if (LOG.isDebugEnabled())            try {        switch(fieldSchema.type) {            case DataType.BAG:                return filterBag(type.asGroupType(), fieldSchema);            case DataType.MAP:                return filterMap(type.asGroupType(), fieldSchema);            case DataType.TUPLE:                return filterTuple(type.asGroupType(), fieldSchema);            default:                return type;        }    } catch (FrontendException e) {        throw new SchemaConversionException("can't filter " + type + " with " + fieldSchema, e);    } catch (RuntimeException e) {        throw new RuntimeException("can't filter " + type + " with " + fieldSchema, e);    }}
private Type parquet-mr_f6499_1(GroupType tupleType, FieldSchema tupleFieldSchema) throws FrontendException
{    if (LOG.isDebugEnabled())            return tupleType.withNewFields(columnAccess.filterTupleSchema(tupleType, tupleFieldSchema.schema, null));}
private Type parquet-mr_f6500_1(GroupType mapType, FieldSchema mapFieldSchema) throws FrontendException
{    if (LOG.isDebugEnabled())            if (mapType.getFieldCount() != 1) {        throw new RuntimeException("not unwrapping the right type, this should be a Map: " + mapType);    }    GroupType nested = mapType.getType(0).asGroupType();    if (nested.getFieldCount() != 2) {        throw new RuntimeException("this should be a Map Key/Value: " + mapType);    }    FieldSchema innerField = mapFieldSchema.schema.getField(0);    return mapType.withNewFields(nested.withNewFields(nested.getType(0), filter(nested.getType(1), innerField)));}
private Type parquet-mr_f6501_1(GroupType bagType, FieldSchema bagFieldSchema) throws FrontendException
{    if (LOG.isDebugEnabled())            if (bagType.getFieldCount() != 1) {        throw new RuntimeException("not unwrapping the right type, this should be a Bag: " + bagType);    }    Type nested = bagType.getType(0);    FieldSchema innerField = bagFieldSchema.schema.getField(0);    if (nested.isPrimitive() || nested.getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.MapLogicalTypeAnnotation || nested.getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.ListLogicalTypeAnnotation) {                innerField = innerField.schema.getField(0);    }    return bagType.withNewFields(filter(nested, innerField));}
public void parquet-mr_f6502_0(Schema schema, DataBag bag)
{    super.add(bag);    size.add(bag.size());    FieldSchema field = getField(schema, 0);    if (bag.size() > 0 && content == null) {        content = new FieldSummaryData();        content.setName(getName(field));    }    for (Tuple tuple : bag) {        content.add(getSchema(field), tuple);    }}
public void parquet-mr_f6503_0(SummaryData other)
{    super.merge(other);    BagSummaryData otherBagSummary = (BagSummaryData) other;    size.merge(otherBagSummary.size);    content = merge(content, otherBagSummary.content);}
public FieldSummaryData parquet-mr_f6504_0()
{    return content;}
public void parquet-mr_f6505_0(FieldSummaryData content)
{    this.content = content;}
public ValueStat parquet-mr_f6506_0()
{    return size;}
public void parquet-mr_f6507_0(ValueStat size)
{    this.size = size;}
public void parquet-mr_f6508_0()
{    ++count;}
public String parquet-mr_f6509_0()
{    return value;}
public void parquet-mr_f6510_0(String value)
{    this.value = value;}
public int parquet-mr_f6511_0()
{    return count;}
public void parquet-mr_f6512_0(int count)
{    this.count = count;}
public void parquet-mr_f6513_0(int countToAdd)
{    this.count += countToAdd;}
public void parquet-mr_f6514_0(String value)
{    if (values != null) {        EnumValueCount enumValueCount = values.get(value);        if (enumValueCount == null) {            enumValueCount = new EnumValueCount(value);            values.put(value, enumValueCount);        }        enumValueCount.add();        checkValues();    }}
public void parquet-mr_f6515_0(EnumStat other)
{    if (values != null) {        if (other.values == null) {            values = null;            return;        }        for (EnumValueCount otherValue : other.getValues()) {            EnumValueCount myValue = values.get(otherValue.value);            if (myValue == null) {                values.put(otherValue.value, otherValue);            } else {                myValue.add(otherValue.count);            }        }        checkValues();    }}
private void parquet-mr_f6516_0()
{    if (values.size() > MAX_COUNT) {        values = null;    }}
public Collection<EnumValueCount> parquet-mr_f6517_0()
{    return values == null ? null : values.values();}
public void parquet-mr_f6518_0(Collection<EnumValueCount> values)
{    if (values == null) {        this.values = null;    } else if (this.values != null) {        for (EnumValueCount value : values) {            this.values.put(value.getValue(), value);        }    }}
public void parquet-mr_f6519_0(SummaryData other)
{    super.merge(other);    FieldSummaryData otherFieldSummaryData = (FieldSummaryData) other;    if (otherFieldSummaryData.name != null) {        setName(otherFieldSummaryData.name);    }    bag = merge(bag, otherFieldSummaryData.bag);    tuple = merge(tuple, otherFieldSummaryData.tuple);    map = merge(map, otherFieldSummaryData.map);    string = merge(string, otherFieldSummaryData.string);    number = merge(number, otherFieldSummaryData.number);    nullCount += otherFieldSummaryData.nullCount;    unknown += otherFieldSummaryData.unknown;    error += otherFieldSummaryData.error;}
public void parquet-mr_f6520_0(Schema schema, Object o)
{    super.add(o);    if (o == null) {        ++nullCount;    } else if (o instanceof DataBag) {        if (bag == null) {            bag = new BagSummaryData();        }        bag.add(schema, (DataBag) o);    } else if (o instanceof Tuple) {        if (tuple == null) {            tuple = new TupleSummaryData();        }        tuple.addTuple(schema, (Tuple) o);    } else if (o instanceof Map<?, ?>) {        if (map == null) {            map = new MapSummaryData();        }        map.add(schema, (Map<?, ?>) o);    } else if (o instanceof String) {        if (string == null) {            string = new StringSummaryData();        }        string.add((String) o);    } else if (o instanceof Number) {        if (number == null) {            number = new NumberSummaryData();        }        number.add((Number) o);    } else {        ++unknown;    }}
public void parquet-mr_f6521_0()
{    ++error;}
public BagSummaryData parquet-mr_f6522_0()
{    return bag;}
public void parquet-mr_f6523_0(BagSummaryData bag)
{    this.bag = bag;}
public TupleSummaryData parquet-mr_f6524_0()
{    return tuple;}
public void parquet-mr_f6525_0(TupleSummaryData tuple)
{    this.tuple = tuple;}
public MapSummaryData parquet-mr_f6526_0()
{    return map;}
public void parquet-mr_f6527_0(MapSummaryData map)
{    this.map = map;}
public StringSummaryData parquet-mr_f6528_0()
{    return string;}
public void parquet-mr_f6529_0(StringSummaryData string)
{    this.string = string;}
public NumberSummaryData parquet-mr_f6530_0()
{    return number;}
public void parquet-mr_f6531_0(NumberSummaryData number)
{    this.number = number;}
public Long parquet-mr_f6532_0()
{    return nullCount == 0 ? null : nullCount;}
public void parquet-mr_f6533_0(long nullCnt)
{    this.nullCount = nullCnt;}
public Long parquet-mr_f6534_0()
{    return unknown == 0 ? null : unknown;}
public void parquet-mr_f6535_0(long unknown)
{    this.unknown = unknown;}
public Long parquet-mr_f6536_0()
{    return error == 0 ? null : error;}
public void parquet-mr_f6537_0(long error)
{    this.error = error;}
public void parquet-mr_f6538_0(String name)
{    if (this.name != null && !this.name.equals(name)) {        throw new IllegalStateException("name mismatch " + this.name + " expected, got " + name);    }    this.name = name;}
public String parquet-mr_f6539_0()
{    return name;}
public void parquet-mr_f6540_0(Schema schema, Map<?, ?> m)
{    super.add(m);    size.add(m.size());    FieldSchema field = getField(schema, 0);    if (m.size() > 0 && key == null) {        key = new FieldSummaryData();        key.setName(getName(field));        value = new FieldSummaryData();        value.setName(getName(field));    }    for (Map.Entry<?, ?> entry : m.entrySet()) {        key.add(null, entry.getKey());        value.add(getSchema(field), entry.getValue());    }}
public void parquet-mr_f6541_0(SummaryData other)
{    super.merge(other);    MapSummaryData otherMapSummaryData = (MapSummaryData) other;    size.merge(otherMapSummaryData.size);    key = merge(key, otherMapSummaryData.key);    value = merge(value, otherMapSummaryData.value);}
public FieldSummaryData parquet-mr_f6542_0()
{    return key;}
public void parquet-mr_f6543_0(FieldSummaryData key)
{    this.key = key;}
public FieldSummaryData parquet-mr_f6544_0()
{    return value;}
public void parquet-mr_f6545_0(FieldSummaryData value)
{    this.value = value;}
public ValueStat parquet-mr_f6546_0()
{    return size;}
public void parquet-mr_f6547_0(ValueStat size)
{    this.size = size;}
public void parquet-mr_f6548_0(Number n)
{    super.add(n);    value.add(n.doubleValue());}
public void parquet-mr_f6549_0(SummaryData other)
{    super.merge(other);    value.merge(((NumberSummaryData) other).value);}
public ValueStat parquet-mr_f6550_0()
{    return value;}
public void parquet-mr_f6551_0(ValueStat value)
{    this.value = value;}
public void parquet-mr_f6552_0(String s)
{    super.add(s);    size.add(s.length());    values.add(s);}
public void parquet-mr_f6553_0(SummaryData other)
{    super.merge(other);    StringSummaryData stringSummaryData = (StringSummaryData) other;    size.merge(stringSummaryData.size);    values.merge(stringSummaryData.values);}
public ValueStat parquet-mr_f6554_0()
{    return size;}
public void parquet-mr_f6555_0(ValueStat size)
{    this.size = size;}
public Collection<EnumValueCount> parquet-mr_f6556_0()
{    Collection<EnumValueCount> values2 = values.getValues();    if (values2 == null) {        return null;    }    List<EnumValueCount> list = new ArrayList<EnumValueCount>(values2);    Collections.sort(list, new Comparator<EnumValueCount>() {        @Override        public int compare(EnumValueCount o1, EnumValueCount o2) {            return o2.getCount() - o1.getCount();        }    });    return list;}
public int parquet-mr_f6557_0(EnumValueCount o1, EnumValueCount o2)
{    return o2.getCount() - o1.getCount();}
public void parquet-mr_f6558_0(Collection<EnumValueCount> values)
{    this.values.setValues(values);}
public Tuple parquet-mr_f6559_0(Tuple t) throws IOException
{    return new JSONTuple(sumUp(getInputSchema(), t));}
public Tuple parquet-mr_f6560_0(Tuple t) throws IOException
{    return new JSONTuple(merge(t));}
public String parquet-mr_f6561_0(Tuple t) throws IOException
{    return SummaryData.toPrettyJSON(merge(t));}
public void parquet-mr_f6562_0(DataInput dataInput) throws IOException
{    throw new UnsupportedOperationException();}
public void parquet-mr_f6563_0(DataOutput dataOutput) throws IOException
{    Tuple t = TF.newTuple(json());    t.write(dataOutput);}
public int parquet-mr_f6564_0(Object o)
{    throw new UnsupportedOperationException();}
public void parquet-mr_f6565_0(Object o)
{    throw new UnsupportedOperationException();}
public Object parquet-mr_f6566_0(int i) throws ExecException
{    if (i == 0) {        return json();    }    throw new ExecException();}
private String parquet-mr_f6567_0()
{    return SummaryData.toJSON(data);}
public List<Object> parquet-mr_f6568_0()
{    return new ArrayList<Object>(Arrays.asList(json()));}
public long parquet-mr_f6569_0()
{        return 100;}
public byte parquet-mr_f6570_0(int i) throws ExecException
{    if (i == 0) {        return DataType.CHARARRAY;    }    throw new ExecException("size is 1");}
public boolean parquet-mr_f6571_0(int i) throws ExecException
{    if (i == 0) {        return false;    }    throw new ExecException("size is 1");}
public void parquet-mr_f6572_0(Tuple t)
{    throw new UnsupportedOperationException();}
public void parquet-mr_f6573_0(int i, Object o) throws ExecException
{    throw new UnsupportedOperationException();}
public int parquet-mr_f6574_0()
{    return 1;}
public String parquet-mr_f6575_0(String delim) throws ExecException
{    return json();}
public Iterator<Object> parquet-mr_f6576_0()
{    return getAll().iterator();}
private static TupleSummaryData parquet-mr_f6577_0(Tuple tuple) throws ExecException
{    if (tuple instanceof JSONTuple) {        return ((JSONTuple) tuple).data;    } else {        return SummaryData.fromJSON((String) tuple.get(0), TupleSummaryData.class);    }}
private static TupleSummaryData parquet-mr_f6578_0(Tuple t) throws IOException
{    TupleSummaryData summaryData = new TupleSummaryData();    DataBag bag = (DataBag) t.get(0);    for (Tuple tuple : bag) {        summaryData.merge(getData(tuple));    }    return summaryData;}
private static TupleSummaryData parquet-mr_f6579_0(Schema schema, Tuple t) throws ExecException
{    TupleSummaryData summaryData = new TupleSummaryData();    DataBag bag = (DataBag) t.get(0);    for (Tuple tuple : bag) {        summaryData.addTuple(schema, tuple);    }    return summaryData;}
public String parquet-mr_f6580_0(Tuple t) throws IOException
{    return SummaryData.toPrettyJSON(sumUp(getInputSchema(), t));}
public String parquet-mr_f6581_0()
{    return Initial.class.getName();}
public String parquet-mr_f6582_0()
{    return Intermediate.class.getName();}
public String parquet-mr_f6583_0()
{    return Final.class.getName();}
public static String parquet-mr_f6584_0(SummaryData summaryData)
{    return toJSON(summaryData, objectMapper);}
public static String parquet-mr_f6585_0(SummaryData summaryData)
{    return toJSON(summaryData, prettyObjectMapper);}
private static String parquet-mr_f6586_0(SummaryData summaryData, ObjectMapper mapper)
{    StringWriter stringWriter = new StringWriter();    try {        mapper.writeValue(stringWriter, summaryData);    } catch (JsonGenerationException e) {        throw new RuntimeException(e);    } catch (JsonMappingException e) {        throw new RuntimeException(e);    } catch (IOException e) {        throw new RuntimeException(e);    }    return stringWriter.toString();}
public static T parquet-mr_f6587_0(String json, Class<T> clazz)
{    try {        return objectMapper.readValue(new StringReader(json), clazz);    } catch (JsonParseException e) {        throw new RuntimeException(e);    } catch (JsonMappingException e) {        throw new RuntimeException(e);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public static T parquet-mr_f6588_0(T s1, T s2)
{    if (s1 == null) {        return s2;    } else if (s2 == null) {        return s1;    } else {        s1.merge(s2);        return s1;    }}
protected FieldSchema parquet-mr_f6589_0(Schema schema, int i)
{    try {        if (schema == null || i >= schema.size()) {            return null;        }        FieldSchema field = schema.getField(i);        return field;    } catch (FrontendException e) {        throw new RuntimeException(e);    }}
protected Schema parquet-mr_f6590_0(FieldSchema field)
{    return field == null ? null : field.schema;}
protected String parquet-mr_f6591_0(FieldSchema field)
{    return field == null ? null : field.alias;}
public void parquet-mr_f6592_0(Object o)
{    ++count;}
public void parquet-mr_f6593_0(SummaryData other)
{    this.count += other.count;}
public long parquet-mr_f6594_0()
{    return count;}
public void parquet-mr_f6595_0(long count)
{    this.count = count;}
public String parquet-mr_f6596_0()
{    return toJSON(this);}
public void parquet-mr_f6597_0(Schema schema, Tuple tuple)
{    super.add(tuple);    int tupleSize = tuple.size();    size.add(tupleSize);    ensureSize(tupleSize);    for (int i = 0; i < tupleSize; i++) {        FieldSummaryData fieldSummaryData = fields.get(i);        try {            FieldSchema field = getField(schema, i);            fieldSummaryData.setName(getName(field));            Object o = tuple.get(i);            fieldSummaryData.add(getSchema(field), o);        } catch (ExecException e) {            LOG.log(Level.WARNING, "Can't get value from tuple", e);            fieldSummaryData.addError();        }    }}
private void parquet-mr_f6598_0(int sizeToEnsure)
{    while (fields.size() < sizeToEnsure) {        fields.add(new FieldSummaryData());    }}
public void parquet-mr_f6599_0(SummaryData other)
{    super.merge(other);    TupleSummaryData otherTupleSummaryData = (TupleSummaryData) other;    size.merge(otherTupleSummaryData.size);    ensureSize(otherTupleSummaryData.fields.size());    for (int i = 0; i < otherTupleSummaryData.fields.size(); i++) {        fields.get(i).merge(otherTupleSummaryData.fields.get(i));    }}
public List<FieldSummaryData> parquet-mr_f6600_0()
{    return fields;}
public void parquet-mr_f6601_0(List<FieldSummaryData> fields)
{    this.fields = fields;}
public ValueStat parquet-mr_f6602_0()
{    return size;}
public void parquet-mr_f6603_0(ValueStat size)
{    this.size = size;}
public void parquet-mr_f6604_0(double v)
{    total += v;    min = Math.min(min, v);    max = Math.max(max, v);}
public void parquet-mr_f6605_0(ValueStat other)
{    total += other.total;    min = Math.min(min, other.min);    max = Math.max(max, other.max);}
public double parquet-mr_f6606_0()
{    return total;}
public void parquet-mr_f6607_0(double total)
{    this.total = total;}
public double parquet-mr_f6608_0()
{    return min;}
public void parquet-mr_f6609_0(double min)
{    this.min = min;}
public double parquet-mr_f6610_0()
{    return max;}
public void parquet-mr_f6611_0(double max)
{    this.max = max;}
 static Schema parquet-mr_f6612_0(Configuration configuration)
{    return parsePigSchema(configuration.get(PARQUET_PIG_SCHEMA));}
 static RequiredFieldList parquet-mr_f6613_0(Configuration configuration)
{    String requiredFieldString = configuration.get(PARQUET_PIG_REQUIRED_FIELDS);    if (requiredFieldString == null) {        return null;    }    try {        return (RequiredFieldList) ObjectSerializer.deserialize(requiredFieldString);    } catch (IOException iOException) {        throw new RuntimeException("Failed to deserialize pushProjection");    }}
 static Schema parquet-mr_f6614_0(MessageType fileSchema, Map<String, Set<String>> keyValueMetaData)
{    Set<String> pigSchemas = PigMetaData.getPigSchemas(keyValueMetaData);    if (pigSchemas == null) {        return pigSchemaConverter.convert(fileSchema);    }    Schema mergedPigSchema = null;    for (String pigSchemaString : pigSchemas) {        try {            mergedPigSchema = union(mergedPigSchema, parsePigSchema(pigSchemaString));        } catch (FrontendException e) {            throw new ParquetDecodingException("can not merge " + pigSchemaString + " into " + mergedPigSchema, e);        }    }    return mergedPigSchema;}
 static Schema parquet-mr_f6615_0(MessageType fileSchema, Map<String, String> keyValueMetaData)
{    PigMetaData pigMetaData = PigMetaData.fromMetaData(keyValueMetaData);    if (pigMetaData == null) {        return pigSchemaConverter.convert(fileSchema);    }    return parsePigSchema(pigMetaData.getPigSchema());}
private static Schema parquet-mr_f6616_0(Schema merged, Schema pigSchema) throws FrontendException
{    List<FieldSchema> fields = new ArrayList<Schema.FieldSchema>();    if (merged == null) {        return pigSchema;    }        for (FieldSchema fieldSchema : merged.getFields()) {        FieldSchema newFieldSchema = pigSchema.getField(fieldSchema.alias);        if (newFieldSchema == null) {            fields.add(fieldSchema);        } else {            fields.add(union(fieldSchema, newFieldSchema));        }    }        for (FieldSchema newFieldSchema : pigSchema.getFields()) {        FieldSchema oldFieldSchema = merged.getField(newFieldSchema.alias);        if (oldFieldSchema == null) {            fields.add(newFieldSchema);        }    }    return new Schema(fields);}
private static FieldSchema parquet-mr_f6617_0(FieldSchema mergedFieldSchema, FieldSchema newFieldSchema)
{    if (!mergedFieldSchema.alias.equals(newFieldSchema.alias) || mergedFieldSchema.type != newFieldSchema.type) {        throw new IncompatibleSchemaModificationException("Incompatible Pig schema change: " + mergedFieldSchema + " can not accept");    }    try {        return new FieldSchema(mergedFieldSchema.alias, union(mergedFieldSchema.schema, newFieldSchema.schema), mergedFieldSchema.type);    } catch (FrontendException e) {        throw new SchemaConversionException(e);    }}
public ReadContext parquet-mr_f6618_0(InitContext initContext)
{    Schema pigSchema = getPigSchema(initContext.getConfiguration());    RequiredFieldList requiredFields = getRequiredFields(initContext.getConfiguration());    boolean columnIndexAccess = initContext.getConfiguration().getBoolean(PARQUET_COLUMN_INDEX_ACCESS, false);    if (pigSchema == null) {        return new ReadContext(initContext.getFileSchema());    } else {                MessageType parquetRequestedSchema = new PigSchemaConverter(columnIndexAccess).filter(initContext.getFileSchema(), pigSchema, requiredFields);        return new ReadContext(parquetRequestedSchema);    }}
public static TupleWriteSupport parquet-mr_f6620_0(String pigSchemaString) throws ParserException
{    return new TupleWriteSupport(Utils.getSchemaFromString(pigSchemaString));}
public String parquet-mr_f6621_0()
{    return "pig";}
public Schema parquet-mr_f6622_0()
{    return rootPigSchema;}
public MessageType parquet-mr_f6623_0()
{    return rootSchema;}
public WriteContext parquet-mr_f6624_0(Configuration configuration)
{    Map<String, String> extraMetaData = new HashMap<String, String>();    new PigMetaData(rootPigSchema).addToMetaData(extraMetaData);    return new WriteContext(rootSchema, extraMetaData);}
public void parquet-mr_f6625_0(RecordConsumer recordConsumer)
{    this.recordConsumer = recordConsumer;}
public void parquet-mr_f6626_0(Tuple t)
{    try {        recordConsumer.startMessage();        writeTuple(rootSchema, rootPigSchema, t);        recordConsumer.endMessage();    } catch (ExecException e) {        throw new RuntimeException(e);    } catch (FrontendException e) {        throw new RuntimeException(e);    }}
private void parquet-mr_f6627_0(GroupType schema, Schema pigSchema, Tuple t) throws ExecException, FrontendException
{    List<Type> fields = schema.getFields();    List<FieldSchema> pigFields = pigSchema.getFields();    assert fields.size() == pigFields.size();    for (int i = 0; i < fields.size(); i++) {        if (t.isNull(i)) {            continue;        }        Type fieldType = fields.get(i);        recordConsumer.startField(fieldType.getName(), i);        FieldSchema pigType = pigFields.get(i);        switch(pigType.type) {            case DataType.BAG:                Type bagType = fieldType.asGroupType().getType(0);                FieldSchema pigBagInnerType = pigType.schema.getField(0);                DataBag bag = (DataBag) t.get(i);                recordConsumer.startGroup();                if (bag.size() > 0) {                    recordConsumer.startField(bagType.getName(), 0);                    for (Tuple tuple : bag) {                        if (bagType.isPrimitive()) {                            writeValue(bagType, pigBagInnerType, tuple, 0);                        } else {                            recordConsumer.startGroup();                            writeTuple(bagType.asGroupType(), pigBagInnerType.schema, tuple);                            recordConsumer.endGroup();                        }                    }                    recordConsumer.endField(bagType.getName(), 0);                }                recordConsumer.endGroup();                break;            case DataType.MAP:                Type mapType = fieldType.asGroupType().getType(0);                FieldSchema pigMapInnerType = pigType.schema.getField(0);                                @SuppressWarnings("unchecked")                Map<String, Object> map = (Map<String, Object>) t.get(i);                recordConsumer.startGroup();                if (map.size() > 0) {                    recordConsumer.startField(mapType.getName(), 0);                    Set<Entry<String, Object>> entrySet = map.entrySet();                    for (Entry<String, Object> entry : entrySet) {                        recordConsumer.startGroup();                        Schema keyValueSchema = new Schema(Arrays.asList(new FieldSchema("key", DataType.CHARARRAY), new FieldSchema("value", pigMapInnerType.schema, pigMapInnerType.type)));                        writeTuple(mapType.asGroupType(), keyValueSchema, TF.newTuple(Arrays.asList(entry.getKey(), entry.getValue())));                        recordConsumer.endGroup();                    }                    recordConsumer.endField(mapType.getName(), 0);                }                recordConsumer.endGroup();                break;            default:                writeValue(fieldType, pigType, t, i);                break;        }        recordConsumer.endField(fieldType.getName(), i);    }}
private void parquet-mr_f6628_0(Type type, FieldSchema pigType, Tuple t, int i)
{    try {        if (type.isPrimitive()) {            switch(type.asPrimitiveType().getPrimitiveTypeName()) {                                case BINARY:                    byte[] bytes;                    if (pigType.type == DataType.BYTEARRAY) {                        bytes = ((DataByteArray) t.get(i)).get();                    } else if (pigType.type == DataType.CHARARRAY) {                        bytes = ((String) t.get(i)).getBytes("UTF-8");                    } else {                        throw new UnsupportedOperationException("can not convert from " + DataType.findTypeName(pigType.type) + " to BINARY ");                    }                    recordConsumer.addBinary(Binary.fromReusedByteArray(bytes));                    break;                case BOOLEAN:                    recordConsumer.addBoolean((Boolean) t.get(i));                    break;                case INT32:                    recordConsumer.addInteger(((Number) t.get(i)).intValue());                    break;                case INT64:                    recordConsumer.addLong(((Number) t.get(i)).longValue());                    break;                case DOUBLE:                    recordConsumer.addDouble(((Number) t.get(i)).doubleValue());                    break;                case FLOAT:                    recordConsumer.addFloat(((Number) t.get(i)).floatValue());                    break;                default:                    throw new UnsupportedOperationException(type.asPrimitiveType().getPrimitiveTypeName().name());            }        } else {            assert pigType.type == DataType.TUPLE;            recordConsumer.startGroup();            writeTuple(type.asGroupType(), pigType.schema, (Tuple) t.get(i));            recordConsumer.endGroup();        }    } catch (Exception e) {        throw new ParquetEncodingException("can not write value at " + i + " in tuple " + t + " from type '" + pigType + "' to type '" + type + "'", e);    }}
public static void parquet-mr_f6629_0(String[] args) throws Exception
{    StringBuilder schemaString = new StringBuilder("a0: chararray");    for (int i = 1; i < COLUMN_COUNT; i++) {        schemaString.append(", a" + i + ": chararray");    }    String out = "target/PerfTest";    {        PigServer pigServer = new PigServer(ExecType.LOCAL);        Data data = Storage.resetData(pigServer);        Collection<Tuple> list = new ArrayList<Tuple>();        for (int i = 0; i < ROW_COUNT; i++) {            Tuple tuple = TupleFactory.getInstance().newTuple(COLUMN_COUNT);            for (int j = 0; j < COLUMN_COUNT; j++) {                tuple.set(j, "a" + i + "_" + j);            }            list.add(tuple);        }        data.set("in", schemaString.toString(), list);        pigServer.setBatchOn();        pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");        pigServer.deleteFile(out);        pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");        if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {            throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());        }    }    load(out, 1);    load(out, 2);    load(out, 3);    load(out, 4);    load(out, 5);    load(out, 10);    load(out, 20);    load(out, 50);    System.out.println(results);}
private static void parquet-mr_f6630_0(String out, int colsToLoad) throws ExecException, IOException
{    long t0 = System.currentTimeMillis();    StringBuilder schemaString = new StringBuilder("a0: chararray");    for (int i = 1; i < colsToLoad; i++) {        schemaString.append(", a" + i + ": chararray");    }    PigServer pigServer = new PigServer(ExecType.LOCAL);    pigServer.registerQuery("B = LOAD '" + out + "' USING " + ParquetLoader.class.getName() + "('" + schemaString + "');");    pigServer.registerQuery("C = FOREACH (GROUP B ALL) GENERATE COUNT(B);");    Iterator<Tuple> it = pigServer.openIterator("C");    if (!it.hasNext()) {        throw new RuntimeException("Job failed: no tuple to read");    }    Long count = (Long) it.next().get(0);    assertEquals(ROW_COUNT, count.longValue());    long t1 = System.currentTimeMillis();    results.append((t1 - t0) + " ms to read " + colsToLoad + " columns\n");}
public static void parquet-mr_f6631_0(String[] args) throws Exception
{    StringBuilder results = new StringBuilder();    String out = "target/PerfTest2";    File outDir = new File(out);    if (outDir.exists()) {        clean(outDir);    }    write(out);    for (int i = 0; i < 2; i++) {        load(out, 1, results);        load(out, 2, results);        load(out, 3, results);        load(out, 4, results);        load(out, 5, results);        load(out, 10, results);        load(out, 20, results);        load(out, 50, results);        results.append("\n");    }    System.out.println(results);}
public static void parquet-mr_f6632_0(String out) throws IOException, ParserException, InterruptedException, ExecException
{    {        StringBuilder schemaString = new StringBuilder("a0: chararray");        for (int i = 1; i < COLUMN_COUNT; i++) {            schemaString.append(", a" + i + ": chararray");        }        String location = out;        String schema = schemaString.toString();        StoreFuncInterface storer = new ParquetStorer();        Job job = new Job(conf);        storer.setStoreFuncUDFContextSignature("sig");        String absPath = storer.relToAbsPathForStoreLocation(location, new Path(new File(".").getAbsoluteFile().toURI()));        storer.setStoreLocation(absPath, job);        storer.checkSchema(new ResourceSchema(Utils.getSchemaFromString(schema)));                @SuppressWarnings("unchecked")        OutputFormat<Void, Tuple> outputFormat = storer.getOutputFormat();                JobContext jobContext = ContextUtil.newJobContext(ContextUtil.getConfiguration(job), new JobID("jt", jobid++));        outputFormat.checkOutputSpecs(jobContext);        if (schema != null) {            ResourceSchema resourceSchema = new ResourceSchema(Utils.getSchemaFromString(schema));            storer.checkSchema(resourceSchema);            if (storer instanceof StoreMetadata) {                ((StoreMetadata) storer).storeSchema(resourceSchema, absPath, job);            }        }        TaskAttemptContext taskAttemptContext = ContextUtil.newTaskAttemptContext(ContextUtil.getConfiguration(job), new TaskAttemptID("jt", jobid, true, 1, 0));        RecordWriter<Void, Tuple> recordWriter = outputFormat.getRecordWriter(taskAttemptContext);        storer.prepareToWrite(recordWriter);        for (int i = 0; i < ROW_COUNT; i++) {            Tuple tuple = TupleFactory.getInstance().newTuple(COLUMN_COUNT);            for (int j = 0; j < COLUMN_COUNT; j++) {                tuple.set(j, "a" + i + "_" + j);            }            storer.putNext(tuple);        }        recordWriter.close(taskAttemptContext);        OutputCommitter outputCommitter = outputFormat.getOutputCommitter(taskAttemptContext);        outputCommitter.commitTask(taskAttemptContext);        outputCommitter.commitJob(jobContext);    }}
 static void parquet-mr_f6633_0(File outDir)
{    if (outDir.isDirectory()) {        File[] listFiles = outDir.listFiles();        for (File file : listFiles) {            clean(file);        }    }    outDir.delete();}
 static void parquet-mr_f6634_0(String out, int colsToLoad, StringBuilder results) throws Exception
{    StringBuilder schemaString = new StringBuilder("a0: chararray");    for (int i = 1; i < colsToLoad; i++) {        schemaString.append(", a" + i + ": chararray");    }    long t0 = System.currentTimeMillis();    Job job = new Job(conf);    int loadjobId = jobid++;    LoadFunc loadFunc = new ParquetLoader(schemaString.toString());    loadFunc.setUDFContextSignature("sigLoader" + loadjobId);    String absPath = loadFunc.relativeToAbsolutePath(out, new Path(new File(".").getAbsoluteFile().toURI()));    loadFunc.setLocation(absPath, job);        @SuppressWarnings("unchecked")    InputFormat<Void, Tuple> inputFormat = loadFunc.getInputFormat();    JobContext jobContext = ContextUtil.newJobContext(ContextUtil.getConfiguration(job), new JobID("jt", loadjobId));    List<InputSplit> splits = inputFormat.getSplits(jobContext);    int i = 0;    int taskid = 0;    for (InputSplit split : splits) {        TaskAttemptContext taskAttemptContext = ContextUtil.newTaskAttemptContext(ContextUtil.getConfiguration(job), new TaskAttemptID("jt", loadjobId, true, taskid++, 0));        RecordReader<Void, Tuple> recordReader = inputFormat.createRecordReader(split, taskAttemptContext);        loadFunc.prepareToRead(recordReader, null);        recordReader.initialize(split, taskAttemptContext);        Tuple t;        while ((t = loadFunc.getNext()) != null) {            if (DEBUG)                System.out.println(t);            ++i;        }    }    assertEquals(ROW_COUNT, i);    long t1 = System.currentTimeMillis();    results.append((t1 - t0) + " ms to read " + colsToLoad + " columns\n");}
public static void parquet-mr_f6635_0(String[] args) throws Exception
{    StringBuilder results = new StringBuilder();    String out = "target/PerfTestReadAllCols";    File outDir = new File(out);    if (outDir.exists()) {        PerfTest2.clean(outDir);    }    PerfTest2.write(out);    for (int i = 0; i < 5; i++) {        PerfTest2.load(out, PerfTest2.COLUMN_COUNT, results);        results.append("\n");    }    System.out.println(results);}
public static Tuple parquet-mr_f6636_0(Object... objects)
{    return tf.newTuple(Arrays.asList(objects));}
public static DataBag parquet-mr_f6637_0(Tuple... tuples)
{    return bf.newDefaultBag(Arrays.asList(tuples));}
public static Map<String, Object> parquet-mr_f6638_0(Object... objects)
{    Map<String, Object> m = new HashMap<String, Object>();    for (int i = 0; i < objects.length; i += 2) {        m.put((String) objects[i], objects[i + 1]);    }    return m;}
public void parquet-mr_f6639_0() throws IOException
{    Summary summary = new Summary();    String result = summary.exec(t(TEST_BAG));    validate(result, 1);}
public void parquet-mr_f6640_0() throws IOException
{    Summary.Initial initial = new Summary.Initial();    Summary.Intermediate intermediate1 = new Summary.Intermediate();    Summary.Intermediate intermediate2 = new Summary.Intermediate();    Summary.Final finall = new Summary.Final();    DataBag combinedRedIn = bf.newDefaultBag();    for (int r = 0; r < 5; r++) {        DataBag combinedMapOut = bf.newDefaultBag();        for (int m = 0; m < 5; m++) {            DataBag mapOut = bf.newDefaultBag();            for (Tuple t : TEST_BAG) {                Tuple exec = initial.exec(t(b(t)));                mapOut.add(exec);            }            Tuple exec = intermediate1.exec(t(mapOut));            validate((String) exec.get(0), 1);            combinedMapOut.add(exec);        }        combinedRedIn.add(intermediate2.exec(t(combinedMapOut)));    }    String result = finall.exec(t(combinedRedIn));    validate(result, 5 * 5);}
private void parquet-mr_f6641_0(String result, int factor) throws IOException
{    TupleSummaryData s = SummaryData.fromJSON(result, TupleSummaryData.class);        assertEquals(9 * factor, s.getCount());    assertEquals(1 * factor, s.getFields().get(0).getNull().longValue());    assertEquals(7 * factor, s.getFields().get(0).getBag().getCount());    assertEquals(18 * factor, s.getFields().get(0).getBag().getContent().getTuple().getFields().get(0).getCount());    MapSummaryData map = s.getFields().get(0).getBag().getContent().getTuple().getFields().get(1).getMap();    assertEquals(2 * factor, map.getCount());    assertEquals(3 * factor, map.getKey().getCount());}
public void parquet-mr_f6642_0() throws Exception
{    PigServer pigServer = new PigServer(ExecType.LOCAL);    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < 1002; i++) {        list.add(t("a", "b" + i, 1l, b(t("a", m("foo", "bar")))));    }    data.set("in", "a:chararray, a1:chararray, b:int, c:{t:(a2:chararray, b2:[])}", list);    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.registerQuery("B = FOREACH (GROUP A ALL) GENERATE " + Summary.class.getName() + "(A);");    pigServer.registerQuery("STORE B INTO 'out' USING mock.Storage();");    System.out.println(data.get("out").get(0).get(0));    TupleSummaryData s = SummaryData.fromJSON((String) data.get("out").get(0).get(0), TupleSummaryData.class);    System.out.println(s);}
public void parquet-mr_f6643_0() throws Exception
{    PigServer pigServer = new PigServer(ExecType.LOCAL);    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < 10; i++) {        list.add(t("a", i - 9));    }    data.set("in", "a:chararray, b:int", list);    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.registerQuery("B = FOREACH (GROUP A ALL) GENERATE " + Summary.class.getName() + "(A);");    pigServer.registerQuery("STORE B INTO 'out' USING mock.Storage();");    TupleSummaryData s = SummaryData.fromJSON((String) data.get("out").get(0).get(0), TupleSummaryData.class);    System.out.println(s);    assertEquals(0, s.getFields().get(1).getNumber().getValue().getMax(), 0);}
private void parquet-mr_f6644_0(double value, int precision, int scale, String stringValue)
{    String originalString = Double.toString(value);    BigDecimal originalValue = new BigDecimal(originalString);    BigDecimal convertedValue = DecimalUtils.binaryToDecimal(Binary.fromByteArray(originalValue.unscaledValue().toByteArray()), precision, scale);    assertEquals(stringValue, convertedValue.toString());}
private void parquet-mr_f6645_0(int value, int precision, int scale, String stringValue)
{    String originalString = Integer.toString(value);    BigDecimal originalValue = new BigDecimal(originalString);    BigDecimal convertedValue = DecimalUtils.binaryToDecimal(Binary.fromByteArray(originalValue.unscaledValue().toByteArray()), precision, scale);    assertEquals(stringValue, convertedValue.toString());}
private void parquet-mr_f6646_0(long value, int precision, int scale, String stringValue)
{    String originalString = Long.toString(value);    BigDecimal originalValue = new BigDecimal(originalString);    BigDecimal convertedValue = DecimalUtils.binaryToDecimal(Binary.fromByteArray(originalValue.unscaledValue().toByteArray()), precision, scale);    assertEquals(stringValue, convertedValue.toString());}
public void parquet-mr_f6647_0() throws Exception
{                testDecimalConversion(Long.MAX_VALUE, 19, 0, "9223372036854775807");    testDecimalConversion(Long.MIN_VALUE, 19, 0, "-9223372036854775808");    testDecimalConversion(0L, 0, 0, "0.0");        testDecimalConversion(Integer.MAX_VALUE, 10, 0, "2147483647");    testDecimalConversion(Integer.MIN_VALUE, 10, 0, "-2147483648");    testDecimalConversion(0, 0, 0, "0.0");        testDecimalConversion(12345678912345678d, 17, 0, "12345678912345678");    testDecimalConversion(123456789123456.78, 17, 2, "123456789123456.78");    testDecimalConversion(0.12345678912345678, 17, 17, "0.12345678912345678");    testDecimalConversion(-0.000102, 6, 6, "-0.000102");}
public void parquet-mr_f6648_0() throws Exception
{    String location = "garbage";    ParquetLoader pLoader = new ParquetLoader("a:chararray, " + "b:{t:(c:chararray, d:chararray)}, " + "p:[(q:chararray, r:chararray)]");    Job job = new Job();    pLoader.getSchema(location, job);    RequiredFieldList list = new RequiredFieldList();    RequiredField field = new RequiredField("a", 0, null, DataType.CHARARRAY);    list.add(field);    field = new RequiredField("b", 0, Arrays.asList(new RequiredField("t", 0, Arrays.asList(new RequiredField("d", 1, null, DataType.CHARARRAY)), DataType.TUPLE)), DataType.BAG);    list.add(field);    pLoader.pushProjection(list);    pLoader.setLocation(location, job);    assertEquals("{a: chararray,b: {t: (d: chararray)}}", TupleReadSupport.getPigSchema(job.getConfiguration()).toString());}
public void parquet-mr_f6649_0() throws Exception
{    PigServer pigServer = new PigServer(ExecType.LOCAL);    pigServer.setValidateEachStatement(true);    String out = "target/out";    int rows = 10;    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple(i, "a" + i, i * 2));    }    data.set("in", "i:int, a:chararray, b:int", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    pigServer.executeBatch();    List<Tuple> expectedList = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        expectedList.add(Storage.tuple("a" + i));    }    pigServer.registerQuery("C = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "();");    pigServer.registerQuery("D = foreach C generate a;");    pigServer.registerQuery("Store D into 'out' using mock.Storage();");    pigServer.executeBatch();    List<Tuple> actualList = data.get("out");    pigServer.registerQuery("C = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('a:chararray, b:int');");    Assert.assertEquals("{a: chararray,b: int}", pigServer.dumpSchema("C").toString());    try {        pigServer.registerQuery("D = foreach C generate i;");        Assert.fail("Frontend Exception expected");    } catch (FrontendException fe) {    }                pigServer = new PigServer(ExecType.LOCAL);    data = Storage.resetData(pigServer);    pigServer.setBatchOn();    pigServer.registerQuery("C = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('a:chararray, b:int');");    pigServer.registerQuery("D = foreach C generate a;");    pigServer.registerQuery("Store D into 'out' using mock.Storage();");    pigServer.executeBatch();    actualList = data.get("out");    Assert.assertEquals(expectedList, actualList);}
public void parquet-mr_f6650_0() throws Exception
{    PigServer pigServer = new PigServer(ExecType.LOCAL);    pigServer.setValidateEachStatement(true);    String out = "target/out";    int rows = 10;    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple(i, "a" + i, i * 2));    }    data.set("in", "i:int, a:chararray, b:int", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    pigServer.executeBatch();        pigServer.registerQuery("C = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('i:int, a:chararray, b:int, n1:int, n2:chararray');");    pigServer.registerQuery("STORE C into 'out' using mock.Storage();");    pigServer.executeBatch();    List<Tuple> actualList = data.get("out");    assertEquals(rows, actualList.size());    for (Tuple t : actualList) {        assertTrue(t.isNull(3));        assertTrue(t.isNull(4));    }        pigServer.registerQuery("D = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('n1:int, a:chararray, n2:chararray, b:int');");    pigServer.registerQuery("STORE D into 'out2' using mock.Storage();");    pigServer.executeBatch();    actualList = data.get("out2");    assertEquals(rows, actualList.size());    for (Tuple t : actualList) {        assertTrue(t.isNull(0));        assertTrue(t.isNull(2));    }}
public void parquet-mr_f6651_0() throws Exception
{    PigServer pigServer = new PigServer(ExecType.LOCAL);    pigServer.setValidateEachStatement(true);    String out = "target/out";    int rows = 10;    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple(i, "a" + i, i * 2));    }    data.set("in", "i:int, a:chararray, b:int", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    pigServer.executeBatch();        pigServer.registerQuery("C = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('i:int, a:chararray, b:int, n1:int, n2:chararray');");    pigServer.registerQuery("G = foreach C generate n1,b,n2,i;");    pigServer.registerQuery("STORE G into 'out' using mock.Storage();");    pigServer.executeBatch();    List<Tuple> actualList = data.get("out");    assertEquals(rows, actualList.size());    for (Tuple t : actualList) {        assertEquals(4, t.size());        assertTrue(t.isNull(0));        assertTrue(t.isNull(2));    }}
public void parquet-mr_f6652_0() throws Exception
{    Properties p = new Properties();    p.setProperty(STRICT_TYPE_CHECKING, Boolean.FALSE.toString());    PigServer pigServer = new PigServer(ExecType.LOCAL, p);    pigServer.setValidateEachStatement(true);    String out = "target/out";    int rows = 10;    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple(i, (long) i, (float) i, (double) i, Integer.toString(i), Boolean.TRUE));    }    data.set("in", "i:int, l:long, f:float, d:double, s:chararray, b:boolean", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    pigServer.executeBatch();    List<Tuple> actualList = null;    byte[] types = { INTEGER, LONG, FLOAT, DOUBLE, CHARARRAY, BOOLEAN };        for (int i = 0; i < types.length; i++) {        String query = "B = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('i:" + DataType.findTypeName(types[i % types.length]) + "," + "  l:" + DataType.findTypeName(types[(i + 1) % types.length]) + "," + "  f:" + DataType.findTypeName(types[(i + 2) % types.length]) + "," + "  d:" + DataType.findTypeName(types[(i + 3) % types.length]) + "," + "  s:" + DataType.findTypeName(types[(i + 4) % types.length]) + "," + "  b:" + DataType.findTypeName(types[(i + 5) % types.length]) + "');";        System.out.println("Query: " + query);        pigServer.registerQuery(query);        pigServer.registerQuery("STORE B into 'out" + i + "' using mock.Storage();");        pigServer.executeBatch();        actualList = data.get("out" + i);        assertEquals(rows, actualList.size());        for (Tuple t : actualList) {            assertTrue(t.getType(0) == types[i % types.length]);            assertTrue(t.getType(1) == types[(i + 1) % types.length]);            assertTrue(t.getType(2) == types[(i + 2) % types.length]);            assertTrue(t.getType(3) == types[(i + 3) % types.length]);            assertTrue(t.getType(4) == types[(i + 4) % types.length]);            assertTrue(t.getType(5) == types[(i + 5) % types.length]);        }    }}
public void parquet-mr_f6653_0() throws Exception
{    PigServer pigServer = new PigServer(ExecType.LOCAL);    pigServer.setValidateEachStatement(true);    String out = "target/out";    int rows = 10;    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple(i, i * 1.0, i * 2L, "v" + i));    }    data.set("in", "c1:int, c2:double, c3:long, c4:chararray", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    pigServer.executeBatch();        pigServer.registerQuery("B = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('n1:int, n2:double, n3:long, n4:chararray', 'true');");    pigServer.registerQuery("STORE B into 'out' using mock.Storage();");    pigServer.executeBatch();    List<Tuple> actualList = data.get("out");    assertEquals(rows, actualList.size());    for (int i = 0; i < rows; i++) {        Tuple t = actualList.get(i);        assertEquals(4, t.size());        assertEquals(i, t.get(0));        assertEquals(i * 1.0, t.get(1));        assertEquals(i * 2L, t.get(2));        assertEquals("v" + i, t.get(3));    }}
public void parquet-mr_f6654_0() throws Exception
{    PigServer pigServer = new PigServer(ExecType.LOCAL);    pigServer.setValidateEachStatement(true);    String out = "target/out";    int rows = 10;    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple(i, i * 1.0, i * 2L, "v" + i));    }    data.set("in", "c1:int, c2:double, c3:long, c4:chararray", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    pigServer.executeBatch();    pigServer.registerQuery("B = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('n1:int, n2:double, n3:long, n4:chararray', 'true');");    pigServer.registerQuery("C = foreach B generate n1, n3;");    pigServer.registerQuery("STORE C into 'out' using mock.Storage();");    pigServer.executeBatch();    List<Tuple> actualList = data.get("out");    assertEquals(rows, actualList.size());    for (int i = 0; i < rows; i++) {        Tuple t = actualList.get(i);        assertEquals(2, t.size());        assertEquals(i, t.get(0));        assertEquals(i * 2L, t.get(1));    }}
public void parquet-mr_f6655_0() throws Exception
{    Configuration conf = new Configuration();    conf.setBoolean(ParquetLoader.ENABLE_PREDICATE_FILTER_PUSHDOWN, true);    PigServer pigServer = new PigServer(ExecType.LOCAL, conf);    pigServer.setValidateEachStatement(true);    String out = "target/out";    String out2 = "target/out2";    int rows = 10;    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple(i, i * 1.0, i * 2L, "v" + i));    }    data.set("in", "c1:int, c2:double, c3:long, c4:chararray", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    pigServer.executeBatch();    pigServer.deleteFile(out2);    pigServer.registerQuery("B = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('c1:int, c2:double, c3:long, c4:chararray');");    pigServer.registerQuery("C = FILTER B by c1 == 1 or c1 == 5;");    pigServer.registerQuery("STORE C into '" + out2 + "' using mock.Storage();");    List<ExecJob> jobs = pigServer.executeBatch();    long recordsRead = jobs.get(0).getStatistics().getInputStats().get(0).getNumberRecords();    assertEquals(2, recordsRead);}
public void parquet-mr_f6656_0() throws ExecException, Exception
{    String out = "target/out";    int rows = 1000;    Properties props = new Properties();    props.setProperty("parquet.compression", "uncompressed");    props.setProperty("parquet.page.size", "1000");    PigServer pigServer = new PigServer(ExecType.LOCAL, props);    Data data = Storage.resetData(pigServer);    Collection<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(tuple("a" + i));    }    data.set("in", "a:chararray", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }    pigServer.registerQuery("B = LOAD '" + out + "' USING " + ParquetLoader.class.getName() + "();");    pigServer.registerQuery("Store B into 'out' using mock.Storage();");    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }    List<Tuple> result = data.get("out");    assertEquals(rows, result.size());    int i = 0;    for (Tuple tuple : result) {        assertEquals("a" + i, tuple.get(0));        ++i;    }}
public void parquet-mr_f6657_0() throws ExecException, Exception
{    String out = "target/out";    int rows = 1000;    Properties props = new Properties();    props.setProperty("parquet.compression", "uncompressed");    props.setProperty("parquet.page.size", "1000");    PigServer pigServer = new PigServer(ExecType.LOCAL, props);    Data data = Storage.resetData(pigServer);    Collection<Tuple> list1 = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list1.add(tuple("a" + i));    }    Collection<Tuple> list2 = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list2.add(tuple("b" + i));    }    data.set("a", "a:chararray", list1);    data.set("b", "b:chararray", list2);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'a' USING mock.Storage();");    pigServer.registerQuery("B = LOAD 'b' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "/a' using " + ParquetStorer.class.getName() + "();");    pigServer.registerQuery("Store B into '" + out + "/b' using " + ParquetStorer.class.getName() + "();");    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }    pigServer.registerQuery("B = LOAD '" + out + "/*' USING " + ParquetLoader.class.getName() + "();");    pigServer.registerQuery("Store B into 'out' using mock.Storage();");    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }    List<Tuple> result = data.get("out");    final Schema schema = data.getSchema("out");    assertEquals(2, schema.size());        int ai;    int bi;    if ("a".equals(schema.getField(0).alias)) {        ai = 0;        bi = 1;        assertEquals("a", schema.getField(0).alias);        assertEquals("b", schema.getField(1).alias);    } else {        ai = 1;        bi = 0;        assertEquals("b", schema.getField(0).alias);        assertEquals("a", schema.getField(1).alias);    }    assertEquals(rows * 2, result.size());    int a = 0;    int b = 0;    for (Tuple tuple : result) {        String fa = (String) tuple.get(ai);        String fb = (String) tuple.get(bi);        if (fa != null) {            assertEquals("a" + a, fa);            ++a;        }        if (fb != null) {            assertEquals("b" + b, fb);            ++b;        }    }}
public void parquet-mr_f6658_0() throws ExecException, Exception
{    String out = "target/out";    int rows = 1000;    Properties props = new Properties();    props.setProperty("parquet.compression", "gzip");    props.setProperty("parquet.page.size", "1000");    PigServer pigServer = new PigServer(ExecType.LOCAL, props);    Data data = Storage.resetData(pigServer);    Collection<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple("a" + i));    }    data.set("in", "a:chararray", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }    pigServer.registerQuery("B = LOAD '" + out + "' USING " + ParquetLoader.class.getName() + "();");    pigServer.registerQuery("Store B into 'out' using mock.Storage();");    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }    List<Tuple> result = data.get("out");    assertEquals(rows, result.size());    int i = 0;    for (Tuple tuple : result) {        assertEquals("a" + i, tuple.get(0));        ++i;    }}
public void parquet-mr_f6659_0() throws ExecException, Exception
{    String out = "target/out";    PigServer pigServer = new PigServer(ExecType.LOCAL);    Data data = Storage.resetData(pigServer);    Collection<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < 1000; i++) {        list.add(tuple("a" + i, bag(tuple("o", "b"))));    }    for (int i = 10; i < 2000; i++) {        list.add(tuple("a" + i, bag(tuple("o", "b"), tuple("o", "b"), tuple("o", "b"), tuple("o", "b"))));    }    for (int i = 20; i < 3000; i++) {        list.add(tuple("a" + i, bag(tuple("o", "b"), tuple("o", null), tuple(null, "b"), tuple(null, null))));    }    for (int i = 30; i < 4000; i++) {        list.add(tuple("a" + i, null));    }    Collections.shuffle((List<?>) list);    data.set("in", "a:chararray, b:{t:(c:chararray, d:chararray)}", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }    {        pigServer.registerQuery("B = LOAD '" + out + "' USING " + ParquetLoader.class.getName() + "();");        pigServer.registerQuery("Store B into 'out' using mock.Storage();");        if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {            throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());        }        List<Tuple> result = data.get("out");        assertEquals(list, result);        final Schema schema = data.getSchema("out");        assertEquals("{a:chararray, b:{t:(c:chararray, d:chararray)}}".replaceAll(" ", ""), schema.toString().replaceAll(" ", ""));    }    {        pigServer.registerQuery("C = LOAD '" + out + "' USING " + ParquetLoader.class.getName() + "('a:chararray');");        pigServer.registerQuery("Store C into 'out2' using mock.Storage();");        if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {            throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());        }        final Function<Tuple, Object> grabFirstColumn = new Function<Tuple, Object>() {            @Override            public Object apply(Tuple input) {                try {                    return input.get(0);                } catch (ExecException e) {                    throw new RuntimeException(e);                }            }        };        List<Tuple> result2 = data.get("out2");                Object[] result2int = Collections2.transform(result2, grabFirstColumn).toArray();        Object[] input2int = Collections2.transform(list, grabFirstColumn).toArray();        assertArrayEquals(input2int, result2int);    }}
public Object parquet-mr_f6660_0(Tuple input)
{    try {        return input.get(0);    } catch (ExecException e) {        throw new RuntimeException(e);    }}
private void parquet-mr_f6661_0(String pigSchemaString) throws Exception
{    Schema pigSchema = Utils.getSchemaFromString(pigSchemaString);    MessageType parquetSchema = pigSchemaConverter.convert(pigSchema);    Schema convertedSchema = pigSchemaConverter.convert(parquetSchema);    assertEquals(pigSchema, convertedSchema);}
public void parquet-mr_f6662_0() throws Exception
{    testPigConversion("b:{t:(a:int)}");}
public void parquet-mr_f6663_0() throws Exception
{    testPigConversion("x:int, b:{t:(a:int,b:chararray)}");}
public void parquet-mr_f6664_0() throws Exception
{    testPigConversion("b:[(c:int)]");}
public void parquet-mr_f6665_0() throws Exception
{    testPigConversion("a:chararray, b:[(c:chararray, d:chararray)]");}
public void parquet-mr_f6666_0() throws Exception
{    testPigConversion("a:map[{bag: (a:int)}]");}
public void parquet-mr_f6667_0() throws Exception
{    for (Type.Repetition repetition : Type.Repetition.values()) {        for (Type.Repetition valueRepetition : Type.Repetition.values()) {            for (PrimitiveType.PrimitiveTypeName primitiveTypeName : PrimitiveType.PrimitiveTypeName.values()) {                if (primitiveTypeName != PrimitiveType.PrimitiveTypeName.INT96) {                                        Types.PrimitiveBuilder<PrimitiveType> value = Types.primitive(primitiveTypeName, valueRepetition);                    if (primitiveTypeName == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY)                        value.length(1);                    GroupType type = Types.buildGroup(repetition).addField(value.named("b")).as(OriginalType.LIST).named("a");                                        pigSchemaConverter.convertField(type);                }            }        }    }}
private void parquet-mr_f6668_0(String pigSchemaString, String schemaString) throws Exception
{    Schema pigSchema = Utils.getSchemaFromString(pigSchemaString);    MessageType schema = pigSchemaConverter.convert(pigSchema);    MessageType expectedMT = MessageTypeParser.parseMessageType(schemaString);    assertEquals("converting " + pigSchemaString + " to " + schemaString, expectedMT, schema);    MessageType filtered = pigSchemaConverter.filter(schema, pigSchema, null);    assertEquals("converting " + pigSchemaString + " to " + schemaString + " and filtering", schema.toString(), filtered.toString());}
public void parquet-mr_f6669_0() throws Exception
{    testConversion("a:chararray, b:{t:(c:chararray, d:chararray)}", "message pig_schema {\n" + "  optional binary a (UTF8);\n" + "  optional group b (LIST) {\n" + "    repeated group t {\n" + "      optional binary c (UTF8);\n" + "      optional binary d (UTF8);\n" + "    }\n" + "  }\n" + "}\n");}
public void parquet-mr_f6670_0() throws Exception
{    testConversion("a:chararray, b:{(c:chararray, d:chararray)}", "message pig_schema {\n" + "  optional binary a (UTF8);\n" + "  optional group b (LIST) {\n" +     "    repeated group bag {\n" + "      optional binary c (UTF8);\n" + "      optional binary d (UTF8);\n" + "    }\n" + "  }\n" + "}\n");}
public void parquet-mr_f6671_0() throws Exception
{    testConversion("a:chararray, b:[(c:chararray, d:chararray)]", "message pig_schema {\n" + "  optional binary a (UTF8);\n" + "  optional group b (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional group value {\n" + "        optional binary c (UTF8);\n" + "        optional binary d (UTF8);\n" + "      }\n" + "    }\n" + "  }\n" + "}\n");}
public void parquet-mr_f6672_0() throws Exception
{    testConversion("a:map[int]", "message pig_schema {\n" + "  optional group a (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional int32 value;" + "    }\n" + "  }\n" + "}\n");}
public void parquet-mr_f6673_0() throws Exception
{    testConversion("a:map[map[int]]", "message pig_schema {\n" + "  optional group a (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional group value (MAP) {\n" + "        repeated group map (MAP_KEY_VALUE) {\n" + "          required binary key (UTF8);\n" + "          optional int32 value;\n" + "        }\n" + "      }\n" + "    }\n" + "  }\n" + "}\n");}
public void parquet-mr_f6674_0() throws Exception
{    testConversion("a:map[bag{(a:int)}]", "message pig_schema {\n" + "  optional group a (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional group value (LIST) {\n" + "        repeated group bag {\n" + "          optional int32 a;\n" + "        }\n" + "      }\n" + "    }\n" + "  }\n" + "}\n");}
public void parquet-mr_f6675_0() throws Exception
{    testFixedConversion("message pig_schema {\n" + "  optional group a (LIST) {\n" + "    repeated binary b (UTF8);\n" + "  }\n" + "}\n", "a:{" + PigSchemaConverter.ARRAY_VALUE_NAME + ":(b: chararray)}");}
private void parquet-mr_f6676_0(String schemaString, String pigSchemaString) throws Exception
{    Schema expectedPigSchema = Utils.getSchemaFromString(pigSchemaString);    MessageType parquetSchema = MessageTypeParser.parseMessageType(schemaString);    Schema pigSchema = pigSchemaConverter.convert(parquetSchema);    assertEquals("converting " + schemaString + " to " + pigSchemaString, expectedPigSchema, pigSchema);}
public void parquet-mr_f6677_0() throws Exception
{    testFixedConversion("message pig_schema {\n" + "  optional binary a;\n" + "  optional group b (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key;\n" + "      optional group value {\n" + "        optional fixed_len_byte_array(5) c;\n" + "        optional fixed_len_byte_array(7) d;\n" + "      }\n" + "    }\n" + "  }\n" + "}\n", "a:bytearray, b:[(c:bytearray, d:bytearray)]");}
public void parquet-mr_f6678_0() throws Exception
{    testFixedConversion("message spark_schema {\n" + "  optional binary a;\n" + "  optional group b (MAP) {\n" + "    repeated group map {\n" + "      required binary key;\n" + "      optional group value {\n" + "        optional fixed_len_byte_array(5) c;\n" + "        optional fixed_len_byte_array(7) d;\n" + "      }\n" + "    }\n" + "  }\n" + "}\n", "a:bytearray, b:[(c:bytearray, d:bytearray)]");}
public void parquet-mr_f6679_0() throws Exception
{    testFixedConversion("message spark_schema {\n" + "  optional int96 datetime;\n" + "}", "datetime:bytearray");}
public void parquet-mr_f6680_0() throws Exception
{    testConversion("a:chararray, int", "message pig_schema {\n" + "  optional binary a (UTF8);\n" + "  optional int32 val_0;\n" + "}\n");}
public void parquet-mr_f6681_0()
{    Map<String, Set<String>> map = new LinkedHashMap<String, Set<String>>();    map.put("pig.schema", new LinkedHashSet<String>(Arrays.asList("a:int, b:int, c:int, d:int, e:int, f:int", "aa:int, aaa:int, b:int, c:int, ee:int")));    Schema result = getPigSchemaFromMultipleFiles(new MessageType("file_schema", new PrimitiveType(OPTIONAL, INT32, "a")), map);    assertEquals("a: int,b: int,c: int,d: int,e: int,f: int,aa: int,aaa: int,ee: int", pigSchemaToString(result));}
public void parquet-mr_f6682_0() throws ExecException, ParserException
{    String pigSchemaString = "DocId:long, " + "Links:(Backward:{(long)}, Forward:{(long)}), " + "Name:{(Language:{(Code:chararray,Country:chararray)}, Url:chararray)}";    SimpleGroup g = new SimpleGroup(getMessageType(pigSchemaString));    g.add("DocId", 1l);    Group links = g.addGroup("Links");    links.addGroup("Backward").addGroup("bag").add(0, 1l);    links.addGroup("Forward").addGroup("bag").add(0, 1l);    Group name = g.addGroup("Name").addGroup("bag");    name.addGroup("Language").addGroup("bag").append("Code", "en").append("Country", "US");    name.add("Url", "http://foo/bar");    testFromGroups(pigSchemaString, Arrays.<Group>asList(g));}
public void parquet-mr_f6683_0() throws ExecException, ParserException
{    String pigSchemaString = "a: {(b: chararray)}";    SimpleGroup g = new SimpleGroup(getMessageType(pigSchemaString));    Group addGroup = g.addGroup("a");    addGroup.addGroup("bag").append("b", "foo");    addGroup.addGroup("bag").append("b", "bar");    testFromGroups(pigSchemaString, Arrays.<Group>asList(g));}
public void parquet-mr_f6684_0() throws ExecException, ParserException
{    String pigSchemaString = "a: [(b: chararray)]";    SimpleGroup g = new SimpleGroup(getMessageType(pigSchemaString));    Group map = g.addGroup("a");    map.addGroup("map").append("key", "foo").addGroup("value").append("b", "foo");    map.addGroup("map").append("key", "bar").addGroup("value").append("b", "bar");    testFromGroups(pigSchemaString, Arrays.<Group>asList(g));}
public void parquet-mr_f6685_0() throws Exception
{    String pigSchemaString = "a:chararray, b:{t:(c:chararray, d:chararray)}";    Tuple t0 = tuple("a" + 0, bag(tuple("o", "b"), tuple("o1", "b1")));    Tuple t1 = tuple("a" + 1, bag(tuple("o", "b"), tuple("o", "b"), tuple("o", "b"), tuple("o", "b")));    Tuple t2 = tuple("a" + 2, bag(tuple("o", "b"), tuple("o", null), tuple(null, "b"), tuple(null, null)));    Tuple t3 = tuple("a" + 3, null);    testFromTuple(pigSchemaString, Arrays.asList(t0, t1, t2, t3));}
public void parquet-mr_f6686_0() throws Exception
{    String pigSchemaString = "a:chararray, b:[(c:chararray, d:chararray)]";    Tuple t0 = tuple("a" + 0, new HashMap() {        {            put("foo", tuple("o", "b"));        }    });    Tuple t1 = tuple("a" + 1, new HashMap() {        {            put("foo", tuple("o", "b"));            put("foo", tuple("o", "b"));            put("foo", tuple("o", "b"));            put("foo", tuple("o", "b"));        }    });    Tuple t2 = tuple("a" + 2, new HashMap() {        {            put("foo", tuple("o", "b"));            put("foo", tuple("o", null));            put("foo", tuple(null, "b"));            put("foo", tuple(null, null));        }    });    Tuple t3 = tuple("a" + 3, null);    testFromTuple(pigSchemaString, Arrays.asList(t0, t1, t2, t3));}
private void parquet-mr_f6687_1(String pigSchemaString, List<Tuple> input) throws Exception
{    List<Tuple> tuples = new ArrayList<Tuple>();    RecordMaterializer<Tuple> recordConsumer = newPigRecordConsumer(pigSchemaString);    TupleWriteSupport tupleWriter = newTupleWriter(pigSchemaString, recordConsumer);    for (Tuple tuple : input) {                tupleWriter.write(tuple);        tuples.add(recordConsumer.getCurrentRecord());    }    assertEquals(input.size(), tuples.size());    for (int i = 0; i < input.size(); i++) {        Tuple in = input.get(i);        Tuple out = tuples.get(i);        assertEquals(in.toString(), out.toString());    }}
private void parquet-mr_f6688_1(String pigSchemaString, List<Group> input) throws ParserException
{    List<Tuple> tuples = new ArrayList<Tuple>();    MessageType schema = getMessageType(pigSchemaString);    RecordMaterializer<Tuple> pigRecordConsumer = newPigRecordConsumer(pigSchemaString);    GroupWriter groupWriter = new GroupWriter(new RecordConsumerLoggingWrapper(new ConverterConsumer(pigRecordConsumer.getRootConverter(), schema)), schema);    for (Group group : input) {        groupWriter.write(group);        final Tuple tuple = pigRecordConsumer.getCurrentRecord();        tuples.add(tuple);            }    List<Group> groups = new ArrayList<Group>();    GroupRecordConverter recordConsumer = new GroupRecordConverter(schema);    TupleWriteSupport tupleWriter = newTupleWriter(pigSchemaString, recordConsumer);    for (Tuple t : tuples) {                tupleWriter.write(t);        groups.add(recordConsumer.getCurrentRecord());    }    assertEquals(input.size(), groups.size());    for (int i = 0; i < input.size(); i++) {        Group in = input.get(i);                Group out = groups.get(i);        assertEquals(in.toString(), out.toString());    }}
private TupleWriteSupport parquet-mr_f6689_0(String pigSchemaString, RecordMaterializer<T> recordConsumer) throws ParserException
{    TupleWriteSupport tupleWriter = TupleWriteSupport.fromPigSchema(pigSchemaString);    tupleWriter.init(null);    tupleWriter.prepareForWrite(new ConverterConsumer(recordConsumer.getRootConverter(), tupleWriter.getParquetSchema()));    return tupleWriter;}
private Map<String, String> parquet-mr_f6690_0(String pigSchemaString)
{    Map<String, String> map = new HashMap<String, String>();    new PigMetaData(pigSchemaString).addToMetaData(map);    return map;}
private RecordMaterializer<Tuple> parquet-mr_f6691_0(String pigSchemaString) throws ParserException
{    TupleReadSupport tupleReadSupport = new TupleReadSupport();    final Configuration configuration = new Configuration(false);    MessageType parquetSchema = getMessageType(pigSchemaString);    final Map<String, String> pigMetaData = pigMetaData(pigSchemaString);    Map<String, Set<String>> globalMetaData = new HashMap<String, Set<String>>();    for (Entry<String, String> entry : pigMetaData.entrySet()) {        globalMetaData.put(entry.getKey(), new HashSet<String>(Arrays.asList(entry.getValue())));    }    configuration.set(PARQUET_PIG_SCHEMA, pigSchemaString);    final ReadContext init = tupleReadSupport.init(new InitContext(configuration, globalMetaData, parquetSchema));    return tupleReadSupport.prepareForRead(configuration, pigMetaData, parquetSchema, init);}
private MessageType parquet-mr_f6692_0(String pigSchemaString) throws ParserException
{    Schema pigSchema = Utils.getSchemaFromString(pigSchemaString);    return new PigSchemaConverter().convert(pigSchema);}
public static void parquet-mr_f6693_0(String[] args) throws Exception
{    String pigSchema = pigSchema(false, false);    String pigSchemaProjected = pigSchema(true, false);    String pigSchemaNoString = pigSchema(true, true);    MessageType schema = new PigSchemaConverter().convert(Utils.getSchemaFromString(pigSchema));    MemPageStore memPageStore = new MemPageStore(0);    ColumnWriteStoreV1 columns = new ColumnWriteStoreV1(memPageStore, ParquetProperties.builder().withPageSize(50 * 1024 * 1024).withDictionaryEncoding(false).build());    write(memPageStore, columns, schema, pigSchema);    columns.flush();    read(memPageStore, pigSchema, pigSchemaProjected, pigSchemaNoString);    System.out.println(columns.getBufferedSize() + " bytes used total");    System.out.println("max col size: " + columns.maxColMemSize() + " bytes");}
private static String parquet-mr_f6694_0(boolean projected, boolean noStrings)
{    StringBuilder sb = new StringBuilder();    for (int i = 0; i < TOP_LEVEL_COLS; i++) {        if (i != 0) {            sb.append(", ");        }        sb.append("i" + i + ":(");        if (!noStrings) {            for (int j = 0; j < (projected ? 2 : 4); j++) {                if (j != 0) {                    sb.append(", ");                }                sb.append("j" + j + ":chararray");            }            sb.append(", ");        }        for (int k = 0; k < (projected ? 2 : 4); k++) {            if (k != 0) {                sb.append(", ");            }            sb.append("k" + k + ":long");        }        for (int l = 0; l < (projected ? 1 : 2); l++) {            sb.append(", ");            sb.append("l" + l + ":{t:(v:int)}");        }        sb.append(")");    }    return sb.toString();}
private static Tuple parquet-mr_f6695_0() throws ExecException
{    TupleFactory tf = TupleFactory.getInstance();    Tuple t = tf.newTuple(TOP_LEVEL_COLS);    for (int i = 0; i < TOP_LEVEL_COLS; i++) {        Tuple ti = tf.newTuple(10);        for (int j = 0; j < 4; j++) {            ti.set(j, "foo" + i + "," + j);        }        for (int k = 0; k < 4; k++) {            ti.set(4 + k, (long) k);        }        for (int l = 0; l < 2; l++) {            DataBag bag = new NonSpillableDataBag();            for (int m = 0; m < 10; m++) {                bag.add(tf.newTuple((Object) new Integer(m)));            }            ti.set(8 + l, bag);        }        t.set(i, ti);    }    return t;}
private static void parquet-mr_f6696_0(PageReadStore columns, String pigSchemaString, String pigSchemaProjected, String pigSchemaProjectedNoStrings) throws ParserException
{    read(columns, pigSchemaString, "read all");    read(columns, pigSchemaProjected, "read projected");    read(columns, pigSchemaProjectedNoStrings, "read projected no Strings");}
private static void parquet-mr_f6697_0(PageReadStore columns, String pigSchemaString, String message) throws ParserException
{    System.out.println(message);    MessageColumnIO columnIO = newColumnFactory(pigSchemaString);    TupleReadSupport tupleReadSupport = new TupleReadSupport();    Map<String, String> pigMetaData = pigMetaData(pigSchemaString);    MessageType schema = new PigSchemaConverter().convert(Utils.getSchemaFromString(pigSchemaString));    ReadContext init = tupleReadSupport.init(null, pigMetaData, schema);    RecordMaterializer<Tuple> recordConsumer = tupleReadSupport.prepareForRead(null, pigMetaData, schema, init);    RecordReader<Tuple> recordReader = columnIO.getRecordReader(columns, recordConsumer);                    read(recordReader, 10000, pigSchemaString);    read(recordReader, 10000, pigSchemaString);    read(recordReader, 10000, pigSchemaString);    read(recordReader, 10000, pigSchemaString);    read(recordReader, 10000, pigSchemaString);    read(recordReader, 100000, pigSchemaString);    read(recordReader, 1000000, pigSchemaString);    System.out.println();}
private static Map<String, String> parquet-mr_f6698_0(String pigSchemaString)
{    Map<String, String> map = new HashMap<String, String>();    new PigMetaData(pigSchemaString).addToMetaData(map);    return map;}
private static void parquet-mr_f6699_0(MemPageStore memPageStore, ColumnWriteStoreV1 columns, MessageType schema, String pigSchemaString) throws ExecException, ParserException
{    MessageColumnIO columnIO = newColumnFactory(pigSchemaString);    TupleWriteSupport tupleWriter = TupleWriteSupport.fromPigSchema(pigSchemaString);    tupleWriter.init(null);    tupleWriter.prepareForWrite(columnIO.getRecordWriter(columns));    write(memPageStore, tupleWriter, 10000);    write(memPageStore, tupleWriter, 10000);    write(memPageStore, tupleWriter, 10000);    write(memPageStore, tupleWriter, 10000);    write(memPageStore, tupleWriter, 10000);    write(memPageStore, tupleWriter, 100000);    write(memPageStore, tupleWriter, 1000000);    System.out.println();}
private static MessageColumnIO parquet-mr_f6700_0(String pigSchemaString) throws ParserException
{    MessageType schema = new PigSchemaConverter().convert(Utils.getSchemaFromString(pigSchemaString));    return new ColumnIOFactory().getColumnIO(schema);}
private static void parquet-mr_f6701_0(RecordReader<Tuple> recordReader, int count, String pigSchemaString) throws ParserException
{    long t0 = System.currentTimeMillis();    Tuple tuple = null;    for (int i = 0; i < count; i++) {        tuple = recordReader.read();    }    if (tuple == null) {        throw new RuntimeException();    }    long t1 = System.currentTimeMillis();    long t = t1 - t0;        float err = (float) 100 * 2 / t;    System.out.printf("read %,9d recs in %,5d ms at %,9d rec/s err: %3.2f%%\n", count, t, t == 0 ? 0 : count * 1000 / t, err);}
private static void parquet-mr_f6702_0(MemPageStore memPageStore, TupleWriteSupport tupleWriter, int count) throws ExecException
{    Tuple tu = tuple();    long t0 = System.currentTimeMillis();    for (int i = 0; i < count; i++) {        tupleWriter.write(tu);    }    long t1 = System.currentTimeMillis();    long t = t1 - t0;    memPageStore.addRowCount(count);    System.out.printf("written %,9d recs in %,5d ms at %,9d rec/s\n", count, t, t == 0 ? 0 : count * 1000 / t);}
public Converter parquet-mr_f6703_0(int fieldIndex)
{    return converters[fieldIndex];}
public void parquet-mr_f6704_0()
{}
public void parquet-mr_f6705_0()
{    parent.add(myBuilder.build());    myBuilder.clear();}
private Converter parquet-mr_f6706_0(final Message.Builder parentBuilder, final Descriptors.FieldDescriptor fieldDescriptor, Type parquetType)
{    boolean isRepeated = fieldDescriptor.isRepeated();    ParentValueContainer parent;    if (isRepeated) {        parent = new ParentValueContainer() {            @Override            public void add(Object value) {                parentBuilder.addRepeatedField(fieldDescriptor, value);            }        };    } else {        parent = new ParentValueContainer() {            @Override            public void add(Object value) {                parentBuilder.setField(fieldDescriptor, value);            }        };    }    LogicalTypeAnnotation logicalTypeAnnotation = parquetType.getLogicalTypeAnnotation();    if (logicalTypeAnnotation == null) {        return newScalarConverter(parent, parentBuilder, fieldDescriptor, parquetType);    }    return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<Converter>() {        @Override        public Optional<Converter> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType) {            return of(new ListConverter(parentBuilder, fieldDescriptor, parquetType));        }        @Override        public Optional<Converter> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType) {            return of(new MapConverter(parentBuilder, fieldDescriptor, parquetType));        }    }).orElseGet(() -> newScalarConverter(parent, parentBuilder, fieldDescriptor, parquetType));}
public void parquet-mr_f6707_0(Object value)
{    parentBuilder.addRepeatedField(fieldDescriptor, value);}
public void parquet-mr_f6708_0(Object value)
{    parentBuilder.setField(fieldDescriptor, value);}
public Optional<Converter> parquet-mr_f6709_0(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    return of(new ListConverter(parentBuilder, fieldDescriptor, parquetType));}
public Optional<Converter> parquet-mr_f6710_0(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    return of(new MapConverter(parentBuilder, fieldDescriptor, parquetType));}
private Converter parquet-mr_f6711_0(ParentValueContainer pvc, Message.Builder parentBuilder, Descriptors.FieldDescriptor fieldDescriptor, Type parquetType)
{    JavaType javaType = fieldDescriptor.getJavaType();    switch(javaType) {        case STRING:            return new ProtoStringConverter(pvc);        case FLOAT:            return new ProtoFloatConverter(pvc);        case DOUBLE:            return new ProtoDoubleConverter(pvc);        case BOOLEAN:            return new ProtoBooleanConverter(pvc);        case BYTE_STRING:            return new ProtoBinaryConverter(pvc);        case ENUM:            return new ProtoEnumConverter(pvc, fieldDescriptor);        case INT:            return new ProtoIntConverter(pvc);        case LONG:            return new ProtoLongConverter(pvc);        case MESSAGE:            {                Message.Builder subBuilder = parentBuilder.newBuilderForField(fieldDescriptor);                return new ProtoMessageConverter(pvc, subBuilder, parquetType.asGroupType());            }    }    throw new UnsupportedOperationException(String.format("Cannot convert type: %s" + " (Parquet type: %s) ", javaType, parquetType));}
public Message.Builder parquet-mr_f6712_0()
{    return myBuilder;}
private Map<Binary, Descriptors.EnumValueDescriptor> parquet-mr_f6713_0(Descriptors.FieldDescriptor enumFieldType)
{    Descriptors.EnumDescriptor enumType = enumFieldType.getEnumType();    Map<Binary, Descriptors.EnumValueDescriptor> lookupStructure = new HashMap<Binary, Descriptors.EnumValueDescriptor>();    List<Descriptors.EnumValueDescriptor> enumValues = enumType.getValues();    for (Descriptors.EnumValueDescriptor value : enumValues) {        String name = value.getName();        lookupStructure.put(Binary.fromString(name), enumType.findValueByName(name));    }    return lookupStructure;}
private Descriptors.EnumValueDescriptor parquet-mr_f6714_0(Binary binaryValue)
{    Descriptors.EnumValueDescriptor protoValue = enumLookup.get(binaryValue);    if (protoValue == null) {        Set<Binary> knownValues = enumLookup.keySet();        String msg = "Illegal enum value \"" + binaryValue + "\"" + " in protocol buffer \"" + fieldType.getFullName() + "\"" + " legal values are: \"" + knownValues + "\"";        throw new InvalidRecordException(msg);    }    return protoValue;}
public final void parquet-mr_f6715_0(Binary binaryValue)
{    Descriptors.EnumValueDescriptor protoValue = translateEnumValue(binaryValue);    parent.add(protoValue);}
public void parquet-mr_f6716_0(int dictionaryId)
{    parent.add(dict[dictionaryId]);}
public boolean parquet-mr_f6717_0()
{    return true;}
public void parquet-mr_f6718_0(Dictionary dictionary)
{    dict = new Descriptors.EnumValueDescriptor[dictionary.getMaxId() + 1];    for (int i = 0; i <= dictionary.getMaxId(); i++) {        Binary binaryValue = dictionary.decodeToBinary(i);        dict[i] = translateEnumValue(binaryValue);    }}
public void parquet-mr_f6719_0(Binary binary)
{    ByteString byteString = ByteString.copyFrom(binary.toByteBuffer());    parent.add(byteString);}
public final void parquet-mr_f6720_0(boolean value)
{    parent.add(value);}
public void parquet-mr_f6721_0(double value)
{    parent.add(value);}
public void parquet-mr_f6722_0(float value)
{    parent.add(value);}
public void parquet-mr_f6723_0(int value)
{    parent.add(value);}
public void parquet-mr_f6724_0(long value)
{    parent.add(value);}
public void parquet-mr_f6725_0(Binary binary)
{    String str = binary.toStringUsingUTF8();    parent.add(str);}
public Converter parquet-mr_f6726_0(int fieldIndex)
{    if (fieldIndex > 0) {        throw new ParquetDecodingException("Unexpected multiple fields in the LIST wrapper");    }    return new GroupConverter() {        @Override        public Converter getConverter(int fieldIndex) {            return converter;        }        @Override        public void start() {        }        @Override        public void end() {        }    };}
public Converter parquet-mr_f6727_0(int fieldIndex)
{    return converter;}
public void parquet-mr_f6728_0()
{}
public void parquet-mr_f6729_0()
{}
public void parquet-mr_f6730_0()
{}
public void parquet-mr_f6731_0()
{}
public Converter parquet-mr_f6732_0(int fieldIndex)
{    if (fieldIndex > 0) {        throw new ParquetDecodingException("Unexpected multiple fields in the MAP wrapper");    }    return converter;}
public void parquet-mr_f6733_0()
{}
public void parquet-mr_f6734_0()
{}
public static void parquet-mr_f6735_0(Job job, String requestedProjection)
{    ProtoReadSupport.setRequestedProjection(ContextUtil.getConfiguration(job), requestedProjection);}
public static void parquet-mr_f6736_0(Job job, Class<? extends Message> protoClass)
{    ProtoWriteSupport.setSchema(ContextUtil.getConfiguration(job), protoClass);}
public static Builder<T> parquet-mr_f6737_0(Path file)
{    return ParquetReader.builder(new ProtoReadSupport(), file);}
public static void parquet-mr_f6738_0(Configuration configuration, String requestedProjection)
{    configuration.set(PB_REQUESTED_PROJECTION, requestedProjection);}
public static void parquet-mr_f6739_0(Configuration configuration, String protobufClass)
{    configuration.set(PB_CLASS, protobufClass);}
public ReadContext parquet-mr_f6740_1(InitContext context)
{    String requestedProjectionString = context.getConfiguration().get(PB_REQUESTED_PROJECTION);    if (requestedProjectionString != null && !requestedProjectionString.trim().isEmpty()) {        MessageType requestedProjection = getSchemaForRead(context.getFileSchema(), requestedProjectionString);                return new ReadContext(requestedProjection);    } else {        MessageType fileSchema = context.getFileSchema();                return new ReadContext(fileSchema);    }}
public RecordMaterializer<T> parquet-mr_f6741_1(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema, ReadContext readContext)
{    String headerProtoClass = keyValueMetaData.get(PB_CLASS);    String configuredProtoClass = configuration.get(PB_CLASS);    if (configuredProtoClass != null) {                headerProtoClass = configuredProtoClass;    }    if (headerProtoClass == null) {        throw new RuntimeException("I Need parameter " + PB_CLASS + " with Protocol Buffer class");    }        MessageType requestedSchema = readContext.getRequestedSchema();    Class<? extends Message> protobufClass = Protobufs.getProtobufClass(headerProtoClass);    return new ProtoRecordMaterializer(requestedSchema, protobufClass);}
public void parquet-mr_f6742_0(Object a)
{    throw new RuntimeException("Should never happen");}
public void parquet-mr_f6743_0()
{    reusedBuilder.clear();    super.start();}
public void parquet-mr_f6744_0()
{}
public T parquet-mr_f6745_0()
{    if (buildBefore) {        return (T) this.reusedBuilder.build();    } else {        return (T) this.reusedBuilder;    }}
public void parquet-mr_f6746_0(boolean buildBefore)
{    this.buildBefore = buildBefore;}
public T parquet-mr_f6747_0()
{    return root.getCurrentRecord();}
public GroupConverter parquet-mr_f6748_0()
{    return root;}
public MessageType parquet-mr_f6749_1(Class<? extends Message> protobufClass)
{        Descriptors.Descriptor descriptor = Protobufs.getMessageDescriptor(protobufClass);    MessageType messageType = convertFields(Types.buildMessage(), descriptor.getFields()).named(descriptor.getFullName());        return messageType;}
private GroupBuilder<T> parquet-mr_f6750_0(GroupBuilder<T> groupBuilder, List<FieldDescriptor> fieldDescriptors)
{    for (FieldDescriptor fieldDescriptor : fieldDescriptors) {        groupBuilder = addField(fieldDescriptor, groupBuilder).id(fieldDescriptor.getNumber()).named(fieldDescriptor.getName());    }    return groupBuilder;}
private Type.Repetition parquet-mr_f6751_0(FieldDescriptor descriptor)
{    if (descriptor.isRequired()) {        return Type.Repetition.REQUIRED;    } else if (descriptor.isRepeated()) {        return Type.Repetition.REPEATED;    } else {        return Type.Repetition.OPTIONAL;    }}
private Builder<? extends Builder<?, GroupBuilder<T>>, GroupBuilder<T>> parquet-mr_f6752_0(FieldDescriptor descriptor, final GroupBuilder<T> builder)
{    if (descriptor.getJavaType() == JavaType.MESSAGE) {        return addMessageField(descriptor, builder);    }    ParquetType parquetType = getParquetType(descriptor);    if (descriptor.isRepeated() && parquetSpecsCompliant) {                return addRepeatedPrimitive(parquetType.primitiveType, parquetType.logicalTypeAnnotation, builder);    }    return builder.primitive(parquetType.primitiveType, getRepetition(descriptor)).as(parquetType.logicalTypeAnnotation);}
private Builder<? extends Builder<?, GroupBuilder<T>>, GroupBuilder<T>> parquet-mr_f6753_0(PrimitiveTypeName primitiveType, LogicalTypeAnnotation logicalTypeAnnotation, final GroupBuilder<T> builder)
{    return builder.group(Type.Repetition.OPTIONAL).as(listType()).group(Type.Repetition.REPEATED).primitive(primitiveType, Type.Repetition.REQUIRED).as(logicalTypeAnnotation).named("element").named("list");}
private GroupBuilder<GroupBuilder<T>> parquet-mr_f6754_0(FieldDescriptor descriptor, GroupBuilder<T> builder)
{    GroupBuilder<GroupBuilder<GroupBuilder<GroupBuilder<T>>>> result = builder.group(Type.Repetition.OPTIONAL).as(listType()).group(Type.Repetition.REPEATED).group(Type.Repetition.OPTIONAL);    convertFields(result, descriptor.getMessageType().getFields());    return result.named("element").named("list");}
private GroupBuilder<GroupBuilder<T>> parquet-mr_f6755_0(FieldDescriptor descriptor, final GroupBuilder<T> builder)
{    if (descriptor.isMapField() && parquetSpecsCompliant) {                return addMapField(descriptor, builder);    }    if (descriptor.isRepeated() && parquetSpecsCompliant) {                return addRepeatedMessage(descriptor, builder);    }        GroupBuilder<GroupBuilder<T>> group = builder.group(getRepetition(descriptor));    convertFields(group, descriptor.getMessageType().getFields());    return group;}
private GroupBuilder<GroupBuilder<T>> parquet-mr_f6756_0(FieldDescriptor descriptor, final GroupBuilder<T> builder)
{    List<FieldDescriptor> fields = descriptor.getMessageType().getFields();    if (fields.size() != 2) {        throw new UnsupportedOperationException("Expected two fields for the map (key/value), but got: " + fields);    }    ParquetType mapKeyParquetType = getParquetType(fields.get(0));    GroupBuilder<GroupBuilder<GroupBuilder<T>>> group = builder.group(Type.Repetition.OPTIONAL).as(    mapType()).group(    Type.Repetition.REPEATED).primitive(mapKeyParquetType.primitiveType, Type.Repetition.REQUIRED).as(mapKeyParquetType.logicalTypeAnnotation).named("key");    return addField(fields.get(1), group).named("value").named("key_value");}
private ParquetType parquet-mr_f6757_0(FieldDescriptor fieldDescriptor)
{    JavaType javaType = fieldDescriptor.getJavaType();    switch(javaType) {        case INT:            return ParquetType.of(INT32);        case LONG:            return ParquetType.of(INT64);        case DOUBLE:            return ParquetType.of(DOUBLE);        case BOOLEAN:            return ParquetType.of(BOOLEAN);        case FLOAT:            return ParquetType.of(FLOAT);        case STRING:            return ParquetType.of(BINARY, stringType());        case ENUM:            return ParquetType.of(BINARY, enumType());        case BYTE_STRING:            return ParquetType.of(BINARY);        default:            throw new UnsupportedOperationException("Cannot convert Protocol Buffer: unknown type " + javaType);    }}
public static ParquetType parquet-mr_f6758_0(PrimitiveTypeName primitiveType, LogicalTypeAnnotation logicalTypeAnnotation)
{    return new ParquetType(primitiveType, logicalTypeAnnotation);}
public static ParquetType parquet-mr_f6759_0(PrimitiveTypeName primitiveType)
{    return of(primitiveType, null);}
public String parquet-mr_f6760_0()
{    return "protobuf";}
public static void parquet-mr_f6761_0(Configuration configuration, Class<? extends Message> protoClass)
{    configuration.setClass(PB_CLASS_WRITE, protoClass, Message.class);}
public static void parquet-mr_f6762_0(Configuration configuration, boolean writeSpecsCompliant)
{    configuration.setBoolean(PB_SPECS_COMPLIANT_WRITE, writeSpecsCompliant);}
public void parquet-mr_f6763_1(T record)
{    recordConsumer.startMessage();    try {        messageWriter.writeTopLevelMessage(record);    } catch (RuntimeException e) {        Message m = (record instanceof Message.Builder) ? ((Message.Builder) record).build() : (Message) record;                throw e;    }    recordConsumer.endMessage();}
public void parquet-mr_f6764_0(RecordConsumer recordConsumer)
{    this.recordConsumer = recordConsumer;}
public WriteContext parquet-mr_f6765_0(Configuration configuration)
{        if (protoMessage == null) {        Class<? extends Message> pbClass = configuration.getClass(PB_CLASS_WRITE, null, Message.class);        if (pbClass != null) {            protoMessage = pbClass;        } else {            String msg = "Protocol buffer class not specified.";            String hint = " Please use method ProtoParquetOutputFormat.setProtobufClass(...) or other similar method.";            throw new BadConfigurationException(msg + hint);        }    }    writeSpecsCompliant = configuration.getBoolean(PB_SPECS_COMPLIANT_WRITE, writeSpecsCompliant);    MessageType rootSchema = new ProtoSchemaConverter(writeSpecsCompliant).convert(protoMessage);    Descriptor messageDescriptor = Protobufs.getMessageDescriptor(protoMessage);    validatedMapping(messageDescriptor, rootSchema);    this.messageWriter = new MessageWriter(messageDescriptor, rootSchema);    Map<String, String> extraMetaData = new HashMap<String, String>();    extraMetaData.put(ProtoReadSupport.PB_CLASS, protoMessage.getName());    extraMetaData.put(ProtoReadSupport.PB_DESCRIPTOR, serializeDescriptor(protoMessage));    extraMetaData.put(PB_SPECS_COMPLIANT_WRITE, String.valueOf(writeSpecsCompliant));    return new WriteContext(rootSchema, extraMetaData);}
 void parquet-mr_f6766_0(String fieldName)
{    this.fieldName = fieldName;}
 void parquet-mr_f6767_0(int index)
{    this.index = index;}
 void parquet-mr_f6768_0(Object value)
{}
 void parquet-mr_f6769_0(Object value)
{    recordConsumer.startField(fieldName, index);    writeRawValue(value);    recordConsumer.endField(fieldName, index);}
private FieldWriter parquet-mr_f6770_0(FieldDescriptor fieldDescriptor, Type type)
{    switch(fieldDescriptor.getJavaType()) {        case STRING:            return new StringWriter();        case MESSAGE:            return createMessageWriter(fieldDescriptor, type);        case INT:            return new IntWriter();        case LONG:            return new LongWriter();        case FLOAT:            return new FloatWriter();        case DOUBLE:            return new DoubleWriter();        case ENUM:            return new EnumWriter();        case BOOLEAN:            return new BooleanWriter();        case BYTE_STRING:            return new BinaryWriter();    }        return unknownType(fieldDescriptor);}
private FieldWriter parquet-mr_f6771_0(FieldDescriptor fieldDescriptor, Type type)
{    if (fieldDescriptor.isMapField() && writeSpecsCompliant) {        return createMapWriter(fieldDescriptor, type);    }    return new MessageWriter(fieldDescriptor.getMessageType(), getGroupType(type));}
private GroupType parquet-mr_f6772_0(Type type)
{    LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();    if (logicalTypeAnnotation == null) {        return type.asGroupType();    }    return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<GroupType>() {        @Override        public Optional<GroupType> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType) {            return ofNullable(type.asGroupType().getType("list").asGroupType().getType("element").asGroupType());        }        @Override        public Optional<GroupType> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType) {            return ofNullable(type.asGroupType().getType("key_value").asGroupType().getType("value").asGroupType());        }    }).orElse(type.asGroupType());}
public Optional<GroupType> parquet-mr_f6773_0(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    return ofNullable(type.asGroupType().getType("list").asGroupType().getType("element").asGroupType());}
public Optional<GroupType> parquet-mr_f6774_0(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    return ofNullable(type.asGroupType().getType("key_value").asGroupType().getType("value").asGroupType());}
private MapWriter parquet-mr_f6775_0(FieldDescriptor fieldDescriptor, Type type)
{    List<FieldDescriptor> fields = fieldDescriptor.getMessageType().getFields();    if (fields.size() != 2) {        throw new UnsupportedOperationException("Expected two fields for the map (key/value), but got: " + fields);    }        FieldDescriptor keyProtoField = fields.get(0);    FieldWriter keyWriter = createWriter(keyProtoField, type);    keyWriter.setFieldName(keyProtoField.getName());    keyWriter.setIndex(0);        FieldDescriptor valueProtoField = fields.get(1);    FieldWriter valueWriter = createWriter(valueProtoField, type);    valueWriter.setFieldName(valueProtoField.getName());    valueWriter.setIndex(1);    return new MapWriter(keyWriter, valueWriter);}
 void parquet-mr_f6776_0(Object value)
{    writeAllFields((MessageOrBuilder) value);}
 final void parquet-mr_f6777_0(Object value)
{    recordConsumer.startGroup();    writeAllFields((MessageOrBuilder) value);    recordConsumer.endGroup();}
 final void parquet-mr_f6778_0(Object value)
{    recordConsumer.startField(fieldName, index);    writeRawValue(value);    recordConsumer.endField(fieldName, index);}
private void parquet-mr_f6779_0(MessageOrBuilder pb)
{        Map<FieldDescriptor, Object> changedPbFields = pb.getAllFields();    for (Map.Entry<FieldDescriptor, Object> entry : changedPbFields.entrySet()) {        FieldDescriptor fieldDescriptor = entry.getKey();        if (fieldDescriptor.isExtension()) {                        throw new UnsupportedOperationException("Cannot convert Protobuf message with extension field(s)");        }        int fieldIndex = fieldDescriptor.getIndex();        fieldWriters[fieldIndex].writeField(entry.getValue());    }}
 final void parquet-mr_f6780_0(Object value)
{    throw new UnsupportedOperationException("Array has no raw value");}
 final void parquet-mr_f6781_0(Object value)
{    recordConsumer.startField(fieldName, index);    recordConsumer.startGroup();    List<?> list = (List<?>) value;        recordConsumer.startField("list", 0);    for (Object listEntry : list) {        recordConsumer.startGroup();                recordConsumer.startField("element", 0);        fieldWriter.writeRawValue(listEntry);        recordConsumer.endField("element", 0);        recordConsumer.endGroup();    }    recordConsumer.endField("list", 0);    recordConsumer.endGroup();    recordConsumer.endField(fieldName, index);}
 final void parquet-mr_f6782_0(Object value)
{    throw new UnsupportedOperationException("Array has no raw value");}
 final void parquet-mr_f6783_0(Object value)
{    recordConsumer.startField(fieldName, index);    List<?> list = (List<?>) value;    for (Object listEntry : list) {        fieldWriter.writeRawValue(listEntry);    }    recordConsumer.endField(fieldName, index);}
private void parquet-mr_f6784_0(Descriptor descriptor, GroupType parquetSchema)
{    List<FieldDescriptor> allFields = descriptor.getFields();    for (FieldDescriptor fieldDescriptor : allFields) {        String fieldName = fieldDescriptor.getName();        int fieldIndex = fieldDescriptor.getIndex();        int parquetIndex = parquetSchema.getFieldIndex(fieldName);        if (fieldIndex != parquetIndex) {            String message = "FieldIndex mismatch name=" + fieldName + ": " + fieldIndex + " != " + parquetIndex;            throw new IncompatibleSchemaModificationException(message);        }    }}
 final void parquet-mr_f6785_0(Object value)
{    Binary binaryString = Binary.fromString((String) value);    recordConsumer.addBinary(binaryString);}
 final void parquet-mr_f6786_0(Object value)
{    recordConsumer.addInteger((Integer) value);}
 final void parquet-mr_f6787_0(Object value)
{    recordConsumer.addLong((Long) value);}
 final void parquet-mr_f6788_0(Object value)
{    recordConsumer.startGroup();        recordConsumer.startField("key_value", 0);    for (Message msg : (Collection<Message>) value) {        recordConsumer.startGroup();        final Descriptor descriptorForType = msg.getDescriptorForType();        final FieldDescriptor keyDesc = descriptorForType.findFieldByName("key");        final FieldDescriptor valueDesc = descriptorForType.findFieldByName("value");        keyWriter.writeField(msg.getField(keyDesc));        valueWriter.writeField(msg.getField(valueDesc));        recordConsumer.endGroup();    }    recordConsumer.endField("key_value", 0);    recordConsumer.endGroup();}
 final void parquet-mr_f6789_0(Object value)
{    recordConsumer.addFloat((Float) value);}
 final void parquet-mr_f6790_0(Object value)
{    recordConsumer.addDouble((Double) value);}
 final void parquet-mr_f6791_0(Object value)
{    Binary binary = Binary.fromString(((Descriptors.EnumValueDescriptor) value).getName());    recordConsumer.addBinary(binary);}
 final void parquet-mr_f6792_0(Object value)
{    recordConsumer.addBoolean((Boolean) value);}
 final void parquet-mr_f6793_0(Object value)
{    ByteString byteString = (ByteString) value;    Binary binary = Binary.fromConstantByteArray(byteString.toByteArray());    recordConsumer.addBinary(binary);}
private FieldWriter parquet-mr_f6794_0(FieldDescriptor fieldDescriptor)
{    String exceptionMsg = "Unknown type with descriptor \"" + fieldDescriptor + "\" and type \"" + fieldDescriptor.getJavaType() + "\".";    throw new InvalidRecordException(exceptionMsg);}
private String parquet-mr_f6795_0(Class<? extends Message> protoClass)
{    Descriptor descriptor = Protobufs.getMessageDescriptor(protoClass);    DescriptorProtos.DescriptorProto asProto = descriptor.toProto();    return TextFormat.printToString(asProto);}
public void parquet-mr_f6796_0() throws Exception
{    TestProtobuf.IOFormatMessage input;    {        TestProtobuf.IOFormatMessage.Builder msg = TestProtobuf.IOFormatMessage.newBuilder();        msg.setOptionalDouble(666);        msg.addRepeatedString("Msg1");        msg.addRepeatedString("Msg2");        msg.getMsgBuilder().setSomeId(323);        input = msg.build();    }    List<Message> result = runMRJobs(input);    assertEquals(1, result.size());    TestProtobuf.IOFormatMessage output = (TestProtobuf.IOFormatMessage) result.get(0);    assertEquals(666, output.getOptionalDouble(), 0.00001);    assertEquals(323, output.getMsg().getSomeId());    assertEquals("Msg1", output.getRepeatedString(0));    assertEquals("Msg2", output.getRepeatedString(1));    assertEquals(input, output);}
public void parquet-mr_f6797_0() throws Exception
{    TestProto3.IOFormatMessage input;    {        TestProto3.IOFormatMessage.Builder msg = TestProto3.IOFormatMessage.newBuilder();        msg.setOptionalDouble(666);        msg.addRepeatedString("Msg1");        msg.addRepeatedString("Msg2");        msg.getMsgBuilder().setSomeId(323);        input = msg.build();    }    List<Message> result = runMRJobs(input);    assertEquals(1, result.size());    TestProto3.IOFormatMessage output = (TestProto3.IOFormatMessage) result.get(0);    assertEquals(666, output.getOptionalDouble(), 0.00001);    assertEquals(323, output.getMsg().getSomeId());    assertEquals("Msg1", output.getRepeatedString(0));    assertEquals("Msg2", output.getRepeatedString(1));    assertEquals(input, output);}
public void parquet-mr_f6798_0() throws Exception
{    TestProtobuf.Document.Builder writtenDocument = TestProtobuf.Document.newBuilder();    writtenDocument.setDocId(12345);    writtenDocument.addNameBuilder().setUrl("http://goout.cz/");    Path outputPath = new WriteUsingMR().write(writtenDocument.build());        ReadUsingMR reader = new ReadUsingMR();    String projection = "message Document {required int64 DocId; }";    reader.setRequestedProjection(projection);    List<Message> output = reader.read(outputPath);    TestProtobuf.Document readDocument = (TestProtobuf.Document) output.get(0);        assertTrue(readDocument.hasDocId());    assertTrue("Found data outside projection.", readDocument.getNameCount() == 0);}
public void parquet-mr_f6799_0() throws Exception
{    TestProto3.Document.Builder writtenDocument = TestProto3.Document.newBuilder();    writtenDocument.setDocId(12345);    writtenDocument.addNameBuilder().setUrl("http://goout.cz/");    Path outputPath = new WriteUsingMR().write(writtenDocument.build());        ReadUsingMR reader = new ReadUsingMR();    String projection = "message Document {optional int64 DocId; }";    reader.setRequestedProjection(projection);    List<Message> output = reader.read(outputPath);    TestProto3.Document readDocument = (TestProto3.Document) output.get(0);        assertTrue(readDocument.getDocId() == 12345);    assertTrue(readDocument.getNameCount() == 0);    assertTrue("Found data outside projection.", readDocument.getNameCount() == 0);}
public void parquet-mr_f6800_0() throws Exception
{    FirstCustomClassMessage.Builder inputMessage;    inputMessage = FirstCustomClassMessage.newBuilder();    inputMessage.setString("writtenString");    Path outputPath = new WriteUsingMR().write(new Message[] { inputMessage.build() });    ReadUsingMR readUsingMR = new ReadUsingMR();    String customClass = SecondCustomClassMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(1, result.size());    Message msg = result.get(0);    assertFalse("Class from header returned.", msg instanceof FirstCustomClassMessage);    assertTrue("Custom class was not used", msg instanceof SecondCustomClassMessage);    String stringValue;    stringValue = ((SecondCustomClassMessage) msg).getString();    assertEquals("writtenString", stringValue);}
public void parquet-mr_f6801_0() throws Exception
{    TestProto3.FirstCustomClassMessage.Builder inputMessage;    inputMessage = TestProto3.FirstCustomClassMessage.newBuilder();    inputMessage.setString("writtenString");    Path outputPath = new WriteUsingMR().write(new Message[] { inputMessage.build() });    ReadUsingMR readUsingMR = new ReadUsingMR();    String customClass = TestProto3.SecondCustomClassMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(1, result.size());    Message msg = result.get(0);    assertFalse("Class from header returned.", msg instanceof TestProto3.FirstCustomClassMessage);    assertTrue("Custom class was not used", msg instanceof TestProto3.SecondCustomClassMessage);    String stringValue;    stringValue = ((TestProto3.SecondCustomClassMessage) msg).getString();    assertEquals("writtenString", stringValue);}
public void parquet-mr_f6802_0() throws Exception
{    TestProtobuf.RepeatedIntMessage msgEmpty = TestProtobuf.RepeatedIntMessage.newBuilder().build();    TestProtobuf.RepeatedIntMessage msgNonEmpty = TestProtobuf.RepeatedIntMessage.newBuilder().addRepeatedInt(1).addRepeatedInt(2).build();    Path outputPath = new WriteUsingMR().write(msgEmpty, msgNonEmpty);    ReadUsingMR readUsingMR = new ReadUsingMR();    String customClass = TestProtobuf.RepeatedIntMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(2, result.size());    assertEquals(msgEmpty, result.get(0));    assertEquals(msgNonEmpty, result.get(1));}
public void parquet-mr_f6803_0() throws Exception
{    TestProtobuf.RepeatedIntMessage msgEmpty = TestProtobuf.RepeatedIntMessage.newBuilder().build();    TestProtobuf.RepeatedIntMessage msgNonEmpty = TestProtobuf.RepeatedIntMessage.newBuilder().addRepeatedInt(1).addRepeatedInt(2).build();    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    Path outputPath = new WriteUsingMR(conf).write(msgEmpty, msgNonEmpty);    ReadUsingMR readUsingMR = new ReadUsingMR();    String customClass = TestProtobuf.RepeatedIntMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(2, result.size());    assertEquals(msgEmpty, result.get(0));    assertEquals(msgNonEmpty, result.get(1));}
public void parquet-mr_f6804_0() throws Exception
{    TestProtobuf.MapIntMessage msgEmpty = TestProtobuf.MapIntMessage.newBuilder().build();    TestProtobuf.MapIntMessage msgNonEmpty = TestProtobuf.MapIntMessage.newBuilder().putMapInt(1, 123).putMapInt(2, 234).build();    Path outputPath = new WriteUsingMR().write(msgEmpty, msgNonEmpty);    ReadUsingMR readUsingMR = new ReadUsingMR();    String customClass = TestProtobuf.MapIntMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(2, result.size());    assertEquals(msgEmpty, result.get(0));    assertEquals(msgNonEmpty, result.get(1));}
public void parquet-mr_f6805_0() throws Exception
{    TestProtobuf.MapIntMessage msgEmpty = TestProtobuf.MapIntMessage.newBuilder().build();    TestProtobuf.MapIntMessage msgNonEmpty = TestProtobuf.MapIntMessage.newBuilder().putMapInt(1, 123).putMapInt(2, 234).build();    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    Path outputPath = new WriteUsingMR(conf).write(msgEmpty, msgNonEmpty);    ReadUsingMR readUsingMR = new ReadUsingMR(conf);    String customClass = TestProtobuf.MapIntMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(2, result.size());    assertEquals(msgEmpty, result.get(0));    assertEquals(msgNonEmpty, result.get(1));}
public void parquet-mr_f6806_0() throws Exception
{    TestProtobuf.RepeatedInnerMessage msgEmpty = TestProtobuf.RepeatedInnerMessage.newBuilder().build();    TestProtobuf.RepeatedInnerMessage msgNonEmpty = TestProtobuf.RepeatedInnerMessage.newBuilder().addRepeatedInnerMessage(TestProtobuf.InnerMessage.newBuilder().setOne("one").build()).addRepeatedInnerMessage(TestProtobuf.InnerMessage.newBuilder().setTwo("two").build()).build();    Path outputPath = new WriteUsingMR().write(msgEmpty, msgNonEmpty);    ReadUsingMR readUsingMR = new ReadUsingMR();    String customClass = TestProtobuf.RepeatedInnerMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(2, result.size());    assertEquals(msgEmpty, result.get(0));    assertEquals(msgNonEmpty, result.get(1));}
public void parquet-mr_f6807_0() throws Exception
{    TestProtobuf.RepeatedInnerMessage msgEmpty = TestProtobuf.RepeatedInnerMessage.newBuilder().build();    TestProtobuf.RepeatedInnerMessage msgNonEmpty = TestProtobuf.RepeatedInnerMessage.newBuilder().addRepeatedInnerMessage(TestProtobuf.InnerMessage.newBuilder().setOne("one").build()).addRepeatedInnerMessage(TestProtobuf.InnerMessage.newBuilder().setTwo("two").build()).build();    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    Path outputPath = new WriteUsingMR(conf).write(msgEmpty, msgNonEmpty);    ReadUsingMR readUsingMR = new ReadUsingMR(conf);    String customClass = TestProtobuf.RepeatedInnerMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(2, result.size());    assertEquals(msgEmpty, result.get(0));    assertEquals(msgNonEmpty, result.get(1));}
public static List<Message> parquet-mr_f6808_0(Message... messages) throws Exception
{    Path outputPath = new WriteUsingMR().write(messages);    List<Message> result = new ReadUsingMR().read(outputPath);    return result;}
public void parquet-mr_f6809_0() throws Exception
{    SchemaConverterAllDatatypes.Builder data;    data = SchemaConverterAllDatatypes.newBuilder();    data.setOptionalBool(true);    data.setOptionalBytes(ByteString.copyFrom("someText", "UTF-8"));    data.setOptionalDouble(0.577);    data.setOptionalFloat(3.1415f);    data.setOptionalEnum(SchemaConverterAllDatatypes.TestEnum.FIRST);    data.setOptionalFixed32(1000 * 1000 * 1);    data.setOptionalFixed64(1000 * 1000 * 1000 * 2);    data.setOptionalInt32(1000 * 1000 * 3);    data.setOptionalInt64(1000L * 1000 * 1000 * 4);    data.setOptionalSFixed32(1000 * 1000 * 5);    data.setOptionalSFixed64(1000L * 1000 * 1000 * 6);    data.setOptionalSInt32(1000 * 1000 * 56);    data.setOptionalSInt64(1000L * 1000 * 1000 * 7);    data.setOptionalString("Good Will Hunting");    data.setOptionalUInt32(1000 * 1000 * 8);    data.setOptionalUInt64(1000L * 1000 * 1000 * 9);    data.getOptionalMessageBuilder().setSomeId(1984);    data.getPbGroupBuilder().setGroupInt(1492);    SchemaConverterAllDatatypes dataBuilt = data.build();    data.clear();    List<TestProtobuf.SchemaConverterAllDatatypes> result;    result = testData(dataBuilt);        SchemaConverterAllDatatypes o = result.get(0);    assertEquals("Good Will Hunting", o.getOptionalString());    assertEquals(true, o.getOptionalBool());    assertEquals(ByteString.copyFrom("someText", "UTF-8"), o.getOptionalBytes());    assertEquals(0.577, o.getOptionalDouble(), 0.00001);    assertEquals(3.1415f, o.getOptionalFloat(), 0.00001);    assertEquals(SchemaConverterAllDatatypes.TestEnum.FIRST, o.getOptionalEnum());    assertEquals(1000 * 1000 * 1, o.getOptionalFixed32());    assertEquals(1000 * 1000 * 1000 * 2, o.getOptionalFixed64());    assertEquals(1000 * 1000 * 3, o.getOptionalInt32());    assertEquals(1000L * 1000 * 1000 * 4, o.getOptionalInt64());    assertEquals(1000 * 1000 * 5, o.getOptionalSFixed32());    assertEquals(1000L * 1000 * 1000 * 6, o.getOptionalSFixed64());    assertEquals(1000 * 1000 * 56, o.getOptionalSInt32());    assertEquals(1000L * 1000 * 1000 * 7, o.getOptionalSInt64());    assertEquals(1000 * 1000 * 8, o.getOptionalUInt32());    assertEquals(1000L * 1000 * 1000 * 9, o.getOptionalUInt64());    assertEquals(1984, o.getOptionalMessage().getSomeId());    assertEquals(1492, o.getPbGroup().getGroupInt());}
public void parquet-mr_f6810_0() throws Exception
{    TestProto3.SchemaConverterAllDatatypes.Builder data;    data = TestProto3.SchemaConverterAllDatatypes.newBuilder();    data.setOptionalBool(true);    data.setOptionalBytes(ByteString.copyFrom("someText", "UTF-8"));    data.setOptionalDouble(0.577);    data.setOptionalFloat(3.1415f);    data.setOptionalEnum(TestProto3.SchemaConverterAllDatatypes.TestEnum.FIRST);    data.setOptionalFixed32(1000 * 1000 * 1);    data.setOptionalFixed64(1000 * 1000 * 1000 * 2);    data.setOptionalInt32(1000 * 1000 * 3);    data.setOptionalInt64(1000L * 1000 * 1000 * 4);    data.setOptionalSFixed32(1000 * 1000 * 5);    data.setOptionalSFixed64(1000L * 1000 * 1000 * 6);    data.setOptionalSInt32(1000 * 1000 * 56);    data.setOptionalSInt64(1000L * 1000 * 1000 * 7);    data.setOptionalString("Good Will Hunting");    data.setOptionalUInt32(1000 * 1000 * 8);    data.setOptionalUInt64(1000L * 1000 * 1000 * 9);    data.getOptionalMessageBuilder().setSomeId(1984);    TestProto3.SchemaConverterAllDatatypes dataBuilt = data.build();    data.clear();    List<TestProto3.SchemaConverterAllDatatypes> result;    result = testData(dataBuilt);        TestProto3.SchemaConverterAllDatatypes o = result.get(0);    assertEquals("Good Will Hunting", o.getOptionalString());    assertEquals(true, o.getOptionalBool());    assertEquals(ByteString.copyFrom("someText", "UTF-8"), o.getOptionalBytes());    assertEquals(0.577, o.getOptionalDouble(), 0.00001);    assertEquals(3.1415f, o.getOptionalFloat(), 0.00001);    assertEquals(TestProto3.SchemaConverterAllDatatypes.TestEnum.FIRST, o.getOptionalEnum());    assertEquals(1000 * 1000 * 1, o.getOptionalFixed32());    assertEquals(1000 * 1000 * 1000 * 2, o.getOptionalFixed64());    assertEquals(1000 * 1000 * 3, o.getOptionalInt32());    assertEquals(1000L * 1000 * 1000 * 4, o.getOptionalInt64());    assertEquals(1000 * 1000 * 5, o.getOptionalSFixed32());    assertEquals(1000L * 1000 * 1000 * 6, o.getOptionalSFixed64());    assertEquals(1000 * 1000 * 56, o.getOptionalSInt32());    assertEquals(1000L * 1000 * 1000 * 7, o.getOptionalSInt64());    assertEquals(1000 * 1000 * 8, o.getOptionalUInt32());    assertEquals(1000L * 1000 * 1000 * 9, o.getOptionalUInt64());    assertEquals(1984, o.getOptionalMessage().getSomeId());}
public void parquet-mr_f6811_0() throws Exception
{    int count = 100;    SchemaConverterAllDatatypes[] input = new SchemaConverterAllDatatypes[count];    for (int i = 0; i < count; i++) {        SchemaConverterAllDatatypes.Builder d = SchemaConverterAllDatatypes.newBuilder();        if (i % 2 != 0)            d.setOptionalBool(true);        if (i % 3 != 0)            d.setOptionalBytes(ByteString.copyFrom("someText " + i, "UTF-8"));        if (i % 4 != 0)            d.setOptionalDouble(0.577 * i);        if (i % 5 != 0)            d.setOptionalFloat(3.1415f * i);        if (i % 6 != 0)            d.setOptionalEnum(SchemaConverterAllDatatypes.TestEnum.FIRST);        if (i % 7 != 0)            d.setOptionalFixed32(1000 * i * 1);        if (i % 8 != 0)            d.setOptionalFixed64(1000 * i * 1000 * 2);        if (i % 9 != 0)            d.setOptionalInt32(1000 * i * 3);        if (i % 2 != 1)            d.setOptionalSFixed32(1000 * i * 5);        if (i % 3 != 1)            d.setOptionalSFixed64(1000 * i * 1000 * 6);        if (i % 4 != 1)            d.setOptionalSInt32(1000 * i * 56);        if (i % 5 != 1)            d.setOptionalSInt64(1000 * i * 1000 * 7);        if (i % 6 != 1)            d.setOptionalString("Good Will Hunting " + i);        if (i % 7 != 1)            d.setOptionalUInt32(1000 * i * 8);        if (i % 8 != 1)            d.setOptionalUInt64(1000 * i * 1000 * 9);        if (i % 9 != 1)            d.getOptionalMessageBuilder().setSomeId(1984 * i);        if (i % 2 != 1)            d.getPbGroupBuilder().setGroupInt(1492 * i);        if (i % 3 != 1)            d.setOptionalInt64(1000 * i * 1000 * 4);        input[i] = d.build();    }    List<TestProtobuf.SchemaConverterAllDatatypes> result;    result = testData(input);        assertEquals("Good Will Hunting 0", result.get(0).getOptionalString());    assertEquals("Good Will Hunting 90", result.get(90).getOptionalString());}
public void parquet-mr_f6812_0() throws Exception
{    int count = 100;    TestProto3.SchemaConverterAllDatatypes[] input = new TestProto3.SchemaConverterAllDatatypes[count];    for (int i = 0; i < count; i++) {        TestProto3.SchemaConverterAllDatatypes.Builder d = TestProto3.SchemaConverterAllDatatypes.newBuilder();        if (i % 2 != 0)            d.setOptionalBool(true);        if (i % 3 != 0)            d.setOptionalBytes(ByteString.copyFrom("someText " + i, "UTF-8"));        if (i % 4 != 0)            d.setOptionalDouble(0.577 * i);        if (i % 5 != 0)            d.setOptionalFloat(3.1415f * i);        if (i % 6 != 0)            d.setOptionalEnum(TestProto3.SchemaConverterAllDatatypes.TestEnum.FIRST);        if (i % 7 != 0)            d.setOptionalFixed32(1000 * i * 1);        if (i % 8 != 0)            d.setOptionalFixed64(1000 * i * 1000 * 2);        if (i % 9 != 0)            d.setOptionalInt32(1000 * i * 3);        if (i % 2 != 1)            d.setOptionalSFixed32(1000 * i * 5);        if (i % 3 != 1)            d.setOptionalSFixed64(1000 * i * 1000 * 6);        if (i % 4 != 1)            d.setOptionalSInt32(1000 * i * 56);        if (i % 5 != 1)            d.setOptionalSInt64(1000 * i * 1000 * 7);        if (i % 6 != 1)            d.setOptionalString("Good Will Hunting " + i);        if (i % 7 != 1)            d.setOptionalUInt32(1000 * i * 8);        if (i % 8 != 1)            d.setOptionalUInt64(1000 * i * 1000 * 9);        if (i % 9 != 1)            d.getOptionalMessageBuilder().setSomeId(1984 * i);        if (i % 3 != 1)            d.setOptionalInt64(1000 * i * 1000 * 4);        input[i] = d.build();    }    List<TestProto3.SchemaConverterAllDatatypes> result;    result = testData(input);        assertEquals("Good Will Hunting 0", result.get(0).getOptionalString());    assertEquals("Good Will Hunting 90", result.get(90).getOptionalString());}
public void parquet-mr_f6813_0() throws Exception
{    SchemaConverterAllDatatypes.Builder data;    data = SchemaConverterAllDatatypes.newBuilder();    List<SchemaConverterAllDatatypes> result = testData(data.build());    SchemaConverterAllDatatypes message = result.get(0);    assertEquals("", message.getOptionalString());    assertEquals(false, message.getOptionalBool());    assertEquals(0, message.getOptionalFixed32());}
public void parquet-mr_f6814_0() throws Exception
{    TestProto3.SchemaConverterAllDatatypes.Builder data;    data = TestProto3.SchemaConverterAllDatatypes.newBuilder();    List<TestProto3.SchemaConverterAllDatatypes> result = testData(data.build());    TestProto3.SchemaConverterAllDatatypes message = result.get(0);    assertEquals("", message.getOptionalString());    assertEquals(false, message.getOptionalBool());    assertEquals(0, message.getOptionalFixed32());}
public void parquet-mr_f6815_0() throws Exception
{    TestProtobuf.TopMessage.Builder top = TestProtobuf.TopMessage.newBuilder();    top.addInnerBuilder().setOne("First inner");    top.addInnerBuilder().setTwo("Second inner");    top.addInnerBuilder().setThree("Third inner");    TestProtobuf.TopMessage result = testData(top.build()).get(0);    assertEquals(3, result.getInnerCount());    TestProtobuf.InnerMessage first = result.getInner(0);    TestProtobuf.InnerMessage second = result.getInner(1);    TestProtobuf.InnerMessage third = result.getInner(2);    assertEquals("First inner", first.getOne());    assertFalse(first.hasTwo());    assertFalse(first.hasThree());    assertEquals("Second inner", second.getTwo());    assertFalse(second.hasOne());    assertFalse(second.hasThree());    assertEquals("Third inner", third.getThree());    assertFalse(third.hasOne());    assertFalse(third.hasTwo());}
public void parquet-mr_f6816_0() throws Exception
{    TestProto3.TopMessage.Builder top = TestProto3.TopMessage.newBuilder();    top.addInnerBuilder().setOne("First inner");    top.addInnerBuilder().setTwo("Second inner");    top.addInnerBuilder().setThree("Third inner");    TestProto3.TopMessage result = testData(top.build()).get(0);    assertEquals(3, result.getInnerCount());    TestProto3.InnerMessage first = result.getInner(0);    TestProto3.InnerMessage second = result.getInner(1);    TestProto3.InnerMessage third = result.getInner(2);    assertEquals("First inner", first.getOne());    assertTrue(first.getTwo().isEmpty());    assertTrue(first.getThree().isEmpty());    assertEquals("Second inner", second.getTwo());    assertTrue(second.getOne().isEmpty());    assertTrue(second.getThree().isEmpty());    assertEquals("Third inner", third.getThree());    assertTrue(third.getOne().isEmpty());    assertTrue(third.getTwo().isEmpty());}
public void parquet-mr_f6817_0() throws Exception
{    TestProtobuf.RepeatedIntMessage.Builder top = TestProtobuf.RepeatedIntMessage.newBuilder();    top.addRepeatedInt(1);    top.addRepeatedInt(2);    top.addRepeatedInt(3);    TestProtobuf.RepeatedIntMessage result = testData(top.build()).get(0);    assertEquals(3, result.getRepeatedIntCount());    assertEquals(1, result.getRepeatedInt(0));    assertEquals(2, result.getRepeatedInt(1));    assertEquals(3, result.getRepeatedInt(2));}
public void parquet-mr_f6818_0() throws Exception
{    TestProto3.RepeatedIntMessage.Builder top = TestProto3.RepeatedIntMessage.newBuilder();    top.addRepeatedInt(1);    top.addRepeatedInt(2);    top.addRepeatedInt(3);    TestProto3.RepeatedIntMessage result = testData(top.build()).get(0);    assertEquals(3, result.getRepeatedIntCount());    assertEquals(1, result.getRepeatedInt(0));    assertEquals(2, result.getRepeatedInt(1));    assertEquals(3, result.getRepeatedInt(2));}
public void parquet-mr_f6819_0() throws Exception
{    TestProtobuf.HighIndexMessage.Builder builder = TestProtobuf.HighIndexMessage.newBuilder();    builder.addRepeatedInt(1);    builder.addRepeatedInt(2);    testData(builder.build());}
public void parquet-mr_f6820_0() throws Exception
{    TestProto3.HighIndexMessage.Builder builder = TestProto3.HighIndexMessage.newBuilder();    builder.addRepeatedInt(1);    builder.addRepeatedInt(2);    testData(builder.build());}
private void parquet-mr_f6821_0(Class<? extends Message> pbClass, String parquetSchemaString, boolean parquetSpecsCompliant) throws Exception
{    ProtoSchemaConverter protoSchemaConverter = new ProtoSchemaConverter(parquetSpecsCompliant);    MessageType schema = protoSchemaConverter.convert(pbClass);    MessageType expectedMT = MessageTypeParser.parseMessageType(parquetSchemaString);    assertEquals(expectedMT.toString(), schema.toString());}
private void parquet-mr_f6822_0(Class<? extends Message> pbClass, String parquetSchemaString) throws Exception
{    testConversion(pbClass, parquetSchemaString, true);}
public void parquet-mr_f6823_0() throws Exception
{    String expectedSchema = "message TestProtobuf.SchemaConverterAllDatatypes {\n" + "  optional double optionalDouble = 1;\n" + "  optional float optionalFloat = 2;\n" + "  optional int32 optionalInt32 = 3;\n" + "  optional int64 optionalInt64 = 4;\n" + "  optional int32 optionalUInt32 = 5;\n" + "  optional int64 optionalUInt64 = 6;\n" + "  optional int32 optionalSInt32 = 7;\n" + "  optional int64 optionalSInt64 = 8;\n" + "  optional int32 optionalFixed32 = 9;\n" + "  optional int64 optionalFixed64 = 10;\n" + "  optional int32 optionalSFixed32 = 11;\n" + "  optional int64 optionalSFixed64 = 12;\n" + "  optional boolean optionalBool = 13;\n" + "  optional binary optionalString (UTF8) = 14;\n" + "  optional binary optionalBytes = 15;\n" + "  optional group optionalMessage = 16 {\n" + "    optional int32 someId = 3;\n" + "  }\n" + "  optional group pbgroup = 17 {\n" + "    optional int32 groupInt = 2;\n" + "  }\n" + " optional binary optionalEnum (ENUM)  = 18;" + "}";    testConversion(TestProtobuf.SchemaConverterAllDatatypes.class, expectedSchema);}
public void parquet-mr_f6824_0() throws Exception
{    String expectedSchema = "message TestProto3.SchemaConverterAllDatatypes {\n" + "  optional double optionalDouble = 1;\n" + "  optional float optionalFloat = 2;\n" + "  optional int32 optionalInt32 = 3;\n" + "  optional int64 optionalInt64 = 4;\n" + "  optional int32 optionalUInt32 = 5;\n" + "  optional int64 optionalUInt64 = 6;\n" + "  optional int32 optionalSInt32 = 7;\n" + "  optional int64 optionalSInt64 = 8;\n" + "  optional int32 optionalFixed32 = 9;\n" + "  optional int64 optionalFixed64 = 10;\n" + "  optional int32 optionalSFixed32 = 11;\n" + "  optional int64 optionalSFixed64 = 12;\n" + "  optional boolean optionalBool = 13;\n" + "  optional binary optionalString (UTF8) = 14;\n" + "  optional binary optionalBytes = 15;\n" + "  optional group optionalMessage = 16 {\n" + "    optional int32 someId = 3;\n" + "  }\n" + "  optional binary optionalEnum (ENUM) = 18;" + "  optional int32 someInt32 = 19;" + "  optional binary someString (UTF8) = 20;" + "  optional group optionalMap (MAP) = 21 {\n" + "    repeated group key_value {\n" + "      required int64 key;\n" + "      optional group value {\n" + "        optional int32 someId = 3;\n" + "      }\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProto3.SchemaConverterAllDatatypes.class, expectedSchema);}
public void parquet-mr_f6825_0() throws Exception
{    String expectedSchema = "message TestProtobuf.SchemaConverterRepetition {\n" + "  optional int32 optionalPrimitive = 1;\n" + "  required int32 requiredPrimitive = 2;\n" + "  optional group repeatedPrimitive (LIST) = 3 {\n" + "    repeated group list {\n" + "      required int32 element;\n" + "    }\n" + "  }\n" + "  optional group optionalMessage = 7 {\n" + "    optional int32 someId = 3;\n" + "  }\n" + "  required group requiredMessage = 8 {\n" + "    optional int32 someId= 3;\n" + "  }\n" + "  optional group repeatedMessage (LIST) = 9 {\n" + "    repeated group list {\n" + "      optional group element {\n" + "        optional int32 someId = 3;\n" + "      }\n" + "    }\n" + "  }" + "}";    testConversion(TestProtobuf.SchemaConverterRepetition.class, expectedSchema);}
public void parquet-mr_f6826_0() throws Exception
{    String expectedSchema = "message TestProto3.SchemaConverterRepetition {\n" + "  optional int32 optionalPrimitive = 1;\n" + "  optional group repeatedPrimitive (LIST) = 3 {\n" + "    repeated group list {\n" + "      required int32 element;\n" + "    }\n" + "  }\n" + "  optional group optionalMessage = 7 {\n" + "    optional int32 someId = 3;\n" + "  }\n" + "  optional group repeatedMessage (LIST) = 9 {\n" + "    repeated group list {\n" + "      optional group element {\n" + "        optional int32 someId = 3;\n" + "      }\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProto3.SchemaConverterRepetition.class, expectedSchema);}
public void parquet-mr_f6827_0() throws Exception
{    String expectedSchema = "message TestProtobuf.RepeatedIntMessage {\n" + "  optional group repeatedInt (LIST) = 1 {\n" + "    repeated group list {\n" + "      required int32 element;\n" + "      }\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProtobuf.RepeatedIntMessage.class, expectedSchema);}
public void parquet-mr_f6828_0() throws Exception
{    String expectedSchema = "message TestProtobuf.RepeatedIntMessage {\n" + "  repeated int32 repeatedInt = 1;\n" + "}";    testConversion(TestProtobuf.RepeatedIntMessage.class, expectedSchema, false);}
public void parquet-mr_f6829_0() throws Exception
{    String expectedSchema = "message TestProto3.RepeatedIntMessage {\n" + "  optional group repeatedInt (LIST) = 1 {\n" + "    repeated group list {\n" + "      required int32 element;\n" + "      }\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProto3.RepeatedIntMessage.class, expectedSchema);}
public void parquet-mr_f6830_0() throws Exception
{    String expectedSchema = "message TestProto3.RepeatedIntMessage {\n" + "  repeated int32 repeatedInt = 1;\n" + "}";    testConversion(TestProto3.RepeatedIntMessage.class, expectedSchema, false);}
public void parquet-mr_f6831_0() throws Exception
{    String expectedSchema = "message TestProtobuf.RepeatedInnerMessage {\n" + "  optional group repeatedInnerMessage (LIST) = 1 {\n" + "    repeated group list {\n" + "      optional group element {\n" + "        optional binary one (UTF8) = 1;\n" + "        optional binary two (UTF8) = 2;\n" + "        optional binary three (UTF8) = 3;\n" + "      }\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProtobuf.RepeatedInnerMessage.class, expectedSchema);}
public void parquet-mr_f6832_0() throws Exception
{    String expectedSchema = "message TestProtobuf.RepeatedInnerMessage {\n" + "  repeated group repeatedInnerMessage = 1 {\n" + "    optional binary one (UTF8) = 1;\n" + "    optional binary two (UTF8) = 2;\n" + "    optional binary three (UTF8) = 3;\n" + "  }\n" + "}";    testConversion(TestProtobuf.RepeatedInnerMessage.class, expectedSchema, false);}
public void parquet-mr_f6833_0() throws Exception
{    String expectedSchema = "message TestProto3.RepeatedInnerMessage {\n" + "  optional group repeatedInnerMessage (LIST) = 1 {\n" + "    repeated group list {\n" + "      optional group element {\n" + "        optional binary one (UTF8) = 1;\n" + "        optional binary two (UTF8) = 2;\n" + "        optional binary three (UTF8) = 3;\n" + "      }\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProto3.RepeatedInnerMessage.class, expectedSchema);}
public void parquet-mr_f6834_0() throws Exception
{    String expectedSchema = "message TestProto3.RepeatedInnerMessage {\n" + "  repeated group repeatedInnerMessage = 1 {\n" + "    optional binary one (UTF8) = 1;\n" + "    optional binary two (UTF8) = 2;\n" + "    optional binary three (UTF8) = 3;\n" + "  }\n" + "}";    testConversion(TestProto3.RepeatedInnerMessage.class, expectedSchema, false);}
public void parquet-mr_f6835_0() throws Exception
{    String expectedSchema = "message TestProtobuf.MapIntMessage {\n" + "  optional group mapInt (MAP) = 1 {\n" + "    repeated group key_value {\n" + "      required int32 key;\n" + "      optional int32 value;\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProtobuf.MapIntMessage.class, expectedSchema);}
public void parquet-mr_f6836_0() throws Exception
{    String expectedSchema = "message TestProtobuf.MapIntMessage {\n" + "  repeated group mapInt = 1 {\n" + "    optional int32 key = 1;\n" + "    optional int32 value = 2;\n" + "  }\n" + "}";    testConversion(TestProtobuf.MapIntMessage.class, expectedSchema, false);}
public void parquet-mr_f6837_0() throws Exception
{    String expectedSchema = "message TestProto3.MapIntMessage {\n" + "  optional group mapInt (MAP) = 1 {\n" + "    repeated group key_value {\n" + "      required int32 key;\n" + "      optional int32 value;\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProto3.MapIntMessage.class, expectedSchema);}
public void parquet-mr_f6838_0() throws Exception
{    String expectedSchema = "message TestProto3.MapIntMessage {\n" + "  repeated group mapInt = 1 {\n" + "    optional int32 key = 1;\n" + "    optional int32 value = 2;\n" + "  }\n" + "}";    testConversion(TestProto3.MapIntMessage.class, expectedSchema, false);}
private ProtoWriteSupport<T> parquet-mr_f6839_0(Class<T> cls, RecordConsumer readConsumerMock)
{    return createReadConsumerInstance(cls, readConsumerMock, new Configuration());}
private ProtoWriteSupport<T> parquet-mr_f6840_0(Class<T> cls, RecordConsumer readConsumerMock, Configuration conf)
{    ProtoWriteSupport support = new ProtoWriteSupport(cls);    support.init(conf);    support.prepareForWrite(readConsumerMock);    return support;}
public void parquet-mr_f6841_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.InnerMessage.class, readConsumerMock);    TestProtobuf.InnerMessage.Builder msg = TestProtobuf.InnerMessage.newBuilder();    msg.setOne("oneValue");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromString("oneValue"));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6842_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.InnerMessage.class, readConsumerMock);    TestProto3.InnerMessage.Builder msg = TestProto3.InnerMessage.newBuilder();    msg.setOne("oneValue");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromString("oneValue"));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6843_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.RepeatedIntMessage.class, readConsumerMock, conf);    TestProtobuf.RepeatedIntMessage.Builder msg = TestProtobuf.RepeatedIntMessage.newBuilder();    msg.addRepeatedInt(1323);    msg.addRepeatedInt(54469);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("repeatedInt", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("list", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).addInteger(1323);    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).addInteger(54469);    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("list", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("repeatedInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6844_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.RepeatedIntMessage.class, readConsumerMock);    TestProtobuf.RepeatedIntMessage.Builder msg = TestProtobuf.RepeatedIntMessage.newBuilder();    msg.addRepeatedInt(1323);    msg.addRepeatedInt(54469);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("repeatedInt", 0);    inOrder.verify(readConsumerMock).addInteger(1323);    inOrder.verify(readConsumerMock).addInteger(54469);    inOrder.verify(readConsumerMock).endField("repeatedInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6845_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.RepeatedIntMessage.class, readConsumerMock, conf);    TestProtobuf.RepeatedIntMessage.Builder msg = TestProtobuf.RepeatedIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6846_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.RepeatedIntMessage.class, readConsumerMock);    TestProtobuf.RepeatedIntMessage.Builder msg = TestProtobuf.RepeatedIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6847_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.RepeatedIntMessage.class, readConsumerMock, conf);    TestProto3.RepeatedIntMessage.Builder msg = TestProto3.RepeatedIntMessage.newBuilder();    msg.addRepeatedInt(1323);    msg.addRepeatedInt(54469);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("repeatedInt", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("list", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).addInteger(1323);    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).addInteger(54469);    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("list", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("repeatedInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6848_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.RepeatedIntMessage.class, readConsumerMock);    TestProto3.RepeatedIntMessage.Builder msg = TestProto3.RepeatedIntMessage.newBuilder();    msg.addRepeatedInt(1323);    msg.addRepeatedInt(54469);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("repeatedInt", 0);    inOrder.verify(readConsumerMock).addInteger(1323);    inOrder.verify(readConsumerMock).addInteger(54469);    inOrder.verify(readConsumerMock).endField("repeatedInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6849_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.RepeatedIntMessage.class, readConsumerMock, conf);    TestProtobuf.RepeatedIntMessage.Builder msg = TestProtobuf.RepeatedIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6850_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.RepeatedIntMessage.class, readConsumerMock);    TestProtobuf.RepeatedIntMessage.Builder msg = TestProtobuf.RepeatedIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6851_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.MapIntMessage.class, readConsumerMock, conf);    TestProtobuf.MapIntMessage.Builder msg = TestProtobuf.MapIntMessage.newBuilder();    msg.putMapInt(123, 1);    msg.putMapInt(234, 2);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("mapInt", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key_value", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(123);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(1);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(234);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(2);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("key_value", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("mapInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6852_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.MapIntMessage.class, readConsumerMock);    TestProtobuf.MapIntMessage.Builder msg = TestProtobuf.MapIntMessage.newBuilder();    msg.putMapInt(123, 1);    msg.putMapInt(234, 2);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("mapInt", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(123);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(1);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(234);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(2);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("mapInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6853_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.MapIntMessage.class, readConsumerMock, conf);    TestProtobuf.MapIntMessage.Builder msg = TestProtobuf.MapIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6854_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.MapIntMessage.class, readConsumerMock);    TestProtobuf.MapIntMessage.Builder msg = TestProtobuf.MapIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6855_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.MapIntMessage.class, readConsumerMock, conf);    TestProto3.MapIntMessage.Builder msg = TestProto3.MapIntMessage.newBuilder();    msg.putMapInt(123, 1);    msg.putMapInt(234, 2);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("mapInt", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key_value", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(123);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(1);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(234);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(2);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("key_value", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("mapInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6856_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.MapIntMessage.class, readConsumerMock);    TestProto3.MapIntMessage.Builder msg = TestProto3.MapIntMessage.newBuilder();    msg.putMapInt(123, 1);    msg.putMapInt(234, 2);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("mapInt", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(123);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(1);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(234);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(2);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("mapInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6857_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.MapIntMessage.class, readConsumerMock, conf);    TestProto3.MapIntMessage.Builder msg = TestProto3.MapIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6858_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.MapIntMessage.class, readConsumerMock);    TestProto3.MapIntMessage.Builder msg = TestProto3.MapIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6859_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.TopMessage.class, readConsumerMock);    TestProtobuf.TopMessage.Builder msg = TestProtobuf.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one").setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6860_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.TopMessage.class, readConsumerMock, conf);    TestProtobuf.TopMessage.Builder msg = TestProtobuf.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one").setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("list", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("list", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6861_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ;    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.TopMessage.class, readConsumerMock);    TestProto3.TopMessage.Builder msg = TestProto3.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one").setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6862_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.TopMessage.class, readConsumerMock, conf);    TestProto3.TopMessage.Builder msg = TestProto3.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one").setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("list", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("list", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6863_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.TopMessage.class, readConsumerMock, conf);    TestProtobuf.TopMessage.Builder msg = TestProtobuf.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one");    msg.addInnerBuilder().setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("list", 0);        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("list", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6864_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.TopMessage.class, readConsumerMock);    TestProtobuf.TopMessage.Builder msg = TestProtobuf.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one");    msg.addInnerBuilder().setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endGroup();        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6865_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.TopMessage.class, readConsumerMock);    TestProto3.TopMessage.Builder msg = TestProto3.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one");    msg.addInnerBuilder().setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endGroup();        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6866_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.TopMessage.class, readConsumerMock, conf);    TestProto3.TopMessage.Builder msg = TestProto3.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one");    msg.addInnerBuilder().setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("list", 0);        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("list", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6867_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.MessageA.class, readConsumerMock);    TestProtobuf.MessageA.Builder msg = TestProtobuf.MessageA.newBuilder();    msg.getInnerBuilder().setOne("one");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6868_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.MessageA.class, readConsumerMock);    TestProto3.MessageA.Builder msg = TestProto3.MessageA.newBuilder();    msg.getInnerBuilder().setOne("one");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
public void parquet-mr_f6869_0() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.Vehicle.class, readConsumerMock);    TestProtobuf.Vehicle.Builder msg = TestProtobuf.Vehicle.newBuilder();    msg.setHorsePower(300);            msg.setExtension(TestProtobuf.Airplane.wingSpan, 50);    instance.write(msg.build());}
public static Path parquet-mr_f6870_0() throws IOException
{    File tmp = File.createTempFile("ParquetProtobuf_unitTest", ".tmp");    tmp.deleteOnExit();    tmp.delete();    return new Path(tmp.getPath());}
public static List<T> parquet-mr_f6871_0(T... records) throws IOException
{    Class<? extends Message> cls = inferRecordsClass(records);    Path file = writeMessages(cls, records);    return readMessages(file);}
public static Class<? extends Message> parquet-mr_f6872_0(MessageOrBuilder[] records)
{    Class<? extends Message> cls = null;    for (MessageOrBuilder record : records) {        Class<? extends Message> recordClass;        if (record instanceof Message.Builder) {            recordClass = ((Message.Builder) record).build().getClass();        } else if (record instanceof Message) {            recordClass = ((Message) record).getClass();        } else {            throw new RuntimeException("Illegal class " + record);        }        if (cls == null) {            cls = recordClass;        } else if (!cls.equals(recordClass)) {            throw new RuntimeException("Class mismatch :" + cls + " and " + recordClass);        }    }    return cls;}
public static List<T> parquet-mr_f6873_0(T... messages) throws IOException
{    checkSameBuilderInstance(messages);    List<MessageOrBuilder> input = cloneList(messages);    List<MessageOrBuilder> output = (List<MessageOrBuilder>) writeAndRead(messages);    List<Message> outputAsMessages = asMessages(output);    assertEquals("The protocol buffers are not same:\n", asMessages(input), outputAsMessages);    return (List<T>) outputAsMessages;}
private static List<MessageOrBuilder> parquet-mr_f6874_0(MessageOrBuilder[] messages)
{    List<MessageOrBuilder> result = new ArrayList<MessageOrBuilder>();    for (MessageOrBuilder mob : messages) {        result.add(asMessage(mob));    }    return result;}
public static List<Message> parquet-mr_f6875_0(List<MessageOrBuilder> mobs)
{    List<Message> result = new ArrayList<Message>();    for (MessageOrBuilder messageOrBuilder : mobs) {        result.add(asMessage(messageOrBuilder));    }    return result;}
public static Message parquet-mr_f6876_0(MessageOrBuilder mob)
{    Message message;    if (mob instanceof Message.Builder) {        message = ((Message.Builder) mob).build();    } else {        message = (Message) mob;    }    return message;}
private static void parquet-mr_f6877_0(MessageOrBuilder[] messages)
{    for (int i = 0; i < messages.length; i++) {        MessageOrBuilder firstMessage = messages[i];        boolean isBuilder = firstMessage instanceof Message.Builder;        if (isBuilder) {            for (int j = 0; j < messages.length; j++) {                MessageOrBuilder secondMessage = messages[j];                if (i != j) {                    boolean isSame = secondMessage == firstMessage;                    if (isSame) {                        fail("Data contains two references to same instance." + secondMessage);                    }                }            }        }    }}
public static List<T> parquet-mr_f6878_0(Path file) throws IOException
{    ProtoParquetReader<T> reader = new ProtoParquetReader<T>(file);    List<T> result = new ArrayList<T>();    boolean hasNext = true;    while (hasNext) {        T item = reader.read();        if (item == null) {            hasNext = false;        } else {            assertNotNull(item);                        result.add((T) asMessage(item).toBuilder());        }    }    reader.close();    return result;}
public static Path parquet-mr_f6879_0(MessageOrBuilder... records) throws IOException
{    return writeMessages(inferRecordsClass(records), records);}
public static Path parquet-mr_f6880_0(Class<? extends Message> cls, MessageOrBuilder... records) throws IOException
{    Path file = someTemporaryFilePath();    ProtoParquetWriter<MessageOrBuilder> writer = new ProtoParquetWriter<MessageOrBuilder>(file, cls);    for (MessageOrBuilder record : records) {        writer.write(record);    }    writer.close();    return file;}
public void parquet-mr_f6881_0(String projection)
{    this.projection = projection;}
public Configuration parquet-mr_f6882_0()
{    return conf;}
protected void parquet-mr_f6883_0(Void key, MessageOrBuilder value, Context context)
{    Message clone = ((Message.Builder) value).build();    outputMessages.add(clone);}
public List<Message> parquet-mr_f6884_0(Path parquetPath) throws Exception
{    synchronized (ReadUsingMR.class) {        outputMessages = new ArrayList<Message>();        final Job job = new Job(conf, "read");        job.setInputFormatClass(ProtoParquetInputFormat.class);        ProtoParquetInputFormat.setInputPaths(job, parquetPath);        if (projection != null) {            ProtoParquetInputFormat.setRequestedProjection(job, projection);        }        job.setMapperClass(ReadingMapper.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(NullOutputFormat.class);        WriteUsingMR.waitForJob(job);        List<Message> result = Collections.unmodifiableList(outputMessages);        outputMessages = null;        return result;    }}
public Configuration parquet-mr_f6885_0()
{    return conf;}
public void parquet-mr_f6886_1(Context context) throws IOException, InterruptedException
{    if (inputMessages == null || inputMessages.size() == 0) {        throw new RuntimeException("No mock data given");    } else {        for (Message msg : inputMessages) {            context.write(null, msg);                    }    }}
public Path parquet-mr_f6887_0(Message... messages) throws Exception
{    synchronized (WriteUsingMR.class) {        outputPath = TestUtils.someTemporaryFilePath();        Path inputPath = TestUtils.someTemporaryFilePath();        FileSystem fileSystem = inputPath.getFileSystem(conf);        fileSystem.create(inputPath);        inputMessages = Collections.unmodifiableList(Arrays.asList(messages));        final Job job = new Job(conf, "write");                TextInputFormat.addInputPath(job, inputPath);        job.setInputFormatClass(TextInputFormat.class);        job.setMapperClass(WritingMapper.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(ProtoParquetOutputFormat.class);        ProtoParquetOutputFormat.setOutputPath(job, outputPath);        ProtoParquetOutputFormat.setProtobufClass(job, TestUtils.inferRecordsClass(messages));        waitForJob(job);        inputMessages = null;        return outputPath;    }}
 static void parquet-mr_f6888_1(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {                sleep(50);    }        if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
public static void parquet-mr_f6889_0(Configuration configuration, Class<? extends ThriftStruct> thriftClass)
{    ScroogeWriteSupport.setScroogeClass(configuration, thriftClass);}
public static Class<? extends ThriftStruct> parquet-mr_f6890_0(Configuration configuration)
{    return ScroogeWriteSupport.getScroogeClass(configuration);}
public void parquet-mr_f6891_0(FlowProcess<JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    DeprecatedParquetOutputFormat.setAsOutputFormat(jobConf);    ParquetOutputFormat.setWriteSupportClass(jobConf, ScroogeWriteSupport.class);    ScroogeWriteSupport.setScroogeClass(jobConf, this.config.getKlass());}
public void parquet-mr_f6892_0(FlowProcess<JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    super.sourceConfInit(fp, tap, jobConf);    jobConf.setInputFormat(DeprecatedParquetInputFormat.class);    ParquetInputFormat.setReadSupportClass(jobConf, ScroogeReadSupport.class);    ThriftReadSupport.setRecordConverterClass(jobConf, ScroogeRecordConverter.class);}
protected MessageType parquet-mr_f6893_0(FieldProjectionFilter fieldProjectionFilter)
{    ThriftType.StructType thriftStruct = new ScroogeStructConverter().convert(thriftClass);    return new ThriftSchemaConverter(fieldProjectionFilter).convert(thriftStruct);}
public T parquet-mr_f6894_0(TProtocol protocol) throws TException
{    return codec.decode(protocol);}
private static ThriftStructCodec<?> parquet-mr_f6895_0(Class<?> klass)
{    Class<?> companionClass;    try {        companionClass = Class.forName(klass.getName() + "$");        Object companionObject = companionClass.getField("MODULE$").get(null);        return (ThriftStructCodec<?>) companionObject;    } catch (Exception t) {        if (t instanceof InterruptedException)            Thread.currentThread().interrupt();        throw new RuntimeException("Unable to create ThriftStructCodec", t);    }}
public ThriftType.StructType parquet-mr_f6896_0(Class scroogeClass)
{    return convertStructFromClass(scroogeClass);}
private static String parquet-mr_f6897_0(String fieldName)
{    return fieldName + "_map_key";}
private static String parquet-mr_f6898_0(String fieldName)
{    return fieldName + "_map_value";}
private static String parquet-mr_f6899_0(String fieldName)
{    return fieldName + "_list_elem";}
private static String parquet-mr_f6900_0(String fieldName)
{    return fieldName + "_set_elem";}
private Class parquet-mr_f6901_0(Class klass)
{    try {        return Class.forName(klass.getName() + "$");    } catch (ClassNotFoundException e) {        throw new ScroogeSchemaConversionException("Can not find companion object for scrooge class " + klass, e);    }}
private ThriftType.StructType parquet-mr_f6902_0(Class klass)
{    return convertCompanionClassToStruct(getCompanionClass(klass));}
private ThriftType.StructType parquet-mr_f6903_0(Class<?> companionClass)
{    ThriftStructCodec<?> companionObject;    try {        companionObject = (ThriftStructCodec<?>) companionClass.getField("MODULE$").get(null);    } catch (NoSuchFieldException e) {        throw new ScroogeSchemaConversionException("Can not get ThriftStructCodec from companion object of " + companionClass.getName(), e);    } catch (IllegalAccessException e) {        throw new ScroogeSchemaConversionException("Can not get ThriftStructCodec from companion object of " + companionClass.getName(), e);    }        List<ThriftField> children = new LinkedList<ThriftField>();    Iterable<ThriftStructFieldInfo> scroogeFields = getFieldInfos(companionObject);    for (ThriftStructFieldInfo field : scroogeFields) {        children.add(toThriftField(field));    }    StructOrUnionType structOrUnionType = isUnion(companionObject.getClass()) ? StructOrUnionType.UNION : StructOrUnionType.STRUCT;    return new ThriftType.StructType(children, structOrUnionType);}
private Iterable<ThriftStructFieldInfo> parquet-mr_f6904_0(ThriftStructCodec<?> c)
{    Class<? extends ThriftStructCodec> klass = c.getClass();    if (isUnion(klass)) {                return getFieldInfosForUnion(klass);    } else {                try {            Object r = klass.getMethod("fieldInfos").invoke(c);            return JavaConversions$.MODULE$.asJavaIterable((scala.collection.Iterable<ThriftStructFieldInfo>) r);        } catch (ClassCastException e) {            throw new ScroogeSchemaConversionException("can not get field Info from: " + c.toString(), e);        } catch (InvocationTargetException e) {            throw new ScroogeSchemaConversionException("can not get field Info from: " + c.toString(), e);        } catch (NoSuchMethodException e) {            throw new ScroogeSchemaConversionException("can not get field Info from: " + c.toString(), e);        } catch (IllegalAccessException e) {            throw new ScroogeSchemaConversionException("can not get field Info from: " + c.toString(), e);        }    }}
private Iterable<ThriftStructFieldInfo> parquet-mr_f6905_0(Class klass)
{    ArrayList<ThriftStructFieldInfo> fields = new ArrayList<ThriftStructFieldInfo>();    for (Field f : klass.getDeclaredFields()) {        if (f.getType().equals(Manifest.class)) {            Class unionClass = (Class) ((ParameterizedType) f.getGenericType()).getActualTypeArguments()[0];            Class companionUnionClass = getCompanionClass(unionClass);            try {                Object companionUnionObj = companionUnionClass.getField("MODULE$").get(null);                ThriftStructFieldInfo info = (ThriftStructFieldInfo) companionUnionClass.getMethod("fieldInfo").invoke(companionUnionObj);                fields.add(info);            } catch (NoSuchFieldException e) {                throw new ScroogeSchemaConversionException("can not find fieldInfo for " + unionClass, e);            } catch (InvocationTargetException e) {                throw new ScroogeSchemaConversionException("can not find fieldInfo for " + unionClass, e);            } catch (NoSuchMethodException e) {                throw new ScroogeSchemaConversionException("can not find fieldInfo for " + unionClass, e);            } catch (IllegalAccessException e) {                throw new ScroogeSchemaConversionException("can not find fieldInfo for " + unionClass, e);            }        }    }    return fields;}
public ThriftField parquet-mr_f6906_0(ThriftStructFieldInfo scroogeField)
{    Requirement requirement = getRequirementType(scroogeField);    String fieldName = scroogeField.tfield().name;    short fieldId = scroogeField.tfield().id;    byte thriftTypeByte = scroogeField.tfield().type;    ThriftTypeID typeId = ThriftTypeID.fromByte(thriftTypeByte);    ThriftType thriftType;    switch(typeId) {        case BOOL:            thriftType = new ThriftType.BoolType();            break;        case BYTE:            thriftType = new ThriftType.ByteType();            break;        case DOUBLE:            thriftType = new ThriftType.DoubleType();            break;        case I16:            thriftType = new ThriftType.I16Type();            break;        case I32:            thriftType = new ThriftType.I32Type();            break;        case I64:            thriftType = new ThriftType.I64Type();            break;        case STRING:            ThriftType.StringType stringType = new ThriftType.StringType();                        if (!String.class.equals(scroogeField.manifest().runtimeClass())) {                stringType.setBinary(true);            }            thriftType = stringType;            break;        case STRUCT:            thriftType = convertStructTypeField(scroogeField);            break;        case MAP:            thriftType = convertMapTypeField(scroogeField, requirement);            break;        case SET:            thriftType = convertSetTypeField(scroogeField, requirement);            break;        case LIST:            thriftType = convertListTypeField(scroogeField, requirement);            break;        case ENUM:            thriftType = convertEnumTypeField(scroogeField);            break;        case STOP:        case VOID:        default:            throw new IllegalArgumentException("can't convert type " + typeId);    }    return new ThriftField(fieldName, fieldId, requirement, thriftType);}
private ThriftType parquet-mr_f6907_0(ThriftStructFieldInfo f, Requirement requirement)
{    return convertSetTypeField(f.tfield().name, f.valueManifest().get(), requirement);}
private ThriftType parquet-mr_f6908_0(String fieldName, Manifest<?> valueManifest, Requirement requirement)
{    String elemName = setElemName(fieldName);    ThriftType elementType = convertClassToThriftType(elemName, requirement, valueManifest);            ThriftField elementField = generateFieldWithoutId(elemName, requirement, elementType);    return new ThriftType.SetType(elementField);}
private ThriftType parquet-mr_f6909_0(ThriftStructFieldInfo f, Requirement requirement)
{    return convertListTypeField(f.tfield().name, f.valueManifest().get(), requirement);}
private ThriftType parquet-mr_f6910_0(String fieldName, Manifest<?> valueManifest, Requirement requirement)
{    String elemName = listElemName(fieldName);    ThriftType elementType = convertClassToThriftType(elemName, requirement, valueManifest);    ThriftField elementField = generateFieldWithoutId(elemName, requirement, elementType);    return new ThriftType.ListType(elementField);}
private ThriftType parquet-mr_f6911_0(ThriftStructFieldInfo f, Requirement requirement)
{    return convertMapTypeField(f.tfield().name, f.keyManifest().get(), f.valueManifest().get(), requirement);}
private ThriftType parquet-mr_f6912_0(String fieldName, Manifest<?> keyManifest, Manifest<?> valueManifest, Requirement requirement)
{    String keyName = mapKeyName(fieldName);    String valueName = mapValueName(fieldName);    ThriftType keyType = convertClassToThriftType(keyName, requirement, keyManifest);    ThriftField keyField = generateFieldWithoutId(keyName, requirement, keyType);    ThriftType valueType = convertClassToThriftType(valueName, requirement, valueManifest);    ThriftField valueField = generateFieldWithoutId(valueName, requirement, valueType);    return new ThriftType.MapType(keyField, valueField);}
private ThriftField parquet-mr_f6913_0(String fieldName, Requirement requirement, ThriftType thriftType)
{    return new ThriftField(fieldName, (short) 1, requirement, thriftType);}
private ThriftType parquet-mr_f6914_0(String name, Requirement requirement, Manifest<?> typeManifest)
{    Class typeClass = typeManifest.runtimeClass();    if (typeManifest.runtimeClass() == boolean.class) {        return new ThriftType.BoolType();    } else if (typeClass == byte.class) {        return new ThriftType.ByteType();    } else if (typeClass == double.class) {        return new ThriftType.DoubleType();    } else if (typeClass == short.class) {        return new ThriftType.I16Type();    } else if (typeClass == int.class) {        return new ThriftType.I32Type();    } else if (typeClass == long.class) {        return new ThriftType.I64Type();    } else if (typeClass == String.class) {        return new ThriftType.StringType();    } else if (typeClass == ByteBuffer.class) {        return new ThriftType.StringType();    } else if (typeClass == scala.collection.Seq.class) {        Manifest<?> a = typeManifest.typeArguments().apply(0);        return convertListTypeField(name, a, requirement);    } else if (typeClass == scala.collection.Set.class) {        Manifest<?> setElementManifest = typeManifest.typeArguments().apply(0);        return convertSetTypeField(name, setElementManifest, requirement);    } else if (typeClass == scala.collection.Map.class) {        List<Manifest<?>> ms = JavaConversions.seqAsJavaList(typeManifest.typeArguments());        Manifest keyManifest = ms.get(0);        Manifest valueManifest = ms.get(1);        return convertMapTypeField(name, keyManifest, valueManifest, requirement);    } else if (com.twitter.scrooge.ThriftEnum.class.isAssignableFrom(typeClass)) {        return convertEnumTypeField(typeClass, name);    } else {        return convertStructFromClass(typeClass);    }}
private ThriftType parquet-mr_f6915_0(ThriftStructFieldInfo f)
{    return convertStructFromClass(f.manifest().runtimeClass());}
private List parquet-mr_f6916_0(String enumName) throws ClassNotFoundException, IllegalAccessException, NoSuchFieldException, NoSuchMethodException, InvocationTargetException
{        enumName += "$";    Class companionObjectClass = Class.forName(enumName);    Object cObject = companionObjectClass.getField("MODULE$").get(null);    Method listMethod = companionObjectClass.getMethod("list", new Class[] {});    Object result = listMethod.invoke(cObject, null);    return JavaConversions.seqAsJavaList((Seq) result);}
public ThriftType parquet-mr_f6917_0(ThriftStructFieldInfo f)
{    return convertEnumTypeField(f.manifest().runtimeClass(), f.tfield().name);}
private ThriftType parquet-mr_f6918_0(Class enumClass, String fieldName)
{    List<ThriftType.EnumValue> enumValues = new ArrayList<ThriftType.EnumValue>();    String enumName = enumClass.getName();    try {        List enumCollection = getEnumList(enumName);        for (Object enumObj : enumCollection) {            ScroogeEnumDesc enumDesc = ScroogeEnumDesc.fromEnum(enumObj);            enumValues.add(new ThriftType.EnumValue(enumDesc.id, enumDesc.originalName));        }        return new ThriftType.EnumType(enumValues);    } catch (RuntimeException e) {        throw new ScroogeSchemaConversionException("Can not convert enum field " + fieldName, e);    } catch (NoSuchMethodException e) {        throw new ScroogeSchemaConversionException("Can not convert enum field " + fieldName, e);    } catch (IllegalAccessException e) {        throw new ScroogeSchemaConversionException("Can not convert enum field " + fieldName, e);    } catch (NoSuchFieldException e) {        throw new ScroogeSchemaConversionException("Can not convert enum field " + fieldName, e);    } catch (InvocationTargetException e) {        throw new ScroogeSchemaConversionException("Can not convert enum field " + fieldName, e);    } catch (ClassNotFoundException e) {        throw new ScroogeSchemaConversionException("Can not convert enum field " + fieldName, e);    }}
private boolean parquet-mr_f6919_0(Class klass)
{    for (Field f : klass.getDeclaredFields()) {        if (f.getName().equals("Union"))            return true;    }    return false;}
private Requirement parquet-mr_f6920_0(ThriftStructFieldInfo f)
{    if (f.isOptional() && !f.isRequired()) {        return OPTIONAL;    } else if (f.isRequired() && !f.isOptional()) {        return REQUIRED;    } else if (!f.isOptional() && !f.isRequired()) {        return DEFAULT;    } else {        throw new ScroogeSchemaConversionException("can not determine requirement type for : " + f.toString() + ", isOptional=" + f.isOptional() + ", isRequired=" + f.isRequired());    }}
public static ScroogeEnumDesc parquet-mr_f6921_0(Object rawScroogeEnum) throws NoSuchMethodException, InvocationTargetException, IllegalAccessException
{    Class enumClass = rawScroogeEnum.getClass();    Method valueMethod = enumClass.getMethod("value", new Class[] {});    Method originalNameMethod = enumClass.getMethod("originalName", new Class[] {});    ScroogeEnumDesc result = new ScroogeEnumDesc();    result.id = (Integer) valueMethod.invoke(rawScroogeEnum, null);    result.originalName = (String) originalNameMethod.invoke(rawScroogeEnum, null);    return result;}
public static void parquet-mr_f6922_0(Configuration configuration, Class<? extends ThriftStruct> thriftClass)
{    AbstractThriftWriteSupport.setGenericThriftClass(configuration, thriftClass);}
public static Class<? extends ThriftStruct> parquet-mr_f6923_0(Configuration configuration)
{    return (Class<? extends ThriftStruct>) AbstractThriftWriteSupport.getGenericThriftClass(configuration);}
public String parquet-mr_f6924_0()
{    return "scrooge";}
protected StructType parquet-mr_f6925_0()
{    ScroogeStructConverter schemaConverter = new ScroogeStructConverter();    return schemaConverter.convert(thriftClass);}
public void parquet-mr_f6926_0(T record)
{    try {        record.write(parquetWriteProtocol);    } catch (TException e) {        throw new ParquetEncodingException(e);    }}
public void parquet-mr_f6927_0() throws Exception
{    RequiredPrimitiveFixture toWrite = new RequiredPrimitiveFixture(true, (byte) 2, (short) 3, 4, (long) 5, 6.0, "7");    toWrite.setInfo_string("it's info");    verifyScroogeRead(thriftRecords(toWrite), org.apache.parquet.scrooge.test.RequiredPrimitiveFixture.class, "RequiredPrimitiveFixture(true,2,3,4,5,6.0,7,Some(it's info))\n", "**");}
public void parquet-mr_f6928_0() throws Exception
{    Map<String, org.apache.parquet.thrift.test.Phone> phoneMap = new HashMap<String, Phone>();    phoneMap.put("key1", new org.apache.parquet.thrift.test.Phone("111", "222"));    org.apache.parquet.thrift.test.TestPersonWithAllInformation toWrite = new org.apache.parquet.thrift.test.TestPersonWithAllInformation(new org.apache.parquet.thrift.test.Name("first"), new Address("my_street", "my_zip"), phoneMap);    toWrite.setInfo("my_info");    String expected = "TestPersonWithAllInformation(Name(first,None),None,Address(my_street,my_zip),None,Some(my_info),Map(key1 -> Phone(111,222)),None,None)\n";    verifyScroogeRead(thriftRecords(toWrite), TestPersonWithAllInformation.class, expected, "**");    String expectedProjected = "TestPersonWithAllInformation(Name(first,None),None,Address(my_street,my_zip),None,Some(my_info),Map(),None,None)\n";    verifyScroogeRead(thriftRecords(toWrite), TestPersonWithAllInformation.class, expectedProjected, "address/*;info;name/first_name");}
public void parquet-mr_f6929_0(FlowProcess flowProcess, FunctionCall functionCall)
{    Object record = functionCall.getArguments().getObject(0);    Tuple result = new Tuple();    result.add(record.toString());    functionCall.getOutputCollector().add(result);}
public void parquet-mr_f6930_0(List<TBase> recordsToWrite, Class<T> readClass, String expectedStr, String projectionFilter) throws Exception
{    Configuration conf = new Configuration();    deleteIfExist(PARQUET_PATH);    deleteIfExist(TXT_OUTPUT_PATH);    final Path parquetFile = new Path(PARQUET_PATH);    writeParquetFile(recordsToWrite, conf, parquetFile);    Scheme sourceScheme = new ParquetScroogeScheme(new Config().withRecordClass(readClass).withProjectionString(projectionFilter));    Tap source = new Hfs(sourceScheme, PARQUET_PATH);    Scheme sinkScheme = new TextLine(new Fields("first", "last"));    Tap sink = new Hfs(sinkScheme, TXT_OUTPUT_PATH);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new ObjectToStringFunction());    Flow flow = new HadoopFlowConnector().connect("namecp", source, sink, assembly);    flow.complete();    String result = FileUtils.readFileToString(new File(TXT_OUTPUT_PATH + "/part-00000"));    assertEquals(expectedStr, result);}
private void parquet-mr_f6931_0(List<TBase> recordsToWrite, Configuration conf, Path parquetFile) throws IOException, InterruptedException, org.apache.thrift.TException
{        final TProtocolFactory protocolFactory = new TCompactProtocol.Factory();    final TaskAttemptID taskId = new TaskAttemptID("local", 0, true, 0, 0);    Class writeClass = recordsToWrite.get(0).getClass();    final ThriftToParquetFileWriter w = new ThriftToParquetFileWriter(parquetFile, ContextUtil.newTaskAttemptContext(conf, taskId), protocolFactory, writeClass);    final ByteArrayOutputStream baos = new ByteArrayOutputStream();    final TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(baos));    for (TBase recordToWrite : recordsToWrite) {        recordToWrite.write(protocol);    }    w.write(new BytesWritable(baos.toByteArray()));    w.close();}
private List<TBase> parquet-mr_f6932_0(TBase... records)
{    List<TBase> result = new ArrayList<TBase>();    for (TBase record : records) {        result.add(record);    }    return result;}
private void parquet-mr_f6933_0(String path) throws IOException
{    Path p = new Path(path);    Configuration conf = new Configuration();    final FileSystem fs = p.getFileSystem(conf);    if (fs.exists(p)) {        fs.delete(p, true);    }}
public void parquet-mr_f6934_0() throws Exception
{    doWrite();    doRead();}
private void parquet-mr_f6935_0() throws Exception
{    Path path = new Path(parquetOutputPath);    final FileSystem fs = path.getFileSystem(new Configuration());    if (fs.exists(path))        fs.delete(path, true);    Scheme sourceScheme = new TextLine(new Fields("first", "last"));    Tap source = new Hfs(sourceScheme, txtInputPath);    Scheme sinkScheme = new ParquetScroogeScheme<Name>(Name.class);    Tap sink = new Hfs(sinkScheme, parquetOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new PackThriftFunction());    Flow flow = new HadoopFlowConnector().connect("namecp", source, sink, assembly);    flow.complete();}
private void parquet-mr_f6936_0() throws Exception
{    Path path = new Path(txtOutputPath);    final FileSystem fs = path.getFileSystem(new Configuration());    if (fs.exists(path))        fs.delete(path, true);    Scheme sourceScheme = new ParquetScroogeScheme<Name>(Name.class);    Tap source = new Hfs(sourceScheme, parquetOutputPath);    Scheme sinkScheme = new TextLine(new Fields("first", "last"));    Tap sink = new Hfs(sinkScheme, txtOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new UnpackThriftFunction());    Flow flow = new HadoopFlowConnector().connect("namecp", source, sink, assembly);    flow.complete();    String result = FileUtils.readFileToString(new File(txtOutputPath + "/part-00000"));    assertEquals("0\tAlice\tPractice\n15\tBob\tHope\n24\tCharlie\tHorse\n", result);}
public void parquet-mr_f6937_0(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Name name = Name$.MODULE$.apply(arguments.getString(0), Option.apply(arguments.getString(1)));    result.add(name);    functionCall.getOutputCollector().add(result);}
public void parquet-mr_f6938_0(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Name name = (Name) arguments.getObject(0);    result.add(name.firstName());    result.add(name.lastName().get());    functionCall.getOutputCollector().add(result);}
public void parquet-mr_f6939_0() throws Exception
{    StringAndBinary expected = new StringAndBinary.Immutable("test", ByteBuffer.wrap(new byte[] { -123, 20, 33 }));    File temp = tempDir.newFile(UUID.randomUUID().toString());    temp.deleteOnExit();    temp.delete();    Path path = new Path(temp.getPath());    ParquetWriter<StringAndBinary> writer = new ParquetWriter<StringAndBinary>(path, new Configuration(), new ScroogeWriteSupport<StringAndBinary>(StringAndBinary.class));    writer.write(expected);    writer.close();        ParquetReader<org.apache.parquet.thrift.test.binary.StringAndBinary> reader = ThriftParquetReader.<org.apache.parquet.thrift.test.binary.StringAndBinary>build(path).withThriftClass(org.apache.parquet.thrift.test.binary.StringAndBinary.class).build();    org.apache.parquet.thrift.test.binary.StringAndBinary record = reader.read();    reader.close();    Assert.assertEquals("String should match after serialization round trip", "test", record.s);    Assert.assertEquals("ByteBuffer should match after serialization round trip", ByteBuffer.wrap(new byte[] { -123, 20, 33 }), record.b);}
public void parquet-mr_f6940_0() throws Exception
{    StringAndBinary expected = new StringAndBinary.Immutable("test", ByteBuffer.wrap(new byte[] { -123, 20, 33 }));    File temp = tempDir.newFile(UUID.randomUUID().toString());    temp.deleteOnExit();    temp.delete();    Path path = new Path(temp.getPath());    ParquetWriter<StringAndBinary> writer = new ParquetWriter<StringAndBinary>(path, new Configuration(), new ScroogeWriteSupport<StringAndBinary>(StringAndBinary.class));    writer.write(expected);    writer.close();    Configuration conf = new Configuration();    conf.set("parquet.thrift.converter.class", ScroogeRecordConverter.class.getName());    ParquetReader<StringAndBinary> reader = ParquetReader.<StringAndBinary>builder(new ScroogeReadSupport(), path).withConf(conf).build();    StringAndBinary record = reader.read();    reader.close();    Assert.assertEquals("String should match after serialization round trip", "test", record.s());    Assert.assertEquals("ByteBuffer should match after serialization round trip", ByteBuffer.wrap(new byte[] { -123, 20, 33 }), record.b());}
private void parquet-mr_f6941_0(Class scroogeClass) throws ClassNotFoundException
{    Class<? extends TBase<?, ?>> thriftClass = (Class<? extends TBase<?, ?>>) Class.forName(scroogeClass.getName().replaceFirst("org.apache.parquet.scrooge.test", "org.apache.parquet.thrift.test"));    ThriftType.StructType structFromThriftSchemaConverter = ThriftSchemaConverter.toStructType(thriftClass);    ThriftType.StructType structFromScroogeSchemaConverter = new ScroogeStructConverter().convert(scroogeClass);    assertEquals(toParquetSchema(structFromThriftSchemaConverter), toParquetSchema(structFromScroogeSchemaConverter));}
private MessageType parquet-mr_f6942_0(ThriftType.StructType struct)
{    return ThriftSchemaConverter.convertWithoutProjection(struct);}
public void parquet-mr_f6943_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestMapPrimitiveKey.class);}
public void parquet-mr_f6944_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(StringAndBinary.class);}
public void parquet-mr_f6945_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestUnion.class);}
public void parquet-mr_f6946_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestMapPrimitiveValue.class);}
public void parquet-mr_f6947_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestListPrimitive.class);}
public void parquet-mr_f6948_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestSetPrimitive.class);}
public void parquet-mr_f6949_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestFieldOfEnum.class);}
public void parquet-mr_f6950_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestMapBinary.class);}
public void parquet-mr_f6951_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestMapComplex.class);}
public void parquet-mr_f6952_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestPersonWithAllInformation.class);}
public void parquet-mr_f6953_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(AddressWithStreetWithDefaultRequirement.class);}
public void parquet-mr_f6954_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestOptionalMap.class);}
public void parquet-mr_f6955_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(NestedList.class);}
public void parquet-mr_f6956_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(ListNestMap.class);}
public void parquet-mr_f6957_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(ListNestEnum.class);}
public void parquet-mr_f6958_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(MapNestList.class);}
public void parquet-mr_f6959_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(MapNestMap.class);}
public void parquet-mr_f6960_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(MapNestSet.class);}
public void parquet-mr_f6961_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(ListNestSet.class);}
public void parquet-mr_f6962_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(SetNestSet.class);}
public void parquet-mr_f6963_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(SetNestList.class);}
public void parquet-mr_f6964_0() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(SetNestMap.class);}
public void parquet-mr_f6965_0(Job job, Path path) throws Exception
{    job.setInputFormatClass(ParquetScroogeInputFormat.class);    ParquetScroogeInputFormat.setInputPaths(job, path);    ParquetScroogeInputFormat.setThriftClass(job.getConfiguration(), StructWithUnionV2.class);    ThriftReadSupport.setRecordConverterClass(job.getConfiguration(), ScroogeRecordConverter.class);    job.setMapperClass(ReadMapper.class);    job.setNumReduceTasks(0);    job.setOutputFormatClass(NullOutputFormat.class);}
protected void parquet-mr_f6966_0(List<org.apache.parquet.thrift.test.compat.StructWithUnionV2> expected, List<Object> found) throws Exception
{    List<StructWithUnionV2> scroogeExpected = new ArrayList<StructWithUnionV2>();    for (org.apache.parquet.thrift.test.compat.StructWithUnionV2 tbase : expected) {        ByteArrayOutputStream baos = new ByteArrayOutputStream();        TProtocol out = new Factory().getProtocol(new TIOStreamTransport(baos));        tbase.write(out);        TProtocol in = new Factory().getProtocol(new TIOStreamTransport(new ByteArrayInputStream(baos.toByteArray())));        scroogeExpected.add(StructWithUnionV2$.MODULE$.decode(in));    }    assertEquals(scroogeExpected, found);}
public static void parquet-mr_f6967_0(Configuration configuration, Class<?> thriftClass)
{    configuration.set(PARQUET_THRIFT_CLASS, thriftClass.getName());}
public static Class parquet-mr_f6968_0(Configuration configuration)
{    final String thriftClassName = configuration.get(PARQUET_THRIFT_CLASS);    if (thriftClassName == null) {        throw new BadConfigurationException("the thrift class conf is missing in job conf at " + PARQUET_THRIFT_CLASS);    }    try {        @SuppressWarnings("unchecked")        Class thriftClass = Class.forName(thriftClassName);        return thriftClass;    } catch (ClassNotFoundException e) {        throw new BadConfigurationException("the class " + thriftClassName + " in job conf at " + PARQUET_THRIFT_CLASS + " could not be found", e);    }}
protected void parquet-mr_f6969_0(Class<T> thriftClass)
{    this.thriftClass = thriftClass;    this.thriftStruct = getThriftStruct();    this.schema = ThriftSchemaConverter.convertWithoutProjection(thriftStruct);    final Map<String, String> extraMetaData = new ThriftMetaData(thriftClass.getName(), thriftStruct).toExtraMetaData();        if (isPigLoaded() && TBase.class.isAssignableFrom(thriftClass)) {        new PigMetaData(new ThriftToPig((Class<? extends TBase<?, ?>>) thriftClass).toSchema()).addToMetaData(extraMetaData);    }    this.writeContext = new WriteContext(schema, extraMetaData);}
protected boolean parquet-mr_f6970_1()
{    try {        Class.forName("org.apache.pig.impl.logicalLayer.schema.Schema");        return true;    } catch (ClassNotFoundException e) {                return false;    }}
public WriteContext parquet-mr_f6971_0(Configuration configuration)
{    if (writeContext == null) {        init(getGenericThriftClass(configuration));    }    return writeContext;}
public void parquet-mr_f6972_0(RecordConsumer recordConsumer)
{    final MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);    this.parquetWriteProtocol = new ParquetWriteProtocol(recordConsumer, columnIO, thriftStruct);}
public static void parquet-mr_f6973_0(Job job, Class<? extends TBase<?, ?>> thriftClass)
{    TBaseWriteSupport.setThriftClass(ContextUtil.getConfiguration(job), thriftClass);}
public static Class<? extends TBase<?, ?>> parquet-mr_f6974_0(Job job)
{    return TBaseWriteSupport.getThriftClass(ContextUtil.getConfiguration(job));}
public static void parquet-mr_f6975_0(Job job, Class<U> tProtocolClass)
{    ThriftBytesWriteSupport.setTProtocolClass(ContextUtil.getConfiguration(job), tProtocolClass);}
public static void parquet-mr_f6976_0(JobConf conf, Class<T> klass)
{    conf.set(ThriftReadSupport.THRIFT_READ_CLASS_KEY, klass.getName());}
public static void parquet-mr_f6977_0(Configuration conf, Class<T> klass)
{    conf.set(ThriftReadSupport.THRIFT_READ_CLASS_KEY, klass.getName());}
public static void parquet-mr_f6978_0(Job job, Class<? extends TBase<?, ?>> thriftClass)
{    ThriftWriteSupport.setThriftClass(ContextUtil.getConfiguration(job), thriftClass);}
public static Class<? extends TBase<?, ?>> parquet-mr_f6979_0(Job job)
{    return ThriftWriteSupport.getThriftClass(ContextUtil.getConfiguration(job));}
public static void parquet-mr_f6980_0(Configuration configuration, Class<U> thriftClass)
{    AbstractThriftWriteSupport.setGenericThriftClass(configuration, thriftClass);}
public static Class<? extends TBase<?, ?>> parquet-mr_f6981_0(Configuration configuration)
{    return (Class<? extends TBase<?, ?>>) AbstractThriftWriteSupport.getGenericThriftClass(configuration);}
public String parquet-mr_f6982_0()
{    return "thrift";}
protected StructType parquet-mr_f6983_0()
{    return ThriftSchemaConverter.toStructType(thriftClass);}
public void parquet-mr_f6984_0(T record)
{    try {        record.write(parquetWriteProtocol);    } catch (TException e) {        throw new ParquetEncodingException(e);    }}
public static void parquet-mr_f6985_0(Configuration conf, Class<U> tProtocolClass)
{    conf.set(PARQUET_PROTOCOL_CLASS, tProtocolClass.getName());}
public static Class<TProtocolFactory> parquet-mr_f6986_0(Configuration conf)
{    final String tProtocolClassName = conf.get(PARQUET_PROTOCOL_CLASS);    if (tProtocolClassName == null) {        throw new BadConfigurationException("the protocol class conf is missing in job conf at " + PARQUET_PROTOCOL_CLASS);    }    try {        @SuppressWarnings("unchecked")        Class<TProtocolFactory> tProtocolFactoryClass = (Class<TProtocolFactory>) Class.forName(tProtocolClassName + "$Factory");        return tProtocolFactoryClass;    } catch (ClassNotFoundException e) {        throw new BadConfigurationException("the Factory for class " + tProtocolClassName + " in job conf at " + PARQUET_PROTOCOL_CLASS + " could not be found", e);    }}
public String parquet-mr_f6987_0()
{    return "thrift";}
public WriteContext parquet-mr_f6988_0(Configuration configuration)
{    if (this.protocolFactory == null) {        try {            this.protocolFactory = getTProtocolFactoryClass(configuration).newInstance();        } catch (InstantiationException e) {            throw new RuntimeException(e);        } catch (IllegalAccessException e) {            throw new RuntimeException(e);        }    }    if (thriftClass != null) {        TBaseWriteSupport.setThriftClass(configuration, thriftClass);    } else {        thriftClass = TBaseWriteSupport.getThriftClass(configuration);    }    this.thriftStruct = ThriftSchemaConverter.toStructType(thriftClass);    this.schema = ThriftSchemaConverter.convertWithoutProjection(thriftStruct);    if (buffered) {        readToWrite = new BufferedProtocolReadToWrite(thriftStruct, errorHandler);    } else {        readToWrite = new ProtocolReadToWrite();    }    return thriftWriteSupport.init(configuration);}
private TProtocol parquet-mr_f6989_1(BytesWritable record)
{    TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(new ByteArrayInputStream(record.getBytes())));    /* Reduce the chance of OOM when data is corrupted. When readBinary is called on TBinaryProtocol, it reads the length of the binary first,     so if the data is corrupted, it could read a big integer as the length of the binary and therefore causes OOM to happen.     Currently this fix only applies to TBinaryProtocol which has the setReadLength defined (thrift 0.7).      */    if (SET_READ_LENGTH != null && protocol instanceof TBinaryProtocol) {        try {            SET_READ_LENGTH.invoke(protocol, new Object[] { record.getLength() });        } catch (IllegalAccessException | IllegalArgumentException | InvocationTargetException e) {                        SET_READ_LENGTH = null;        }    }    return protocol;}
public void parquet-mr_f6990_0(RecordConsumer recordConsumer)
{    final MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);    this.parquetWriteProtocol = new ParquetWriteProtocol(recordConsumer, columnIO, thriftStruct);    thriftWriteSupport.prepareForWrite(recordConsumer);}
public void parquet-mr_f6991_0(BytesWritable record)
{    try {        readToWrite.readOne(protocol(record), parquetWriteProtocol);    } catch (TException e) {        throw new ParquetEncodingException(e);    }}
public static void parquet-mr_f6992_0(JobConf conf, Class<?> klass)
{    setRecordConverterClass((Configuration) conf, klass);}
public static void parquet-mr_f6993_0(Configuration conf, Class<?> klass)
{    conf.set(RECORD_CONVERTER_CLASS_KEY, klass.getName());}
public static void parquet-mr_f6994_0(JobConf jobConf, String projectionString)
{    jobConf.set(THRIFT_COLUMN_FILTER_KEY, projectionString);}
public static void parquet-mr_f6995_0(Configuration conf, String semicolonDelimitedGlobs)
{    conf.set(STRICT_THRIFT_COLUMN_FILTER_KEY, semicolonDelimitedGlobs);}
public static FieldProjectionFilter parquet-mr_f6996_1(Configuration conf)
{    String deprecated = conf.get(THRIFT_COLUMN_FILTER_KEY);    String strict = conf.get(STRICT_THRIFT_COLUMN_FILTER_KEY);    if (Strings.isNullOrEmpty(deprecated) && Strings.isNullOrEmpty(strict)) {        return null;    }    if (!Strings.isNullOrEmpty(deprecated) && !Strings.isNullOrEmpty(strict)) {        throw new ThriftProjectionException("You cannot provide both " + THRIFT_COLUMN_FILTER_KEY + " and " + STRICT_THRIFT_COLUMN_FILTER_KEY + "! " + THRIFT_COLUMN_FILTER_KEY + " is deprecated.");    }    if (!Strings.isNullOrEmpty(deprecated)) {                return new DeprecatedFieldProjectionFilter(deprecated);    }    return StrictFieldProjectionFilter.fromSemicolonDelimitedString(strict);}
public org.apache.parquet.hadoop.api.ReadSupport.ReadContext parquet-mr_f6997_0(InitContext context)
{    final Configuration configuration = context.getConfiguration();    final MessageType fileMessageType = context.getFileSchema();    MessageType requestedProjection = fileMessageType;    String partialSchemaString = configuration.get(ReadSupport.PARQUET_READ_SCHEMA);    FieldProjectionFilter projectionFilter = getFieldProjectionFilter(configuration);    if (partialSchemaString != null && projectionFilter != null) {        throw new ThriftProjectionException(String.format("You cannot provide both a partial schema and field projection filter." + "Only one of (%s, %s, %s) should be set.", PARQUET_READ_SCHEMA, STRICT_THRIFT_COLUMN_FILTER_KEY, THRIFT_COLUMN_FILTER_KEY));    }        if (partialSchemaString != null) {        requestedProjection = getSchemaForRead(fileMessageType, partialSchemaString);    } else if (projectionFilter != null) {        try {            initThriftClassFromMultipleFiles(context.getKeyValueMetadata(), configuration);            requestedProjection = getProjectedSchema(projectionFilter);        } catch (ClassNotFoundException e) {            throw new ThriftProjectionException("can not find thriftClass from configuration", e);        }    }    MessageType schemaForRead = getSchemaForRead(fileMessageType, requestedProjection);    return new ReadContext(schemaForRead);}
protected MessageType parquet-mr_f6998_0(FieldProjectionFilter fieldProjectionFilter)
{    return new ThriftSchemaConverter(fieldProjectionFilter).convert((Class<TBase<?, ?>>) thriftClass);}
private void parquet-mr_f6999_0(Map<String, Set<String>> fileMetadata, Configuration conf) throws ClassNotFoundException
{    if (thriftClass != null) {        return;    }    String className = conf.get(THRIFT_READ_CLASS_KEY, null);    if (className == null) {        Set<String> names = ThriftMetaData.getThriftClassNames(fileMetadata);        if (names == null || names.size() != 1) {            throw new ParquetDecodingException("Could not read file as the Thrift class is not provided and could not be resolved from the file: " + names);        }        className = names.iterator().next();    }    thriftClass = (Class<T>) Class.forName(className);}
private void parquet-mr_f7000_0(ThriftMetaData metadata, Configuration conf) throws ClassNotFoundException
{    if (thriftClass != null) {        return;    }    String className = conf.get(THRIFT_READ_CLASS_KEY, null);    if (className == null) {        if (metadata == null) {            throw new ParquetDecodingException("Could not read file as the Thrift class is not provided and could not be resolved from the file");        }        thriftClass = (Class<T>) metadata.getThriftClass();    } else {        thriftClass = (Class<T>) Class.forName(className);    }}
public RecordMaterializer<T> parquet-mr_f7001_0(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema, org.apache.parquet.hadoop.api.ReadSupport.ReadContext readContext)
{    ThriftMetaData thriftMetaData = ThriftMetaData.fromExtraMetaData(keyValueMetaData);    try {        initThriftClass(thriftMetaData, configuration);    } catch (ClassNotFoundException e) {        throw new RuntimeException("Cannot find Thrift object class for metadata: " + thriftMetaData, e);    }        if (thriftMetaData == null) {        thriftMetaData = ThriftMetaData.fromThriftClass(thriftClass);    }    String converterClassName = configuration.get(RECORD_CONVERTER_CLASS_KEY, RECORD_CONVERTER_DEFAULT);    return getRecordConverterInstance(converterClassName, thriftClass, readContext.getRequestedSchema(), thriftMetaData.getDescriptor(), configuration);}
private static ThriftRecordConverter<T> parquet-mr_f7002_0(String converterClassName, Class<T> thriftClass, MessageType requestedSchema, StructType descriptor, Configuration conf)
{    Class<ThriftRecordConverter<T>> converterClass;    try {        converterClass = (Class<ThriftRecordConverter<T>>) Class.forName(converterClassName);    } catch (ClassNotFoundException e) {        throw new RuntimeException("Cannot find Thrift converter class: " + converterClassName, e);    }    try {                try {            Constructor<ThriftRecordConverter<T>> constructor = converterClass.getConstructor(Class.class, MessageType.class, StructType.class, Configuration.class);            return constructor.newInstance(thriftClass, requestedSchema, descriptor, conf);        } catch (IllegalAccessException e) {                } catch (NoSuchMethodException e) {                }        Constructor<ThriftRecordConverter<T>> constructor = converterClass.getConstructor(Class.class, MessageType.class, StructType.class);        return constructor.newInstance(thriftClass, requestedSchema, descriptor);    } catch (InstantiationException e) {        throw new RuntimeException("Failed to construct Thrift converter class: " + converterClassName, e);    } catch (InvocationTargetException e) {        throw new RuntimeException("Failed to construct Thrift converter class: " + converterClassName, e);    } catch (IllegalAccessException e) {        throw new RuntimeException("Cannot access constructor for Thrift converter class: " + converterClassName, e);    } catch (NoSuchMethodException e) {        throw new RuntimeException("Cannot find constructor for Thrift converter class: " + converterClassName, e);    }}
public void parquet-mr_f7003_0(BytesWritable bytes) throws IOException, InterruptedException
{    recordWriter.write(null, bytes);}
public void parquet-mr_f7004_0() throws IOException
{    try {        recordWriter.close(taskAttemptContext);    } catch (InterruptedException e) {        Thread.interrupted();        throw new IOException("The thread was interrupted", e);    }}
public static void parquet-mr_f7005_0(Configuration configuration, Class<U> thriftClass)
{    TBaseWriteSupport.setThriftClass(configuration, thriftClass);}
public static Class<? extends TBase<?, ?>> parquet-mr_f7006_0(Configuration configuration)
{    return TBaseWriteSupport.getThriftClass(configuration);}
public String parquet-mr_f7007_0()
{    return writeSupport.getName();}
public WriteContext parquet-mr_f7008_0(Configuration configuration)
{    return this.writeSupport.init(configuration);}
public void parquet-mr_f7009_0(RecordConsumer recordConsumer)
{    this.writeSupport.prepareForWrite(recordConsumer);}
public void parquet-mr_f7010_0(T record)
{    this.writeSupport.write(record);}
public void parquet-mr_f7011_0(TProtocol out) throws TException
{    out.writeFieldStop();    out.writeStructEnd();}
public String parquet-mr_f7012_0()
{    return ")";}
public void parquet-mr_f7013_0(TProtocol out) throws TException
{    out.writeFieldEnd();}
public String parquet-mr_f7014_0()
{    return ";";}
public void parquet-mr_f7015_0(TProtocol out) throws TException
{    out.writeMapEnd();}
public String parquet-mr_f7016_0()
{    return "]";}
public void parquet-mr_f7017_0(TProtocol out) throws TException
{    out.writeListEnd();}
public String parquet-mr_f7018_0()
{    return "}";}
public void parquet-mr_f7019_0(TProtocol out) throws TException
{    out.writeSetEnd();}
public String parquet-mr_f7020_0()
{    return "*}";}
public void parquet-mr_f7021_0(TProtocol in, TProtocol out) throws TException
{    List<Action> buffer = new LinkedList<Action>();    try {        boolean hasFieldsIgnored = readOneStruct(in, buffer, thriftType);        if (hasFieldsIgnored) {            notifyRecordHasFieldIgnored();        }    } catch (Exception e) {        throw new SkippableException(error("Error while reading", buffer), e);    }    try {        for (Action a : buffer) {            a.write(out);        }    } catch (Exception e) {        throw new TException(error("Can not write record", buffer), e);    }}
private void parquet-mr_f7022_0()
{    if (errorHandler != null) {        errorHandler.handleRecordHasFieldIgnored();    }}
private void parquet-mr_f7023_0(TField field)
{    if (errorHandler != null) {        errorHandler.handleFieldIgnored(field);    }}
private String parquet-mr_f7024_0(String message, List<Action> buffer)
{    StringBuilder sb = new StringBuilder(message).append(": ");    for (Action action : buffer) {        sb.append(action.toDebugString());    }    return sb.toString();}
private boolean parquet-mr_f7025_0(TProtocol in, byte type, List<Action> buffer, ThriftType expectedType) throws TException
{    if (expectedType != null && expectedType.getType().getSerializedThriftType() != type) {        throw new DecodingSchemaMismatchException("the data type does not match the expected thrift structure: expected " + expectedType + " got " + typeName(type));    }    boolean hasFieldsIgnored = false;    switch(type) {        case TType.LIST:            hasFieldsIgnored = readOneList(in, buffer, (ListType) expectedType);            break;        case TType.MAP:            hasFieldsIgnored = readOneMap(in, buffer, (MapType) expectedType);            break;        case TType.SET:            hasFieldsIgnored = readOneSet(in, buffer, (SetType) expectedType);            break;        case TType.STRUCT:            hasFieldsIgnored = readOneStruct(in, buffer, (StructType) expectedType);            break;        case TType.STOP:            break;        case TType.BOOL:            final boolean bool = in.readBool();            writeBoolAction(buffer, bool);            break;        case TType.BYTE:            final byte b = in.readByte();            writeByteAction(buffer, b);            break;        case TType.DOUBLE:            final double d = in.readDouble();            writeDoubleAction(buffer, d);            break;        case TType.I16:            final short s = in.readI16();            writeShortAction(buffer, s);            break;                case TType.ENUM:        case TType.I32:            final int i = in.readI32();            checkEnum(expectedType, i);            writeIntAction(buffer, i);            break;        case TType.I64:            final long l = in.readI64();            writeLongAction(buffer, l);            break;        case TType.STRING:            final ByteBuffer bin = in.readBinary();            writeStringAction(buffer, bin);            break;        case TType.VOID:            break;        default:            throw new TException("Unknown type: " + type);    }    return hasFieldsIgnored;}
private void parquet-mr_f7026_0(List<Action> buffer, final ByteBuffer bin)
{    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeBinary(bin);        }        @Override        public String toDebugString() {            return String.valueOf(bin);        }    });}
public void parquet-mr_f7027_0(TProtocol out) throws TException
{    out.writeBinary(bin);}
public String parquet-mr_f7028_0()
{    return String.valueOf(bin);}
private void parquet-mr_f7029_0(List<Action> buffer, final long l)
{    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeI64(l);        }        @Override        public String toDebugString() {            return String.valueOf(l);        }    });}
public void parquet-mr_f7030_0(TProtocol out) throws TException
{    out.writeI64(l);}
public String parquet-mr_f7031_0()
{    return String.valueOf(l);}
private void parquet-mr_f7032_0(List<Action> buffer, final int i)
{    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeI32(i);        }        @Override        public String toDebugString() {            return String.valueOf(i);        }    });}
public void parquet-mr_f7033_0(TProtocol out) throws TException
{    out.writeI32(i);}
public String parquet-mr_f7034_0()
{    return String.valueOf(i);}
private void parquet-mr_f7035_0(List<Action> buffer, final short s)
{    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeI16(s);        }        @Override        public String toDebugString() {            return String.valueOf(s);        }    });}
public void parquet-mr_f7036_0(TProtocol out) throws TException
{    out.writeI16(s);}
public String parquet-mr_f7037_0()
{    return String.valueOf(s);}
private void parquet-mr_f7038_0(List<Action> buffer, final double d)
{    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeDouble(d);        }        @Override        public String toDebugString() {            return String.valueOf(d);        }    });}
public void parquet-mr_f7039_0(TProtocol out) throws TException
{    out.writeDouble(d);}
public String parquet-mr_f7040_0()
{    return String.valueOf(d);}
private void parquet-mr_f7041_0(List<Action> buffer, final byte b)
{    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeByte(b);        }        @Override        public String toDebugString() {            return String.valueOf(b);        }    });}
public void parquet-mr_f7042_0(TProtocol out) throws TException
{    out.writeByte(b);}
public String parquet-mr_f7043_0()
{    return String.valueOf(b);}
private void parquet-mr_f7044_0(List<Action> buffer, final boolean bool)
{    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeBool(bool);        }        @Override        public String toDebugString() {            return String.valueOf(bool);        }    });}
public void parquet-mr_f7045_0(TProtocol out) throws TException
{    out.writeBool(bool);}
public String parquet-mr_f7046_0()
{    return String.valueOf(bool);}
private String parquet-mr_f7047_0(byte type)
{    try {        return ThriftTypeID.fromByte(type).name();    } catch (RuntimeException e) {        return String.valueOf(type);    }}
private boolean parquet-mr_f7048_0(TProtocol in, List<Action> buffer, StructType type) throws TException
{    final TStruct struct = in.readStructBegin();    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeStructBegin(struct);        }        @Override        public String toDebugString() {            return "(";        }    });    TField field;    boolean hasFieldsIgnored = false;    int childFieldsPresent = 0;    while ((field = in.readFieldBegin()).type != TType.STOP) {        final TField currentField = field;        ThriftField expectedField;        if ((expectedField = type.getChildById(field.id)) == null) {            handleUnrecognizedField(field, type, in);            hasFieldsIgnored |= true;            continue;        }        childFieldsPresent++;        buffer.add(new Action() {            @Override            public void write(TProtocol out) throws TException {                out.writeFieldBegin(currentField);            }            @Override            public String toDebugString() {                return "f=" + currentField.id + "<t=" + typeName(currentField.type) + ">: ";            }        });        hasFieldsIgnored |= readOneValue(in, field.type, buffer, expectedField.getType());        in.readFieldEnd();        buffer.add(FIELD_END);    }        assertUnionHasExactlyOneChild(type, childFieldsPresent);    in.readStructEnd();    buffer.add(STRUCT_END);    return hasFieldsIgnored;}
public void parquet-mr_f7049_0(TProtocol out) throws TException
{    out.writeStructBegin(struct);}
public String parquet-mr_f7050_0()
{    return "(";}
public void parquet-mr_f7051_0(TProtocol out) throws TException
{    out.writeFieldBegin(currentField);}
public String parquet-mr_f7052_0()
{    return "f=" + currentField.id + "<t=" + typeName(currentField.type) + ">: ";}
private void parquet-mr_f7053_0(TField field, StructType type, TProtocol in) throws TException
{    switch(type.getStructOrUnionType()) {        case STRUCT:                        notifyIgnoredFieldsOfRecord(field);                        new ProtocolReadToWrite().readOneValue(in, new NullProtocol(), field.type);            break;        case UNION:                        throw new DecodingSchemaMismatchException("Unrecognized union member with id: " + field.id + " for struct:\n" + type);        case UNKNOWN:            throw unknownStructOrUnion(type);        default:            throw unrecognizedStructOrUnion(type.getStructOrUnionType());    }}
private void parquet-mr_f7054_0(StructType type, int childFieldsPresent)
{    switch(type.getStructOrUnionType()) {        case STRUCT:                        break;        case UNION:                        if (childFieldsPresent != 1) {                if (childFieldsPresent == 0) {                    throw new DecodingSchemaMismatchException("Cannot write a TUnion with no set value in :\n" + type);                } else {                    throw new DecodingSchemaMismatchException("Cannot write a TUnion with more than 1 set value in :\n" + type);                }            }            break;        case UNKNOWN:            throw unknownStructOrUnion(type);        default:            throw unrecognizedStructOrUnion(type.getStructOrUnionType());    }}
private static ShouldNeverHappenException parquet-mr_f7055_0(StructOrUnionType type)
{    return new ShouldNeverHappenException("Unrecognized StructOrUnionType: " + type);}
private static ShouldNeverHappenException parquet-mr_f7056_0(StructType type)
{    return new ShouldNeverHappenException("This should never happen! " + "Don't know if this field is a union, was the deprecated constructor of StructType used?\n" + type);}
private boolean parquet-mr_f7057_0(TProtocol in, List<Action> buffer, MapType mapType) throws TException
{    final TMap map = in.readMapBegin();    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeMapBegin(map);        }        @Override        public String toDebugString() {            return "<k=" + map.keyType + ", v=" + map.valueType + ", s=" + map.size + ">[";        }    });    boolean hasFieldIgnored = false;    for (int i = 0; i < map.size; i++) {        hasFieldIgnored |= readOneValue(in, map.keyType, buffer, mapType.getKey().getType());        hasFieldIgnored |= readOneValue(in, map.valueType, buffer, mapType.getValue().getType());    }    in.readMapEnd();    buffer.add(MAP_END);    return hasFieldIgnored;}
public void parquet-mr_f7058_0(TProtocol out) throws TException
{    out.writeMapBegin(map);}
public String parquet-mr_f7059_0()
{    return "<k=" + map.keyType + ", v=" + map.valueType + ", s=" + map.size + ">[";}
private boolean parquet-mr_f7060_0(TProtocol in, List<Action> buffer, SetType expectedType) throws TException
{    final TSet set = in.readSetBegin();    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeSetBegin(set);        }        @Override        public String toDebugString() {            return "<e=" + set.elemType + ", s=" + set.size + ">{*";        }    });    boolean hasFieldsIgnored = readCollectionElements(in, set.size, set.elemType, buffer, expectedType.getValues().getType());    in.readSetEnd();    buffer.add(SET_END);    return hasFieldsIgnored;}
public void parquet-mr_f7061_0(TProtocol out) throws TException
{    out.writeSetBegin(set);}
public String parquet-mr_f7062_0()
{    return "<e=" + set.elemType + ", s=" + set.size + ">{*";}
private boolean parquet-mr_f7063_0(TProtocol in, List<Action> buffer, ListType expectedType) throws TException
{    final TList list = in.readListBegin();    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeListBegin(list);        }        @Override        public String toDebugString() {            return "<e=" + list.elemType + ", s=" + list.size + ">{";        }    });    boolean hasFieldsIgnored = readCollectionElements(in, list.size, list.elemType, buffer, expectedType.getValues().getType());    in.readListEnd();    buffer.add(LIST_END);    return hasFieldsIgnored;}
public void parquet-mr_f7064_0(TProtocol out) throws TException
{    out.writeListBegin(list);}
public String parquet-mr_f7065_0()
{    return "<e=" + list.elemType + ", s=" + list.size + ">{";}
private boolean parquet-mr_f7066_0(TProtocol in, final int size, final byte elemType, List<Action> buffer, ThriftType expectedType) throws TException
{    boolean hasFieldIgnored = false;    for (int i = 0; i < size; i++) {        hasFieldIgnored |= readOneValue(in, elemType, buffer, expectedType);    }    return hasFieldIgnored;}
private void parquet-mr_f7067_0(ThriftType expectedType, int i)
{    if (expectedType.getType() == ThriftTypeID.ENUM) {        ThriftType.EnumType expectedEnumType = (ThriftType.EnumType) expectedType;        if (expectedEnumType.getEnumValueById(i) == null) {            throw new DecodingSchemaMismatchException("can not find index " + i + " in enum " + expectedType);        }    }}
public void parquet-mr_f7068_0(TMessage tMessage) throws TException
{}
public void parquet-mr_f7069_0() throws TException
{}
public void parquet-mr_f7070_0(TStruct tStruct) throws TException
{}
public void parquet-mr_f7071_0() throws TException
{}
public void parquet-mr_f7072_0(TField tField) throws TException
{}
public void parquet-mr_f7073_0() throws TException
{}
public void parquet-mr_f7074_0() throws TException
{}
public void parquet-mr_f7075_0(TMap tMap) throws TException
{}
public void parquet-mr_f7076_0() throws TException
{}
public void parquet-mr_f7077_0(TList tList) throws TException
{}
public void parquet-mr_f7078_0() throws TException
{}
public void parquet-mr_f7079_0(TSet tSet) throws TException
{}
public void parquet-mr_f7080_0() throws TException
{}
public void parquet-mr_f7081_0(boolean b) throws TException
{}
public void parquet-mr_f7082_0(byte b) throws TException
{}
public void parquet-mr_f7083_0(short i) throws TException
{}
public void parquet-mr_f7084_0(int i) throws TException
{}
public void parquet-mr_f7085_0(long l) throws TException
{}
public void parquet-mr_f7086_0(double v) throws TException
{}
public void parquet-mr_f7087_0(String s) throws TException
{}
public void parquet-mr_f7088_0(ByteBuffer byteBuffer) throws TException
{}
public TMessage parquet-mr_f7089_0() throws TException
{    return null;}
public void parquet-mr_f7090_0() throws TException
{}
public TStruct parquet-mr_f7091_0() throws TException
{    return null;}
public void parquet-mr_f7092_0() throws TException
{}
public TField parquet-mr_f7093_0() throws TException
{    return null;}
public void parquet-mr_f7094_0() throws TException
{}
public TMap parquet-mr_f7095_0() throws TException
{    return null;}
public void parquet-mr_f7096_0() throws TException
{}
public TList parquet-mr_f7097_0() throws TException
{    return null;}
public void parquet-mr_f7098_0() throws TException
{}
public TSet parquet-mr_f7099_0() throws TException
{    return null;}
public void parquet-mr_f7100_0() throws TException
{}
public boolean parquet-mr_f7101_0() throws TException
{    return false;}
public byte parquet-mr_f7102_0() throws TException
{    return 0;}
public short parquet-mr_f7103_0() throws TException
{    return 0;}
public int parquet-mr_f7104_0() throws TException
{    return 0;}
public long parquet-mr_f7105_0() throws TException
{    return 0;}
public double parquet-mr_f7106_0() throws TException
{    return 0;}
public String parquet-mr_f7107_0() throws TException
{    return null;}
public ByteBuffer parquet-mr_f7108_0() throws TException
{    return null;}
public boolean parquet-mr_f7109_0()
{    return false;}
public Keep parquet-mr_f7110_0()
{    throw new ShouldNeverHappenException("asKeep called on " + this);}
public boolean parquet-mr_f7111_0()
{    return false;}
public Drop parquet-mr_f7112_0()
{    throw new ShouldNeverHappenException("asDrop called on " + this);}
public boolean parquet-mr_f7113_0()
{    return false;}
public SentinelUnion parquet-mr_f7114_0()
{    throw new ShouldNeverHappenException("asSentinelUnion called on " + this);}
public FieldsPath parquet-mr_f7115_0()
{    return path;}
public boolean parquet-mr_f7116_0()
{    return true;}
public Keep parquet-mr_f7117_0()
{    return this;}
public Type parquet-mr_f7118_0()
{    return type;}
public boolean parquet-mr_f7119_0()
{    return true;}
public SentinelUnion parquet-mr_f7120_0()
{    return this;}
public Type parquet-mr_f7121_0()
{    return type;}
public boolean parquet-mr_f7122_0()
{    return true;}
public Drop parquet-mr_f7123_0()
{    return this;}
public void parquet-mr_f7124_0()
{}
public void parquet-mr_f7125_0(TField field)
{}
public boolean parquet-mr_f7126_0(FieldsPath path)
{    if (found) {        return false;    }    found = true;    return true;}
public void parquet-mr_f7127_0() throws ThriftProjectionException
{}
private String parquet-mr_f7128_0()
{    final Class<? extends ParquetProtocol> clazz = getClass();    final Method enclosingMethod = clazz.getEnclosingMethod();    if (enclosingMethod != null) {        return clazz.getName() + " in " + enclosingMethod.toGenericString();    }    return clazz.getName();}
private TException parquet-mr_f7129_0()
{    String message = name == null ? "in " + getClassInfo() : "when we expected " + name + " in " + getClassInfo();    return new TException(new UnsupportedOperationException(new Exception().getStackTrace()[1].getMethodName() + " was called " + message));}
public void parquet-mr_f7130_0(TMessage message) throws TException
{    throw exception();}
public void parquet-mr_f7131_0() throws TException
{    throw exception();}
public void parquet-mr_f7132_0(TStruct struct) throws TException
{    throw exception();}
public void parquet-mr_f7133_0() throws TException
{    throw exception();}
public void parquet-mr_f7134_0(TField field) throws TException
{    throw exception();}
public void parquet-mr_f7135_0() throws TException
{    throw exception();}
public void parquet-mr_f7136_0() throws TException
{    throw exception();}
public void parquet-mr_f7137_0(TMap map) throws TException
{    throw exception();}
public void parquet-mr_f7138_0() throws TException
{    throw exception();}
public void parquet-mr_f7139_0(TList list) throws TException
{    throw exception();}
public void parquet-mr_f7140_0() throws TException
{    throw exception();}
public void parquet-mr_f7141_0(TSet set) throws TException
{    throw exception();}
public void parquet-mr_f7142_0() throws TException
{    throw exception();}
public void parquet-mr_f7143_0(boolean b) throws TException
{    throw exception();}
public void parquet-mr_f7144_0(byte b) throws TException
{    throw exception();}
public void parquet-mr_f7145_0(short i16) throws TException
{    throw exception();}
public void parquet-mr_f7146_0(int i32) throws TException
{    throw exception();}
public void parquet-mr_f7147_0(long i64) throws TException
{    throw exception();}
public void parquet-mr_f7148_0(double dub) throws TException
{    throw exception();}
public void parquet-mr_f7149_0(String str) throws TException
{    throw exception();}
public void parquet-mr_f7150_0(ByteBuffer buf) throws TException
{    throw exception();}
public TMessage parquet-mr_f7151_0() throws TException
{    throw exception();}
public void parquet-mr_f7152_0() throws TException
{    throw exception();}
public TStruct parquet-mr_f7153_0() throws TException
{    throw exception();}
public void parquet-mr_f7154_0() throws TException
{    throw exception();}
public TField parquet-mr_f7155_0() throws TException
{    throw exception();}
public void parquet-mr_f7156_0() throws TException
{    throw exception();}
public TMap parquet-mr_f7157_0() throws TException
{    throw exception();}
public void parquet-mr_f7158_0() throws TException
{    throw exception();}
public TList parquet-mr_f7159_0() throws TException
{    throw exception();}
public void parquet-mr_f7160_0() throws TException
{    throw exception();}
public TSet parquet-mr_f7161_0() throws TException
{    throw exception();}
public void parquet-mr_f7162_0() throws TException
{    throw exception();}
public boolean parquet-mr_f7163_0() throws TException
{    throw exception();}
public byte parquet-mr_f7164_0() throws TException
{    throw exception();}
public short parquet-mr_f7165_0() throws TException
{    throw exception();}
public int parquet-mr_f7166_0() throws TException
{    throw exception();}
public long parquet-mr_f7167_0() throws TException
{    throw exception();}
public double parquet-mr_f7168_0() throws TException
{    throw exception();}
public String parquet-mr_f7169_0() throws TException
{    throw exception();}
public ByteBuffer parquet-mr_f7170_0() throws TException
{    throw exception();}
public void parquet-mr_f7171_0(TProtocol p)
{    events.addLast(p);}
public void parquet-mr_f7172_0(Collection<TProtocol> events)
{    this.events.addAll(events);}
public void parquet-mr_f7173_0()
{    this.events.clear();}
private TProtocol parquet-mr_f7174_0()
{    return events.removeFirst();}
public TMessage parquet-mr_f7175_1() throws TException
{        return next().readMessageBegin();}
public void parquet-mr_f7176_1() throws TException
{        next().readMessageEnd();}
public TStruct parquet-mr_f7177_1() throws TException
{        return next().readStructBegin();}
public void parquet-mr_f7178_1() throws TException
{        next().readStructEnd();}
public TField parquet-mr_f7179_1() throws TException
{        return next().readFieldBegin();}
public void parquet-mr_f7180_1() throws TException
{        next().readFieldEnd();}
public TMap parquet-mr_f7181_1() throws TException
{        return next().readMapBegin();}
public void parquet-mr_f7182_1() throws TException
{        next().readMapEnd();}
public TList parquet-mr_f7183_1() throws TException
{        return next().readListBegin();}
public void parquet-mr_f7184_1() throws TException
{        next().readListEnd();}
public TSet parquet-mr_f7185_1() throws TException
{        return next().readSetBegin();}
public void parquet-mr_f7186_1() throws TException
{        next().readSetEnd();}
public boolean parquet-mr_f7187_1() throws TException
{        return next().readBool();}
public byte parquet-mr_f7188_1() throws TException
{        return next().readByte();}
public short parquet-mr_f7189_1() throws TException
{        return next().readI16();}
public int parquet-mr_f7190_1() throws TException
{        return next().readI32();}
public long parquet-mr_f7191_1() throws TException
{        return next().readI64();}
public double parquet-mr_f7192_1() throws TException
{        return next().readDouble();}
public String parquet-mr_f7193_1() throws TException
{        return next().readString();}
public ByteBuffer parquet-mr_f7194_1() throws TException
{        return next().readBinary();}
 void parquet-mr_f7195_0()
{    this.returnClause.start();}
 void parquet-mr_f7196_0()
{    this.returnClause.end();}
public void parquet-mr_f7197_0(int i32) throws TException
{    start();    EnumValue value = type.getEnumValueById(i32);    if (value == null) {        throw new ParquetEncodingException("Can not find enum value of index " + i32 + " for field:" + columnIO.toString());    }    recordConsumer.addBinary(Binary.fromString(value.getName()));    end();}
public void parquet-mr_f7198_0()
{}
public void parquet-mr_f7199_0()
{    ++consumedRecords;    if (consumedRecords == size) {        currentProtocol = ListWriteProtocol.this;        consumedRecords = 0;    }}
private void parquet-mr_f7200_0()
{    start();    recordConsumer.startGroup();    if (size > 0) {        recordConsumer.startField(listContent.getType().getName(), 0);        currentProtocol = contentProtocol;    }}
private void parquet-mr_f7201_0()
{    if (size > 0) {        recordConsumer.endField(listContent.getType().getName(), 0);    }    recordConsumer.endGroup();    end();}
public void parquet-mr_f7202_0(TList list) throws TException
{    size = list.size;    startListWrapper();}
public void parquet-mr_f7203_0() throws TException
{    endListWrapper();}
public void parquet-mr_f7204_0(TSet set) throws TException
{    size = set.size;    startListWrapper();}
public void parquet-mr_f7205_0() throws TException
{    endListWrapper();}
public void parquet-mr_f7206_0()
{    recordConsumer.startGroup();    recordConsumer.startField(key.getName(), key.getIndex());}
public void parquet-mr_f7207_0()
{    recordConsumer.endField(key.getName(), key.getIndex());    currentProtocol = valueProtocol;}
public void parquet-mr_f7208_0()
{    recordConsumer.startField(value.getName(), value.getIndex());}
public void parquet-mr_f7209_0()
{    consumed++;    recordConsumer.endField(value.getName(), value.getIndex());    recordConsumer.endGroup();    if (consumed == countToConsume) {        currentProtocol = MapWriteProtocol.this;        consumed = 0;    } else {        currentProtocol = keyProtocol;    }}
public void parquet-mr_f7210_0(TMap map) throws TException
{    start();    recordConsumer.startGroup();    countToConsume = map.size;    if (countToConsume > 0) {        recordConsumer.startField(mapContent.getType().getName(), 0);        currentProtocol = keyProtocol;    }}
public void parquet-mr_f7211_0() throws TException
{    if (countToConsume > 0) {        recordConsumer.endField(mapContent.getType().getName(), 0);    }    recordConsumer.endGroup();    end();}
public void parquet-mr_f7212_0(boolean b) throws TException
{    start();    recordConsumer.addBoolean(b);    end();}
public void parquet-mr_f7213_0(byte b) throws TException
{    start();    recordConsumer.addInteger(b);    end();}
public void parquet-mr_f7214_0(short i16) throws TException
{    start();    recordConsumer.addInteger(i16);    end();}
public void parquet-mr_f7215_0(int i32) throws TException
{    start();    recordConsumer.addInteger(i32);    end();}
public void parquet-mr_f7216_0(long i64) throws TException
{    start();    recordConsumer.addLong(i64);    end();}
public void parquet-mr_f7217_0(double dub) throws TException
{    start();    recordConsumer.addDouble(dub);    end();}
public void parquet-mr_f7218_0(String str) throws TException
{    start();    writeStringToRecordConsumer(str);    end();}
public void parquet-mr_f7219_0(ByteBuffer buf) throws TException
{    start();    writeBinaryToRecordConsumer(buf);    end();}
public void parquet-mr_f7220_0()
{}
public void parquet-mr_f7221_0()
{    currentProtocol = StructWriteProtocol.this;}
public void parquet-mr_f7222_0(TStruct struct) throws TException
{    start();    recordConsumer.startGroup();}
public void parquet-mr_f7223_0() throws TException
{    recordConsumer.endGroup();    end();}
public void parquet-mr_f7224_0(TField field) throws TException
{    if (field.type == TType.STOP) {        return;    }    try {        currentType = thriftFieldIdToParquetField[field.id];        if (currentType == null) {            throw new ParquetEncodingException("field " + field.id + " was not found in " + thriftType + " and " + schema.getType());        }        final int index = currentType.getIndex();        recordConsumer.startField(currentType.getName(), index);        currentProtocol = children[index];    } catch (ArrayIndexOutOfBoundsException e) {        throw new ParquetEncodingException("field " + field.id + " was not found in " + thriftType + " and " + schema.getType());    }}
public void parquet-mr_f7225_0() throws TException
{}
public void parquet-mr_f7226_0() throws TException
{    recordConsumer.endField(currentType.getName(), currentType.getIndex());}
public void parquet-mr_f7227_0(TStruct struct) throws TException
{    recordConsumer.startMessage();}
public void parquet-mr_f7228_0() throws TException
{    recordConsumer.endMessage();}
private String parquet-mr_f7229_0(TStruct struct)
{    return "<TStruct name:" + struct.name + ">";}
private String parquet-mr_f7230_0(TList list)
{    return "<TList elemType:" + list.elemType + " size:" + list.size + ">";}
private String parquet-mr_f7231_0(TMap map)
{    return "<TMap keyType:" + map.keyType + " valueType:" + map.valueType + " size:" + map.size + ">";}
public void parquet-mr_f7232_1(TMessage message) throws TException
{        currentProtocol.writeMessageBegin(message);}
public void parquet-mr_f7233_1() throws TException
{        currentProtocol.writeMessageEnd();}
public void parquet-mr_f7234_1(TStruct struct) throws TException
{    if (LOG.isDebugEnabled())            currentProtocol.writeStructBegin(struct);}
public void parquet-mr_f7235_1() throws TException
{        currentProtocol.writeStructEnd();}
public void parquet-mr_f7236_1(TField field) throws TException
{        currentProtocol.writeFieldBegin(field);}
public void parquet-mr_f7237_1() throws TException
{        currentProtocol.writeFieldEnd();}
public void parquet-mr_f7238_1() throws TException
{        currentProtocol.writeFieldStop();}
public void parquet-mr_f7239_1(TMap map) throws TException
{    if (LOG.isDebugEnabled())            currentProtocol.writeMapBegin(map);}
public void parquet-mr_f7240_1() throws TException
{        currentProtocol.writeMapEnd();}
public void parquet-mr_f7241_1(TList list) throws TException
{    if (LOG.isDebugEnabled())            currentProtocol.writeListBegin(list);}
public void parquet-mr_f7242_1() throws TException
{        currentProtocol.writeListEnd();}
public void parquet-mr_f7243_1(TSet set) throws TException
{        currentProtocol.writeSetBegin(set);}
public void parquet-mr_f7244_1() throws TException
{        currentProtocol.writeSetEnd();}
public void parquet-mr_f7245_1(boolean b) throws TException
{        currentProtocol.writeBool(b);}
public void parquet-mr_f7246_1(byte b) throws TException
{        currentProtocol.writeByte(b);}
public void parquet-mr_f7247_1(short i16) throws TException
{        currentProtocol.writeI16(i16);}
public void parquet-mr_f7248_1(int i32) throws TException
{        currentProtocol.writeI32(i32);}
public void parquet-mr_f7249_1(long i64) throws TException
{        currentProtocol.writeI64(i64);}
public void parquet-mr_f7250_1(double dub) throws TException
{        currentProtocol.writeDouble(dub);}
public void parquet-mr_f7251_1(String str) throws TException
{        currentProtocol.writeString(str);}
public void parquet-mr_f7252_1(ByteBuffer buf) throws TException
{        currentProtocol.writeBinary(buf);}
private void parquet-mr_f7253_0(ByteBuffer buf)
{    recordConsumer.addBinary(Binary.fromReusedByteArray(buf.array(), buf.position(), buf.limit() - buf.position()));}
private void parquet-mr_f7254_0(String str)
{    recordConsumer.addBinary(Binary.fromString(str));}
private TProtocol parquet-mr_f7255_0(ThriftField field, ColumnIO columnIO, Events returnClause)
{    TProtocol p;    final ThriftType type = field.getType();    switch(type.getType()) {        case STOP:        case VOID:        default:            throw new UnsupportedOperationException("can't convert type of " + field);        case BOOL:        case BYTE:        case DOUBLE:        case I16:        case I32:        case I64:        case STRING:            p = new PrimitiveWriteProtocol((PrimitiveColumnIO) columnIO, returnClause);            break;        case STRUCT:            p = new StructWriteProtocol((GroupColumnIO) columnIO, (StructType) type, returnClause);            break;        case MAP:            p = new MapWriteProtocol((GroupColumnIO) columnIO, (MapType) type, returnClause);            break;        case SET:            p = new ListWriteProtocol((GroupColumnIO) columnIO, ((SetType) type).getValues(), returnClause);            break;        case LIST:            p = new ListWriteProtocol((GroupColumnIO) columnIO, ((ListType) type).getValues(), returnClause);            break;        case ENUM:            p = new EnumWriteProtocol((PrimitiveColumnIO) columnIO, (EnumType) type, returnClause);            break;    }    return p;}
public OutputFormat<Void, Tuple> parquet-mr_f7256_0() throws IOException
{    return new ParquetOutputFormat<Tuple>(new TupleToThriftWriteSupport(className));}
public void parquet-mr_f7257_0(RecordWriter recordWriter) throws IOException
{    this.recordWriter = recordWriter;}
public void parquet-mr_f7258_0(Tuple tuple) throws IOException
{    try {        this.recordWriter.write(null, tuple);    } catch (InterruptedException e) {        throw new ParquetEncodingException("Interrupted while writing", e);    }}
public void parquet-mr_f7259_0(String location, Job job) throws IOException
{    FileOutputFormat.setOutputPath(job, new Path(location));}
public String parquet-mr_f7260_0()
{    return "thrift";}
public WriteContext parquet-mr_f7261_0(Configuration configuration)
{    try {        Class<?> clazz = configuration.getClassByName(className).asSubclass(TBase.class);        thriftWriteSupport = new ThriftWriteSupport(clazz);        pigToThrift = new PigToThrift(clazz);        return thriftWriteSupport.init(configuration);    } catch (ClassNotFoundException e) {        throw new BadConfigurationException("The thrift class name was not found: " + className, e);    } catch (ClassCastException e) {        throw new BadConfigurationException("The thrift class name should extend TBase: " + className, e);    }}
public void parquet-mr_f7262_0(RecordConsumer recordConsumer)
{    thriftWriteSupport.prepareForWrite(recordConsumer);}
public void parquet-mr_f7263_0(Tuple t)
{    thriftWriteSupport.write(pigToThrift.getThriftObject(t));}
public Void parquet-mr_f7264_0(ThriftType.MapType mapType, Void v)
{    dummyEvents.add(new ParquetProtocol("readMapBegin()") {        @Override        public TMap readMapBegin() throws TException {            return new TMap();        }    });    dummyEvents.add(new ParquetProtocol("readMapEnd()") {        @Override        public void readMapEnd() throws TException {        }    });    return null;}
public TMap parquet-mr_f7265_0() throws TException
{    return new TMap();}
public void parquet-mr_f7266_0() throws TException
{}
public Void parquet-mr_f7267_0(final ThriftType.SetType setType, Void v)
{    dummyEvents.add(new ParquetProtocol("readSetBegin()") {        @Override        public TSet readSetBegin() throws TException {            return new TSet();        }    });    dummyEvents.add(new ParquetProtocol("readSetEnd()") {        @Override        public void readSetEnd() throws TException {        }    });    return null;}
public TSet parquet-mr_f7268_0() throws TException
{    return new TSet();}
public void parquet-mr_f7269_0() throws TException
{}
public Void parquet-mr_f7270_0(final ThriftType.ListType listType, Void v)
{    dummyEvents.add(new ParquetProtocol("readListBegin()") {        @Override        public TList readListBegin() throws TException {            return new TList();        }    });    dummyEvents.add(new ParquetProtocol("readListEnd()") {        @Override        public void readListEnd() throws TException {        }    });    return null;}
public TList parquet-mr_f7271_0() throws TException
{    return new TList();}
public void parquet-mr_f7272_0() throws TException
{}
public Void parquet-mr_f7273_0(ThriftType.StructType structType, Void v)
{    dummyEvents.add(new StructBeginProtocol("struct"));    List<ThriftField> children = structType.getChildren();    for (ThriftField child : children) {        dummyEvents.add(new ReadFieldBeginProtocol(child));                child.getType().accept(this, null);        dummyEvents.add(DefaultProtocolEventsGenerator.READ_FIELD_END);    }    dummyEvents.add(DefaultProtocolEventsGenerator.READ_FIELD_STOP);    dummyEvents.add(DefaultProtocolEventsGenerator.READ_STRUCT_END);    return null;}
public Void parquet-mr_f7274_0(ThriftType.EnumType enumType, Void v)
{    dummyEvents.add(new ParquetProtocol("readI32() enum") {        @Override        public int readI32() throws TException {            return 0;        }    });    return null;}
public int parquet-mr_f7275_0() throws TException
{    return 0;}
public Void parquet-mr_f7276_0(ThriftType.BoolType boolType, Void v)
{    dummyEvents.add(new ParquetProtocol("readBool()") {        @Override        public boolean readBool() throws TException {            return false;        }    });    return null;}
public boolean parquet-mr_f7277_0() throws TException
{    return false;}
public Void parquet-mr_f7278_0(ThriftType.ByteType byteType, Void v)
{    dummyEvents.add(new ParquetProtocol("readByte() int") {        @Override        public byte readByte() throws TException {            return (byte) 0;        }    });    return null;}
public byte parquet-mr_f7279_0() throws TException
{    return (byte) 0;}
public Void parquet-mr_f7280_0(ThriftType.DoubleType doubleType, Void v)
{    dummyEvents.add(new ParquetProtocol("readDouble()") {        @Override        public double readDouble() throws TException {            return 0.0;        }    });    return null;}
public double parquet-mr_f7281_0() throws TException
{    return 0.0;}
public Void parquet-mr_f7282_0(ThriftType.I16Type i16Type, Void v)
{    dummyEvents.add(new ParquetProtocol("readI16()") {        @Override        public short readI16() throws TException {            return (short) 0;        }    });    return null;}
public short parquet-mr_f7283_0() throws TException
{    return (short) 0;}
public Void parquet-mr_f7284_0(ThriftType.I32Type i32Type, Void v)
{    dummyEvents.add(new ParquetProtocol("readI32()") {        @Override        public int readI32() throws TException {            return 0;        }    });    return null;}
public int parquet-mr_f7285_0() throws TException
{    return 0;}
public Void parquet-mr_f7286_0(ThriftType.I64Type i64Type, Void v)
{    dummyEvents.add(new ParquetProtocol("readI64()") {        @Override        public long readI64() throws TException {            return 0;        }    });    return null;}
public long parquet-mr_f7287_0() throws TException
{    return 0;}
public Void parquet-mr_f7288_0(ThriftType.StringType stringType, Void v)
{    dummyEvents.add(new StringProtocol(""));    return null;}
public List<ParquetProtocol> parquet-mr_f7289_0()
{    return dummyEvents;}
public TStruct parquet-mr_f7290_0() throws TException
{    return new TStruct(structName);}
public String parquet-mr_f7291_0() throws TException
{    return str;}
public ByteBuffer parquet-mr_f7292_0() throws TException
{    return ByteBuffer.wrap("str".getBytes());}
public TField parquet-mr_f7293_0() throws TException
{    return stop;}
public void parquet-mr_f7294_0() throws TException
{}
public void parquet-mr_f7295_0() throws TException
{}
public List<TProtocol> parquet-mr_f7296_0(ThriftField missingField)
{    TProtocol fieldBegin = new ReadFieldBeginProtocol(missingField);    createdEvents.add(fieldBegin);    DefaultEventsVisitor dummyCreatorvisitor = new DefaultEventsVisitor();    missingField.getType().accept(dummyCreatorvisitor, null);    createdEvents.addAll(dummyCreatorvisitor.getEvents());    createdEvents.add(READ_FIELD_END);    return createdEvents;}
public List<TProtocol> parquet-mr_f7297_0(StructType recordThriftType) throws TException
{    Iterator<TProtocol> protocolIter = rootEvents.iterator();    checkStruct(protocolIter, recordThriftType);    return fixedEvents;}
private TProtocol parquet-mr_f7298_0(TProtocol p)
{    this.fixedEvents.add(p);    return p;}
private void parquet-mr_f7299_0(Iterator<TProtocol> eventIter, ThriftType.StructType thriftStructType) throws TException
{    TStruct tStruct = acceptProtocol(eventIter.next()).readStructBegin();    List<ThriftField> childrenFields = thriftStructType.getChildren();    Set<Short> includedFieldsIds = new HashSet<Short>();    while (true) {        TProtocol next = eventIter.next();        TField field = next.readFieldBegin();        if (isStopField(field))            break;        acceptProtocol(next);        includedFieldsIds.add(field.id);        ThriftField fieldDefinition = thriftStructType.getChildById(field.id);        checkField(field.type, eventIter, fieldDefinition);        acceptProtocol(eventIter.next()).readFieldEnd();    }    for (ThriftField requiredField : childrenFields) {        if (!isRequired(requiredField)) {            continue;        }        if (!includedFieldsIds.contains(requiredField.getFieldId())) {            fixedEvents.addAll(new DefaultProtocolEventsGenerator().createProtocolEventsForField(requiredField));        }    }    acceptProtocol(DefaultProtocolEventsGenerator.READ_FIELD_STOP);    acceptProtocol(eventIter.next()).readStructEnd();}
private void parquet-mr_f7300_0(byte type, Iterator<TProtocol> eventIter, ThriftField fieldDefinition) throws TException
{    switch(type) {        case TType.STRUCT:            checkStruct(eventIter, (ThriftType.StructType) fieldDefinition.getType());            return;        case TType.LIST:            checkList(eventIter, fieldDefinition);            return;        case TType.MAP:            checkMap(eventIter, fieldDefinition);            return;        case TType.SET:            checkSet(eventIter, fieldDefinition);            return;    }    checkPrimitiveField(type, eventIter);}
private void parquet-mr_f7301_0(Iterator<TProtocol> eventIter, ThriftField setFieldDefinition) throws TException
{    TSet thriftSet = acceptProtocol(eventIter.next()).readSetBegin();    ThriftField elementFieldDefinition = ((ThriftType.SetType) setFieldDefinition.getType()).getValues();    int setSize = thriftSet.size;    for (int i = 0; i < setSize; i++) {        checkField(thriftSet.elemType, eventIter, elementFieldDefinition);    }    acceptProtocol(eventIter.next()).readSetEnd();}
private void parquet-mr_f7302_0(Iterator<TProtocol> eventIter, ThriftField mapFieldForWriting) throws TException
{    TMap thriftMap = acceptProtocol(eventIter.next()).readMapBegin();    ThriftField keyFieldForWriting = ((ThriftType.MapType) mapFieldForWriting.getType()).getKey();    ThriftField valueFieldForWriting = ((ThriftType.MapType) mapFieldForWriting.getType()).getValue();    int mapSize = thriftMap.size;    for (int i = 0; i < mapSize; i++) {                checkField(thriftMap.keyType, eventIter, keyFieldForWriting);                checkField(thriftMap.valueType, eventIter, valueFieldForWriting);    }    acceptProtocol(eventIter.next()).readMapEnd();}
private void parquet-mr_f7303_0(Iterator<TProtocol> eventIter, ThriftField listFieldUsedForWriting) throws TException
{    ThriftField valueFieldForWriting = ((ThriftType.ListType) listFieldUsedForWriting.getType()).getValues();    TList thriftList = acceptProtocol(eventIter.next()).readListBegin();    int listSize = thriftList.size;    for (int i = 0; i < listSize; i++) {        checkField(thriftList.elemType, eventIter, valueFieldForWriting);    }    acceptProtocol(eventIter.next()).readListEnd();}
private void parquet-mr_f7304_0(byte type, Iterator<TProtocol> eventIter) throws TException
{    acceptProtocol(eventIter.next());}
private boolean parquet-mr_f7305_0(TField field)
{    return field.type == TType.STOP;}
private boolean parquet-mr_f7306_0(ThriftField requiredField)
{    return requiredField.getRequirement() == ThriftField.Requirement.REQUIRED;}
public TField parquet-mr_f7307_0() throws TException
{    return new TField(field.getName(), thriftType, field.getFieldId());}
public boolean parquet-mr_f7308_0(String path)
{    if (this.pattern.matches(path)) {        this.hasMatchingPath = true;        return true;    } else {        return false;    }}
public boolean parquet-mr_f7309_0(FieldsPath path)
{    if (filterPatterns.size() == 0)        return true;    for (PathGlobPatternStatus pattern : filterPatterns) {        if (pattern.matches(path.toDelimitedString("/")))            return true;    }    return false;}
public void parquet-mr_f7310_0() throws ThriftProjectionException
{    List<PathGlobPattern> unmatched = new LinkedList<PathGlobPattern>();    for (PathGlobPatternStatus p : filterPatterns) {        if (!p.hasMatchingPath) {            unmatched.add(p.pattern);        }    }    if (!unmatched.isEmpty()) {        StringBuilder message = new StringBuilder("The following projection patterns did not match any columns in this schema:\n");        for (PathGlobPattern p : unmatched) {            message.append(p);            message.append('\n');        }        throw new ThriftProjectionException(message.toString());    }}
public static Pattern parquet-mr_f7311_0(String globPattern)
{    return new GlobPattern(globPattern).compiled();}
private static void parquet-mr_f7312_0(String message, String pattern, int pos)
{    throw new PatternSyntaxException(message, pattern, pos);}
public Pattern parquet-mr_f7313_0()
{    return compiled;}
public boolean parquet-mr_f7314_0(CharSequence s)
{    return compiled.matcher(s).matches();}
public void parquet-mr_f7315_0(String glob)
{    StringBuilder regex = new StringBuilder();    int setOpen = 0;    int curlyOpen = 0;    int len = glob.length();    hasWildcard = false;    for (int i = 0; i < len; i++) {        char c = glob.charAt(i);        switch(c) {            case BACKSLASH:                if (++i >= len) {                    error("Missing escaped character", glob, i);                }                regex.append(c).append(glob.charAt(i));                continue;            case '.':            case '$':            case '(':            case ')':            case '|':            case '+':                                regex.append(BACKSLASH);                break;            case '*':                if (i + 1 < len && glob.charAt(i + 1) == '*') {                    regex.append('.');                    i++;                    break;                }                regex.append("[^" + PATH_SEPARATOR + "]");                hasWildcard = true;                break;            case '?':                regex.append('.');                hasWildcard = true;                continue;            case             '{':                                regex.append("(?:");                curlyOpen++;                hasWildcard = true;                continue;            case ',':                regex.append(curlyOpen > 0 ? '|' : c);                continue;            case '}':                if (curlyOpen > 0) {                                        curlyOpen--;                    regex.append(")");                    continue;                }                break;            case '[':                if (setOpen > 0) {                    error("Unclosed character class", glob, i);                }                setOpen++;                hasWildcard = true;                break;            case             '^':                if (setOpen == 0) {                    regex.append(BACKSLASH);                }                break;            case             '!':                regex.append(setOpen > 0 && '[' == glob.charAt(i - 1) ? '^' : '!');                continue;            case ']':                                                                setOpen = 0;                break;            default:        }        regex.append(c);    }    if (setOpen > 0) {        error("Unclosed character class", glob, len);    }    if (curlyOpen > 0) {        error("Unclosed group", glob, len);    }    compiled = Pattern.compile(regex.toString());}
public String parquet-mr_f7316_0()
{    return compiled.toString();}
public boolean parquet-mr_f7317_0()
{    return hasWildcard;}
public boolean parquet-mr_f7318_0(FieldsPath path)
{    return true;}
public void parquet-mr_f7319_0() throws ThriftProjectionException
{}
public FieldsPath parquet-mr_f7320_0(ThriftField f)
{    ArrayList<ThriftField> copy = new ArrayList<ThriftField>(fields);    copy.add(f);    return new FieldsPath(copy);}
public String parquet-mr_f7321_0(String delim)
{    StringBuilder delimited = new StringBuilder();    for (int i = 0; i < fields.size(); i++) {        ThriftField currentField = fields.get(i);        if (i > 0) {            ThriftField previousField = fields.get(i - 1);            if (FieldsPath.isKeyFieldOfMap(currentField, previousField)) {                delimited.append("key");                delimited.append(delim);                continue;            } else if (FieldsPath.isValueFieldOfMap(currentField, previousField)) {                delimited.append("value");                delimited.append(delim);                continue;            }        }        delimited.append(currentField.getName()).append(delim);    }    if (delimited.length() == 0) {        return "";    } else {        return delimited.substring(0, delimited.length() - 1);    }}
public String parquet-mr_f7322_0()
{    return toDelimitedString(".");}
private static boolean parquet-mr_f7323_0(ThriftField currentField, ThriftField previousField)
{    ThriftType previousType = previousField.getType();    return previousType instanceof ThriftType.MapType && ((ThriftType.MapType) previousType).getValue() == currentField;}
private static boolean parquet-mr_f7324_0(ThriftField currentField, ThriftField previousField)
{    ThriftType previousType = previousField.getType();    return previousType instanceof ThriftType.MapType && ((ThriftType.MapType) previousType).getKey() == currentField;}
 static List<String> parquet-mr_f7325_0(String columnsToKeepGlobs)
{    String[] splits = columnsToKeepGlobs.split(GLOB_SEPARATOR);    List<String> globs = new ArrayList<String>();    for (String s : splits) {        if (!s.isEmpty()) {            globs.add(s);        }    }    if (globs.isEmpty()) {        throw new ThriftProjectionException(String.format("Semicolon delimited string '%s' contains 0 glob strings", columnsToKeepGlobs));    }    return globs;}
public static StrictFieldProjectionFilter parquet-mr_f7326_0(String columnsToKeepGlobs)
{    return new StrictFieldProjectionFilter(parseSemicolonDelimitedString(columnsToKeepGlobs));}
public boolean parquet-mr_f7327_0(FieldsPath path)
{    return keep(path.toDelimitedString("."));}
 boolean parquet-mr_f7328_0(String path)
{    WildcardPath match = null;        for (WildcardPathStatus wp : columnsToKeep) {        if (wp.matches(path)) {            if (match != null && !match.getParentGlobPath().equals(wp.getWildcardPath().getParentGlobPath())) {                String message = "Field path: '%s' matched more than one glob path pattern. First match: " + "'%s' (when expanded to '%s') second match:'%s' (when expanded to '%s')";                warn(String.format(message, path, match.getParentGlobPath(), match.getOriginalPattern(), wp.getWildcardPath().getParentGlobPath(), wp.getWildcardPath().getOriginalPattern()));            } else {                match = wp.getWildcardPath();            }        }    }    return match != null;}
protected void parquet-mr_f7329_1(String warning)
{    }
private List<WildcardPath> parquet-mr_f7330_0()
{    List<WildcardPath> unmatched = new ArrayList<WildcardPath>();    for (WildcardPathStatus wp : columnsToKeep) {        if (!wp.hasMatched()) {            unmatched.add(wp.getWildcardPath());        }    }    return unmatched;}
public void parquet-mr_f7331_0() throws ThriftProjectionException
{    List<WildcardPath> unmatched = getUnmatchedPatterns();    if (!unmatched.isEmpty()) {        StringBuilder message = new StringBuilder("The following projection patterns did not match any columns in this schema:\n");        for (WildcardPath wp : unmatched) {            message.append(String.format("Pattern: '%s' (when expanded to '%s')", wp.getParentGlobPath(), wp.getOriginalPattern()));            message.append('\n');        }        throw new ThriftProjectionException(message.toString());    }}
public boolean parquet-mr_f7332_0(String path)
{    boolean matches = wildcardPath.matches(path);    this.hasMatched = hasMatched || matches;    return matches;}
public WildcardPath parquet-mr_f7333_0()
{    return wildcardPath;}
public boolean parquet-mr_f7334_0()
{    return hasMatched;}
public void parquet-mr_f7335_0(TProtocol in, TProtocol out) throws TException
{    readOneStruct(in, out);}
 void parquet-mr_f7336_0(TProtocol in, TProtocol out, byte type) throws TException
{    switch(type) {        case TType.LIST:            readOneList(in, out);            break;        case TType.MAP:            readOneMap(in, out);            break;        case TType.SET:            readOneSet(in, out);            break;        case TType.STRUCT:            readOneStruct(in, out);            break;        case TType.STOP:            break;        case TType.BOOL:            out.writeBool(in.readBool());            break;        case TType.BYTE:            out.writeByte(in.readByte());            break;        case TType.DOUBLE:            out.writeDouble(in.readDouble());            break;        case TType.I16:            out.writeI16(in.readI16());            break;                case TType.ENUM:        case TType.I32:            out.writeI32(in.readI32());            break;        case TType.I64:            out.writeI64(in.readI64());            break;        case TType.STRING:            out.writeBinary(in.readBinary());            break;        case TType.VOID:            break;        default:            throw new TException("Unknown type: " + type);    }}
private void parquet-mr_f7337_0(TProtocol in, TProtocol out) throws TException
{    final TStruct struct = in.readStructBegin();    out.writeStructBegin(struct);    TField field;    while ((field = in.readFieldBegin()).type != TType.STOP) {        out.writeFieldBegin(field);        readOneValue(in, out, field.type);        in.readFieldEnd();        out.writeFieldEnd();    }    out.writeFieldStop();    in.readStructEnd();    out.writeStructEnd();}
private void parquet-mr_f7338_0(TProtocol in, TProtocol out) throws TException
{    final TMap map = in.readMapBegin();    out.writeMapBegin(map);    for (int i = 0; i < map.size; i++) {        readOneValue(in, out, map.keyType);        readOneValue(in, out, map.valueType);    }    in.readMapEnd();    out.writeMapEnd();}
private void parquet-mr_f7339_0(TProtocol in, TProtocol out) throws TException
{    final TSet set = in.readSetBegin();    out.writeSetBegin(set);    readCollectionElements(in, out, set.size, set.elemType);    in.readSetEnd();    out.writeSetEnd();}
private void parquet-mr_f7340_0(TProtocol in, TProtocol out) throws TException
{    final TList list = in.readListBegin();    out.writeListBegin(list);    readCollectionElements(in, out, list.size, list.elemType);    in.readListEnd();    out.writeListEnd();}
private void parquet-mr_f7341_0(TProtocol in, TProtocol out, final int size, final byte elemType) throws TException
{    for (int i = 0; i < size; i++) {        readOneValue(in, out, elemType);    }}
public CompatibilityReport parquet-mr_f7342_0(ThriftType.StructType oldStruct, ThriftType.StructType newStruct)
{    CompatibleCheckerVisitor visitor = new CompatibleCheckerVisitor();    newStruct.accept(visitor, new State(oldStruct, new FieldsPath()));    return visitor.getReport();}
public boolean parquet-mr_f7343_0()
{    return isCompatible;}
public boolean parquet-mr_f7344_0()
{    return hasEmptyStruct;}
public void parquet-mr_f7345_0(String message)
{    messages.add(message);    isCompatible = false;}
public void parquet-mr_f7346_0(String message)
{    messages.add(message);    hasEmptyStruct = true;}
public List<String> parquet-mr_f7347_0()
{    return messages;}
public String parquet-mr_f7348_0()
{    return Strings.join(messages, "\n");}
public String parquet-mr_f7349_0()
{    return "CompatibilityReport{" + "isCompatible=" + isCompatible + ", hasEmptyStruct=" + hasEmptyStruct + ", messages=\n" + prettyMessages() + '}';}
public CompatibilityReport parquet-mr_f7350_0()
{    return report;}
public Void parquet-mr_f7351_0(ThriftType.MapType mapType, State state)
{    ThriftType.MapType oldMapType = ((ThriftType.MapType) state.oldType);    ThriftField oldKeyField = oldMapType.getKey();    ThriftField newKeyField = mapType.getKey();    ThriftField newValueField = mapType.getValue();    ThriftField oldValueField = oldMapType.getValue();    checkField(oldKeyField, newKeyField, state.path);    checkField(oldValueField, newValueField, state.path);    return null;}
public Void parquet-mr_f7352_0(ThriftType.SetType setType, State state)
{    ThriftType.SetType oldSetType = ((ThriftType.SetType) state.oldType);    ThriftField oldField = oldSetType.getValues();    ThriftField newField = setType.getValues();    checkField(oldField, newField, state.path);    return null;}
public Void parquet-mr_f7353_0(ThriftType.ListType listType, State state)
{    ThriftType.ListType currentOldType = ((ThriftType.ListType) state.oldType);    ThriftField oldField = currentOldType.getValues();    ThriftField newField = listType.getValues();    checkField(oldField, newField, state.path);    return null;}
public void parquet-mr_f7354_0(String message, FieldsPath path)
{    report.fail("at " + path + ":" + message);}
private void parquet-mr_f7355_0(ThriftField oldField, ThriftField newField, FieldsPath path)
{    if (!newField.getType().getType().equals(oldField.getType().getType())) {        incompatible("type is not compatible: " + oldField.getType().getType() + " vs " + newField.getType().getType(), path);        return;    }    if (!newField.getName().equals(oldField.getName())) {        incompatible("field names are different: " + oldField.getName() + " vs " + newField.getName(), path);        return;    }    if (firstIsMoreRestirctive(newField.getRequirement(), oldField.getRequirement())) {        incompatible("new field is more restrictive: " + newField.getName(), path);        return;    }    newField.getType().accept(this, new State(oldField.getType(), path.push(newField)));}
private boolean parquet-mr_f7356_0(ThriftField.Requirement firstReq, ThriftField.Requirement secReq)
{    if (firstReq == ThriftField.Requirement.REQUIRED && secReq != ThriftField.Requirement.REQUIRED) {        return true;    } else {        return false;    }}
public Void parquet-mr_f7357_0(ThriftType.StructType newStruct, State state)
{    ThriftType.StructType oldStructType = ((ThriftType.StructType) state.oldType);    short oldMaxId = 0;    if (newStruct.getChildren().isEmpty()) {        report.emptyStruct("encountered an empty struct: " + state.path);    }    for (ThriftField oldField : oldStructType.getChildren()) {        short fieldId = oldField.getFieldId();        if (fieldId > oldMaxId) {            oldMaxId = fieldId;        }        ThriftField newField = newStruct.getChildById(fieldId);        if (newField == null) {            incompatible("can not find index in new Struct: " + fieldId + " in " + newStruct, state.path);            return null;        }        checkField(oldField, newField, state.path);    }        for (ThriftField newField : newStruct.getChildren()) {                if (newField.getRequirement() != ThriftField.Requirement.REQUIRED)                        continue;        short newFieldId = newField.getFieldId();        if (newFieldId > oldMaxId) {            incompatible("new required field " + newField.getName() + " is added", state.path);            return null;        }        if (newFieldId < oldMaxId && oldStructType.getChildById(newFieldId) == null) {            incompatible("new required field " + newField.getName() + " is added", state.path);            return null;        }    }    return null;}
public Void parquet-mr_f7358_0(EnumType enumType, State state)
{    return null;}
public Void parquet-mr_f7359_0(BoolType boolType, State state)
{    return null;}
public Void parquet-mr_f7360_0(ByteType byteType, State state)
{    return null;}
public Void parquet-mr_f7361_0(DoubleType doubleType, State state)
{    return null;}
public Void parquet-mr_f7362_0(I16Type i16Type, State state)
{    return null;}
public Void parquet-mr_f7363_0(I32Type i32Type, State state)
{    return null;}
public Void parquet-mr_f7364_0(I64Type i64Type, State state)
{    return null;}
public Void parquet-mr_f7365_0(StringType stringType, State state)
{    return null;}
public static void parquet-mr_f7366_0(String[] args) throws Exception
{    LinkedList<String> arguments = new LinkedList<String>(Arrays.asList(args));    String operator = arguments.pollFirst();    if (operator.equals("generate-json")) {                generateJson(arguments);    }    if (operator.equals("compare-json")) {        compareJson(arguments);    }}
private static void parquet-mr_f7367_0(LinkedList<String> arguments) throws IOException
{    String oldJsonPath = arguments.pollFirst();    String newJsonPath = arguments.pollFirst();    File oldJsonFile = new File(oldJsonPath);    checkExist(oldJsonFile);    File newJsonFile = new File(newJsonPath);    checkExist(newJsonFile);    ObjectMapper mapper = new ObjectMapper();    ThriftType.StructType oldStruct = mapper.readValue(oldJsonFile, ThriftType.StructType.class);    ThriftType.StructType newStruct = mapper.readValue(newJsonFile, ThriftType.StructType.class);    CompatibilityReport report = new CompatibilityChecker().checkCompatibility(oldStruct, newStruct);    if (!report.isCompatible) {        System.err.println("schema not compatible");        System.err.println(report.getMessages());        System.exit(1);    }    if (report.hasEmptyStruct()) {        System.err.println("schema contains empty struct");        System.err.println(report.getMessages());        System.exit(1);    }    System.out.println("[success] schema is compatible");}
private static void parquet-mr_f7368_0(File f)
{    if (!f.exists())        throw new RuntimeException("can not find file " + f);}
private static void parquet-mr_f7369_0(LinkedList<String> arguments) throws ClassNotFoundException, IOException
{    String catName = arguments.pollFirst();    String className = arguments.pollFirst();    String storedPath = arguments.pollFirst();    File storeDir = new File(storedPath);    ThriftType.StructType structType = ThriftSchemaConverter.toStructType((Class<? extends TBase<?, ?>>) Class.forName(className));    ObjectMapper mapper = new ObjectMapper();    String fileName = catName + ".json";    mapper.writerWithDefaultPrettyPrinter().writeValue(new File(storeDir, fileName), structType);}
 static T parquet-mr_f7370_0(String json, Class<T> clzz)
{    try {        return om.readValue(json, clzz);    } catch (IOException e) {        throw new RuntimeException(e);    }}
 static String parquet-mr_f7371_0(Object o)
{    try (final StringWriter sw = new StringWriter()) {        om.writeValue(sw, o);        return sw.toString();    } catch (IOException e) {        throw new RuntimeException(e);    }}
public byte parquet-mr_f7372_0()
{    return requirement;}
public static Requirement parquet-mr_f7373_0(byte fieldRequirementType)
{    for (Requirement req : Requirement.values()) {        if (req.requirement == fieldRequirementType) {            return req;        }    }    throw new RuntimeException("Unknown requirement " + fieldRequirementType);}
public String parquet-mr_f7374_0()
{    return name;}
public short parquet-mr_f7375_0()
{    return fieldId;}
public ThriftType parquet-mr_f7376_0()
{    return type;}
public Requirement parquet-mr_f7377_0()
{    return requirement;}
public String parquet-mr_f7378_0()
{    return JSON.toJSON(this);}
public boolean parquet-mr_f7379_0(Object o)
{    if (this == o)        return true;    if (!(o instanceof ThriftField))        return false;    ThriftField that = (ThriftField) o;    if (fieldId != that.fieldId)        return false;    if (!name.equals(that.name))        return false;    if (requirement != that.requirement)        return false;    if (!type.equals(that.type))        return false;    return true;}
public int parquet-mr_f7380_0()
{    int result = name.hashCode();    result = 31 * result + (int) fieldId;    result = 31 * result + requirement.hashCode();    result = 31 * result + type.hashCode();    return result;}
public boolean parquet-mr_f7381_0(Object o)
{    if (this == o)        return true;    if (!(o instanceof ThriftType))        return false;    ThriftType that = (ThriftType) o;    if (type != that.type)        return false;    return true;}
public int parquet-mr_f7382_0()
{    return type != null ? type.hashCode() : 0;}
public static ThriftType parquet-mr_f7383_0(String json)
{    return JSON.fromJSON(json, ThriftType.class);}
public String parquet-mr_f7384_0()
{    return JSON.toJSON(this);}
public String parquet-mr_f7385_0()
{    return toJSON();}
public final void parquet-mr_f7386_0(EnumType enumType)
{    throw new IllegalArgumentException("Expected complex type");}
public final void parquet-mr_f7387_0(BoolType boolType)
{    throw new IllegalArgumentException("Expected complex type");}
public final void parquet-mr_f7388_0(ByteType byteType)
{    throw new IllegalArgumentException("Expected complex type");}
public final void parquet-mr_f7389_0(DoubleType doubleType)
{    throw new IllegalArgumentException("Expected complex type");}
public final void parquet-mr_f7390_0(I16Type i16Type)
{    throw new IllegalArgumentException("Expected complex type");}
public final void parquet-mr_f7391_0(I32Type i32Type)
{    throw new IllegalArgumentException("Expected complex type");}
public final void parquet-mr_f7392_0(I64Type i64Type)
{    throw new IllegalArgumentException("Expected complex type");}
public final void parquet-mr_f7393_0(StringType stringType)
{    throw new IllegalArgumentException("Expected complex type");}
public List<ThriftField> parquet-mr_f7394_0()
{    return children;}
public ThriftField parquet-mr_f7395_0(short id)
{    if (id >= childById.length) {        return null;    } else {        return childById[id];    }}
public StructOrUnionType parquet-mr_f7396_0()
{    return structOrUnionType;}
public R parquet-mr_f7397_0(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
public void parquet-mr_f7398_0(TypeVisitor visitor)
{    visitor.visit(this);}
public boolean parquet-mr_f7399_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    StructType that = (StructType) o;    if (!Arrays.equals(childById, that.childById))        return false;    return true;}
public int parquet-mr_f7400_0()
{    int result = childById != null ? Arrays.hashCode(childById) : 0;    return result;}
public ThriftField parquet-mr_f7401_0()
{    return key;}
public ThriftField parquet-mr_f7402_0()
{    return value;}
public R parquet-mr_f7403_0(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
public void parquet-mr_f7404_0(TypeVisitor visitor)
{    visitor.visit(this);}
public boolean parquet-mr_f7405_0(Object o)
{    if (this == o)        return true;    if (!(o instanceof MapType))        return false;    if (!super.equals(o))        return false;    MapType mapType = (MapType) o;    if (!key.equals(mapType.key))        return false;    if (!value.equals(mapType.value))        return false;    return true;}
public int parquet-mr_f7406_0()
{    int result = super.hashCode();    result = 31 * result + key.hashCode();    result = 31 * result + value.hashCode();    return result;}
public ThriftField parquet-mr_f7407_0()
{    return values;}
public R parquet-mr_f7408_0(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
public void parquet-mr_f7409_0(TypeVisitor visitor)
{    visitor.visit(this);}
public boolean parquet-mr_f7410_0(Object o)
{    if (this == o)        return true;    if (!(o instanceof SetType))        return false;    if (!super.equals(o))        return false;    SetType setType = (SetType) o;    if (!values.equals(setType.values))        return false;    return true;}
public int parquet-mr_f7411_0()
{    int result = super.hashCode();    result = 31 * result + values.hashCode();    return result;}
public ThriftField parquet-mr_f7412_0()
{    return values;}
public R parquet-mr_f7413_0(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
public void parquet-mr_f7414_0(TypeVisitor visitor)
{    visitor.visit(this);}
public boolean parquet-mr_f7415_0(Object o)
{    if (this == o)        return true;    if (!(o instanceof ListType))        return false;    if (!super.equals(o))        return false;    ListType listType = (ListType) o;    if (!values.equals(listType.values))        return false;    return true;}
public int parquet-mr_f7416_0()
{    int result = super.hashCode();    result = 31 * result + values.hashCode();    return result;}
public int parquet-mr_f7417_0()
{    return id;}
public String parquet-mr_f7418_0()
{    return name;}
public boolean parquet-mr_f7419_0(Object o)
{    if (this == o)        return true;    if (!(o instanceof EnumValue))        return false;    EnumValue enumValue = (EnumValue) o;    if (id != enumValue.id)        return false;    if (name != null ? !name.equals(enumValue.name) : enumValue.name != null)        return false;    return true;}
public int parquet-mr_f7420_0()
{    int result = id;    result = 31 * result + (name != null ? name.hashCode() : 0);    return result;}
public Iterable<EnumValue> parquet-mr_f7421_0()
{    return new Iterable<EnumValue>() {        @Override        public Iterator<EnumValue> iterator() {            return values.iterator();        }    };}
public Iterator<EnumValue> parquet-mr_f7422_0()
{    return values.iterator();}
public EnumValue parquet-mr_f7423_0(int id)
{    prepareEnumLookUp();    return idEnumLookup.get(id);}
private void parquet-mr_f7424_0()
{    if (idEnumLookup == null) {        idEnumLookup = new HashMap<Integer, EnumValue>();        for (EnumValue value : values) {            idEnumLookup.put(value.getId(), value);        }    }}
public R parquet-mr_f7425_0(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
public void parquet-mr_f7426_0(TypeVisitor visitor)
{    visitor.visit(this);}
public boolean parquet-mr_f7427_0(Object o)
{    if (this == o)        return true;    if (!(o instanceof EnumType))        return false;    if (!super.equals(o))        return false;    EnumType enumType = (EnumType) o;    if (!values.equals(enumType.values))        return false;    return true;}
public int parquet-mr_f7428_0()
{    int result = super.hashCode();    result = 31 * result + values.hashCode();    return result;}
public R parquet-mr_f7429_0(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
public void parquet-mr_f7430_0(TypeVisitor visitor)
{    visitor.visit(this);}
public R parquet-mr_f7431_0(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
public void parquet-mr_f7432_0(TypeVisitor visitor)
{    visitor.visit(this);}
public R parquet-mr_f7433_0(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
public void parquet-mr_f7434_0(TypeVisitor visitor)
{    visitor.visit(this);}
public R parquet-mr_f7435_0(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
public void parquet-mr_f7436_0(TypeVisitor visitor)
{    visitor.visit(this);}
public R parquet-mr_f7437_0(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
public void parquet-mr_f7438_0(TypeVisitor visitor)
{    visitor.visit(this);}
public R parquet-mr_f7439_0(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
public void parquet-mr_f7440_0(TypeVisitor visitor)
{    visitor.visit(this);}
public boolean parquet-mr_f7441_0()
{    return binary;}
public void parquet-mr_f7442_0(boolean binary)
{    this.binary = binary;}
public R parquet-mr_f7443_0(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
public void parquet-mr_f7444_0(TypeVisitor visitor)
{    visitor.visit(this);}
public ThriftTypeID parquet-mr_f7445_0()
{    return this.type;}
public byte parquet-mr_f7446_0()
{    return thriftType;}
public boolean parquet-mr_f7447_0()
{    return complex;}
public Class<? extends ThriftType> parquet-mr_f7448_0()
{    return clss;}
public static ThriftTypeID parquet-mr_f7449_0(byte type)
{    return types[type];}
public byte parquet-mr_f7450_0()
{    return serializedThriftType;}
public T parquet-mr_f7451_0(TProtocol protocol) throws TException
{    try {        T thriftObject = thriftClass.newInstance();        thriftObject.read(protocol);        return thriftObject;    } catch (InstantiationException e) {        throw new ParquetDecodingException("Could not instantiate Thrift " + thriftClass, e);    } catch (IllegalAccessException e) {        throw new ParquetDecodingException("Thrift class or constructor not public " + thriftClass, e);    }}
public Class<?> parquet-mr_f7452_0()
{    if (thriftClass == null) {        thriftClass = getThriftClass(thriftClassName);    }    return thriftClass;}
public static Class<?> parquet-mr_f7453_0(String thriftClassName)
{    try {        Class<?> thriftClass = Class.forName(thriftClassName);        return thriftClass;    } catch (ClassNotFoundException e) {        throw new BadConfigurationException("Could not instantiate thrift class " + thriftClassName, e);    }}
public StructType parquet-mr_f7454_0()
{    return descriptor;}
public static ThriftMetaData parquet-mr_f7455_0(Map<String, String> extraMetaData)
{    final String thriftClassName = extraMetaData.get(THRIFT_CLASS);    final String thriftDescriptorString = extraMetaData.get(THRIFT_DESCRIPTOR);    if (thriftClassName == null || thriftDescriptorString == null) {        return null;    }    final StructType descriptor = parseDescriptor(thriftDescriptorString);    return new ThriftMetaData(thriftClassName, descriptor);}
public static ThriftMetaData parquet-mr_f7456_0(Class<?> thriftClass)
{    if (thriftClass != null && TBase.class.isAssignableFrom(thriftClass)) {        Class<? extends TBase<?, ?>> tClass = (Class<? extends TBase<?, ?>>) thriftClass;        StructType descriptor = new ThriftSchemaConverter().toStructType(tClass);        return new ThriftMetaData(thriftClass.getName(), descriptor);    }    return null;}
private static StructType parquet-mr_f7457_0(String json)
{    try {        return (StructType) ThriftType.fromJSON(json);    } catch (RuntimeException e) {        throw new BadConfigurationException("Could not read the thrift descriptor " + json, e);    }}
public Map<String, String> parquet-mr_f7458_0()
{    final Map<String, String> map = new HashMap<String, String>();    map.put(THRIFT_CLASS, getThriftClass().getName());    map.put(THRIFT_DESCRIPTOR, descriptor.toJSON());    return map;}
public static Set<String> parquet-mr_f7459_0(Map<String, Set<String>> fileMetadata)
{    return fileMetadata.get(THRIFT_CLASS);}
public String parquet-mr_f7460_0()
{    return String.format("ThriftMetaData(thriftClassName: %s, descriptor: %s)", thriftClassName, descriptor);}
public static Builder<T> parquet-mr_f7461_0(Path file)
{    return new Builder<T>(file);}
public Builder<T> parquet-mr_f7462_0(Configuration conf)
{    this.conf = checkNotNull(conf, "conf");    return this;}
public Builder<T> parquet-mr_f7463_0(Filter filter)
{    this.filter = checkNotNull(filter, "filter");    return this;}
public Builder<T> parquet-mr_f7464_0(Class<T> thriftClass)
{    this.thriftClass = checkNotNull(thriftClass, "thriftClass");    return this;}
public ParquetReader<T> parquet-mr_f7465_0() throws IOException
{    ReadSupport<T> readSupport;    if (thriftClass != null) {        readSupport = new ThriftReadSupport<T>(thriftClass);    } else {        readSupport = new ThriftReadSupport<T>();    }    return ParquetReader.builder(readSupport, file).withConf(conf).withFilter(filter).build();}
public void parquet-mr_f7466_0() throws TException
{}
private void parquet-mr_f7467_0()
{    events.add(readFieldBegin);}
private void parquet-mr_f7468_0()
{    events.add(readFieldEnd);}
public TField parquet-mr_f7469_0() throws TException
{    return new TField(field.getName(), thriftType, field.getFieldId());}
public void parquet-mr_f7470_0(Binary value)
{    startField();    delegate.addBinary(value);    endField();}
public void parquet-mr_f7471_0(boolean value)
{    startField();    delegate.addBoolean(value);    endField();}
public void parquet-mr_f7472_0(double value)
{    startField();    delegate.addDouble(value);    endField();}
public void parquet-mr_f7473_0(float value)
{    startField();    delegate.addFloat(value);    endField();}
public void parquet-mr_f7474_0(int value)
{    startField();    delegate.addInt(value);    endField();}
public void parquet-mr_f7475_0(long value)
{    startField();    delegate.addLong(value);    endField();}
public TField parquet-mr_f7476_0() throws TException
{    return new TField(field.getName(), field.getType().getType().getThriftType(), field.getFieldId());}
public Converter parquet-mr_f7477_0(int fieldIndex)
{    return delegate.getConverter(fieldIndex);}
public void parquet-mr_f7478_0()
{    events.add(readFieldBegin);    delegate.start();}
public void parquet-mr_f7479_0()
{    delegate.end();    events.add(readFieldEnd);}
public Converter parquet-mr_f7480_0(int fieldIndex)
{    return delegate.getConverter(fieldIndex);}
public void parquet-mr_f7481_0()
{    delegate.start();}
public void parquet-mr_f7482_0()
{    delegate.end();    ++count;}
public void parquet-mr_f7483_0()
{    count = 0;}
public int parquet-mr_f7484_0()
{    return count;}
public void parquet-mr_f7485_0(Binary value)
{    delegate.addBinary(value);    ++count;}
public void parquet-mr_f7486_0(boolean value)
{    delegate.addBoolean(value);    ++count;}
public void parquet-mr_f7487_0(double value)
{    delegate.addDouble(value);    ++count;}
public void parquet-mr_f7488_0(float value)
{    delegate.addFloat(value);    ++count;}
public void parquet-mr_f7489_0(int value)
{    delegate.addInt(value);    ++count;}
public void parquet-mr_f7490_0(long value)
{    delegate.addLong(value);    ++count;}
public void parquet-mr_f7491_0()
{    count = 0;}
public int parquet-mr_f7492_0()
{    return count;}
public void parquet-mr_f7493_0(final boolean value)
{    events.add(new ParquetProtocol("readBool()") {        @Override        public boolean readBool() throws TException {            return value;        }    });}
public boolean parquet-mr_f7494_0() throws TException
{    return value;}
public void parquet-mr_f7495_0(final double value)
{    events.add(new ParquetProtocol("readDouble()") {        @Override        public double readDouble() throws TException {            return value;        }    });}
public double parquet-mr_f7496_0() throws TException
{    return value;}
public void parquet-mr_f7497_0(final float value)
{        events.add(new ParquetProtocol("readDouble() float") {        @Override        public double readDouble() throws TException {            return value;        }    });}
public double parquet-mr_f7498_0() throws TException
{    return value;}
public void parquet-mr_f7499_0(final int value)
{        switch(type) {        case BYTE:            events.add(new ParquetProtocol("readByte() int") {                @Override                public byte readByte() throws TException {                    return (byte) value;                }            });            break;        case I16:            events.add(new ParquetProtocol("readI16()") {                @Override                public short readI16() throws TException {                    return (short) value;                }            });            break;        case I32:            events.add(new ParquetProtocol("readI32()") {                @Override                public int readI32() throws TException {                    return value;                }            });            break;        default:            throw new UnsupportedOperationException("not convertible type " + type);    }}
public byte parquet-mr_f7500_0() throws TException
{    return (byte) value;}
public short parquet-mr_f7501_0() throws TException
{    return (short) value;}
public int parquet-mr_f7502_0() throws TException
{    return value;}
public void parquet-mr_f7503_0(final long value)
{    events.add(new ParquetProtocol("readI64()") {        @Override        public long readI64() throws TException {            return value;        }    });}
public long parquet-mr_f7504_0() throws TException
{    return value;}
public void parquet-mr_f7505_0(final Binary value)
{    events.add(new ParquetProtocol("readString() binary") {        @Override        public String readString() throws TException {            return value.toStringUsingUTF8();        }        @Override        public ByteBuffer readBinary() throws TException {            return value.toByteBuffer();        }    });}
public String parquet-mr_f7506_0() throws TException
{    return value.toStringUsingUTF8();}
public ByteBuffer parquet-mr_f7507_0() throws TException
{    return value.toByteBuffer();}
public void parquet-mr_f7508_0(final Binary value)
{    final Integer id = enumLookup.get(value);    if (id == null) {        throw new ParquetDecodingException("Unrecognized enum value: " + value.toStringUsingUTF8() + " known values: " + enumLookup + " in " + this.field);    }    events.add(new ParquetProtocol("readI32() enum") {        @Override        public int readI32() throws TException {            return id;        }    });}
public int parquet-mr_f7509_0() throws TException
{    return id;}
public Converter parquet-mr_f7510_0(int fieldIndex)
{    if (fieldIndex != 0) {        throw new IllegalArgumentException("lists have only one field. can't reach " + fieldIndex);    }    return child;}
public void parquet-mr_f7511_0()
{    child.startCounting();}
public void parquet-mr_f7512_0() throws TException
{}
public void parquet-mr_f7513_0()
{    final int count = child.getCount();    parentEvents.add(new ParquetProtocol("readMapBegin()") {        @Override        public TMap readMapBegin() throws TException {            return new TMap(keyType, valueType, count);        }    });    parentEvents.addAll(mapEvents);    mapEvents.clear();    parentEvents.add(readMapEnd);}
public TMap parquet-mr_f7514_0() throws TException
{    return new TMap(keyType, valueType, count);}
public Converter parquet-mr_f7515_0(int fieldIndex)
{    switch(fieldIndex) {        case 0:            return keyConverter;        case 1:            return valueConverter;        default:            throw new IllegalArgumentException("only key (0) and value (1) are supported. got " + fieldIndex);    }}
public void parquet-mr_f7516_0()
{}
public void parquet-mr_f7517_0()
{}
public void parquet-mr_f7518_0() throws TException
{}
 void parquet-mr_f7519_0(final int count, final byte type)
{    parentEvents.add(new ParquetProtocol("readSetBegin()") {        @Override        public TSet readSetBegin() throws TException {            return new TSet(type, count);        }    });}
public TSet parquet-mr_f7520_0() throws TException
{    return new TSet(type, count);}
 void parquet-mr_f7521_0()
{    parentEvents.add(readSetEnd);}
public void parquet-mr_f7522_0() throws TException
{}
 void parquet-mr_f7523_0(final int count, final byte type)
{    parentEvents.add(new ParquetProtocol("readListBegin()") {        @Override        public TList readListBegin() throws TException {            return new TList(type, count);        }    });}
public TList parquet-mr_f7524_0() throws TException
{    return new TList(type, count);}
 void parquet-mr_f7525_0()
{    parentEvents.add(readListEnd);}
public Converter parquet-mr_f7526_0(int fieldIndex)
{    if (fieldIndex != 0) {        throw new IllegalArgumentException("lists have only one field. can't reach " + fieldIndex);    }    return child;}
public void parquet-mr_f7527_0()
{    childCounter.startCounting();}
public void parquet-mr_f7528_0()
{    int count = childCounter.getCount();    if (elementConverter != null) {        count -= elementConverter.getNullElementCount();    }    collectionStart(count, valuesType.getThriftType());    parentEvents.addAll(listEvents);    listEvents.clear();    collectionEnd();}
public Converter parquet-mr_f7529_0(int fieldIndex)
{    Preconditions.checkArgument(fieldIndex == 0, "Illegal field index: %s", fieldIndex);    return elementConverter;}
public void parquet-mr_f7530_0()
{    elementEvents.clear();}
public void parquet-mr_f7531_0()
{    if (elementEvents.size() > 0) {        listEvents.addAll(elementEvents);    } else {        nullElementCount += 1;    }}
public int parquet-mr_f7532_0()
{    return nullElementCount;}
public Converter parquet-mr_f7533_0(int fieldIndex)
{    return converters[fieldIndex];}
public TStruct parquet-mr_f7534_0() throws TException
{    return tStruct;}
public void parquet-mr_f7535_0()
{    events.add(readStructBegin);}
public TField parquet-mr_f7536_0() throws TException
{    return stop;}
public void parquet-mr_f7537_0() throws TException
{}
public void parquet-mr_f7538_0()
{    events.add(readFieldStop);    events.add(readStructEnd);}
private boolean parquet-mr_f7539_0(GroupType requested, GroupType fullSchema)
{    for (Type field : fullSchema.getFields()) {        if (requested.containsField(field.getName())) {            Type requestedType = requested.getType(field.getName());                        if (!field.isPrimitive()) {                if (hasMissingRequiredFieldInGroupType(requestedType.asGroupType(), field.asGroupType())) {                    return true;                } else {                                        continue;                }            }        } else {            if (field.getRepetition() == Type.Repetition.REQUIRED) {                                return true;            } else {                                continue;            }        }    }    return false;}
public T parquet-mr_f7540_0()
{    try {        if (missingRequiredFieldsInProjection) {            List<TProtocol> fixedEvents = new ProtocolEventsAmender(rootEvents).amendMissingRequiredFields(thriftType);            protocol.addAll(fixedEvents);        } else {            protocol.addAll(rootEvents);        }        rootEvents.clear();        return thriftReader.readOneRecord(protocol);    } catch (TException e) {        protocol.clear();        rootEvents.clear();        throw new RecordMaterializationException("Could not read thrift object from protocol", e);    }}
public void parquet-mr_f7541_0()
{    rootEvents.clear();}
public GroupConverter parquet-mr_f7542_0()
{    return structConverter;}
private Converter parquet-mr_f7543_0(List<TProtocol> events, Type type, ThriftField field)
{    switch(field.getType().getType()) {        case LIST:            return new ListConverter(events, type.asGroupType(), field);        case SET:            return new SetConverter(events, type.asGroupType(), field);        case MAP:            return new MapConverter(events, type.asGroupType(), field);        case STRUCT:            return new StructConverter(events, type.asGroupType(), field);        case STRING:            return new FieldStringConverter(events, field);        case ENUM:            return new FieldEnumConverter(events, field);        default:            return new FieldPrimitiveConverter(events, field);    }}
public MessageType parquet-mr_f7544_0(Class<? extends TBase<?, ?>> thriftClass)
{    return convert(toStructType(thriftClass));}
public MessageType parquet-mr_f7545_0(StructType struct)
{    MessageType messageType = ThriftSchemaConvertVisitor.convert(struct, fieldProjectionFilter, true);    fieldProjectionFilter.assertNoUnmatchedPatterns();    return messageType;}
public static MessageType parquet-mr_f7546_0(StructType struct)
{    return ThriftSchemaConvertVisitor.convert(struct, FieldProjectionFilter.ALL_COLUMNS, false);}
public static StructOrUnionType parquet-mr_f7547_0(Class<T> klass)
{    return TUnion.class.isAssignableFrom(klass) ? StructOrUnionType.UNION : StructOrUnionType.STRUCT;}
public static ThriftType.StructType parquet-mr_f7548_0(Class<? extends TBase<?, ?>> thriftClass)
{    final TStructDescriptor struct = TStructDescriptor.getInstance(thriftClass);    return toStructType(struct);}
private static StructType parquet-mr_f7549_0(TStructDescriptor struct)
{    List<Field> fields = struct.getFields();    List<ThriftField> children = new ArrayList<ThriftField>(fields.size());    for (Field field : fields) {        Requirement req = field.getFieldMetaData() == null ? Requirement.OPTIONAL : Requirement.fromType(field.getFieldMetaData().requirementType);        children.add(toThriftField(field.getName(), field, req));    }    return new StructType(children, structOrUnionType(struct.getThriftClass()));}
 static boolean parquet-mr_f7550_0(Type repeatedType, ThriftField thriftElement)
{    if (repeatedType.isPrimitive() || (repeatedType.asGroupType().getFieldCount() != 1) || (repeatedType.asGroupType().getType(0).isRepetition(REPEATED))) {                return true;    } else if (thriftElement != null && thriftElement.getType() instanceof StructType) {        Set<String> fieldNames = new HashSet<String>();        for (ThriftField field : ((StructType) thriftElement.getType()).getChildren()) {            fieldNames.add(field.getName());        }                return fieldNames.contains(repeatedType.asGroupType().getFieldName(0));    }    return false;}
private static ThriftField parquet-mr_f7551_0(String name, Field field, ThriftField.Requirement requirement)
{    ThriftType type;    switch(ThriftTypeID.fromByte(field.getType())) {        case STOP:        case VOID:        default:            throw new UnsupportedOperationException("can't convert type of " + field);        case BOOL:            type = new BoolType();            break;        case BYTE:            type = new ByteType();            break;        case DOUBLE:            type = new DoubleType();            break;        case I16:            type = new I16Type();            break;        case I32:            type = new I32Type();            break;        case I64:            type = new I64Type();            break;        case STRING:            StringType stringType = new StringType();            FieldMetaData fieldMetaData = field.getFieldMetaData();                        if (fieldMetaData != null && fieldMetaData.valueMetaData.isBinary()) {                stringType.setBinary(true);            }            type = stringType;            break;        case STRUCT:            type = toStructType(field.gettStructDescriptor());            break;        case MAP:            final Field mapKeyField = field.getMapKeyField();            final Field mapValueField = field.getMapValueField();            type = new ThriftType.MapType(toThriftField(mapKeyField.getName(), mapKeyField, requirement), toThriftField(mapValueField.getName(), mapValueField, requirement));            break;        case SET:            final Field setElemField = field.getSetElemField();            type = new ThriftType.SetType(toThriftField(setElemField.getName(), setElemField, requirement));            break;        case LIST:            final Field listElemField = field.getListElemField();            type = new ThriftType.ListType(toThriftField(listElemField.getName(), listElemField, requirement));            break;        case ENUM:            Collection<TEnum> enumValues = field.getEnumValues();            List<EnumValue> values = new ArrayList<ThriftType.EnumValue>();            for (TEnum tEnum : enumValues) {                values.add(new EnumValue(tEnum.getValue(), tEnum.toString()));            }            type = new EnumType(values);            break;    }    return new ThriftField(name, field.getId(), requirement, type);}
public static MessageType parquet-mr_f7552_0(StructType struct, FieldProjectionFilter filter)
{    return convert(struct, filter, true);}
public static MessageType parquet-mr_f7553_0(StructType struct, FieldProjectionFilter filter, boolean keepOneOfEachUnion)
{    State state = new State(new FieldsPath(), REPEATED, "ParquetSchema");    ConvertedField converted = struct.accept(new ThriftSchemaConvertVisitor(filter, true, keepOneOfEachUnion), state);    if (!converted.isKeep()) {        throw new ThriftProjectionException("No columns have been selected");    }    return new MessageType(state.name, converted.asKeep().getType().asGroupType().getFields());}
public FieldProjectionFilter parquet-mr_f7554_0()
{    return fieldProjectionFilter;}
public ConvertedField parquet-mr_f7555_0(MapType mapType, State state)
{    ThriftField keyField = mapType.getKey();    ThriftField valueField = mapType.getValue();    State keyState = new State(state.path.push(keyField), REQUIRED, "key");                State valueState = new State(state.path.push(valueField), OPTIONAL, "value");    ConvertedField convertedKey = keyField.getType().accept(this, keyState);    ConvertedField convertedValue = valueField.getType().accept(this, valueState);    if (!convertedKey.isKeep()) {        if (convertedValue.isKeep()) {            throw new ThriftProjectionException("Cannot select only the values of a map, you must keep the keys as well: " + state.path);        }                return new Drop(state.path);    }        if (doProjection) {        ConvertedField fullConvKey = keyField.getType().accept(new ThriftSchemaConvertVisitor(FieldProjectionFilter.ALL_COLUMNS, false, keepOneOfEachUnion), keyState);        if (!fullConvKey.asKeep().getType().equals(convertedKey.asKeep().getType())) {            throw new ThriftProjectionException("Cannot select only a subset of the fields in a map key, " + "for path " + state.path);        }    }    if (convertedValue.isKeep()) {                Type mapField = mapType(state.repetition, state.name, convertedKey.asKeep().getType(), convertedValue.asKeep().getType());        return new Keep(state.path, mapField);    }        ConvertedField sentinelValue = valueField.getType().accept(new ThriftSchemaConvertVisitor(new KeepOnlyFirstPrimitiveFilter(), true, keepOneOfEachUnion), valueState);    Type mapField = mapType(state.repetition, state.name, convertedKey.asKeep().getType(),     sentinelValue.asKeep().getType());    return new Keep(state.path, mapField);}
private ConvertedField parquet-mr_f7556_0(ThriftField listLike, State state, boolean isSet)
{    State childState = new State(state.path, REPEATED, state.name + "_tuple");    ConvertedField converted = listLike.getType().accept(this, childState);    if (converted.isKeep()) {                if (isSet && doProjection) {            ConvertedField fullConv = listLike.getType().accept(new ThriftSchemaConvertVisitor(FieldProjectionFilter.ALL_COLUMNS, false, keepOneOfEachUnion), childState);            if (!converted.asKeep().getType().equals(fullConv.asKeep().getType())) {                throw new ThriftProjectionException("Cannot select only a subset of the fields in a set, " + "for path " + state.path);            }        }        return new Keep(state.path, listType(state.repetition, state.name, converted.asKeep().getType()));    }    return new Drop(state.path);}
public ConvertedField parquet-mr_f7557_0(SetType setType, State state)
{    return visitListLike(setType.getValues(), state, true);}
public ConvertedField parquet-mr_f7558_0(ListType listType, State state)
{    return visitListLike(listType.getValues(), state, false);}
public ConvertedField parquet-mr_f7559_0(StructType structType, State state)
{                final boolean needsToKeepOneOfEachUnion = keepOneOfEachUnion && isUnion(structType.getStructOrUnionType());    boolean hasSentinelUnionColumns = false;    boolean hasNonSentinelUnionColumns = false;    List<Type> convertedChildren = new ArrayList<Type>();    for (ThriftField child : structType.getChildren()) {        State childState = new State(state.path.push(child), getRepetition(child), child.getName());        ConvertedField converted = child.getType().accept(this, childState);        if (!converted.isKeep() && needsToKeepOneOfEachUnion) {                                                                                    ConvertedField firstPrimitive = child.getType().accept(new ThriftSchemaConvertVisitor(new KeepOnlyFirstPrimitiveFilter(), true, keepOneOfEachUnion), childState);            convertedChildren.add(firstPrimitive.asKeep().getType().withId(child.getFieldId()));            hasSentinelUnionColumns = true;        }        if (converted.isSentinelUnion()) {                        if (childState.repetition == REQUIRED) {                                convertedChildren.add(converted.asSentinelUnion().getType().withId(child.getFieldId()));                hasSentinelUnionColumns = true;            }        } else if (converted.isKeep()) {                        convertedChildren.add(converted.asKeep().getType().withId(child.getFieldId()));            hasNonSentinelUnionColumns = true;        }    }    if (!hasNonSentinelUnionColumns && hasSentinelUnionColumns) {                return new SentinelUnion(state.path, new GroupType(state.repetition, state.name, convertedChildren));    }    if (hasNonSentinelUnionColumns) {                return new Keep(state.path, new GroupType(state.repetition, state.name, convertedChildren));    } else {                return new Drop(state.path);    }}
private ConvertedField parquet-mr_f7560_0(PrimitiveTypeName type, State state)
{    return visitPrimitiveType(type, null, state);}
private ConvertedField parquet-mr_f7561_0(PrimitiveTypeName type, LogicalTypeAnnotation orig, State state)
{    PrimitiveBuilder<PrimitiveType> b = primitive(type, state.repetition);    if (orig != null) {        b = b.as(orig);    }    if (fieldProjectionFilter.keep(state.path)) {        return new Keep(state.path, b.named(state.name));    } else {        return new Drop(state.path);    }}
public ConvertedField parquet-mr_f7562_0(EnumType enumType, State state)
{    return visitPrimitiveType(BINARY, enumType(), state);}
public ConvertedField parquet-mr_f7563_0(BoolType boolType, State state)
{    return visitPrimitiveType(BOOLEAN, state);}
public ConvertedField parquet-mr_f7564_0(ByteType byteType, State state)
{    return visitPrimitiveType(INT32, state);}
public ConvertedField parquet-mr_f7565_0(DoubleType doubleType, State state)
{    return visitPrimitiveType(DOUBLE, state);}
public ConvertedField parquet-mr_f7566_0(I16Type i16Type, State state)
{    return visitPrimitiveType(INT32, state);}
public ConvertedField parquet-mr_f7567_0(I32Type i32Type, State state)
{    return visitPrimitiveType(INT32, state);}
public ConvertedField parquet-mr_f7568_0(I64Type i64Type, State state)
{    return visitPrimitiveType(INT64, state);}
public ConvertedField parquet-mr_f7569_0(StringType stringType, State state)
{    return stringType.isBinary() ? visitPrimitiveType(BINARY, state) : visitPrimitiveType(BINARY, stringType(), state);}
private static boolean parquet-mr_f7570_0(StructOrUnionType s)
{    switch(s) {        case STRUCT:            return false;        case UNION:            return true;        case UNKNOWN:            throw new ShouldNeverHappenException("Encountered UNKNOWN StructOrUnionType");        default:            throw new ShouldNeverHappenException("Unrecognized type: " + s);    }}
private Type.Repetition parquet-mr_f7571_0(ThriftField thriftField)
{    switch(thriftField.getRequirement()) {        case REQUIRED:            return REQUIRED;        case OPTIONAL:            return OPTIONAL;        case DEFAULT:            return OPTIONAL;        default:            throw new IllegalArgumentException("unknown requirement type: " + thriftField.getRequirement());    }}
public void parquet-mr_f7572_0() throws Exception
{    Path test = writeDirect("message UnannotatedListOfPrimitives {" + "  repeated int32 list_of_ints;" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("list_of_ints", 0);            rc.addInteger(34);            rc.addInteger(35);            rc.addInteger(36);            rc.endField("list_of_ints", 0);            rc.endMessage();        }    });}
public void parquet-mr_f7573_0(RecordConsumer rc)
{    rc.startMessage();    rc.startField("list_of_ints", 0);    rc.addInteger(34);    rc.addInteger(35);    rc.addInteger(36);    rc.endField("list_of_ints", 0);    rc.endMessage();}
public void parquet-mr_f7574_0() throws Exception
{    Path test = writeDirect("message UnannotatedListOfGroups {" + "  repeated group list_of_points {" + "    required float x;" + "    required float y;" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("list_of_points", 0);            rc.startGroup();            rc.startField("x", 0);            rc.addFloat(1.0f);            rc.endField("x", 0);            rc.startField("y", 1);            rc.addFloat(1.0f);            rc.endField("y", 1);            rc.endGroup();            rc.startGroup();            rc.startField("x", 0);            rc.addFloat(2.0f);            rc.endField("x", 0);            rc.startField("y", 1);            rc.addFloat(2.0f);            rc.endField("y", 1);            rc.endGroup();            rc.endField("list_of_points", 0);            rc.endMessage();        }    });}
public void parquet-mr_f7575_0(RecordConsumer rc)
{    rc.startMessage();    rc.startField("list_of_points", 0);    rc.startGroup();    rc.startField("x", 0);    rc.addFloat(1.0f);    rc.endField("x", 0);    rc.startField("y", 1);    rc.addFloat(1.0f);    rc.endField("y", 1);    rc.endGroup();    rc.startGroup();    rc.startField("x", 0);    rc.addFloat(2.0f);    rc.endField("x", 0);    rc.startField("y", 1);    rc.addFloat(2.0f);    rc.endField("y", 1);    rc.endGroup();    rc.endField("list_of_points", 0);    rc.endMessage();}
public void parquet-mr_f7576_0() throws Exception
{    Path test = writeDirect("message RepeatedPrimitiveInList {" + "  required group list_of_ints (LIST) {" + "    repeated int32 array;" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("list_of_ints", 0);            rc.startGroup();            rc.startField("array", 0);            rc.addInteger(34);            rc.addInteger(35);            rc.addInteger(36);            rc.endField("array", 0);            rc.endGroup();            rc.endField("list_of_ints", 0);            rc.endMessage();        }    });    ListOfInts expected = new ListOfInts(Lists.newArrayList(34, 35, 36));    ListOfInts actual = reader(test, ListOfInts.class).read();    Assert.assertEquals("Should read record correctly", expected, actual);}
public void parquet-mr_f7577_0(RecordConsumer rc)
{    rc.startMessage();    rc.startField("list_of_ints", 0);    rc.startGroup();    rc.startField("array", 0);    rc.addInteger(34);    rc.addInteger(35);    rc.addInteger(36);    rc.endField("array", 0);    rc.endGroup();    rc.endField("list_of_ints", 0);    rc.endMessage();}
public void parquet-mr_f7578_0() throws Exception
{        Path test = writeDirect("message MultiFieldGroupInList {" + "  optional group locations (LIST) {" + "    repeated group element {" + "      required double latitude;" + "      required double longitude;" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(0.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(180.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLocations expected = new ListOfLocations();    expected.addToLocations(new Location(0.0, 0.0));    expected.addToLocations(new Location(0.0, 180.0));    assertReaderContains(reader(test, ListOfLocations.class), expected);}
public void parquet-mr_f7579_0(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(0.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(180.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
public void parquet-mr_f7580_0() throws Exception
{            Path test = writeDirect("message SingleFieldGroupInList {" + "  optional group single_element_groups (LIST) {" + "    repeated group single_element_group {" + "      required int64 count;" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("single_element_groups", 0);            rc.startGroup();                        rc.startField("single_element_group", 0);            rc.startGroup();            rc.startField("count", 0);            rc.addLong(1234L);            rc.endField("count", 0);            rc.endGroup();            rc.startGroup();            rc.startField("count", 0);            rc.addLong(2345L);            rc.endField("count", 0);            rc.endGroup();                        rc.endField("single_element_group", 0);            rc.endGroup();            rc.endField("single_element_groups", 0);            rc.endMessage();        }    });            ListOfSingleElementGroups expectedOldBehavior = new ListOfSingleElementGroups();    expectedOldBehavior.addToSingle_element_groups(new SingleElementGroup(1234L));    expectedOldBehavior.addToSingle_element_groups(new SingleElementGroup(2345L));    assertReaderContains(reader(test, ListOfSingleElementGroups.class), expectedOldBehavior);        ListOfCounts expectedNewBehavior = new ListOfCounts();    expectedNewBehavior.addToSingle_element_groups(1234L);    expectedNewBehavior.addToSingle_element_groups(2345L);    assertReaderContains(reader(test, ListOfCounts.class), expectedNewBehavior);}
public void parquet-mr_f7581_0(RecordConsumer rc)
{    rc.startMessage();    rc.startField("single_element_groups", 0);    rc.startGroup();        rc.startField("single_element_group", 0);    rc.startGroup();    rc.startField("count", 0);    rc.addLong(1234L);    rc.endField("count", 0);    rc.endGroup();    rc.startGroup();    rc.startField("count", 0);    rc.addLong(2345L);    rc.endField("count", 0);    rc.endGroup();        rc.endField("single_element_group", 0);    rc.endGroup();    rc.endField("single_element_groups", 0);    rc.endMessage();}
public void parquet-mr_f7582_0() throws Exception
{    Path test = writeDirect("message NewOptionalGroupInList {" + "  optional group locations (LIST) {" + "    repeated group list {" + "      optional group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();                        rc.startField("list", 0);                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(0.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                                    rc.startGroup();                        rc.endGroup();                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(180.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                        rc.endField("list", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLocations expected = new ListOfLocations();    expected.addToLocations(new Location(0.0, 0.0));            expected.addToLocations(new Location(0.0, 180.0));    try {        assertReaderContains(reader(test, ListOfLocations.class), expected);        fail("Should fail: locations are optional and not ignored");    } catch (RuntimeException e) {                assertTrue(e.getCause().getCause().getMessage().contains("locations"));    }    assertReaderContains(readerIgnoreNulls(test, ListOfLocations.class), expected);}
public void parquet-mr_f7583_0(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();        rc.startField("list", 0);            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(0.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();            rc.startGroup();        rc.endGroup();            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(180.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();        rc.endField("list", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
public void parquet-mr_f7584_0() throws Exception
{    Path test = writeDirect("message NewRequiredGroupInList {" + "  optional group locations (LIST) {" + "    repeated group list {" + "      required group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();                        rc.startField("list", 0);                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(180.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(0.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                        rc.endField("list", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLocations expected = new ListOfLocations();    expected.addToLocations(new Location(0.0, 180.0));    expected.addToLocations(new Location(0.0, 0.0));    assertReaderContains(reader(test, ListOfLocations.class), expected);}
public void parquet-mr_f7585_0(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();        rc.startField("list", 0);            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(180.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(0.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();        rc.endField("list", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
public void parquet-mr_f7586_0() throws Exception
{    Path test = writeDirect("message AvroCompatRequiredGroupInList {" + "  optional group locations (LIST) {" + "    repeated group array {" + "      required group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();                        rc.startField("array", 0);                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(90.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(180.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(-90.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(0.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                        rc.endField("array", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLocations expected = new ListOfLocations();    expected.addToLocations(new Location(90.0, 180.0));    expected.addToLocations(new Location(-90.0, 0.0));    assertReaderContains(reader(test, ListOfLocations.class), expected);}
public void parquet-mr_f7587_0(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();        rc.startField("array", 0);            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(90.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(180.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(-90.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(0.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();        rc.endField("array", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
public void parquet-mr_f7588_0() throws Exception
{    Path test = writeDirect("message AvroCompatListInList {" + "  optional group listOfLists (LIST) {" + "    repeated group array (LIST) {" + "      repeated int32 array;" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();                        rc.startField("array", 0);            rc.startGroup();                        rc.startField("array", 0);                        rc.addInteger(34);            rc.addInteger(35);            rc.addInteger(36);                        rc.endField("array", 0);            rc.endGroup();                        rc.startGroup();            rc.endGroup();            rc.startGroup();                        rc.startField("array", 0);                        rc.addInteger(32);            rc.addInteger(33);            rc.addInteger(34);                        rc.endField("array", 0);            rc.endGroup();                        rc.endField("array", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLists expected = new ListOfLists();    expected.addToListOfLists(Arrays.asList(34, 35, 36));    expected.addToListOfLists(Arrays.<Integer>asList());    expected.addToListOfLists(Arrays.asList(32, 33, 34));        assertReaderContains(reader(test, ListOfLists.class), expected);}
public void parquet-mr_f7589_0(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();        rc.startField("array", 0);    rc.startGroup();        rc.startField("array", 0);        rc.addInteger(34);    rc.addInteger(35);    rc.addInteger(36);        rc.endField("array", 0);    rc.endGroup();        rc.startGroup();    rc.endGroup();    rc.startGroup();        rc.startField("array", 0);        rc.addInteger(32);    rc.addInteger(33);    rc.addInteger(34);        rc.endField("array", 0);    rc.endGroup();        rc.endField("array", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
public void parquet-mr_f7590_0() throws Exception
{    Path test = writeDirect("message ThriftCompatListInList {" + "  optional group listOfLists (LIST) {" + "    repeated group listOfLists_tuple (LIST) {" + "      repeated int32 listOfLists_tuple_tuple;" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();                        rc.startField("listOfLists_tuple", 0);            rc.startGroup();                        rc.startField("listOfLists_tuple_tuple", 0);                        rc.addInteger(34);            rc.addInteger(35);            rc.addInteger(36);                        rc.endField("listOfLists_tuple_tuple", 0);            rc.endGroup();                        rc.startGroup();            rc.endGroup();            rc.startGroup();                        rc.startField("listOfLists_tuple_tuple", 0);                        rc.addInteger(32);            rc.addInteger(33);            rc.addInteger(34);                        rc.endField("listOfLists_tuple_tuple", 0);            rc.endGroup();                        rc.endField("listOfLists_tuple", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLists expected = new ListOfLists();    expected.addToListOfLists(Arrays.asList(34, 35, 36));    expected.addToListOfLists(Arrays.<Integer>asList());    expected.addToListOfLists(Arrays.asList(32, 33, 34));        assertReaderContains(reader(test, ListOfLists.class), expected);}
public void parquet-mr_f7591_0(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();        rc.startField("listOfLists_tuple", 0);    rc.startGroup();        rc.startField("listOfLists_tuple_tuple", 0);        rc.addInteger(34);    rc.addInteger(35);    rc.addInteger(36);        rc.endField("listOfLists_tuple_tuple", 0);    rc.endGroup();        rc.startGroup();    rc.endGroup();    rc.startGroup();        rc.startField("listOfLists_tuple_tuple", 0);        rc.addInteger(32);    rc.addInteger(33);    rc.addInteger(34);        rc.endField("listOfLists_tuple_tuple", 0);    rc.endGroup();        rc.endField("listOfLists_tuple", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
public void parquet-mr_f7592_0() throws Exception
{    Path test = writeDirect("message OldThriftCompatRequiredGroupInList {" + "  optional group locations (LIST) {" + "    repeated group locations_tuple {" + "      required group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();                        rc.startField("locations_tuple", 0);                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(180.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(0.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                        rc.endField("locations_tuple", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLocations expected = new ListOfLocations();    expected.addToLocations(new Location(0.0, 180.0));    expected.addToLocations(new Location(0.0, 0.0));    assertReaderContains(reader(test, ListOfLocations.class), expected);}
public void parquet-mr_f7593_0(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();        rc.startField("locations_tuple", 0);            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(180.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(0.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();        rc.endField("locations_tuple", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
public void parquet-mr_f7594_0() throws Exception
{    Path test = writeDirect("message HiveCompatOptionalGroupInList {" + "  optional group locations (LIST) {" + "    repeated group bag {" + "      optional group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();                        rc.startField("bag", 0);                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(180.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(0.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                        rc.endField("bag", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLocations expected = new ListOfLocations();    expected.addToLocations(new Location(0.0, 180.0));    expected.addToLocations(new Location(0.0, 0.0));    try {        assertReaderContains(reader(test, ListOfLocations.class), expected);        fail("Should fail: locations are optional and not ignored");    } catch (RuntimeException e) {                assertTrue(e.getCause().getCause().getMessage().contains("locations"));    }    assertReaderContains(readerIgnoreNulls(test, ListOfLocations.class), expected);}
public void parquet-mr_f7595_0(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();        rc.startField("bag", 0);            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(180.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(0.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();        rc.endField("bag", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
public ParquetReader<T> parquet-mr_f7596_0(Path file, Class<T> thriftClass) throws IOException
{    return ThriftParquetReader.<T>build(file).withThriftClass(thriftClass).build();}
public ParquetReader<T> parquet-mr_f7597_0(Path file, Class<T> thriftClass) throws IOException
{    Configuration conf = new Configuration();    conf.setBoolean(ThriftRecordConverter.IGNORE_NULL_LIST_ELEMENTS, true);    return ThriftParquetReader.<T>build(file).withThriftClass(thriftClass).withConf(conf).build();}
public void parquet-mr_f7598_0(ParquetReader<T> reader, T... expected) throws IOException
{    T record;    List<T> actual = Lists.newArrayList();    while ((record = reader.read()) != null) {        actual.add(record);    }    Assert.assertEquals("Should match exepected records", Lists.newArrayList(expected), actual);}
public void parquet-mr_f7599_0() throws IOException
{    StringAndBinary expected = new StringAndBinary("test", ByteBuffer.wrap(new byte[] { -123, 20, 33 }));    File temp = tempDir.newFile(UUID.randomUUID().toString());    temp.deleteOnExit();    temp.delete();    Path path = new Path(temp.getPath());    ThriftParquetWriter<StringAndBinary> writer = new ThriftParquetWriter<StringAndBinary>(path, StringAndBinary.class, CompressionCodecName.SNAPPY);    writer.write(expected);    writer.close();    ParquetReader<StringAndBinary> reader = ThriftParquetReader.<StringAndBinary>build(path).withThriftClass(StringAndBinary.class).build();    StringAndBinary record = reader.read();    reader.close();    assertSchema(ParquetFileReader.readFooter(new Configuration(), path));    assertEquals("Should match after serialization round trip", expected, record);}
private void parquet-mr_f7600_0(ParquetMetadata parquetMetadata)
{    List<Type> fields = parquetMetadata.getFileMetaData().getSchema().getFields();    assertEquals(2, fields.size());    assertEquals(Types.required(PrimitiveType.PrimitiveTypeName.BINARY).as(OriginalType.UTF8).id(1).named("s"), fields.get(0));    assertEquals(Types.required(PrimitiveType.PrimitiveTypeName.BINARY).id(2).named("b"), fields.get(1));}
protected void parquet-mr_f7601_0(Context context) throws IOException, InterruptedException
{    records = new ArrayList<Object>();}
protected void parquet-mr_f7602_0(Void key, T value, Context context) throws IOException, InterruptedException
{    records.add(value);}
public static StructWithAStructThatLooksLikeUnionV2 parquet-mr_f7603_0(int i)
{    AStructThatLooksLikeUnionV2 validUnion = new AStructThatLooksLikeUnionV2();    switch(i % 3) {        case 0:            validUnion.setALong(new ALong(17L));            break;        case 1:            validUnion.setANewBool(new ABool(false));            break;        case 2:            validUnion.setAString(new AString("bar"));            break;    }    return new StructWithAStructThatLooksLikeUnionV2("foo" + i, validUnion);}
public static StructWithUnionV2 parquet-mr_f7604_0(int i)
{    UnionV2 validUnion = new UnionV2();    switch(i % 3) {        case 0:            validUnion.setALong(new ALong(17L));            break;        case 1:            validUnion.setANewBool(new ABool(false));            break;        case 2:            validUnion.setAString(new AString("bar"));            break;    }    return new StructWithUnionV2("foo" + i, validUnion);}
public static StructWithAStructThatLooksLikeUnionV2 parquet-mr_f7605_0(int i)
{    AStructThatLooksLikeUnionV2 invalid = new AStructThatLooksLikeUnionV2();    if (i % 2 == 0) {                invalid.setALong(new ALong(18l));        invalid.setANewBool(new ABool(false));    } else {        }    return new StructWithAStructThatLooksLikeUnionV2("foo" + i, invalid);}
protected void parquet-mr_f7606_0(Job job, Path path) throws Exception
{    job.setInputFormatClass(ParquetThriftInputFormat.class);    ParquetThriftInputFormat.setInputPaths(job, path);    ParquetThriftInputFormat.setThriftClass(job.getConfiguration(), StructWithUnionV2.class);    job.setMapperClass(ReadMapper.class);    job.setNumReduceTasks(0);    job.setOutputFormatClass(NullOutputFormat.class);}
protected void parquet-mr_f7607_0(List<StructWithUnionV2> expected, List<Object> found) throws Exception
{    assertEquals(expected, found);}
private Path parquet-mr_f7608_0(int numCorrupt, List<StructWithUnionV2> collectExpectedRecords) throws Exception
{                Path outputPath = new Path(new File(tempDir.getRoot(), "corrupt_out").getAbsolutePath());    ParquetWriter<StructWithAStructThatLooksLikeUnionV2> writer = new ThriftParquetWriter<StructWithAStructThatLooksLikeUnionV2>(outputPath, StructWithAStructThatLooksLikeUnionV2.class, CompressionCodecName.UNCOMPRESSED);    int numRecords = 0;    for (int i = 0; i < 100; i++) {        StructWithAStructThatLooksLikeUnionV2 valid = makeValid(numRecords);        StructWithUnionV2 expected = makeExpectedValid(numRecords);        numRecords++;        collectExpectedRecords.add(expected);        writer.write(valid);    }    for (int i = 0; i < numCorrupt; i++) {        writer.write(makeInvalid(numRecords++));    }    for (int i = 0; i < 100; i++) {        StructWithAStructThatLooksLikeUnionV2 valid = makeValid(numRecords);        StructWithUnionV2 expected = makeExpectedValid(numRecords);        numRecords++;        collectExpectedRecords.add(expected);        writer.write(valid);    }    writer.close();    return outputPath;}
private void parquet-mr_f7609_0(Path path, Configuration conf, String name) throws Exception
{    Job job = new Job(conf, name);    setupJob(job, path);    waitForJob(job);}
public void parquet-mr_f7610_0() throws Exception
{    ArrayList<StructWithUnionV2> expected = new ArrayList<StructWithUnionV2>();    try {        readFile(writeFileWithCorruptRecords(1, expected), new Configuration(), "testDefaultsToNoTolerance");        fail("This should throw");    } catch (RuntimeException e) {                assertEquals(100, ReadMapper.records.size());        assertEqualsExcepted(expected.subList(0, 100), ReadMapper.records);    }}
public void parquet-mr_f7611_0() throws Exception
{    Configuration conf = new Configuration();    conf.setFloat(UnmaterializableRecordCounter.BAD_RECORD_THRESHOLD_CONF_KEY, 0.1f);    List<StructWithUnionV2> expected = new ArrayList<StructWithUnionV2>();    readFile(writeFileWithCorruptRecords(4, expected), conf, "testCanTolerateBadRecords");    assertEquals(200, ReadMapper.records.size());    assertEqualsExcepted(expected, ReadMapper.records);}
public void parquet-mr_f7612_0() throws Exception
{    Configuration conf = new Configuration();    conf.setFloat(UnmaterializableRecordCounter.BAD_RECORD_THRESHOLD_CONF_KEY, 0.1f);    ArrayList<StructWithUnionV2> expected = new ArrayList<StructWithUnionV2>();    try {        readFile(writeFileWithCorruptRecords(300, expected), conf, "testThrowsWhenTooManyBadRecords");        fail("This should throw");    } catch (RuntimeException e) {                assertEquals(100, ReadMapper.records.size());        assertEqualsExcepted(expected.subList(0, 100), ReadMapper.records);    }}
public static AddressBook parquet-mr_f7613_0(int i)
{    final ArrayList<Person> persons = new ArrayList<Person>();    for (int j = 0; j < i % 3; j++) {        final ArrayList<PhoneNumber> phones = new ArrayList<PhoneNumber>();        for (int k = 0; k < i % 4; k++) {            phones.add(new PhoneNumber("12345" + i));        }        persons.add(new Person(new Name("John" + i, "Roberts"), i, "John@example.com" + i, phones));    }    AddressBook a = new AddressBook(persons);    return a;}
public void parquet-mr_f7614_0(org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Void, AddressBook>.Context context) throws IOException, InterruptedException
{    for (int i = 0; i < 10; i++) {        AddressBook a = TestInputOutputFormat.nextAddressbook(i);        context.write(null, a);    }}
protected void parquet-mr_f7615_0(Void key, AddressBook value, Mapper<Void, Group, LongWritable, Text>.Context context) throws IOException, InterruptedException
{    context.write(null, new Text(value.toString()));}
public void parquet-mr_f7616_0() throws Exception
{    final Configuration conf = new Configuration();    final Path inputPath = new Path("src/test/java/org/apache/parquet/hadoop/thrift/TestInputOutputFormat.java");    final Path parquetPath = new Path("target/test/thrift/TestInputOutputFormat/parquet");    final Path outputPath = new Path("target/test/thrift/TestInputOutputFormat/out");    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        final Job job = new Job(conf, "write");                TextInputFormat.addInputPath(job, inputPath);        job.setInputFormatClass(TextInputFormat.class);        job.setMapperClass(TestInputOutputFormat.MyMapper.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(ParquetThriftOutputFormat.class);        ParquetThriftOutputFormat.setCompression(job, CompressionCodecName.GZIP);        ParquetThriftOutputFormat.setOutputPath(job, parquetPath);        ParquetThriftOutputFormat.setThriftClass(job, AddressBook.class);        waitForJob(job);    }    {        final Job job = new Job(conf, "read");        job.setInputFormatClass(ParquetThriftInputFormat.class);        ParquetThriftInputFormat.setInputPaths(job, parquetPath);        job.setMapperClass(TestInputOutputFormat.MyMapper2.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(TextOutputFormat.class);        TextOutputFormat.setOutputPath(job, outputPath);        waitForJob(job);    }    final BufferedReader out = new BufferedReader(new FileReader(new File(outputPath.toString(), "part-m-00000")));    String lineOut = null;    int lineNumber = 0;    while ((lineOut = out.readLine()) != null) {        lineOut = lineOut.substring(lineOut.indexOf("\t") + 1);        AddressBook a = nextAddressbook(lineNumber);        assertEquals("line " + lineNumber, a.toString(), lineOut);        ++lineNumber;    }    assertNull("line " + lineNumber, out.readLine());    out.close();}
protected void parquet-mr_f7617_0(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Void, StructV1>.Context context) throws IOException, InterruptedException
{    context.write(null, new StructV1(value.toString() + 1));}
protected void parquet-mr_f7618_0(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Void, StructV2>.Context context) throws IOException, InterruptedException
{    final StructV2 s = new StructV2(value.toString() + 2);    s.setAge("undetermined");    context.write(null, s);}
protected void parquet-mr_f7619_0(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Void, StructV3>.Context context) throws IOException, InterruptedException
{    final StructV3 s = new StructV3(value.toString() + 3);    s.setAge("average");    s.setGender("unavailable");    context.write(null, s);}
protected void parquet-mr_f7620_0(LongWritable key, StructV3 value, org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Void, Text>.Context context) throws IOException, InterruptedException
{    context.write(null, new Text(value.toString()));}
public void parquet-mr_f7621_0() throws Exception
{    final Configuration conf = new Configuration();    final Path inputPath = new Path("target/test/thrift/schema_evolution/in");    final Path parquetPath = new Path("target/test/thrift/schema_evolution/parquet");    final Path outputPath = new Path("target/test/thrift/schema_evolution/out");    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(inputPath, true);    final FSDataOutputStream in = fileSystem.create(inputPath);    in.writeUTF("Alice\nBob\nCharles\n");    in.close();    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        write(conf, inputPath, new Path(parquetPath, "V1"), TestInputOutputFormat.SchemaEvolutionMapper1.class, StructV1.class);        write(conf, inputPath, new Path(parquetPath, "V2"), TestInputOutputFormat.SchemaEvolutionMapper2.class, StructV2.class);        write(conf, inputPath, new Path(parquetPath, "V3"), TestInputOutputFormat.SchemaEvolutionMapper3.class, StructV3.class);    }    {        final Job job = new Job(conf, "read");        job.setInputFormatClass(ParquetThriftInputFormat.class);        ParquetThriftInputFormat.setInputPaths(job, new Path(parquetPath, "*"));        ParquetThriftInputFormat.setThriftClass(job.getConfiguration(), StructV3.class);        job.setMapperClass(TestInputOutputFormat.SchemaEvolutionReadMapper.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(TextOutputFormat.class);        TextOutputFormat.setOutputPath(job, outputPath);        waitForJob(job);    }    read(outputPath + "/part-m-00000", 3);    read(outputPath + "/part-m-00001", 3);    read(outputPath + "/part-m-00002", 3);}
private void parquet-mr_f7622_0(String outputPath, int expected) throws FileNotFoundException, IOException
{    final BufferedReader out = new BufferedReader(new FileReader(new File(outputPath.toString())));    String lineOut = null;    int lineNumber = 0;    while ((lineOut = out.readLine()) != null) {        lineOut = lineOut.substring(lineOut.indexOf("\t") + 1);        System.out.println(lineOut);        ++lineNumber;    }    out.close();    Assert.assertEquals(expected, lineNumber);}
private void parquet-mr_f7623_0(final Configuration conf, final Path inputPath, final Path parquetPath, Class<? extends Mapper> mapperClass, Class<? extends TBase<?, ?>> outputClass) throws IOException, Exception
{    final Job job = new Job(conf, "write");        TextInputFormat.addInputPath(job, inputPath);    job.setInputFormatClass(TextInputFormat.class);    job.setMapperClass(mapperClass);    job.setNumReduceTasks(0);    job.setOutputFormatClass(ParquetThriftOutputFormat.class);    ParquetThriftOutputFormat.setCompression(job, CompressionCodecName.GZIP);    ParquetThriftOutputFormat.setOutputPath(job, parquetPath);    ParquetThriftOutputFormat.setThriftClass(job, outputClass);    waitForJob(job);}
public static void parquet-mr_f7624_1(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {                sleep(100);    }        if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
public void parquet-mr_f7625_0() throws Exception
{        Configuration conf = new Configuration();    final String readProjectionSchema = "message AddressBook {\n" + "  optional group persons {\n" + "    repeated group persons_tuple {\n" + "      required group name {\n" + "        optional binary first_name;\n" + "        optional binary last_name;\n" + "      }\n" + "      optional int32 id;\n" + "    }\n" + "  }\n" + "}";    conf.set(ReadSupport.PARQUET_READ_SCHEMA, readProjectionSchema);    TBase toWrite = new AddressBook(Arrays.asList(new Person(new Name("Bob", "Roberts"), 0, "bob.roberts@example.com", Arrays.asList(new PhoneNumber("1234567890")))));    TBase toRead = new AddressBook(Arrays.asList(new Person(new Name("Bob", "Roberts"), 0, null, null)));    shouldDoProjection(conf, toWrite, toRead, AddressBook.class);}
public void parquet-mr_f7626_0() throws Exception
{    final String projectionFilterDesc = "persons/{id};persons/email";    TBase toWrite = new AddressBook(Arrays.asList(new Person(new Name("Bob", "Roberts"), 0, "bob.roberts@example.com", Arrays.asList(new PhoneNumber("1234567890")))));            TBase toRead = new AddressBook(Arrays.asList(new Person(new Name("", ""), 0, "bob.roberts@example.com", null)));    shouldDoProjectionWithThriftColumnFilter(projectionFilterDesc, toWrite, toRead, AddressBook.class);}
public void parquet-mr_f7627_0() throws Exception
{    final String projectionFilter = "**";    StructWithReorderedOptionalFields toWrite = new StructWithReorderedOptionalFields();    toWrite.setFieldOne(1);    toWrite.setFieldTwo(2);    toWrite.setFieldThree(3);    shouldDoProjectionWithThriftColumnFilter(projectionFilter, toWrite, toWrite, StructWithReorderedOptionalFields.class);}
public void parquet-mr_f7628_0() throws Exception
{    final String projectionFilterDesc = "persons/name/*";    TBase toWrite = new AddressBook(Arrays.asList(new Person(new Name("Bob", "Roberts"), 0, "bob.roberts@example.com", Arrays.asList(new PhoneNumber("1234567890")))));        TBase toRead = new AddressBook(Arrays.asList(new Person(new Name("Bob", "Roberts"), 0, null, null)));    shouldDoProjectionWithThriftColumnFilter(projectionFilterDesc, toWrite, toRead, AddressBook.class);}
public void parquet-mr_f7629_0() throws Exception
{    String filter = "name";    Map<String, String> mapValue = new HashMap<String, String>();    mapValue.put("a", "1");    mapValue.put("b", "2");    RequiredMapFixture toWrite = new RequiredMapFixture(mapValue);    toWrite.setName("testName");    RequiredMapFixture toRead = new RequiredMapFixture(new HashMap<String, String>());    toRead.setName("testName");    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, RequiredMapFixture.class);}
public void parquet-mr_f7630_0() throws Exception
{    String filter = "mavalue/key";    Map<String, String> mapValue = new HashMap<String, String>();    mapValue.put("a", "1");    mapValue.put("b", "2");    RequiredMapFixture toWrite = new RequiredMapFixture(mapValue);    toWrite.setName("testName");            Map<String, String> readValue = new HashMap<String, String>();    readValue.put("a", "1");    readValue.put("b", "2");    RequiredMapFixture toRead = new RequiredMapFixture(readValue);    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, RequiredMapFixture.class);}
private StructV4WithExtracStructField parquet-mr_f7631_0(String id)
{    StructV4WithExtracStructField sv4 = new StructV4WithExtracStructField();    StructV3 sv3 = new StructV3();    sv3.setAge("age " + id);    sv3.setGender("gender" + id);    sv3.setName("inner name " + id);    sv4.setAge("outer age " + id);    sv4.setAddedStruct(sv3);    sv4.setGender("outer gender " + id);    sv4.setName("outer name " + id);    return sv4;}
public void parquet-mr_f7632_0() throws Exception
{    String filter = "reqMap/key";    Map<String, StructV4WithExtracStructField> mapValue = new HashMap<String, StructV4WithExtracStructField>();    StructV4WithExtracStructField v1 = makeStructV4WithExtracStructField("1");    StructV4WithExtracStructField v2 = makeStructV4WithExtracStructField("2");    mapValue.put("key 1", v1);    mapValue.put("key 2", v2);    MapWithStructValue toWrite = new MapWithStructValue(mapValue);        HashMap<String, StructV4WithExtracStructField> readValue = new HashMap<String, StructV4WithExtracStructField>();    readValue.put("key 1", new StructV4WithExtracStructField("outer name 1"));    readValue.put("key 2", new StructV4WithExtracStructField("outer name 2"));    MapWithStructValue toRead = new MapWithStructValue(readValue);    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, MapWithStructValue.class);}
public void parquet-mr_f7633_0() throws Exception
{    String filter = "reqMap/key";    Map<String, Map<String, String>> mapValue = new HashMap<String, Map<String, String>>();    Map<String, String> innerValue1 = new HashMap<String, String>();    innerValue1.put("inner key (1, 1)", "inner (1, 1)");    innerValue1.put("inner key (1, 2)", "inner (1, 2)");    Map<String, String> innerValue2 = new HashMap<String, String>();    innerValue2.put("inner key (2, 1)", "inner (2, 1)");    innerValue2.put("inner key (2, 2)", "inner (2, 2)");    mapValue.put("outer key 1", innerValue1);    mapValue.put("outer key 2", innerValue2);    MapWithPrimMapValue toWrite = new MapWithPrimMapValue(mapValue);    Map<String, Map<String, String>> expected = new HashMap<String, Map<String, String>>();    Map<String, String> expectedInnerValue1 = new HashMap<String, String>();    expectedInnerValue1.put("inner key (1, 1)", "inner (1, 1)");    expectedInnerValue1.put("inner key (1, 2)", "inner (1, 2)");    Map<String, String> expectedInnerValue2 = new HashMap<String, String>();    expectedInnerValue2.put("inner key (2, 1)", "inner (2, 1)");    expectedInnerValue2.put("inner key (2, 2)", "inner (2, 2)");    expected.put("outer key 1", expectedInnerValue1);    expected.put("outer key 2", expectedInnerValue2);    MapWithPrimMapValue toRead = new MapWithPrimMapValue(expected);    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, MapWithPrimMapValue.class);}
public void parquet-mr_f7634_0() throws Exception
{    String filter = "reqMap/key";    Map<String, Map<String, StructV4WithExtracStructField>> mapValue = new HashMap<String, Map<String, StructV4WithExtracStructField>>();    Map<String, StructV4WithExtracStructField> innerValue1 = new HashMap<String, StructV4WithExtracStructField>();    innerValue1.put("inner key (1, 1)", makeStructV4WithExtracStructField("inner (1, 1)"));    innerValue1.put("inner key (1, 2)", makeStructV4WithExtracStructField("inner (1, 2)"));    Map<String, StructV4WithExtracStructField> innerValue2 = new HashMap<String, StructV4WithExtracStructField>();    innerValue2.put("inner key (2, 1)", makeStructV4WithExtracStructField("inner (2, 1)"));    innerValue2.put("inner key (2, 2)", makeStructV4WithExtracStructField("inner (2, 2)"));    mapValue.put("outer key 1", innerValue1);    mapValue.put("outer key 2", innerValue2);    MapWithStructMapValue toWrite = new MapWithStructMapValue(mapValue);    Map<String, Map<String, StructV4WithExtracStructField>> expected = new HashMap<String, Map<String, StructV4WithExtracStructField>>();    Map<String, StructV4WithExtracStructField> expectedInnerValue1 = new HashMap<String, StructV4WithExtracStructField>();    expectedInnerValue1.put("inner key (1, 1)", new StructV4WithExtracStructField("outer name inner (1, 1)"));    expectedInnerValue1.put("inner key (1, 2)", new StructV4WithExtracStructField("outer name inner (1, 2)"));    Map<String, StructV4WithExtracStructField> expectedInnerValue2 = new HashMap<String, StructV4WithExtracStructField>();    expectedInnerValue2.put("inner key (2, 1)", new StructV4WithExtracStructField("outer name inner (2, 1)"));    expectedInnerValue2.put("inner key (2, 2)", new StructV4WithExtracStructField("outer name inner (2, 2)"));    expected.put("outer key 1", expectedInnerValue1);    expected.put("outer key 2", expectedInnerValue2);    MapWithStructMapValue toRead = new MapWithStructMapValue(expected);    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, MapWithStructMapValue.class);}
public void parquet-mr_f7635_0() throws Exception
{    String filter = "info";    RequiredListFixture toWrite = new RequiredListFixture(Arrays.asList(new org.apache.parquet.thrift.test.Name("first_name")));    toWrite.setInfo("test_info");    RequiredListFixture toRead = new RequiredListFixture(new ArrayList<org.apache.parquet.thrift.test.Name>());    toRead.setInfo("test_info");    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, RequiredListFixture.class);}
public void parquet-mr_f7636_0() throws Exception
{    String filter = "info";    RequiredSetFixture toWrite = new RequiredSetFixture(new HashSet<org.apache.parquet.thrift.test.Name>(Arrays.asList(new org.apache.parquet.thrift.test.Name("first_name"))));    toWrite.setInfo("test_info");    RequiredSetFixture toRead = new RequiredSetFixture(new HashSet<org.apache.parquet.thrift.test.Name>());    toRead.setInfo("test_info");    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, RequiredSetFixture.class);}
public void parquet-mr_f7637_0() throws Exception
{    String filter = "info_string";    RequiredPrimitiveFixture toWrite = new RequiredPrimitiveFixture(true, (byte) 2, (short) 3, 4, (long) 5, (double) 6.0, "7");    toWrite.setInfo_string("it's info");    RequiredPrimitiveFixture toRead = new RequiredPrimitiveFixture(false, (byte) 0, (short) 0, 0, (long) 0, (double) 0.0, "");    toRead.setInfo_string("it's info");    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, RequiredPrimitiveFixture.class);}
private void parquet-mr_f7638_0(String filterDesc, TBase toWrite, TBase toRead, Class<? extends TBase<?, ?>> thriftClass) throws Exception
{    Configuration conf = new Configuration();    conf.set(ThriftReadSupport.THRIFT_COLUMN_FILTER_KEY, filterDesc);    shouldDoProjection(conf, toWrite, toRead, thriftClass);}
private void parquet-mr_f7639_1(Configuration conf, T recordToWrite, T exptectedReadResult, Class<? extends TBase<?, ?>> thriftClass) throws Exception
{    final Path parquetFile = new Path("target/test/TestParquetToThriftReadWriteAndProjection/file.parquet");    final FileSystem fs = parquetFile.getFileSystem(conf);    if (fs.exists(parquetFile)) {        fs.delete(parquetFile, true);    }        final TProtocolFactory protocolFactory = new TCompactProtocol.Factory();    final TaskAttemptID taskId = new TaskAttemptID("local", 0, true, 0, 0);    final ThriftToParquetFileWriter w = new ThriftToParquetFileWriter(parquetFile, ContextUtil.newTaskAttemptContext(conf, taskId), protocolFactory, thriftClass);    final ByteArrayOutputStream baos = new ByteArrayOutputStream();    final TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(baos));    recordToWrite.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    w.close();    final ParquetThriftInputFormat<T> parquetThriftInputFormat = new ParquetThriftInputFormat<T>();    final Job job = new Job(conf, "read");    job.setInputFormatClass(ParquetThriftInputFormat.class);    ParquetThriftInputFormat.setInputPaths(job, parquetFile);    final JobID jobID = new JobID("local", 1);    List<InputSplit> splits = parquetThriftInputFormat.getSplits(ContextUtil.newJobContext(ContextUtil.getConfiguration(job), jobID));    T readValue = null;    for (InputSplit split : splits) {        TaskAttemptContext taskAttemptContext = ContextUtil.newTaskAttemptContext(ContextUtil.getConfiguration(job), new TaskAttemptID(new TaskID(jobID, true, 1), 0));        final RecordReader<Void, T> reader = parquetThriftInputFormat.createRecordReader(split, taskAttemptContext);        reader.initialize(split, taskAttemptContext);        if (reader.nextKeyValue()) {            readValue = reader.getCurrentValue();                    }    }    assertEquals(exptectedReadResult, readValue);}
public void parquet-mr_f7640_0() throws IOException, InterruptedException, TException
{    final AddressBook a = new AddressBook(Arrays.asList(new Person(new Name("Bob", "Roberts"), 0, "bob.roberts@example.com", Arrays.asList(new PhoneNumber("1234567890")))));    final Path fileToCreate = createFile(a);    ParquetReader<Group> reader = createRecordReader(fileToCreate);    Group g = null;    int i = 0;    while ((g = reader.read()) != null) {        assertEquals(a.persons.size(), g.getFieldRepetitionCount("persons"));        assertEquals(a.persons.get(0).email, g.getGroup("persons", 0).getGroup(0, 0).getString("email", 0));                ++i;    }    assertEquals("read 1 record", 1, i);}
public void parquet-mr_f7641_0() throws Exception
{        IntStatistics intStatsSmall = new IntStatistics();    intStatsSmall.setMinMax(2, 100);    LongStatistics longStatsSmall = new LongStatistics();    longStatsSmall.setMinMax(-17l, 287L);    DoubleStatistics doubleStatsSmall = new DoubleStatistics();    doubleStatsSmall.setMinMax(-15.55d, 9.63d);    BinaryStatistics binaryStatsSmall = new BinaryStatistics();    binaryStatsSmall.setMinMax(Binary.fromString("as"), Binary.fromString("world"));    BooleanStatistics boolStats = new BooleanStatistics();    boolStats.setMinMax(false, true);        Path p = createFile(new RequiredPrimitiveFixture(false, (byte) 32, (short) 32, 2, 90l, -15.55d, "as"), new RequiredPrimitiveFixture(false, (byte) 100, (short) 100, 100, 287l, -9.0d, "world"), new RequiredPrimitiveFixture(true, (byte) 2, (short) 2, 9, -17l, 9.63d, "hello"));    final Configuration configuration = new Configuration();    configuration.setBoolean("parquet.strings.signed-min-max.enabled", true);    final FileSystem fs = p.getFileSystem(configuration);    FileStatus fileStatus = fs.getFileStatus(p);    ParquetMetadata footer = ParquetFileReader.readFooter(configuration, p);    for (BlockMetaData bmd : footer.getBlocks()) {        for (ColumnChunkMetaData cmd : bmd.getColumns()) {            switch(cmd.getType()) {                case INT32:                    TestUtils.assertStatsValuesEqual(intStatsSmall, cmd.getStatistics());                    break;                case INT64:                    TestUtils.assertStatsValuesEqual(longStatsSmall, cmd.getStatistics());                    break;                case DOUBLE:                    TestUtils.assertStatsValuesEqual(doubleStatsSmall, cmd.getStatistics());                    break;                case BOOLEAN:                    TestUtils.assertStatsValuesEqual(boolStats, cmd.getStatistics());                    break;                case BINARY:                                        if (cmd.getPath().toString() == "[test_string]")                        TestUtils.assertStatsValuesEqual(binaryStatsSmall, cmd.getStatistics());                    break;            }        }    }        IntStatistics intStatsLarge = new IntStatistics();    intStatsLarge.setMinMax(-Integer.MAX_VALUE, Integer.MAX_VALUE);    LongStatistics longStatsLarge = new LongStatistics();    longStatsLarge.setMinMax(-Long.MAX_VALUE, Long.MAX_VALUE);    DoubleStatistics doubleStatsLarge = new DoubleStatistics();    doubleStatsLarge.setMinMax(-Double.MAX_VALUE, Double.MAX_VALUE);    BinaryStatistics binaryStatsLarge = new BinaryStatistics();    binaryStatsLarge.setMinMax(Binary.fromString("some small string"), Binary.fromString("some very large string here to test in this function"));        Path p_large = createFile(new RequiredPrimitiveFixture(false, (byte) 2, (short) 32, -Integer.MAX_VALUE, -Long.MAX_VALUE, -Double.MAX_VALUE, "some small string"), new RequiredPrimitiveFixture(false, (byte) 100, (short) 100, Integer.MAX_VALUE, Long.MAX_VALUE, Double.MAX_VALUE, "some very large string here to test in this function"), new RequiredPrimitiveFixture(true, (byte) 2, (short) 2, 9, -17l, 9.63d, "hello"));        final Configuration configuration_large = new Configuration();    configuration.setBoolean("parquet.strings.signed-min-max.enabled", true);    final FileSystem fs_large = p_large.getFileSystem(configuration_large);    FileStatus fileStatus_large = fs_large.getFileStatus(p_large);    ParquetMetadata footer_large = ParquetFileReader.readFooter(configuration_large, p_large);    for (BlockMetaData bmd : footer_large.getBlocks()) {        for (ColumnChunkMetaData cmd : bmd.getColumns()) {            switch(cmd.getType()) {                case INT32:                                        if (cmd.getPath().toString() == "[test_i32]")                        TestUtils.assertStatsValuesEqual(intStatsLarge, cmd.getStatistics());                    break;                case INT64:                    TestUtils.assertStatsValuesEqual(longStatsLarge, cmd.getStatistics());                    break;                case DOUBLE:                    TestUtils.assertStatsValuesEqual(doubleStatsLarge, cmd.getStatistics());                    break;                case BOOLEAN:                    TestUtils.assertStatsValuesEqual(boolStats, cmd.getStatistics());                    break;                case BINARY:                                        if (cmd.getPath().toString() == "[test_string]")                        TestUtils.assertStatsValuesEqual(binaryStatsLarge, cmd.getStatistics());                    break;            }        }    }}
public void parquet-mr_f7642_0() throws IOException, InterruptedException, TException
{    Map<String, String> map1 = new HashMap<String, String>();    map1.put("key11", "value11");    map1.put("key12", "value12");    Map<String, String> map2 = new HashMap<String, String>();    map2.put("key21", "value21");    final TestMapInList listMap = new TestMapInList("listmap", Arrays.asList(map1, map2));    final Path fileToCreate = createFile(listMap);    ParquetReader<Group> reader = createRecordReader(fileToCreate);    Group g = null;    while ((g = reader.read()) != null) {        assertEquals(listMap.names.size(), g.getGroup("names", 0).getFieldRepetitionCount("names_tuple"));        assertEquals(listMap.names.get(0).size(), g.getGroup("names", 0).getGroup("names_tuple", 0).getFieldRepetitionCount("map"));        assertEquals(listMap.names.get(1).size(), g.getGroup("names", 0).getGroup("names_tuple", 1).getFieldRepetitionCount("map"));    }}
public void parquet-mr_f7643_0() throws IOException, InterruptedException, TException
{    Map<String, List<String>> map = new HashMap<String, List<String>>();    map.put("key", Arrays.asList("val1", "val2"));    final TestListInMap mapList = new TestListInMap("maplist", map);    final Path fileToCreate = createFile(mapList);    ParquetReader<Group> reader = createRecordReader(fileToCreate);    Group g = null;    while ((g = reader.read()) != null) {        assertEquals("key", g.getGroup("names", 0).getGroup("map", 0).getBinary("key", 0).toStringUsingUTF8());        assertEquals(map.get("key").size(), g.getGroup("names", 0).getGroup("map", 0).getGroup("value", 0).getFieldRepetitionCount(0));    }}
public void parquet-mr_f7644_0() throws IOException, InterruptedException, TException
{    Map<List<String>, List<String>> map = new HashMap<List<String>, List<String>>();    map.put(Arrays.asList("key1", "key2"), Arrays.asList("val1", "val2"));    final TestListsInMap mapList = new TestListsInMap("maplists", map);    final Path fileToCreate = createFile(mapList);    ParquetReader<Group> reader = createRecordReader(fileToCreate);    Group g = null;    while ((g = reader.read()) != null) {        assertEquals("key1", g.getGroup("names", 0).getGroup("map", 0).getGroup("key", 0).getBinary("key_tuple", 0).toStringUsingUTF8());        assertEquals("key2", g.getGroup("names", 0).getGroup("map", 0).getGroup("key", 0).getBinary("key_tuple", 1).toStringUsingUTF8());        assertEquals("val1", g.getGroup("names", 0).getGroup("map", 0).getGroup("value", 0).getBinary("value_tuple", 0).toStringUsingUTF8());        assertEquals("val2", g.getGroup("names", 0).getGroup("map", 0).getGroup("value", 0).getBinary("value_tuple", 1).toStringUsingUTF8());    }}
private ParquetReader<Group> parquet-mr_f7645_0(Path parquetFilePath) throws IOException
{    Configuration configuration = new Configuration(true);    GroupReadSupport readSupport = new GroupReadSupport();    ParquetMetadata readFooter = ParquetFileReader.readFooter(configuration, parquetFilePath);    MessageType schema = readFooter.getFileMetaData().getSchema();    readSupport.init(configuration, null, schema);    return new ParquetReader<Group>(parquetFilePath, readSupport);}
private Path parquet-mr_f7646_1(T... tObjs) throws IOException, InterruptedException, TException
{    final Path fileToCreate = new Path("target/test/TestThriftToParquetFileWriter/" + tObjs[0].getClass() + ".parquet");        Configuration conf = new Configuration();    final FileSystem fs = fileToCreate.getFileSystem(conf);    if (fs.exists(fileToCreate)) {        fs.delete(fileToCreate, true);    }    TProtocolFactory protocolFactory = new TCompactProtocol.Factory();    TaskAttemptID taskId = new TaskAttemptID("local", 0, true, 0, 0);    ThriftToParquetFileWriter w = new ThriftToParquetFileWriter(fileToCreate, ContextUtil.newTaskAttemptContext(conf, taskId), protocolFactory, (Class<? extends TBase<?, ?>>) tObjs[0].getClass());    for (T tObj : tObjs) {        final ByteArrayOutputStream baos = new ByteArrayOutputStream();        final TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(baos));        tObj.write(protocol);        w.write(new BytesWritable(baos.toByteArray()));    }    w.close();    return fileToCreate;}
public void parquet-mr_f7647_0() throws ExecException, Exception
{    String out = "target/out";    int rows = 1000;    Properties props = new Properties();    props.setProperty("parquet.compression", "uncompressed");    props.setProperty("parquet.page.size", "1000");    PigServer pigServer = new PigServer(ExecType.LOCAL, props);    Data data = Storage.resetData(pigServer);    Collection<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(tuple("bob", "roberts" + i));    }    data.set("in", "fn:chararray, ln:chararray", list);    pigServer.deleteFile(out);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetThriftStorer.class.getName() + "('" + Name.class.getName() + "');");    execBatch(pigServer);    pigServer.registerQuery("B = LOAD '" + out + "' USING " + ParquetLoader.class.getName() + "();");    pigServer.registerQuery("Store B into 'out' using mock.Storage();");    execBatch(pigServer);    List<Tuple> result = data.get("out");    assertEquals(rows, result.size());    int i = 0;    for (Tuple tuple : result) {        assertEquals(tuple("bob", "roberts" + i), tuple);        ++i;    }}
private void parquet-mr_f7648_0(PigServer pigServer) throws IOException
{    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }}
public void parquet-mr_f7649_0()
{    PathGlobPattern g = new PathGlobPattern("a/**/b");    assertFalse(g.matches("a/b"));    assertTrue(g.matches("a/asd/b"));    assertTrue(g.matches("a/asd/ss/b"));    g = new PathGlobPattern("a/**");    assertTrue(g.matches("a/as"));    assertTrue(g.matches("a/asd/b"));    assertTrue(g.matches("a/asd/ss/b"));}
public void parquet-mr_f7650_0()
{    PathGlobPattern g = new PathGlobPattern("a/*");    assertTrue(g.matches("a/as"));    assertFalse(g.matches("a/asd/b"));    assertFalse(g.matches("a/asd/ss/b"));    g = new PathGlobPattern("a/{bb,cc}/d");    assertTrue(g.matches("a/bb/d"));    assertTrue(g.matches("a/cc/d"));    assertFalse(g.matches("a/cc/bb/d"));    assertFalse(g.matches("a/d"));}
public void parquet-mr_f7651_0()
{    StructType person = ThriftSchemaConverter.toStructType(Person.class);    List<String> paths = PrimitivePathVisitor.visit(person, ".");    assertEquals(Arrays.asList("name.first_name", "name.last_name", "id", "email", "phones.number", "phones.type"), paths);    paths = PrimitivePathVisitor.visit(person, "/");    assertEquals(Arrays.asList("name/first_name", "name/last_name", "id", "email", "phones/number", "phones/type"), paths);    StructType structInMap = ThriftSchemaConverter.toStructType(TestStructInMap.class);    paths = PrimitivePathVisitor.visit(structInMap, ".");    assertEquals(Arrays.asList("name", "names.key", "names.value.name.first_name", "names.value.name.last_name", "names.value.phones.key", "names.value.phones.value", "name_to_id.key", "name_to_id.value"), paths);    paths = PrimitivePathVisitor.visit(structInMap, "/");    assertEquals(Arrays.asList("name", "names/key", "names/value/name/first_name", "names/value/name/last_name", "names/value/phones/key", "names/value/phones/value", "name_to_id/key", "name_to_id/value"), paths);}
public static List<String> parquet-mr_f7652_0(StructType s, String delim)
{    PrimitivePathVisitor v = new PrimitivePathVisitor(delim);    return s.accept(v, new FieldsPath());}
public List<String> parquet-mr_f7653_0(MapType mapType, FieldsPath path)
{    List<String> ret = new ArrayList<String>();    ThriftField key = mapType.getKey();    ThriftField value = mapType.getValue();    ret.addAll(key.getType().accept(this, path.push(key)));    ret.addAll(value.getType().accept(this, path.push(value)));    return ret;}
public List<String> parquet-mr_f7654_0(SetType setType, FieldsPath path)
{    return setType.getValues().getType().accept(this, path);}
public List<String> parquet-mr_f7655_0(ListType listType, FieldsPath path)
{    return listType.getValues().getType().accept(this, path);}
public List<String> parquet-mr_f7656_0(StructType structType, FieldsPath path)
{    List<String> ret = new ArrayList<String>();    for (ThriftField child : structType.getChildren()) {        ret.addAll(child.getType().accept(this, path.push(child)));    }    return ret;}
private List<String> parquet-mr_f7657_0(FieldsPath path)
{    return Arrays.asList(path.toDelimitedString(delim));}
public List<String> parquet-mr_f7658_0(EnumType enumType, FieldsPath path)
{    return visitPrimitive(path);}
public List<String> parquet-mr_f7659_0(BoolType boolType, FieldsPath path)
{    return visitPrimitive(path);}
public List<String> parquet-mr_f7660_0(ByteType byteType, FieldsPath path)
{    return visitPrimitive(path);}
public List<String> parquet-mr_f7661_0(DoubleType doubleType, FieldsPath path)
{    return visitPrimitive(path);}
public List<String> parquet-mr_f7662_0(I16Type i16Type, FieldsPath path)
{    return visitPrimitive(path);}
public List<String> parquet-mr_f7663_0(I32Type i32Type, FieldsPath path)
{    return visitPrimitive(path);}
public List<String> parquet-mr_f7664_0(I64Type i64Type, FieldsPath path)
{    return visitPrimitive(path);}
public List<String> parquet-mr_f7665_0(StringType stringType, FieldsPath path)
{    return visitPrimitive(path);}
public void parquet-mr_f7666_0()
{    List<String> globs = StrictFieldProjectionFilter.parseSemicolonDelimitedString(";x.y.z;*.a.b.c*;;foo;;;;bar;");    assertEquals(Arrays.asList("x.y.z", "*.a.b.c*", "foo", "bar"), globs);    try {        StrictFieldProjectionFilter.parseSemicolonDelimitedString(";;");        fail("this should throw");    } catch (ThriftProjectionException e) {        assertEquals("Semicolon delimited string ';;' contains 0 glob strings", e.getMessage());    }}
private static void parquet-mr_f7667_0(StrictFieldProjectionFilter filter, String... strings)
{    for (String s : strings) {        if (!filter.keep(s)) {            fail(String.format("String '%s' was expected to match", s));        }    }}
private static void parquet-mr_f7668_0(StrictFieldProjectionFilter filter, String... strings)
{    for (String s : strings) {        if (filter.keep(s)) {            fail(String.format("String '%s' was not expected to match", s));        }    }}
public void parquet-mr_f7669_0()
{    StrictFieldProjectionFilter filter = StrictFieldProjectionFilter.fromSemicolonDelimitedString("home.phone_number;home.address;work.address.zip;base_info;*.average;a.b.c.pre{x,y,z{a,b,c}}post");    assertMatches(filter, "home.phone_number", "home.address", "work.address.zip", "base_info", "foo.average", "bar.x.y.z.average", "base_info.nested.field", "a.b.c.prexpost", "a.b.c.prezapost");    assertDoesNotMatch(filter, "home2.phone_number", "home2.address", "work.address", "base_info2", "foo_average", "bar.x.y.z_average", "base_info_nested.field", "hi", "average", "a.b.c.pre{x,y,z{a,b,c}}post", "");}
public void parquet-mr_f7670_0()
{    StrictFieldProjectionFilter filter = StrictFieldProjectionFilter.fromSemicolonDelimitedString("home.phone_number;a.b.c.pre{x,y,z{a,b,c}}post;bar.*.average");    assertMatches(filter, "home.phone_number", "bar.foo.average", "a.b.c.prexpost", "a.b.c.prezcpost");    assertDoesNotMatch(filter, "hello");    try {        filter.assertNoUnmatchedPatterns();        fail("this should throw");    } catch (ThriftProjectionException e) {        String expectedMessage = "The following projection patterns did not match any columns in this schema:\n" + "Pattern: 'a.b.c.pre{x,y,z{a,b,c}}post' (when expanded to 'a.b.c.preypost')\n" + "Pattern: 'a.b.c.pre{x,y,z{a,b,c}}post' (when expanded to 'a.b.c.prezapost')\n" + "Pattern: 'a.b.c.pre{x,y,z{a,b,c}}post' (when expanded to 'a.b.c.prezbpost')\n";        assertEquals(expectedMessage, e.getMessage());    }}
public void parquet-mr_f7671_0()
{    StrictFieldProjectionFilter filter = createMockBuilder(StrictFieldProjectionFilter.class).withConstructor(Arrays.asList("a.b.c.{x_average,z_average}", "a.*_average")).addMockedMethod("warn").createMock();        filter.warn("Field path: 'a.b.c.x_average' matched more than one glob path pattern. " + "First match: 'a.b.c.{x_average,z_average}' (when expanded to 'a.b.c.x_average') " + "second match:'a.*_average' (when expanded to 'a.*_average')");    filter.warn("Field path: 'a.b.c.z_average' matched more than one glob path pattern. " + "First match: 'a.b.c.{x_average,z_average}' (when expanded to 'a.b.c.z_average') " + "second match:'a.*_average' (when expanded to 'a.*_average')");    replay(filter);    assertMatches(filter, "a.b.c.x_average", "a.b.c.z_average", "a.other.w_average");    assertDoesNotMatch(filter, "hello");    verify(filter);}
public void parquet-mr_f7672_0()
{    verifyCompatible(StructV1.class, StructV2.class, true);}
public void parquet-mr_f7673_0()
{    verifyCompatible(StructV2.class, StructV1.class, false);}
public void parquet-mr_f7674_0()
{    verifyCompatible(StructV1.class, RenameStructV1.class, false);}
public void parquet-mr_f7675_0()
{    verifyCompatible(StructV1.class, TypeChangeStructV1.class, false);}
public void parquet-mr_f7676_0()
{        verifyCompatible(StructV1.class, OptionalStructV1.class, true);    verifyCompatible(StructV1.class, DefaultStructV1.class, true);        verifyCompatible(OptionalStructV1.class, StructV1.class, false);    verifyCompatible(DefaultStructV1.class, StructV1.class, false);}
public void parquet-mr_f7677_0()
{    verifyCompatible(StructV1.class, AddRequiredStructV1.class, false);}
public void parquet-mr_f7678_0()
{        verifyCompatible(MapStructV1.class, MapStructV2.class, true);    verifyCompatible(MapValueStructV1.class, MapValueStructV2.class, true);        verifyCompatible(MapStructV2.class, MapStructV1.class, false);    verifyCompatible(MapValueStructV2.class, MapValueStructV1.class, false);        verifyCompatible(MapStructV2.class, MapAddRequiredStructV1.class, false);}
public void parquet-mr_f7679_0()
{    verifyCompatible(SetStructV2.class, SetStructV1.class, false);    verifyCompatible(SetStructV1.class, SetStructV2.class, true);}
public void parquet-mr_f7680_0()
{    verifyCompatible(ListStructV2.class, ListStructV1.class, false);    verifyCompatible(ListStructV1.class, ListStructV2.class, true);}
public void parquet-mr_f7681_0()
{    CompatibilityReport report = getCompatibilityReport(NestedEmptyStruct.class, NestedEmptyStruct.class);    assertEquals("encountered an empty struct: required_empty\nencountered an empty struct: optional_empty", report.prettyMessages());    assertTrue(report.hasEmptyStruct());}
private ThriftType.StructType parquet-mr_f7682_0(Class thriftClass)
{    return ThriftSchemaConverter.toStructType(thriftClass);}
private CompatibilityReport parquet-mr_f7683_0(Class oldClass, Class newClass)
{    CompatibilityChecker checker = new CompatibilityChecker();    CompatibilityReport report = checker.checkCompatibility(struct(oldClass), struct(newClass));    return report;}
private void parquet-mr_f7684_0(Class oldClass, Class newClass, boolean expectCompatible)
{    CompatibilityReport report = getCompatibilityReport(oldClass, newClass);    assertEquals(expectCompatible, report.isCompatible());}
public void parquet-mr_f7685_0() throws Exception
{    StructType st = new StructType(new LinkedList<ThriftField>(), null);    assertEquals("{\n" + "  \"id\" : \"STRUCT\",\n" + "  \"children\" : [ ],\n" + "  \"structOrUnionType\" : \"STRUCT\"\n" + "}", st.toJSON());    st = new StructType(new LinkedList<ThriftField>(), StructOrUnionType.UNION);    assertEquals("{\n" + "  \"id\" : \"STRUCT\",\n" + "  \"children\" : [ ],\n" + "  \"structOrUnionType\" : \"UNION\"\n" + "}", st.toJSON());    st = new StructType(new LinkedList<ThriftField>(), StructOrUnionType.STRUCT);    assertEquals("{\n" + "  \"id\" : \"STRUCT\",\n" + "  \"children\" : [ ],\n" + "  \"structOrUnionType\" : \"STRUCT\"\n" + "}", st.toJSON());}
public void parquet-mr_f7686_0() throws Exception
{    StructType st = (StructType) StructType.fromJSON("{\"id\": \"STRUCT\", \"children\":[], \"structOrUnionType\": \"UNION\"}");    assertEquals(st.getStructOrUnionType(), StructOrUnionType.UNION);    st = (StructType) StructType.fromJSON("{\"id\": \"STRUCT\", \"children\":[], \"structOrUnionType\": \"STRUCT\"}");    assertEquals(st.getStructOrUnionType(), StructOrUnionType.STRUCT);    st = (StructType) StructType.fromJSON("{\"id\": \"STRUCT\", \"children\":[]}");    assertEquals(st.getStructOrUnionType(), StructOrUnionType.STRUCT);    st = (StructType) StructType.fromJSON("{\"id\": \"STRUCT\", \"children\":[], \"structOrUnionType\": \"UNKNOWN\"}");    assertEquals(st.getStructOrUnionType(), StructOrUnionType.UNKNOWN);}
public void parquet-mr_f7687_0() throws TException
{    final List<String> names = new ArrayList<String>();    names.add("John");    names.add("Jack");    final TestNameList o = new TestNameList("name", names);    validate(o);}
public void parquet-mr_f7688_0() throws TException
{    final Set<String> names = new HashSet<String>();    names.add("John");    names.add("Jack");    final TestNameSet o = new TestNameSet("name", names);    validate(o);}
public void parquet-mr_f7689_0() throws Exception
{    AddressBook expected = new AddressBook();    validate(expected);}
public void parquet-mr_f7690_0() throws TException
{    final List<Byte> bytes = new ArrayList<Byte>();    bytes.add((byte) 1);    final List<Short> shorts = new ArrayList<Short>();    shorts.add((short) 1);    final List<Long> longs = new ArrayList<Long>();    longs.add((long) 1);    OneOfEach a = new OneOfEach(true, false, (byte) 8, (short) 16, (int) 32, (long) 64, (double) 1234, "string", "å", false, ByteBuffer.wrap("a".getBytes()), bytes, shorts, longs);    validate(a);}
public void parquet-mr_f7691_0() throws Exception
{    final PhoneNumber phoneNumber = new PhoneNumber("5555555555");    phoneNumber.type = MOBILE;    List<Person> persons = Arrays.asList(new Person(new Name("john", "johson"), 1, "john@johnson.org", Arrays.asList(phoneNumber)), new Person(new Name("jack", "jackson"), 2, "jack@jackson.org", Arrays.asList(new PhoneNumber("5555555556"))));    AddressBook expected = new AddressBook(persons);    validate(expected);}
public void parquet-mr_f7692_0() throws Exception
{    final Map<String, String> map = new HashMap<String, String>();    map.put("foo", "bar");    TestMap testMap = new TestMap("map_name", map);    validate(testMap);}
public void parquet-mr_f7693_0() throws Exception
{    final Map<String, TestPerson> map = new HashMap<String, TestPerson>();    map.put("foo", new TestPerson(new TestName("john", "johnson"), new HashMap<TestPhoneType, String>()));    final Map<String, Integer> stringToIntMap = Collections.singletonMap("bar", 10);    TestStructInMap testMap = new TestStructInMap("map_name", map, stringToIntMap);    validate(testMap);}
private void parquet-mr_f7694_1(T expected) throws TException
{    @SuppressWarnings("unchecked")    final Class<T> thriftClass = (Class<T>) expected.getClass();    final MemPageStore memPageStore = new MemPageStore(1);    final ThriftSchemaConverter schemaConverter = new ThriftSchemaConverter();    final MessageType schema = schemaConverter.convert(thriftClass);        final MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    final ColumnWriteStoreV1 columns = new ColumnWriteStoreV1(memPageStore, ParquetProperties.builder().withPageSize(10000).withDictionaryEncoding(false).build());    final RecordConsumer recordWriter = columnIO.getRecordWriter(columns);    final StructType thriftType = schemaConverter.toStructType(thriftClass);    ParquetWriteProtocol parquetWriteProtocol = new ParquetWriteProtocol(recordWriter, columnIO, thriftType);    expected.write(parquetWriteProtocol);    recordWriter.flush();    columns.flush();    ThriftRecordConverter<T> converter = new TBaseRecordConverter<T>(thriftClass, schema, thriftType);    final RecordReader<T> recordReader = columnIO.getRecordReader(memPageStore, converter);    final T result = recordReader.read();    assertEquals(expected, result);}
public void parquet-mr_f7695_0() throws Exception
{    String[] expectations = { "startMessage()", "startField(name, 0)", "addBinary(map_name)", "endField(name, 0)", "startField(names, 1)", "startGroup()", "startField(map, 0)", "startGroup()", "startField(key, 0)", "addBinary(foo)", "endField(key, 0)", "startField(value, 1)", "addBinary(bar)", "endField(value, 1)", "endGroup()", "startGroup()", "startField(key, 0)", "addBinary(foo2)", "endField(key, 0)", "startField(value, 1)", "addBinary(bar2)", "endField(value, 1)", "endGroup()", "endField(map, 0)", "endGroup()", "endField(names, 1)", "endMessage()" };    String[] expectationsAlt = { "startMessage()", "startField(name, 0)", "addBinary(map_name)", "endField(name, 0)", "startField(names, 1)", "startGroup()", "startField(map, 0)", "startGroup()", "startField(key, 0)", "addBinary(foo2)", "endField(key, 0)", "startField(value, 1)", "addBinary(bar2)", "endField(value, 1)", "endGroup()", "startGroup()", "startField(key, 0)", "addBinary(foo)", "endField(key, 0)", "startField(value, 1)", "addBinary(bar)", "endField(value, 1)", "endGroup()", "endField(map, 0)", "endGroup()", "endField(names, 1)", "endMessage()" };    final Map<String, String> map = new TreeMap<String, String>();    map.put("foo", "bar");    map.put("foo2", "bar2");    TestMap testMap = new TestMap("map_name", map);    try {        validatePig(expectations, testMap);    } catch (ComparisonFailure e) {                                validatePig(expectationsAlt, testMap);    }    validateThrift(expectations, testMap);}
public void parquet-mr_f7696_0() throws Exception
{    String[] pigExpectations = { "startMessage()", "startField(name, 0)", "addBinary(top)", "endField(name, 0)",     "startField(names, 1)", "startGroup()",     "startField(t, 0)", "startGroup()",     "startField(names_tuple, 0)", "startGroup()",     "startField(map, 0)", "startGroup()",     "startField(key, 0)", "addBinary(foo)", "endField(key, 0)",     "startField(value, 1)", "addBinary(bar)", "endField(value, 1)", "endGroup()", "endField(map, 0)", "endGroup()", "endField(names_tuple, 0)", "endGroup()", "endField(t, 0)", "endGroup()", "endField(names, 1)", "endMessage()" };    final Set<Map<String, String>> set = new HashSet<Map<String, String>>();    final Map<String, String> map = new HashMap<String, String>();    map.put("foo", "bar");    set.add(map);    TestMapInSet o = new TestMapInSet("top", set);    validatePig(pigExpectations, o);    String[] expectationsThrift = { "startMessage()", "startField(name, 0)", "addBinary(top)", "endField(name, 0)",     "startField(names, 1)", "startGroup()",     "startField(names_tuple, 0)", "startGroup()",     "startField(map, 0)", "startGroup()",     "startField(key, 0)", "addBinary(foo)", "endField(key, 0)",     "startField(value, 1)", "addBinary(bar)", "endField(value, 1)", "endGroup()", "endField(map, 0)", "endGroup()", "endField(names_tuple, 0)", "endGroup()", "endField(names, 1)", "endMessage()" };    validateThrift(expectationsThrift, o);}
public void parquet-mr_f7697_0() throws TException
{    final List<String> names = new ArrayList<String>();    names.add("John");    names.add("Jack");    final TestNameList o = new TestNameList("name", names);    String[] pigExpectations = { "startMessage()", "startField(name, 0)", "addBinary(name)", "endField(name, 0)", "startField(names, 1)", "startGroup()", "startField(t, 0)", "startGroup()", "startField(names_tuple, 0)", "addBinary(John)", "endField(names_tuple, 0)", "endGroup()", "startGroup()", "startField(names_tuple, 0)", "addBinary(Jack)", "endField(names_tuple, 0)", "endGroup()", "endField(t, 0)", "endGroup()", "endField(names, 1)", "endMessage()" };    validatePig(pigExpectations, o);    String[] expectations = { "startMessage()", "startField(name, 0)", "addBinary(name)", "endField(name, 0)", "startField(names, 1)", "startGroup()", "startField(names_tuple, 0)", "addBinary(John)", "addBinary(Jack)", "endField(names_tuple, 0)", "endGroup()", "endField(names, 1)", "endMessage()" };    validateThrift(expectations, o);}
public void parquet-mr_f7698_0() throws Exception
{    String[] expectations = { "startMessage()", "startField(name, 0)", "addBinary(map_name)", "endField(name, 0)", "startField(names, 1)", "startGroup()", "startField(map, 0)", "startGroup()", "startField(key, 0)", "addBinary(foo)", "endField(key, 0)", "startField(value, 1)", "startGroup()", "startField(name, 0)", "startGroup()", "startField(first_name, 0)", "addBinary(john)", "endField(first_name, 0)", "startField(last_name, 1)", "addBinary(johnson)", "endField(last_name, 1)", "endGroup()", "endField(name, 0)", "startField(phones, 1)", "startGroup()", "endGroup()", "endField(phones, 1)", "endGroup()", "endField(value, 1)", "endGroup()", "endField(map, 0)", "endGroup()", "endField(names, 1)", "startField(name_to_id, 2)", "startGroup()", "startField(map, 0)", "startGroup()", "startField(key, 0)", "addBinary(bar)", "endField(key, 0)", "startField(value, 1)", "addInt(10)", "endField(value, 1)", "endGroup()", "endField(map, 0)", "endGroup()", "endField(name_to_id, 2)", "endMessage()" };    final Map<String, TestPerson> map = new HashMap<String, TestPerson>();    map.put("foo", new TestPerson(new TestName("john", "johnson"), new HashMap<TestPhoneType, String>()));    final Map<String, Integer> stringToIntMap = Collections.singletonMap("bar", 10);    TestStructInMap testMap = new TestStructInMap("map_name", map, stringToIntMap);    validatePig(expectations, testMap);    validateThrift(expectations, testMap);}
public void parquet-mr_f7699_0() throws Exception
{    String[] expectations = { "startMessage()", "startField(persons, 0)", "startGroup()", "endGroup()", "endField(persons, 0)", "endMessage()" };    AddressBook a = new AddressBook(new ArrayList<Person>());    validatePig(expectations, a);    validateThrift(expectations, a);}
public void parquet-mr_f7700_0() throws Exception
{    String[] expectations = {     "startMessage()",     "startField(persons, 0)", "startGroup()",     "startField(t, 0)", "startGroup()", "startField(name, 0)",     "startGroup()", "startField(first_name, 0)", "addBinary(Bob)", "endField(first_name, 0)", "startField(last_name, 1)", "addBinary(Roberts)", "endField(last_name, 1)", "endGroup()", "endField(name, 0)", "startField(id, 1)", "addInt(1)", "endField(id, 1)", "startField(email, 2)", "addBinary(bob@roberts.com)", "endField(email, 2)", "startField(phones, 3)", "startGroup()", "startField(t, 0)", "startGroup()", "startField(number, 0)", "addBinary(555 999 9999)", "endField(number, 0)", "endGroup()", "startGroup()", "startField(number, 0)", "addBinary(555 999 9998)", "endField(number, 0)", "startField(type, 1)", "addBinary(HOME)", "endField(type, 1)", "endGroup()", "endField(t, 0)", "endGroup()", "endField(phones, 3)", "endGroup()", "startGroup()", "startField(name, 0)", "startGroup()", "startField(first_name, 0)", "addBinary(Dick)", "endField(first_name, 0)", "startField(last_name, 1)", "addBinary(Richardson)", "endField(last_name, 1)", "endGroup()", "endField(name, 0)", "startField(id, 1)", "addInt(2)", "endField(id, 1)", "startField(email, 2)", "addBinary(dick@richardson.com)", "endField(email, 2)", "startField(phones, 3)", "startGroup()", "startField(t, 0)", "startGroup()", "startField(number, 0)", "addBinary(555 999 9997)", "endField(number, 0)", "endGroup()", "startGroup()", "startField(number, 0)", "addBinary(555 999 9996)", "endField(number, 0)", "endGroup()", "endField(t, 0)", "endGroup()", "endField(phones, 3)", "endGroup()", "endField(t, 0)", "endGroup()", "endField(persons, 0)", "endMessage()" };    ArrayList<Person> persons = new ArrayList<Person>();    final PhoneNumber phoneNumber = new PhoneNumber("555 999 9998");    phoneNumber.type = PhoneType.HOME;    persons.add(new Person(new Name("Bob", "Roberts"), 1, "bob@roberts.com", Arrays.asList(new PhoneNumber("555 999 9999"), phoneNumber)));    persons.add(new Person(new Name("Dick", "Richardson"), 2, "dick@richardson.com", Arrays.asList(new PhoneNumber("555 999 9997"), new PhoneNumber("555 999 9996"))));    AddressBook a = new AddressBook(persons);    validatePig(expectations, a);        String[] expectationsThrift = Arrays.copyOf(expectations, expectations.length, String[].class);    expectationsThrift[3] = "startField(persons_tuple, 0)";    expectationsThrift[23] = "startField(phones_tuple, 0)";    expectationsThrift[37] = "endField(phones_tuple, 0)";    expectationsThrift[60] = "startField(phones_tuple, 0)";    expectationsThrift[71] = "endField(phones_tuple, 0)";    expectationsThrift[75] = "endField(persons_tuple, 0)";    validateThrift(expectationsThrift, a);}
public void parquet-mr_f7701_0() throws TException
{    String[] expectations = { "startMessage()", "startField(im_true, 0)", "addInt(1)", "endField(im_true, 0)", "startField(im_false, 1)", "addInt(0)", "endField(im_false, 1)", "startField(a_bite, 2)", "addInt(8)", "endField(a_bite, 2)", "startField(integer16, 3)", "addInt(16)", "endField(integer16, 3)", "startField(integer32, 4)", "addInt(32)", "endField(integer32, 4)", "startField(integer64, 5)", "addLong(64)", "endField(integer64, 5)", "startField(double_precision, 6)", "addDouble(1234.0)", "endField(double_precision, 6)", "startField(some_characters, 7)", "addBinary(string)", "endField(some_characters, 7)", "startField(zomg_unicode, 8)", "addBinary(å)", "endField(zomg_unicode, 8)", "startField(what_who, 9)", "addInt(0)", "endField(what_who, 9)", "startField(base64, 10)", "addBinary(a)", "endField(base64, 10)", "startField(byte_list, 11)", "startGroup()", "endGroup()", "endField(byte_list, 11)", "startField(i16_list, 12)", "startGroup()", "endGroup()", "endField(i16_list, 12)", "startField(i64_list, 13)", "startGroup()", "endGroup()", "endField(i64_list, 13)", "endMessage()" };    OneOfEach a = new OneOfEach(true, false, (byte) 8, (short) 16, (int) 32, (long) 64, (double) 1234, "string", "å", false, ByteBuffer.wrap("a".getBytes()), new ArrayList<Byte>(), new ArrayList<Short>(), new ArrayList<Long>());    validatePig(expectations, a);    String[] thriftExpectations = Arrays.copyOf(expectations, expectations.length, String[].class);        thriftExpectations[2] = "addBoolean(true)";    thriftExpectations[5] = "addBoolean(false)";    thriftExpectations[29] = "addBoolean(false)";    validateThrift(thriftExpectations, a);}
private void parquet-mr_f7702_1(String[] expectations, TBase<?, ?> a) throws TException
{    final ThriftSchemaConverter thriftSchemaConverter = new ThriftSchemaConverter();        final Class<TBase<?, ?>> class1 = (Class<TBase<?, ?>>) a.getClass();    final MessageType schema = thriftSchemaConverter.convert(class1);        final StructType structType = thriftSchemaConverter.toStructType(class1);    ExpectationValidatingRecordConsumer recordConsumer = new ExpectationValidatingRecordConsumer(new ArrayDeque<String>(Arrays.asList(expectations)));    final MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);    ParquetWriteProtocol p = new ParquetWriteProtocol(new RecordConsumerLoggingWrapper(recordConsumer), columnIO, structType);    a.write(p);}
private MessageType parquet-mr_f7703_1(String[] expectations, TBase<?, ?> a)
{    ThriftToPig<TBase<?, ?>> thriftToPig = new ThriftToPig(a.getClass());    ExpectationValidatingRecordConsumer recordConsumer = new ExpectationValidatingRecordConsumer(new ArrayDeque<String>(Arrays.asList(expectations)));    Schema pigSchema = thriftToPig.toSchema();        MessageType schema = new PigSchemaConverter().convert(pigSchema);        TupleWriteSupport tupleWriteSupport = new TupleWriteSupport(pigSchema);    tupleWriteSupport.init(null);    tupleWriteSupport.prepareForWrite(recordConsumer);    final Tuple pigTuple = thriftToPig.getPigTuple(a);        tupleWriteSupport.write(pigTuple);    return schema;}
public void parquet-mr_f7704_0() throws Exception
{    OneOfEach a = new OneOfEach(true, false, (byte) 8, (short) 16, (int) 32, (long) 64, (double) 1234, "string", "å", false, ByteBuffer.wrap("a".getBytes()), new ArrayList<Byte>(), new ArrayList<Short>(), new ArrayList<Long>());    writeReadCompare(a);}
public void parquet-mr_f7705_0() throws Exception
{    ArrayList<Person> persons = new ArrayList<Person>();    final PhoneNumber phoneNumber = new PhoneNumber("555 999 9998");    phoneNumber.type = PhoneType.HOME;    persons.add(new Person(new Name("Bob", "Roberts"), 1, "bob@roberts.com", Arrays.asList(new PhoneNumber("555 999 9999"), phoneNumber)));    persons.add(new Person(new Name("Dick", "Richardson"), 2, "dick@richardson.com", Arrays.asList(new PhoneNumber("555 999 9997"), new PhoneNumber("555 999 9996"))));    AddressBook a = new AddressBook(persons);    writeReadCompare(a);}
public void parquet-mr_f7706_0() throws Exception
{    AddressBook a = new AddressBook();    writeReadCompare(a);}
public void parquet-mr_f7707_0() throws Exception
{    final Set<Map<String, String>> set = new HashSet<Map<String, String>>();    final Map<String, String> map = new HashMap<String, String>();    map.put("foo", "bar");    set.add(map);    TestMapInSet a = new TestMapInSet("top", set);    writeReadCompare(a);}
private void parquet-mr_f7708_0(TBase<?, ?> a) throws TException, InstantiationException, IllegalAccessException
{    ProtocolPipe[] pipes = { new ProtocolReadToWrite(), new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType((Class<TBase<?, ?>>) a.getClass())) };    for (ProtocolPipe p : pipes) {        final ByteArrayOutputStream in = new ByteArrayOutputStream();        final ByteArrayOutputStream out = new ByteArrayOutputStream();        a.write(protocol(in));        p.readOne(protocol(new ByteArrayInputStream(in.toByteArray())), protocol(out));        TBase<?, ?> b = a.getClass().newInstance();        b.read(protocol(new ByteArrayInputStream(out.toByteArray())));        assertEquals(p.getClass().getSimpleName(), a, b);    }}
public void parquet-mr_f7709_0() throws Exception
{        CountingErrorHandler countingHandler = new CountingErrorHandler();    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(AddressBook.class), countingHandler);    final ByteArrayOutputStream in = new ByteArrayOutputStream();    final ByteArrayOutputStream out = new ByteArrayOutputStream();    OneOfEach a = new OneOfEach(true, false, (byte) 8, (short) 16, (int) 32, (long) 64, (double) 1234, "string", "å", false, ByteBuffer.wrap("a".getBytes()), new ArrayList<Byte>(), new ArrayList<Short>(), new ArrayList<Long>());    a.write(protocol(in));    try {        p.readOne(protocol(new ByteArrayInputStream(in.toByteArray())), protocol(out));        fail("this should throw");    } catch (SkippableException e) {        Throwable cause = e.getCause();        assertTrue(cause instanceof DecodingSchemaMismatchException);        assertTrue(cause.getMessage().contains("the data type does not match the expected thrift structure"));        assertTrue(cause.getMessage().contains("got BOOL"));    }    assertEquals(0, countingHandler.recordCountOfMissingFields);    assertEquals(0, countingHandler.fieldIgnoredCount);}
public void parquet-mr_f7710_0() throws Exception
{    CountingErrorHandler countingHandler = new CountingErrorHandler();    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructWithUnionV1.class), countingHandler);    final ByteArrayOutputStream in = new ByteArrayOutputStream();    final ByteArrayOutputStream out = new ByteArrayOutputStream();    StructWithUnionV1 validUnion = new StructWithUnionV1("a valid struct", UnionV1.aLong(new ALong(17L)));    StructWithUnionV2 invalidUnion = new StructWithUnionV2("a struct with new union member", UnionV2.aNewBool(new ABool(true)));    validUnion.write(protocol(in));    invalidUnion.write(protocol(in));    ByteArrayInputStream baos = new ByteArrayInputStream(in.toByteArray());        p.readOne(protocol(baos), protocol(out));    try {        p.readOne(protocol(baos), protocol(out));        fail("this should throw");    } catch (SkippableException e) {        Throwable cause = e.getCause();        assertEquals(DecodingSchemaMismatchException.class, cause.getClass());        assertTrue(cause.getMessage().startsWith("Unrecognized union member with id: 3 for struct:"));    }    assertEquals(0, countingHandler.recordCountOfMissingFields);    assertEquals(0, countingHandler.fieldIgnoredCount);}
public void parquet-mr_f7711_0() throws Exception
{    CountingErrorHandler countingHandler = new CountingErrorHandler();    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructWithUnionV2.class), countingHandler);    ByteArrayOutputStream in = new ByteArrayOutputStream();    final ByteArrayOutputStream out = new ByteArrayOutputStream();    StructWithUnionV2 validUnion = new StructWithUnionV2("a valid struct", UnionV2.aLong(new ALong(17L)));    StructWithAStructThatLooksLikeUnionV2 allMissing = new StructWithAStructThatLooksLikeUnionV2("all missing", new AStructThatLooksLikeUnionV2());    AStructThatLooksLikeUnionV2 extra = new AStructThatLooksLikeUnionV2();    extra.setALong(new ALong(18L));    extra.setANewBool(new ABool(false));    StructWithAStructThatLooksLikeUnionV2 hasExtra = new StructWithAStructThatLooksLikeUnionV2("has extra", new AStructThatLooksLikeUnionV2(extra));    validUnion.write(protocol(in));    allMissing.write(protocol(in));    ByteArrayInputStream baos = new ByteArrayInputStream(in.toByteArray());        p.readOne(protocol(baos), protocol(out));    try {        p.readOne(protocol(baos), protocol(out));        fail("this should throw");    } catch (SkippableException e) {        Throwable cause = e.getCause();        assertEquals(DecodingSchemaMismatchException.class, cause.getClass());        assertTrue(cause.getMessage().startsWith("Cannot write a TUnion with no set value in"));    }    assertEquals(0, countingHandler.recordCountOfMissingFields);    assertEquals(0, countingHandler.fieldIgnoredCount);    in = new ByteArrayOutputStream();    validUnion.write(protocol(in));    hasExtra.write(protocol(in));    baos = new ByteArrayInputStream(in.toByteArray());        p.readOne(protocol(baos), protocol(out));    try {        p.readOne(protocol(baos), protocol(out));        fail("this should throw");    } catch (SkippableException e) {        Throwable cause = e.getCause();        assertEquals(DecodingSchemaMismatchException.class, cause.getClass());        assertTrue(cause.getMessage().startsWith("Cannot write a TUnion with more than 1 set value in"));    }    assertEquals(0, countingHandler.recordCountOfMissingFields);    assertEquals(0, countingHandler.fieldIgnoredCount);}
public void parquet-mr_f7712_0() throws Exception
{    CountingErrorHandler countingHandler = new CountingErrorHandler();    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(UnionV3.class), countingHandler);    ByteArrayOutputStream in = new ByteArrayOutputStream();    final ByteArrayOutputStream out = new ByteArrayOutputStream();    UnionV3 validUnion = UnionV3.aStruct(new StructV1("a valid struct"));    StructV2 structV2 = new StructV2("a valid struct");    structV2.setAge("a valid age");    UnionThatLooksLikeUnionV3 unionWithUnknownStructField = UnionThatLooksLikeUnionV3.aStruct(structV2);    validUnion.write(protocol(in));    unionWithUnknownStructField.write(protocol(in));    ByteArrayInputStream baos = new ByteArrayInputStream(in.toByteArray());        p.readOne(protocol(baos), protocol(out));    p.readOne(protocol(baos), protocol(out));    assertEquals(1, countingHandler.recordCountOfMissingFields);    assertEquals(1, countingHandler.fieldIgnoredCount);    in = new ByteArrayOutputStream();    validUnion.write(protocol(in));    unionWithUnknownStructField.write(protocol(in));    baos = new ByteArrayInputStream(in.toByteArray());        p.readOne(protocol(baos), protocol(out));    p.readOne(protocol(baos), protocol(out));    assertEquals(2, countingHandler.recordCountOfMissingFields);    assertEquals(2, countingHandler.fieldIgnoredCount);}
public void parquet-mr_f7713_0() throws Exception
{    CountingErrorHandler countingHandler = new CountingErrorHandler();    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructWithEnum.class), countingHandler);    final ByteArrayOutputStream in = new ByteArrayOutputStream();    final ByteArrayOutputStream out = new ByteArrayOutputStream();    StructWithMoreEnum enumDefinedInOldDefinition = new StructWithMoreEnum(NumberEnumWithMoreValue.THREE);    StructWithMoreEnum extraEnumDefinedInNewDefinition = new StructWithMoreEnum(NumberEnumWithMoreValue.FOUR);    enumDefinedInOldDefinition.write(protocol(in));    extraEnumDefinedInNewDefinition.write(protocol(in));    ByteArrayInputStream baos = new ByteArrayInputStream(in.toByteArray());        p.readOne(protocol(baos), protocol(out));    try {        p.readOne(protocol(baos), protocol(out));        fail("this should throw");    } catch (SkippableException e) {        Throwable cause = e.getCause();        assertEquals(DecodingSchemaMismatchException.class, cause.getClass());        assertTrue(cause.getMessage().contains("can not find index 4 in enum"));    }    assertEquals(0, countingHandler.recordCountOfMissingFields);    assertEquals(0, countingHandler.fieldIgnoredCount);}
public void parquet-mr_f7714_0() throws Exception
{    CountingErrorHandler countingHandler = new CountingErrorHandler() {        @Override        public void handleFieldIgnored(TField field) {            assertEquals(field.id, 4);            fieldIgnoredCount++;        }    };    BufferedProtocolReadToWrite structForRead = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructV3.class), countingHandler);        final ByteArrayOutputStream in = new ByteArrayOutputStream();    StructV4WithExtracStructField dataWithNewSchema = new StructV4WithExtracStructField("name");    dataWithNewSchema.setAge("10");    dataWithNewSchema.setGender("male");    StructV3 structV3 = new StructV3("name");    structV3.setAge("10");    dataWithNewSchema.setAddedStruct(structV3);    dataWithNewSchema.write(protocol(in));        final ByteArrayOutputStream out = new ByteArrayOutputStream();    structForRead.readOne(protocol(new ByteArrayInputStream(in.toByteArray())), protocol(out));        assertEquals(1, countingHandler.recordCountOfMissingFields);    assertEquals(1, countingHandler.fieldIgnoredCount);    StructV4WithExtracStructField b = StructV4WithExtracStructField.class.newInstance();    b.read(protocol(new ByteArrayInputStream(out.toByteArray())));    assertEquals(dataWithNewSchema.getName(), b.getName());    assertEquals(dataWithNewSchema.getAge(), b.getAge());    assertEquals(dataWithNewSchema.getGender(), b.getGender());    assertEquals(null, b.getAddedStruct());}
public void parquet-mr_f7715_0(TField field)
{    assertEquals(field.id, 4);    fieldIgnoredCount++;}
public void parquet-mr_f7716_0() throws Exception
{    CountingErrorHandler countingHandler = new CountingErrorHandler() {        @Override        public void handleFieldIgnored(TField field) {            assertEquals(3, field.id);            fieldIgnoredCount++;        }    };    BufferedProtocolReadToWrite structForRead = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructWithIndexStartsFrom4.class), countingHandler);        final ByteArrayOutputStream in = new ByteArrayOutputStream();    StructWithExtraField dataWithNewExtraField = new StructWithExtraField(new Phone("111", "222"), new Phone("333", "444"));    dataWithNewExtraField.write(protocol(in));        final ByteArrayOutputStream out = new ByteArrayOutputStream();    structForRead.readOne(protocol(new ByteArrayInputStream(in.toByteArray())), protocol(out));    assertEquals(1, countingHandler.recordCountOfMissingFields);    assertEquals(1, countingHandler.fieldIgnoredCount);}
public void parquet-mr_f7717_0(TField field)
{    assertEquals(3, field.id);    fieldIgnoredCount++;}
private TCompactProtocol parquet-mr_f7718_0(OutputStream to)
{    return new TCompactProtocol(new TIOStreamTransport(to));}
private TCompactProtocol parquet-mr_f7719_0(InputStream from)
{    return new TCompactProtocol(new TIOStreamTransport(from));}
public void parquet-mr_f7720_0()
{    recordCountOfMissingFields++;}
public void parquet-mr_f7721_0(TField field)
{    fieldIgnoredCount++;}
public void parquet-mr_f7722_0()
{    StructType descriptor = new StructType(new ArrayList<ThriftField>(), StructOrUnionType.STRUCT);    ThriftMetaData tmd = new ThriftMetaData("non existent class!!!", descriptor);    assertEquals("ThriftMetaData(thriftClassName: non existent class!!!, descriptor: {\n" + "  \"id\" : \"STRUCT\",\n" + "  \"children\" : [ ],\n" + "  \"structOrUnionType\" : \"STRUCT\"\n" + "})", tmd.toString());    tmd = new ThriftMetaData("non existent class!!!", null);    assertEquals("ThriftMetaData(thriftClassName: non existent class!!!, descriptor: null)", tmd.toString());}
public void parquet-mr_f7723_0() throws IOException
{    Configuration configuration = new Configuration();    Path f = new Path("target/test/TestThriftParquetReaderWriter");    FileSystem fs = f.getFileSystem(configuration);    if (fs.exists(f)) {        fs.delete(f, true);    }    AddressBook original = new AddressBook(Arrays.asList(new Person(new Name("Bob", "Roberts"), 1, "bob@roberts.com", Arrays.asList(new PhoneNumber("5555555555")))));    {                ThriftParquetWriter<AddressBook> thriftParquetWriter = new ThriftParquetWriter<AddressBook>(f, AddressBook.class, CompressionCodecName.UNCOMPRESSED);        thriftParquetWriter.write(original);        thriftParquetWriter.close();    }    {                ThriftParquetReader<AddressBook> thriftParquetReader = new ThriftParquetReader<AddressBook>(f, AddressBook.class);        AddressBook read = thriftParquetReader.read();        Assert.assertEquals(original, read);        thriftParquetReader.close();    }    {                ThriftParquetReader<AddressBook> thriftParquetReader = new ThriftParquetReader<AddressBook>(f);        AddressBook read = thriftParquetReader.read();        Assert.assertEquals(original, read);        thriftParquetReader.close();    }}
public void parquet-mr_f7724_0() throws Exception
{    EnumType et = new EnumType(Arrays.asList(new EnumValue(77, "hello")));    ThriftField field = new ThriftField("name", (short) 1, Requirement.REQUIRED, et);    ArrayList<TProtocol> events = new ArrayList<TProtocol>();    FieldEnumConverter conv = new FieldEnumConverter(events, field);    conv.addBinary(Binary.fromString("hello"));    assertEquals(1, events.size());    assertEquals(77, events.get(0).readI32());    try {        conv.addBinary(Binary.fromString("FAKE_ENUM_VALUE"));        fail("this should throw");    } catch (ParquetDecodingException e) {        assertEquals("Unrecognized enum value: FAKE_ENUM_VALUE known values: {Binary{\"hello\"}=77} in {\n" + "  \"name\" : \"name\",\n" + "  \"fieldId\" : 1,\n" + "  \"requirement\" : \"REQUIRED\",\n" + "  \"type\" : {\n" + "    \"id\" : \"ENUM\",\n" + "    \"values\" : [ {\n" + "      \"id\" : 77,\n" + "      \"name\" : \"hello\"\n" + "    } ]\n" + "  }\n" + "}", e.getMessage());    }}
public void parquet-mr_f7725_0() throws Exception
{    String jsonWithNoStructOrUnionMeta = Strings.join(Files.readAllLines(new File("src/test/resources/org/apache/parquet/thrift/StructWithUnionV1NoStructOrUnionMeta.json").toPath(), StandardCharsets.UTF_8), "\n");    StructType noStructOrUnionMeta = (StructType) ThriftType.fromJSON(jsonWithNoStructOrUnionMeta);        new ThriftRecordConverter<StructWithUnionV1>(new ThriftReader<StructWithUnionV1>() {        @Override        public StructWithUnionV1 readOneRecord(TProtocol protocol) throws TException {            return null;        }    }, "name", new ThriftSchemaConverter().convert(StructWithUnionV1.class), noStructOrUnionMeta);}
public StructWithUnionV1 parquet-mr_f7726_0(TProtocol protocol) throws TException
{    return null;}
public void parquet-mr_f7727_0() throws Exception
{    String expected = "message ParquetSchema {\n" + "  optional group persons (LIST) = 1 {\n" + "    repeated group persons_tuple {\n" + "      required group name = 1 {\n" + "        optional binary first_name (UTF8) = 1;\n" + "        optional binary last_name (UTF8) = 2;\n" + "      }\n" + "      optional int32 id = 2;\n" + "      optional binary email (UTF8) = 3;\n" + "      optional group phones (LIST) = 4 {\n" + "        repeated group phones_tuple {\n" + "          optional binary number (UTF8) = 1;\n" + "          optional binary type (ENUM) = 2;\n" + "        }\n" + "      }\n" + "    }\n" + "  }\n" + "}";    ThriftSchemaConverter schemaConverter = new ThriftSchemaConverter();    final MessageType converted = schemaConverter.convert(AddressBook.class);    assertEquals(MessageTypeParser.parseMessageType(expected), converted);}
public void parquet-mr_f7728_0()
{    shouldGetProjectedSchema("name/first_name", "name.first_name", "message ParquetSchema {" + "  required group name = 1 {" + "    optional binary first_name (UTF8) = 1;" + "  }}", Person.class);    shouldGetProjectedSchema("name/first_name;name/last_name", "name.first_name;name.last_name", "message ParquetSchema {" + "  required group name = 1 {" + "    optional binary first_name (UTF8) = 1;" + "    optional binary last_name (UTF8) = 2;" + "  }}", Person.class);    shouldGetProjectedSchema("name/{first,last}_name;", "name.{first,last}_name;", "message ParquetSchema {" + "  required group name = 1 {" + "    optional binary first_name (UTF8) = 1;" + "    optional binary last_name (UTF8) = 2;" + "  }}", Person.class);    shouldGetProjectedSchema("name/*", "name", "message ParquetSchema {" + "  required group name = 1 {" + "    optional binary first_name (UTF8) = 1;" + "    optional binary last_name (UTF8) = 2;" + "  }" + "}", Person.class);    shouldGetProjectedSchema("*/*_name", "*.*_name", "message ParquetSchema {" + "  required group name = 1 {" + "    optional binary first_name (UTF8) = 1;" + "    optional binary last_name (UTF8) = 2;" + "  }" + "}", Person.class);    shouldGetProjectedSchema("name/first_*", "name.first_*", "message ParquetSchema {" + "  required group name = 1 {" + "    optional binary first_name (UTF8) = 1;" + "  }" + "}", Person.class);    shouldGetProjectedSchema("*/*", "*.*", "message ParquetSchema {" + "  required group name = 1 {" + "  optional binary first_name (UTF8) = 1;" + "  optional binary last_name (UTF8) = 2;" + "} " + "  optional group phones (LIST) = 4 {" + "    repeated group phones_tuple {" + "      optional binary number (UTF8) = 1;" + "      optional binary type (ENUM) = 2;" + "    }" + "}}", Person.class);}
public void parquet-mr_f7729_0()
{        shouldGetProjectedSchema("name;names/key*;names/value/**", "name;names.key*;names.value", "message ParquetSchema {\n" + "  optional binary name (UTF8) = 1;\n" + "  optional group names (MAP) = 2 {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional group value {\n" + "        optional group name = 1 {\n" + "          optional binary first_name (UTF8) = 1;\n" + "          optional binary last_name (UTF8) = 2;\n" + "        }\n" + "        optional group phones (MAP) = 2 {\n" + "          repeated group map (MAP_KEY_VALUE) {\n" + "            required binary key (ENUM);\n" + "            optional binary value (UTF8);\n" + "          }\n" + "        }\n" + "      }\n" + "    }\n" + "  }\n" + "}", TestStructInMap.class);        shouldGetProjectedSchema("name;names/key;names/value/name/*", "name;names.key;names.value.name", "message ParquetSchema {\n" + "  optional binary name (UTF8) = 1;\n" + "  optional group names (MAP) = 2 {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional group value {\n" + "        optional group name = 1 {\n" + "          optional binary first_name (UTF8) = 1;\n" + "          optional binary last_name (UTF8) = 2;\n" + "        }\n" + "      }\n" + "    }\n" + "  }\n" + "}", TestStructInMap.class);}
public void parquet-mr_f7730_0()
{    shouldGetProjectedSchema("name;names/key", "name;names.key", "message ParquetSchema {\n" + "  optional binary name (UTF8) = 1;\n" + "  optional group names (MAP) = 2 {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional group value {\n" + "        optional group name = 1 {\n" + "          optional binary first_name (UTF8) = 1;\n" + "        }\n" + "      }" + "    }\n" + "  }\n" + "}", TestStructInMap.class);}
private void parquet-mr_f7731_0(String filters, String unmatchedFilter, Class<? extends TBase<?, ?>> thriftClass)
{    try {        getDeprecatedFilteredSchema(filters, thriftClass);        fail("should throw projection exception when filter matches nothing");    } catch (ThriftProjectionException e) {        assertEquals("The following projection patterns did not match any columns in this schema:\n" + unmatchedFilter + "\n", e.getMessage());    }}
private void parquet-mr_f7732_0(String filters, Class<? extends TBase<?, ?>> thriftClass)
{    try {        getDeprecatedFilteredSchema(filters, thriftClass);        fail("should throw projection exception when no columns are selected");    } catch (ThriftProjectionException e) {        assertEquals("No columns have been selected", e.getMessage());    }}
public void parquet-mr_f7733_0()
{    shouldThrowWhenNoColumnsAreSelected("non_existing", TestStructInMap.class);}
public void parquet-mr_f7734_0()
{    shouldThrowWhenProjectionFilterMatchesNothing("name;non_existing", "non_existing", TestStructInMap.class);    shouldThrowWhenProjectionFilterMatchesNothing("**;non_existing", "non_existing", TestStructInMap.class);    shouldThrowWhenProjectionFilterMatchesNothing("**;names/non_existing", "names/non_existing", TestStructInMap.class);    shouldThrowWhenProjectionFilterMatchesNothing("**;names/non_existing;non_existing", "names/non_existing\nnon_existing", TestStructInMap.class);}
public void parquet-mr_f7735_0()
{    try {        getDeprecatedFilteredSchema("name;names/value/**", TestStructInMap.class);        fail("this should throw");    } catch (ThriftProjectionException e) {        assertEquals("Cannot select only the values of a map, you must keep the keys as well: names", e.getMessage());    }    try {        getStrictFilteredSchema("name;names.value", TestStructInMap.class);        fail("this should throw");    } catch (ThriftProjectionException e) {        assertEquals("Cannot select only the values of a map, you must keep the keys as well: names", e.getMessage());    }}
private void parquet-mr_f7736_0(String deprecated, String strict)
{    try {        getDeprecatedFilteredSchema(deprecated, MapStructV2.class);        fail("this should throw");    } catch (ThriftProjectionException e) {        assertEquals("Cannot select only a subset of the fields in a map key, for path map1", e.getMessage());    }    try {        getStrictFilteredSchema(strict, MapStructV2.class);        fail("this should throw");    } catch (ThriftProjectionException e) {        assertEquals("Cannot select only a subset of the fields in a map key, for path map1", e.getMessage());    }}
public void parquet-mr_f7737_0()
{    doTestPartialKeyProjection("map1/key/age", "map1.key.age");    doTestPartialKeyProjection("map1/key/age;map1/value/**", "map1.{key.age,value}");}
public void parquet-mr_f7738_0()
{    try {        getDeprecatedFilteredSchema("set1/age", SetStructV2.class);        fail("this should throw");    } catch (ThriftProjectionException e) {        assertEquals("Cannot select only a subset of the fields in a set, for path set1", e.getMessage());    }    try {        getStrictFilteredSchema("set1.age", SetStructV2.class);        fail("this should throw");    } catch (ThriftProjectionException e) {        assertEquals("Cannot select only a subset of the fields in a set, for path set1", e.getMessage());    }}
public void parquet-mr_f7739_0()
{    String expected = "message ParquetSchema {\n" + "  required binary a (UTF8) = 1;\n" + "  required binary b (UTF8) = 2;\n" + "}\n";    ThriftSchemaConverter converter = new ThriftSchemaConverter();    StructType structType = new StructType(Arrays.asList(new ThriftField("a", (short) 1, REQUIRED, new ThriftType.StringType()), new ThriftField("b", (short) 2, REQUIRED, new ThriftType.StringType())));    final MessageType converted = converter.convert(structType);    assertEquals(MessageTypeParser.parseMessageType(expected), converted);}
public static void parquet-mr_f7740_0(String deprecatedFilterDesc, String strictFilterDesc, String expectedSchemaStr, Class<? extends TBase<?, ?>> thriftClass)
{    MessageType depRequestedSchema = getDeprecatedFilteredSchema(deprecatedFilterDesc, thriftClass);    MessageType strictRequestedSchema = getStrictFilteredSchema(strictFilterDesc, thriftClass);    MessageType expectedSchema = parseMessageType(expectedSchemaStr);    assertEquals(expectedSchema, depRequestedSchema);    assertEquals(expectedSchema, strictRequestedSchema);}
private static MessageType parquet-mr_f7741_0(String filterDesc, Class<? extends TBase<?, ?>> thriftClass)
{    DeprecatedFieldProjectionFilter fieldProjectionFilter = new DeprecatedFieldProjectionFilter(filterDesc);    return new ThriftSchemaConverter(fieldProjectionFilter).convert(thriftClass);}
private static MessageType parquet-mr_f7742_0(String semicolonDelimitedString, Class<? extends TBase<?, ?>> thriftClass)
{    StrictFieldProjectionFilter fieldProjectionFilter = StrictFieldProjectionFilter.fromSemicolonDelimitedString(semicolonDelimitedString);    return new ThriftSchemaConverter(fieldProjectionFilter).convert(thriftClass);}
public void parquet-mr_f7743_0() throws Exception
{    final StructType converted = ThriftSchemaConverter.toStructType(AddressBook.class);    final String json = converted.toJSON();    final ThriftType fromJSON = StructType.fromJSON(json);    assertEquals(json, fromJSON.toJSON());}
public void parquet-mr_f7744_0()
{        shouldGetProjectedSchema("aLong/**", "aLong", "message ParquetSchema {\n" + "  optional group aString = 1 {\n" + "    required binary s (UTF8) = 1;\n" + "  }\n" + "  optional group aLong = 2 {\n" + "    required int64 l = 1;\n" + "  }\n" + "  optional group aNewBool = 3 {\n" + "    required boolean b = 1;\n" + "  }\n" + "}", UnionV2.class);            shouldGetProjectedSchema("aNewBool/**", "aNewBool", "message ParquetSchema {\n" + "  optional group structV3 = 1 {\n" + "    required binary name (UTF8) = 1;\n" + "  }\n" + "  optional group structV4 = 2 {\n" + "    required binary name (UTF8) = 1;\n" + "  }\n" + "  optional group aNewBool = 3 {\n" + "    required boolean b = 1;\n" + "  }\n" + "}", UnionOfStructs.class);}
public void parquet-mr_f7745_0()
{    shouldGetProjectedSchema("name", "name", "message ParquetSchema {\n" + "  required binary name (UTF8) = 1;\n" + "}", StructWithOptionalUnionOfStructs.class);}
public void parquet-mr_f7746_0()
{    shouldGetProjectedSchema("name", "name", "message ParquetSchema {\n" + "  required binary name (UTF8) = 1;\n" + "}", OptionalInsideRequired.class);}
public void parquet-mr_f7747_0()
{        shouldGetProjectedSchema("name", "name", "message ParquetSchema {\n" + "  required binary name (UTF8) = 1;\n" + "}", RequiredInsideOptional.class);}
public void parquet-mr_f7748_0()
{    shouldGetProjectedSchema("aStruct/name", "aStruct.name", "message ParquetSchema {\n" + "  optional group aStruct = 2 {\n" + "    required binary name (UTF8) = 1;\n" + "    required group aUnion = 2 {\n" + "      optional group structV3 = 1 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group structV4 = 2 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group aNewBool = 3 {\n" + "        required boolean b = 1;\n" + "      }\n" + "    }\n" + "  }\n" + "}", RequiredInsideOptional.class);}
public void parquet-mr_f7749_0()
{    shouldGetProjectedSchema("aUnion/structV4/addedStruct/gender", "aUnion.structV4.addedStruct.gender", "message ParquetSchema {\n" + "  optional group aUnion = 2 {\n" + "    optional group structV3 = 1 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group structV4 = 2 {\n" + "      optional group addedStruct = 4 {\n" + "        optional binary gender (UTF8) = 3;\n" + "      }\n" + "    }\n" + "    optional group aNewBool = 3 {\n" + "      required boolean b = 1;\n" + "    }\n" + "  }\n" + "}", StructWithOptionalUnionOfStructs.class);}
public void parquet-mr_f7750_0()
{    shouldGetProjectedSchema("structV3/age", "structV3.age", "message ParquetSchema {\n" + "  optional group structV3 = 1 {\n" + "    optional binary age (UTF8) = 2;\n" + "  }\n" + "  optional group unionOfStructs = 2 {\n" + "    optional group structV3 = 1 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group structV4 = 2 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group aNewBool = 3 {\n" + "      required boolean b = 1;\n" + "    }\n" + "  }\n" + "  optional group aLong = 3 {\n" + "    required int64 l = 1;\n" + "  }\n" + "}", NestedUnion.class);    shouldGetProjectedSchema("unionOfStructs/structV4/addedStruct/gender", "unionOfStructs.structV4.addedStruct.gender", "message ParquetSchema {\n" + "  optional group structV3 = 1 {\n" + "    required binary name (UTF8) = 1;\n" + "  }\n" + "  optional group unionOfStructs = 2 {\n" + "    optional group structV3 = 1 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group structV4 = 2 {\n" + "      optional group addedStruct = 4 {\n" + "        optional binary gender (UTF8) = 3;\n" + "      }\n" + "    }\n" + "    optional group aNewBool = 3 {\n" + "      required boolean b = 1;\n" + "    }\n" + "  }\n" + "  optional group aLong = 3 {\n" + "    required int64 l = 1;\n" + "  }\n" + "}\n", NestedUnion.class);    shouldGetProjectedSchema("unionV2/aLong/**", "unionV2.aLong", "message ParquetSchema {\n" + "  optional group nestedUnion = 1 {\n" + "    optional group structV3 = 1 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group unionOfStructs = 2 {\n" + "      optional group structV3 = 1 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group structV4 = 2 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group aNewBool = 3 {\n" + "        required boolean b = 1;\n" + "      }\n" + "    }\n" + "    optional group aLong = 3 {\n" + "      required int64 l = 1;\n" + "    }\n" + "  }\n" + "  optional group unionV2 = 2 {\n" + "    optional group aString = 1 {\n" + "      required binary s (UTF8) = 1;\n" + "    }\n" + "    optional group aLong = 2 {\n" + "      required int64 l = 1;\n" + "    }\n" + "    optional group aNewBool = 3 {\n" + "      required boolean b = 1;\n" + "    }\n" + "  }\n" + "}", NestedNestedUnion.class);}
public void parquet-mr_f7751_0()
{                    shouldGetProjectedSchema("optListUnion/structV3/age", "optListUnion.structV3.age", "message ParquetSchema {\n" + "  optional group optListUnion (LIST) = 1 {\n" + "    repeated group optListUnion_tuple {\n" + "      optional group structV3 = 1 {\n" + "        optional binary age (UTF8) = 2;\n" + "      }\n" + "      optional group structV4 = 2 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group aNewBool = 3 {\n" + "        required boolean b = 1;\n" + "      }\n" + "    }\n" + "  }\n" + "}", ListOfUnions.class);            shouldGetProjectedSchema("reqListUnion/structV3/age", "reqListUnion.structV3.age", "message ParquetSchema {\n" + "  required group reqListUnion (LIST) = 2 {\n" + "    repeated group reqListUnion_tuple {\n" + "      optional group structV3 = 1 {\n" + "        optional binary age (UTF8) = 2;\n" + "      }\n" + "      optional group structV4 = 2 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group aNewBool = 3 {\n" + "        required boolean b = 1;\n" + "      }\n" + "    }\n" + "  }\n" + "}", ListOfUnions.class);}
public void parquet-mr_f7752_0()
{    shouldGetProjectedSchema("optMapWithUnionKey/key/**", "optMapWithUnionKey.key", "message ParquetSchema {\n" + "  optional group optMapWithUnionKey (MAP) = 1 {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required group key {\n" + "        optional group structV3 = 1 {\n" + "          required binary name (UTF8) = 1;\n" + "          optional binary age (UTF8) = 2;\n" + "          optional binary gender (UTF8) = 3;\n" + "        }\n" + "        optional group structV4 = 2 {\n" + "          required binary name (UTF8) = 1;\n" + "          optional binary age (UTF8) = 2;\n" + "          optional binary gender (UTF8) = 3;\n" + "          optional group addedStruct = 4 {\n" + "            required binary name (UTF8) = 1;\n" + "            optional binary age (UTF8) = 2;\n" + "            optional binary gender (UTF8) = 3;\n" + "          }\n" + "        }\n" + "        optional group aNewBool = 3 {\n" + "          required boolean b = 1;\n" + "        }\n" + "      }\n" + "      optional group value {\n" + "        required binary name (UTF8) = 1;\n" + "      } " + "    }\n" + "  }\n" + "}", MapWithUnionKey.class);    shouldGetProjectedSchema("optMapWithUnionKey/key/**;optMapWithUnionKey/value/gender", "optMapWithUnionKey.{key,value.gender}", "message ParquetSchema {\n" + "  optional group optMapWithUnionKey (MAP) = 1 {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required group key {\n" + "        optional group structV3 = 1 {\n" + "          required binary name (UTF8) = 1;\n" + "          optional binary age (UTF8) = 2;\n" + "          optional binary gender (UTF8) = 3;\n" + "        }\n" + "        optional group structV4 = 2 {\n" + "          required binary name (UTF8) = 1;\n" + "          optional binary age (UTF8) = 2;\n" + "          optional binary gender (UTF8) = 3;\n" + "          optional group addedStruct = 4 {\n" + "            required binary name (UTF8) = 1;\n" + "            optional binary age (UTF8) = 2;\n" + "            optional binary gender (UTF8) = 3;\n" + "          }\n" + "        }\n" + "        optional group aNewBool = 3 {\n" + "          required boolean b = 1;\n" + "        }\n" + "      }\n" + "      optional group value {\n" + "        optional binary gender (UTF8) = 3;\n" + "      }\n" + "    }\n" + "  }\n" + "}", MapWithUnionKey.class);}
public void parquet-mr_f7753_0()
{    shouldGetProjectedSchema("optMapWithUnionValue/key/**;optMapWithUnionValue/value/structV4/addedStruct/gender", "optMapWithUnionValue.{key,value.structV4.addedStruct.gender}", "message ParquetSchema {\n" + "  optional group optMapWithUnionValue (MAP) = 1 {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required group key {\n" + "        required binary name (UTF8) = 1;\n" + "        optional binary age (UTF8) = 2;\n" + "        optional binary gender (UTF8) = 3;\n" + "      }\n" + "      optional group value {\n" + "        optional group structV3 = 1 {\n" + "          required binary name (UTF8) = 1;\n" + "        }\n" + "        optional group structV4 = 2 {\n" + "          optional group addedStruct = 4 {\n" + "            optional binary gender (UTF8) = 3;\n" + "          }\n" + "        }\n" + "        optional group aNewBool = 3 {\n" + "          required boolean b = 1;\n" + "        }\n" + "      }\n" + "    }\n" + "  }\n" + "}", MapWithUnionValue.class);}
public void parquet-mr_f7754_0()
{    shouldGetProjectedSchema("reqStructWithUnionV2/name", "reqStructWithUnionV2.name", "message ParquetSchema {\n" + "  required group reqUnionOfStructs = 2 {\n" + "    optional group structV3 = 1 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group structV4 = 2 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group aNewBool = 3 {\n" + "      required boolean b = 1;\n" + "    }\n" + "  }\n" + "  required group reqNestedUnion = 5 {\n" + "    optional group structV3 = 1 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group unionOfStructs = 2 {\n" + "      optional group structV3 = 1 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group structV4 = 2 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group aNewBool = 3 {\n" + "        required boolean b = 1;\n" + "      }\n" + "    }\n" + "    optional group aLong = 3 {\n" + "      required int64 l = 1;\n" + "    }\n" + "  }\n" + "  required group reqStructWithUnionV2 = 8 {\n" + "    required binary name (UTF8) = 1;\n" + "    required group aUnion = 2 {\n" + "      optional group aString = 1 {\n" + "        required binary s (UTF8) = 1;\n" + "      }\n" + "      optional group aLong = 2 {\n" + "        required int64 l = 1;\n" + "      }\n" + "      optional group aNewBool = 3 {\n" + "        required boolean b = 1;\n" + "      }\n" + "    }\n" + "  }\n" + "  required group reqUnionStructUnion = 11 {\n" + "    optional group structV3 = 1 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group structWithUnionOfStructs = 2 {\n" + "      required binary name (UTF8) = 1;\n" + "      required group aUnion = 2 {\n" + "        optional group structV3 = 1 {\n" + "          required binary name (UTF8) = 1;\n" + "        }\n" + "        optional group structV4 = 2 {\n" + "          required binary name (UTF8) = 1;\n" + "        }\n" + "        optional group aNewBool = 3 {\n" + "          required boolean b = 1;\n" + "        }\n" + "      }\n" + "    }\n" + "    optional group aLong = 3 {\n" + "      required int64 l = 1;\n" + "    }\n" + "  }\n" + "}", StructWithNestedUnion.class);}
public void parquet-mr_f7755_0() throws Exception
{    Map<String, String> map = new TreeMap<String, String>();    map.put("foo", "bar");    map.put("foo2", "bar2");    TestMap testMap = new TestMap("map_name", map);    validateSameTupleAsEB(testMap);}
public void parquet-mr_f7756_0() throws Exception
{    final Set<Map<String, String>> set = new HashSet<Map<String, String>>();    final Map<String, String> map = new HashMap<String, String>();    map.put("foo", "bar");    set.add(map);    TestMapInSet o = new TestMapInSet("top", set);    validateSameTupleAsEB(o);}
public void parquet-mr_f7757_0() throws Exception
{    final Map<String, TestPerson> map = new HashMap<String, TestPerson>();    map.put("foo", new TestPerson(new TestName("john", "johnson"), new HashMap<TestPhoneType, String>()));    final Map<String, Integer> stringToIntMap = Collections.singletonMap("bar", 10);    TestStructInMap testMap = new TestStructInMap("map_name", map, stringToIntMap);    validateSameTupleAsEB(testMap);}
public void parquet-mr_f7758_0() throws Exception
{    AddressBook a = new AddressBook(new ArrayList<Person>());    validateSameTupleAsEB(a);}
public void parquet-mr_f7759_0() throws Exception
{    ArrayList<Person> persons = new ArrayList<Person>();    final PhoneNumber phoneNumber = new PhoneNumber("555 999 9998");    phoneNumber.type = PhoneType.HOME;    persons.add(new Person(new Name("Bob", "Roberts"), 1, "bob@roberts.com", Arrays.asList(new PhoneNumber("555 999 9999"), phoneNumber)));    persons.add(new Person(new Name("Dick", "Richardson"), 2, "dick@richardson.com", Arrays.asList(new PhoneNumber("555 999 9997"), new PhoneNumber("555 999 9996"))));    AddressBook a = new AddressBook(persons);    validateSameTupleAsEB(a);}
public void parquet-mr_f7760_0() throws Exception
{    OneOfEach a = new OneOfEach(true, false, (byte) 8, (short) 16, (int) 32, (long) 64, (double) 1234, "string", "å", false, ByteBuffer.wrap("a".getBytes()), new ArrayList<Byte>(), new ArrayList<Short>(), new ArrayList<Long>());    validateSameTupleAsEB(a);}
public void parquet-mr_f7761_0() throws Exception
{    final List<String> names = new ArrayList<String>();    names.add("John");    names.add("Jack");    TestNameList o = new TestNameList("name", names);    validateSameTupleAsEB(o);}
public static void parquet-mr_f7762_0(T o) throws TException
{    final ThriftSchemaConverter thriftSchemaConverter = new ThriftSchemaConverter();    @SuppressWarnings("unchecked")    final Class<T> class1 = (Class<T>) o.getClass();    final MessageType schema = thriftSchemaConverter.convert(class1);    final StructType structType = ThriftSchemaConverter.toStructType(class1);    final ThriftToPig<T> thriftToPig = new ThriftToPig<T>(class1);    final Schema pigSchema = thriftToPig.toSchema();    final TupleRecordMaterializer tupleRecordConverter = new TupleRecordMaterializer(schema, pigSchema, true);    RecordConsumer recordConsumer = new ConverterConsumer(tupleRecordConverter.getRootConverter(), schema);    final MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);    ParquetWriteProtocol p = new ParquetWriteProtocol(new RecordConsumerLoggingWrapper(recordConsumer), columnIO, structType);    o.write(p);    final Tuple t = tupleRecordConverter.getCurrentRecord();    final Tuple expected = thriftToPig.getPigTuple(o);    assertEquals(expected.toString(), t.toString());    final MessageType filtered = new PigSchemaConverter().filter(schema, pigSchema);    assertEquals(schema.toString(), filtered.toString());}
public Options parquet-mr_f7763_0()
{    return null;}
public boolean parquet-mr_f7764_0()
{    return true;}
public void parquet-mr_f7765_0(CommandLine options) throws Exception
{    String[] args = options.getArgs();    if (args.length < min) {        throw new MissingArgumentException("missing required arguments");    }    if (args.length > max) {        throw new UnrecognizedOptionException("unknown extra argument \"" + args[max] + "\"");    }}
public String[] parquet-mr_f7766_0()
{    return USAGE;}
public String parquet-mr_f7767_0()
{    return "Prints the content of a Parquet file. The output contains only the data, no metadata is displayed";}
public Options parquet-mr_f7768_0()
{    return OPTIONS;}
public void parquet-mr_f7769_0(CommandLine options) throws Exception
{    super.execute(options);    String[] args = options.getArgs();    String input = args[0];    ParquetReader<SimpleRecord> reader = null;    try {        PrintWriter writer = new PrintWriter(Main.out, true);        reader = ParquetReader.builder(new SimpleReadSupport(), new Path(input)).build();        ParquetMetadata metadata = ParquetFileReader.readFooter(new Configuration(), new Path(input));        JsonRecordFormatter.JsonGroupFormatter formatter = JsonRecordFormatter.fromSchema(metadata.getFileMetaData().getSchema());        for (SimpleRecord value = reader.read(); value != null; value = reader.read()) {            if (options.hasOption('j')) {                writer.write(formatter.formatRecord(value));            } else {                value.prettyPrint(writer);            }            writer.println();        }    } finally {        if (reader != null) {            try {                reader.close();            } catch (Exception ex) {            }        }    }}
public String[] parquet-mr_f7770_0()
{    return USAGE;}
public String parquet-mr_f7771_0()
{    return "Prints the column and offset indexes of a Parquet file.";}
public Options parquet-mr_f7772_0()
{    return OPTIONS;}
public void parquet-mr_f7773_0(CommandLine options) throws Exception
{    super.execute(options);    String[] args = options.getArgs();    InputFile in = HadoopInputFile.fromPath(new Path(args[0]), new Configuration());    PrintWriter out = new PrintWriter(Main.out, true);    String rowGroupValue = options.getOptionValue("r");    Set<String> indexes = new HashSet<>();    if (rowGroupValue != null) {        indexes.addAll(Arrays.asList(rowGroupValue.split("\\s*,\\s*")));    }    boolean showColumnIndex = options.hasOption("i");    boolean showOffsetIndex = options.hasOption("o");    if (!showColumnIndex && !showOffsetIndex) {        showColumnIndex = true;        showOffsetIndex = true;    }    try (ParquetFileReader reader = ParquetFileReader.open(in)) {        boolean firstBlock = true;        int rowGroupIndex = 0;        for (BlockMetaData block : reader.getFooter().getBlocks()) {            if (!indexes.isEmpty() && !indexes.contains(Integer.toString(rowGroupIndex))) {                ++rowGroupIndex;                continue;            }            if (!firstBlock) {                out.println();                firstBlock = false;            }            out.format("row group %d:%n", rowGroupIndex);            for (ColumnChunkMetaData column : getColumns(block, options)) {                String path = column.getPath().toDotString();                if (showColumnIndex) {                    out.format("column index for column %s:%n", path);                    ColumnIndex columnIndex = reader.readColumnIndex(column);                    if (columnIndex == null) {                        out.println("NONE");                    } else {                        out.println(columnIndex);                    }                }                if (showOffsetIndex) {                    out.format("offset index for column %s:%n", path);                    OffsetIndex offsetIndex = reader.readOffsetIndex(column);                    if (offsetIndex == null) {                        out.println("NONE");                    } else {                        out.println(offsetIndex);                    }                }            }            ++rowGroupIndex;        }    }}
private static List<ColumnChunkMetaData> parquet-mr_f7774_0(BlockMetaData block, CommandLine options)
{    List<ColumnChunkMetaData> columns = block.getColumns();    String pathValue = options.getOptionValue("c");    if (pathValue == null) {        return columns;    }    String[] paths = pathValue.split("\\s*,\\s*");    Map<String, ColumnChunkMetaData> pathMap = new HashMap<>();    for (ColumnChunkMetaData column : columns) {        pathMap.put(column.getPath().toDotString(), column);    }    List<ColumnChunkMetaData> filtered = new ArrayList<>();    for (String path : paths) {        ColumnChunkMetaData column = pathMap.get(path);        if (column != null) {            filtered.add(column);        }    }    return filtered;}
public Options parquet-mr_f7775_0()
{    return OPTIONS;}
public String[] parquet-mr_f7776_0()
{    return USAGE;}
public String parquet-mr_f7777_0()
{    return "Prints the content and metadata of a Parquet file";}
public void parquet-mr_f7778_0(CommandLine options) throws Exception
{    super.execute(options);    String[] args = options.getArgs();    String input = args[0];    Configuration conf = new Configuration();    Path inpath = new Path(input);    ParquetMetadata metaData = ParquetFileReader.readFooter(conf, inpath, NO_FILTER);    MessageType schema = metaData.getFileMetaData().getSchema();    boolean showmd = !options.hasOption('m');    boolean showdt = !options.hasOption('d');    boolean cropoutput = !options.hasOption('n');    Set<String> showColumns = null;    if (options.hasOption('c')) {        String[] cols = options.getOptionValues('c');        showColumns = new HashSet<String>(Arrays.asList(cols));    }    PrettyPrintWriter out = prettyPrintWriter(cropoutput);    dump(out, metaData, schema, inpath, showmd, showdt, showColumns);}
public static void parquet-mr_f7779_0(PrettyPrintWriter out, ParquetMetadata meta, MessageType schema, Path inpath, boolean showmd, boolean showdt, Set<String> showColumns) throws IOException
{    Configuration conf = new Configuration();    List<BlockMetaData> blocks = meta.getBlocks();    List<ColumnDescriptor> columns = schema.getColumns();    if (showColumns != null) {        columns = new ArrayList<ColumnDescriptor>();        for (ColumnDescriptor column : schema.getColumns()) {            String path = Joiner.on('.').skipNulls().join(column.getPath());            if (showColumns.contains(path)) {                columns.add(column);            }        }    }    ParquetFileReader freader = null;    if (showmd) {        try {            long group = 0;            for (BlockMetaData block : blocks) {                if (group != 0)                    out.println();                out.format("row group %d%n", group++);                out.rule('-');                List<ColumnChunkMetaData> ccmds = block.getColumns();                if (showColumns != null) {                    ccmds = new ArrayList<ColumnChunkMetaData>();                    for (ColumnChunkMetaData ccmd : block.getColumns()) {                        String path = Joiner.on('.').skipNulls().join(ccmd.getPath().toArray());                        if (showColumns.contains(path)) {                            ccmds.add(ccmd);                        }                    }                }                MetadataUtils.showDetails(out, ccmds);                List<BlockMetaData> rblocks = Collections.singletonList(block);                freader = new ParquetFileReader(conf, meta.getFileMetaData(), inpath, rblocks, columns);                PageReadStore store = freader.readNextRowGroup();                while (store != null) {                    out.incrementTabLevel();                    for (ColumnDescriptor column : columns) {                        out.println();                        dump(out, store, column);                    }                    out.decrementTabLevel();                    store = freader.readNextRowGroup();                }                out.flushColumns();            }        } finally {            if (freader != null) {                freader.close();            }        }    }    if (showdt) {        boolean first = true;        for (ColumnDescriptor column : columns) {            if (!first || showmd)                out.println();            first = false;            out.format("%s %s%n", column.getType(), Joiner.on('.').skipNulls().join(column.getPath()));            out.rule('-');            try {                long page = 1;                long total = blocks.size();                long offset = 1;                freader = new ParquetFileReader(conf, meta.getFileMetaData(), inpath, blocks, Collections.singletonList(column));                PageReadStore store = freader.readNextRowGroup();                while (store != null) {                    ColumnReadStoreImpl crstore = new ColumnReadStoreImpl(store, new DumpGroupConverter(), schema, meta.getFileMetaData().getCreatedBy());                    dump(out, crstore, column, page++, total, offset);                    offset += store.getRowCount();                    store = freader.readNextRowGroup();                }                out.flushColumns();            } finally {                out.flushColumns();                if (freader != null) {                    freader.close();                }            }        }    }}
private static boolean parquet-mr_f7780_0(int referenceCrc, byte[] bytes)
{    crc.reset();    crc.update(bytes);    return crc.getValue() == ((long) referenceCrc & 0xffffffffL);}
public static void parquet-mr_f7781_0(final PrettyPrintWriter out, PageReadStore store, ColumnDescriptor column) throws IOException
{    PageReader reader = store.getPageReader(column);    long vc = reader.getTotalValueCount();    int rmax = column.getMaxRepetitionLevel();    int dmax = column.getMaxDefinitionLevel();    out.format("%s TV=%d RL=%d DL=%d", Joiner.on('.').skipNulls().join(column.getPath()), vc, rmax, dmax);    DictionaryPage dict = reader.readDictionaryPage();    if (dict != null) {        out.format(" DS:%d", dict.getDictionarySize());        out.format(" DE:%s", dict.getEncoding());    }    out.println();    out.rule('-');    DataPage page = reader.readPage();    for (long count = 0; page != null; count++) {        out.format("page %d:", count);        page.accept(new Visitor<Void>() {            @Override            public Void visit(DataPageV1 pageV1) {                out.format(" DLE:%s", pageV1.getDlEncoding());                out.format(" RLE:%s", pageV1.getRlEncoding());                out.format(" VLE:%s", pageV1.getValueEncoding());                Statistics<?> statistics = pageV1.getStatistics();                if (statistics != null) {                    out.format(" ST:[%s]", statistics);                } else {                    out.format(" ST:[none]");                }                if (pageV1.getCrc().isPresent()) {                    try {                        out.format(" CRC:%s", verifyCrc(pageV1.getCrc().getAsInt(), pageV1.getBytes().toByteArray()) ? "[verified]" : "[PAGE CORRUPT]");                    } catch (IOException e) {                        out.format(" CRC:[error getting page bytes]");                    }                } else {                    out.format(" CRC:[none]");                }                return null;            }            @Override            public Void visit(DataPageV2 pageV2) {                out.format(" DLE:RLE");                out.format(" RLE:RLE");                out.format(" VLE:%s", pageV2.getDataEncoding());                Statistics<?> statistics = pageV2.getStatistics();                if (statistics != null) {                    out.format(" ST:[%s]", statistics);                } else {                    out.format(" ST:[none]");                }                return null;            }        });        out.format(" SZ:%d", page.getUncompressedSize());        out.format(" VC:%d", page.getValueCount());        out.println();        page = reader.readPage();    }}
public Void parquet-mr_f7782_0(DataPageV1 pageV1)
{    out.format(" DLE:%s", pageV1.getDlEncoding());    out.format(" RLE:%s", pageV1.getRlEncoding());    out.format(" VLE:%s", pageV1.getValueEncoding());    Statistics<?> statistics = pageV1.getStatistics();    if (statistics != null) {        out.format(" ST:[%s]", statistics);    } else {        out.format(" ST:[none]");    }    if (pageV1.getCrc().isPresent()) {        try {            out.format(" CRC:%s", verifyCrc(pageV1.getCrc().getAsInt(), pageV1.getBytes().toByteArray()) ? "[verified]" : "[PAGE CORRUPT]");        } catch (IOException e) {            out.format(" CRC:[error getting page bytes]");        }    } else {        out.format(" CRC:[none]");    }    return null;}
public Void parquet-mr_f7783_0(DataPageV2 pageV2)
{    out.format(" DLE:RLE");    out.format(" RLE:RLE");    out.format(" VLE:%s", pageV2.getDataEncoding());    Statistics<?> statistics = pageV2.getStatistics();    if (statistics != null) {        out.format(" ST:[%s]", statistics);    } else {        out.format(" ST:[none]");    }    return null;}
public static void parquet-mr_f7784_0(PrettyPrintWriter out, ColumnReadStoreImpl crstore, ColumnDescriptor column, long page, long total, long offset) throws IOException
{    int dmax = column.getMaxDefinitionLevel();    ColumnReader creader = crstore.getColumnReader(column);    out.format("*** row group %d of %d, values %d to %d ***%n", page, total, offset, offset + creader.getTotalValueCount() - 1);    for (long i = 0, e = creader.getTotalValueCount(); i < e; ++i) {        int rlvl = creader.getCurrentRepetitionLevel();        int dlvl = creader.getCurrentDefinitionLevel();        out.format("value %d: R:%d D:%d V:", offset + i, rlvl, dlvl);        if (dlvl == dmax) {            PrimitiveStringifier stringifier = column.getPrimitiveType().stringifier();            switch(column.getType()) {                case FIXED_LEN_BYTE_ARRAY:                case INT96:                case BINARY:                    out.print(stringifier.stringify(creader.getBinary()));                    break;                case BOOLEAN:                    out.print(stringifier.stringify(creader.getBoolean()));                    break;                case DOUBLE:                    out.print(stringifier.stringify(creader.getDouble()));                    break;                case FLOAT:                    out.print(stringifier.stringify(creader.getFloat()));                    break;                case INT32:                    out.print(stringifier.stringify(creader.getInteger()));                    break;                case INT64:                    out.print(stringifier.stringify(creader.getLong()));                    break;            }        } else {            out.format("<null>");        }        out.println();        creader.consume();    }}
public static String parquet-mr_f7785_0(Binary value)
{    byte[] data = value.getBytesUnsafe();    if (data == null)        return null;    try {        CharBuffer buffer = UTF8_DECODER.decode(value.toByteBuffer());        return buffer.toString();    } catch (Exception ex) {    }    return "<bytes...>";}
public static BigInteger parquet-mr_f7786_0(Binary value)
{    byte[] data = value.getBytesUnsafe();    if (data == null)        return null;    return new BigInteger(data);}
private static PrettyPrintWriter parquet-mr_f7787_0(boolean cropOutput)
{    PrettyPrintWriter.Builder builder = PrettyPrintWriter.stdoutPrettyPrinter().withAutoColumn().withWhitespaceHandler(WhiteSpaceHandler.ELIMINATE_NEWLINES).withColumnPadding(1).withMaxBufferedLines(1000000).withFlushOnTab();    if (cropOutput) {        builder.withAutoCrop();    }    return builder.build();}
public void parquet-mr_f7788_0()
{}
public void parquet-mr_f7789_0()
{}
public Converter parquet-mr_f7790_0(int fieldIndex)
{    return new DumpConverter();}
public GroupConverter parquet-mr_f7791_0()
{    return new DumpGroupConverter();}
public Options parquet-mr_f7792_0()
{    return OPTIONS;}
public String[] parquet-mr_f7793_0()
{    return USAGE;}
public String parquet-mr_f7794_0()
{    return "Prints the first n record of the Parquet file";}
public void parquet-mr_f7795_0(CommandLine options) throws Exception
{    super.execute(options);    long num = DEFAULT;    if (options.hasOption('n')) {        num = Long.parseLong(options.getOptionValue('n'));    }    String[] args = options.getArgs();    String input = args[0];    ParquetReader<SimpleRecord> reader = null;    try {        PrintWriter writer = new PrintWriter(Main.out, true);        reader = ParquetReader.builder(new SimpleReadSupport(), new Path(input)).build();        for (SimpleRecord value = reader.read(); value != null && num-- > 0; value = reader.read()) {            value.prettyPrint(writer);            writer.println();        }    } finally {        if (reader != null) {            try {                reader.close();            } catch (Exception ex) {            }        }    }}
public String[] parquet-mr_f7796_0()
{    return USAGE;}
public String parquet-mr_f7797_0()
{    return "Merges multiple Parquet files into one. " + "The command doesn't merge row groups, just places one after the other. " + "When used to merge many small files, the resulting file will still contain small row groups, " + "which usually leads to bad query performance.";}
public void parquet-mr_f7798_0(CommandLine options) throws Exception
{        List<String> args = options.getArgList();    List<Path> inputFiles = getInputFiles(args.subList(0, args.size() - 1));    Path outputFile = new Path(args.get(args.size() - 1));        FileMetaData mergedMeta = mergedMetadata(inputFiles);    PrintWriter out = new PrintWriter(Main.out, true);        ParquetFileWriter writer = new ParquetFileWriter(conf, mergedMeta.getSchema(), outputFile, ParquetFileWriter.Mode.CREATE);    writer.start();    boolean tooSmallFilesMerged = false;    for (Path input : inputFiles) {        if (input.getFileSystem(conf).getFileStatus(input).getLen() < TOO_SMALL_FILE_THRESHOLD) {            out.format("Warning: file %s is too small, length: %d\n", input, input.getFileSystem(conf).getFileStatus(input).getLen());            tooSmallFilesMerged = true;        }        writer.appendFile(HadoopInputFile.fromPath(input, conf));    }    if (tooSmallFilesMerged) {        out.println("Warning: you merged too small files. " + "Although the size of the merged file is bigger, it STILL contains small row groups, thus you don't have the advantage of big row groups, " + "which usually leads to bad query performance!");    }    writer.end(mergedMeta.getKeyValueMetaData());}
private FileMetaData parquet-mr_f7799_0(List<Path> inputFiles) throws IOException
{    return ParquetFileWriter.mergeMetadataFiles(inputFiles, conf).getFileMetaData();}
private List<Path> parquet-mr_f7800_0(List<String> input) throws IOException
{    List<Path> inputFiles = null;    if (input.size() == 1) {        Path p = new Path(input.get(0));        FileSystem fs = p.getFileSystem(conf);        FileStatus status = fs.getFileStatus(p);        if (status.isDir()) {            inputFiles = getInputFilesFromDirectory(status);        }    } else {        inputFiles = parseInputFiles(input);    }    checkParquetFiles(inputFiles);    return inputFiles;}
private void parquet-mr_f7801_0(List<Path> inputFiles) throws IOException
{    if (inputFiles == null || inputFiles.size() <= 1) {        throw new IllegalArgumentException("Not enough files to merge");    }    for (Path inputFile : inputFiles) {        FileSystem fs = inputFile.getFileSystem(conf);        FileStatus status = fs.getFileStatus(inputFile);        if (status.isDir()) {            throw new IllegalArgumentException("Illegal parquet file: " + inputFile.toUri());        }    }}
private List<Path> parquet-mr_f7802_0(FileStatus partitionDir) throws IOException
{    FileSystem fs = partitionDir.getPath().getFileSystem(conf);    FileStatus[] inputFiles = fs.listStatus(partitionDir.getPath(), HiddenFileFilter.INSTANCE);    List<Path> input = new ArrayList<Path>();    for (FileStatus f : inputFiles) {        input.add(f.getPath());    }    return input;}
private List<Path> parquet-mr_f7803_0(List<String> input)
{    List<Path> inputFiles = new ArrayList<Path>();    for (String name : input) {        inputFiles.add(new Path(name));    }    return inputFiles;}
 static void parquet-mr_f7804_0(PrettyPrintWriter out, ParquetMetadata meta, boolean showOriginalTypes)
{    showDetails(out, meta.getFileMetaData(), showOriginalTypes);    long i = 1;    for (BlockMetaData bmeta : meta.getBlocks()) {        out.println();        showDetails(out, bmeta, i++);    }}
 static void parquet-mr_f7805_0(PrettyPrintWriter out, FileMetaData meta, boolean showOriginalTypes)
{    out.format("creator: %s%n", meta.getCreatedBy());    Map<String, String> extra = meta.getKeyValueMetaData();    if (extra != null) {        for (Map.Entry<String, String> entry : meta.getKeyValueMetaData().entrySet()) {            out.print("extra: ");            out.incrementTabLevel();            out.format("%s = %s%n", entry.getKey(), entry.getValue());            out.decrementTabLevel();        }    }    out.println();    out.format("file schema: %s%n", meta.getSchema().getName());    out.rule('-');    showDetails(out, meta.getSchema(), showOriginalTypes);}
private static void parquet-mr_f7806_0(PrettyPrintWriter out, BlockMetaData meta, Long num)
{    long rows = meta.getRowCount();    long tbs = meta.getTotalByteSize();    long offset = meta.getStartingPos();    out.format("row group%s: RC:%d TS:%d OFFSET:%d%n", (num == null ? "" : " " + num), rows, tbs, offset);    out.rule('-');    showDetails(out, meta.getColumns());}
 static void parquet-mr_f7807_0(PrettyPrintWriter out, List<ColumnChunkMetaData> ccmeta)
{    Map<String, Object> chunks = new LinkedHashMap<String, Object>();    for (ColumnChunkMetaData cmeta : ccmeta) {        String[] path = cmeta.getPath().toArray();        Map<String, Object> current = chunks;        for (int i = 0; i < path.length - 1; ++i) {            String next = path[i];            if (!current.containsKey(next)) {                current.put(next, new LinkedHashMap<String, Object>());            }            current = (Map<String, Object>) current.get(next);        }        current.put(path[path.length - 1], cmeta);    }    showColumnChunkDetails(out, chunks, 0);}
private static void parquet-mr_f7808_0(PrettyPrintWriter out, Map<String, Object> current, int depth)
{    for (Map.Entry<String, Object> entry : current.entrySet()) {        String name = Strings.repeat(".", depth) + entry.getKey();        Object value = entry.getValue();        if (value instanceof Map) {            out.println(name + ": ");            showColumnChunkDetails(out, (Map<String, Object>) value, depth + 1);        } else {            out.print(name + ": ");            showDetails(out, (ColumnChunkMetaData) value, false);        }    }}
private static void parquet-mr_f7809_0(PrettyPrintWriter out, ColumnChunkMetaData meta, boolean name)
{    long doff = meta.getDictionaryPageOffset();    long foff = meta.getFirstDataPageOffset();    long tsize = meta.getTotalSize();    long usize = meta.getTotalUncompressedSize();    long count = meta.getValueCount();    double ratio = usize / (double) tsize;    String encodings = Joiner.on(',').skipNulls().join(meta.getEncodings());    if (name) {        String path = Joiner.on('.').skipNulls().join(meta.getPath());        out.format("%s: ", path);    }    out.format(" %s", meta.getType());    out.format(" %s", meta.getCodec());    out.format(" DO:%d", doff);    out.format(" FPO:%d", foff);    out.format(" SZ:%d/%d/%.2f", tsize, usize, ratio);    out.format(" VC:%d", count);    if (!encodings.isEmpty())        out.format(" ENC:%s", encodings);    Statistics<?> stats = meta.getStatistics();    if (stats != null) {        out.format(" ST:[%s]", stats);    } else {        out.format(" ST:[none]");    }    out.println();}
 static void parquet-mr_f7810_0(PrettyPrintWriter out, MessageType type, boolean showOriginalTypes)
{    List<String> cpath = new ArrayList<String>();    for (Type ftype : type.getFields()) {        showDetails(out, ftype, 0, type, cpath, showOriginalTypes);    }}
private static void parquet-mr_f7811_0(PrettyPrintWriter out, GroupType type, int depth, MessageType container, List<String> cpath, boolean showOriginalTypes)
{    String name = Strings.repeat(".", depth) + type.getName();    Repetition rep = type.getRepetition();    int fcount = type.getFieldCount();    out.format("%s: %s F:%d%n", name, rep, fcount);    cpath.add(type.getName());    for (Type ftype : type.getFields()) {        showDetails(out, ftype, depth + 1, container, cpath, showOriginalTypes);    }    cpath.remove(cpath.size() - 1);}
private static void parquet-mr_f7812_0(PrettyPrintWriter out, PrimitiveType type, int depth, MessageType container, List<String> cpath, boolean showOriginalTypes)
{    String name = Strings.repeat(".", depth) + type.getName();    Repetition rep = type.getRepetition();    PrimitiveTypeName ptype = type.getPrimitiveTypeName();    out.format("%s: %s %s", name, rep, ptype);    if (showOriginalTypes) {        OriginalType otype;        try {            otype = type.getOriginalType();        } catch (Exception e) {            otype = null;        }        if (otype != null)            out.format(" O:%s", otype);    } else {        LogicalTypeAnnotation ltype = type.getLogicalTypeAnnotation();        if (ltype != null)            out.format(" L:%s", ltype);    }    if (container != null) {        cpath.add(type.getName());        String[] paths = cpath.toArray(new String[cpath.size()]);        cpath.remove(cpath.size() - 1);        ColumnDescriptor desc = container.getColumnDescription(paths);        int defl = desc.getMaxDefinitionLevel();        int repl = desc.getMaxRepetitionLevel();        out.format(" R:%d D:%d", repl, defl);    }    out.println();}
private static void parquet-mr_f7813_0(PrettyPrintWriter out, Type type, int depth, MessageType container, List<String> cpath, boolean showOriginalTypes)
{    if (type instanceof GroupType) {        showDetails(out, type.asGroupType(), depth, container, cpath, showOriginalTypes);        return;    } else if (type instanceof PrimitiveType) {        showDetails(out, type.asPrimitiveType(), depth, container, cpath, showOriginalTypes);        return;    }}
public static Map<String, Command> parquet-mr_f7814_0()
{    Map<String, Command> results = new LinkedHashMap<String, Command>();    for (Map.Entry<String, Class<? extends Command>> entry : registry.entrySet()) {        try {            results.put(entry.getKey(), entry.getValue().newInstance());        } catch (Exception ex) {        }    }    return results;}
public static Command parquet-mr_f7815_0(String name)
{    Class<? extends Command> clazz = registry.get(name);    if (clazz == null) {        return null;    }    try {        return clazz.newInstance();    } catch (Exception ex) {        return null;    }}
public Options parquet-mr_f7816_0()
{    return OPTIONS;}
public String[] parquet-mr_f7817_0()
{    return USAGE;}
public String parquet-mr_f7818_0()
{    return "Prints the count of rows in Parquet file(s)";}
public void parquet-mr_f7819_0(CommandLine options) throws Exception
{    super.execute(options);    String[] args = options.getArgs();    String input = args[0];    out = new PrintWriter(Main.out, true);    inputPath = new Path(input);    conf = new Configuration();    inputFileStatuses = inputPath.getFileSystem(conf).globStatus(inputPath);    long rowCount = 0;    for (FileStatus fs : inputFileStatuses) {        long fileRowCount = 0;        for (Footer f : ParquetFileReader.readFooters(conf, fs, false)) {            for (BlockMetaData b : f.getParquetMetadata().getBlocks()) {                rowCount += b.getRowCount();                fileRowCount += b.getRowCount();            }        }        if (options.hasOption('d')) {            out.format("%s row count: %d\n", fs.getPath().getName(), fileRowCount);        }    }    out.format("Total RowCount: %d", rowCount);    out.println();}
public String[] parquet-mr_f7820_0()
{    return USAGE;}
public String parquet-mr_f7821_0()
{    return "Prints the metadata of Parquet file(s)";}
public Options parquet-mr_f7822_0()
{    return OPTIONS;}
public void parquet-mr_f7823_0(CommandLine options) throws Exception
{    super.execute(options);    String[] args = options.getArgs();    String input = args[0];    boolean showOriginalTypes = options.hasOption('o');    Configuration conf = new Configuration();    Path inputPath = new Path(input);    FileStatus inputFileStatus = inputPath.getFileSystem(conf).getFileStatus(inputPath);    List<Footer> footers = ParquetFileReader.readFooters(conf, inputFileStatus, false);    PrettyPrintWriter out = PrettyPrintWriter.stdoutPrettyPrinter().withAutoColumn().withWhitespaceHandler(WhiteSpaceHandler.COLLAPSE_WHITESPACE).withColumnPadding(1).build();    for (Footer f : footers) {        out.format("file: %s%n", f.getFile());        MetadataUtils.showDetails(out, f.getParquetMetadata(), showOriginalTypes);        out.flushColumns();    }}
public String[] parquet-mr_f7824_0()
{    return USAGE;}
public String parquet-mr_f7825_0()
{    return "Prints the schema of Parquet file(s)";}
public Options parquet-mr_f7826_0()
{    return OPTIONS;}
public void parquet-mr_f7827_0(CommandLine options) throws Exception
{    super.execute(options);    String[] args = options.getArgs();    String input = args[0];    Configuration conf = new Configuration();    ParquetMetadata metaData;    Path path = new Path(input);    FileSystem fs = path.getFileSystem(conf);    Path file;    if (fs.isDirectory(path)) {        FileStatus[] statuses = fs.listStatus(path, HiddenFileFilter.INSTANCE);        if (statuses.length == 0) {            throw new RuntimeException("Directory " + path.toString() + " is empty");        }        file = statuses[0].getPath();    } else {        file = path;    }    metaData = ParquetFileReader.readFooter(conf, file, NO_FILTER);    MessageType schema = metaData.getFileMetaData().getSchema();    Main.out.println(schema);    if (options.hasOption('d')) {        boolean showOriginalTypes = options.hasOption('o');        PrettyPrintWriter out = PrettyPrintWriter.stdoutPrettyPrinter().build();        MetadataUtils.showDetails(out, metaData, showOriginalTypes);    }}
public Options parquet-mr_f7828_0()
{    return OPTIONS;}
public String[] parquet-mr_f7829_0()
{    return USAGE;}
public String parquet-mr_f7830_0()
{    return "Prints the size of Parquet file(s)";}
public void parquet-mr_f7831_0(CommandLine options) throws Exception
{    super.execute(options);    String[] args = options.getArgs();    String input = args[0];    out = new PrintWriter(Main.out, true);    inputPath = new Path(input);    conf = new Configuration();    inputFileStatuses = inputPath.getFileSystem(conf).globStatus(inputPath);    long size = 0;    for (FileStatus fs : inputFileStatuses) {        long fileSize = 0;        for (Footer f : ParquetFileReader.readFooters(conf, fs, false)) {            for (BlockMetaData b : f.getParquetMetadata().getBlocks()) {                size += (options.hasOption('u') ? b.getTotalByteSize() : b.getCompressedSize());                fileSize += (options.hasOption('u') ? b.getTotalByteSize() : b.getCompressedSize());            }        }        if (options.hasOption('d')) {            if (options.hasOption('p')) {                out.format("%s: %s\n", fs.getPath().getName(), getPrettySize(fileSize));            } else {                out.format("%s: %d bytes\n", fs.getPath().getName(), fileSize);            }        }    }    if (options.hasOption('p')) {        out.format("Total Size: %s", getPrettySize(size));    } else {        out.format("Total Size: %d bytes", size);    }    out.println();}
public String parquet-mr_f7832_0(long bytes)
{    if (bytes / ONE_KB < 1) {        return String.format("%d", bytes) + " bytes";    }    if (bytes / ONE_MB < 1) {        return String.format("%.3f", bytes / ONE_KB) + " KB";    }    if (bytes / ONE_GB < 1) {        return String.format("%.3f", bytes / ONE_MB) + " MB";    }    if (bytes / ONE_TB < 1) {        return String.format("%.3f", bytes / ONE_GB) + " GB";    }    if (bytes / ONE_PB < 1) {        return String.format("%.3f", bytes / ONE_TB) + " TB";    }    return String.format("%.3f", bytes / ONE_PB) + " PB";}
protected Object parquet-mr_f7833_0(List<Object> listOfValues)
{    if (super.typeInfo.getRepetition() == Type.Repetition.REPEATED) {        return listOfValues;    } else {        return listOfValues.get(SINGLE_VALUE);    }}
private Map<String, JsonRecordFormatter> parquet-mr_f7834_0(GroupType groupSchema)
{    Map<String, JsonRecordFormatter> writers = new LinkedHashMap<String, JsonRecordFormatter>();    for (Type type : groupSchema.getFields()) {        if (type.isPrimitive()) {            writers.put(type.getName(), new JsonPrimitiveWriter(type));        } else {            writers.put(type.getName(), new JsonGroupFormatter((GroupType) type));        }    }    return writers;}
private Object parquet-mr_f7835_0(SimpleRecord record)
{    return formatEntries(collateEntries(record));}
private Map<String, List<Object>> parquet-mr_f7836_0(SimpleRecord record)
{    Map<String, List<Object>> collatedEntries = new LinkedHashMap<String, List<Object>>();    for (SimpleRecord.NameValue value : record.getValues()) {        if (collatedEntries.containsKey(value.getName())) {            collatedEntries.get(value.getName()).add(value.getValue());        } else {            List<Object> newResultListForKey = new ArrayList<Object>();            newResultListForKey.add(value.getValue());            collatedEntries.put(value.getName(), newResultListForKey);        }    }    return collatedEntries;}
private Object parquet-mr_f7837_0(Map<String, List<Object>> entries)
{    Map<String, Object> results = new LinkedHashMap<String, Object>();    for (Map.Entry<String, List<Object>> entry : entries.entrySet()) {        JsonRecordFormatter formatter = formatters.get(entry.getKey());        results.put(entry.getKey(), formatter.formatResults(entry.getValue()));    }    return results;}
protected Object parquet-mr_f7838_0(List<SimpleRecord> values)
{    if (super.typeInfo.getRepetition() == Type.Repetition.REPEATED) {        List<Object> results = new ArrayList<Object>();        for (SimpleRecord object : values) {            results.add(add(object));        }        return results;    } else {        return add(values.get(SINGLE_VALUE));    }}
public String parquet-mr_f7839_0(SimpleRecord value) throws IOException
{    ObjectMapper mapper = new ObjectMapper();    return mapper.writeValueAsString(add(value));}
public static JsonGroupFormatter parquet-mr_f7840_0(MessageType messageType)
{    return new JsonGroupFormatter(messageType);}
public static void parquet-mr_f7841_0(Options opt, Options opts)
{    if (opts == null) {        return;    }    Collection<Option> all = opts.getOptions();    if (all != null && !all.isEmpty()) {        for (Option o : all) {            opt.addOption(o);        }    }}
public static Options parquet-mr_f7842_0(Options opt, Options... opts)
{    Options results = new Options();    mergeOptionsInto(results, opt);    for (Options o : opts) {        mergeOptionsInto(results, o);    }    return results;}
public static void parquet-mr_f7843_0(HelpFormatter format, PrintWriter err, String name, Command command)
{    Options options = mergeOptions(OPTIONS, command.getOptions());    String[] usage = command.getUsageDescription();    String ustr = name + " [option...]";    if (usage != null && usage.length >= 1) {        ustr = ustr + " " + usage[0];    }    format.printWrapped(err, WIDTH, name + ":\n" + command.getCommandDescription());    format.printUsage(err, WIDTH, ustr);    format.printWrapped(err, WIDTH, LEFT_PAD, "where option is one of:");    format.printOptions(err, WIDTH, options, LEFT_PAD, DESC_PAD);    if (usage != null && usage.length >= 2) {        for (int i = 1; i < usage.length; ++i) {            format.printWrapped(err, WIDTH, LEFT_PAD, usage[i]);        }    }}
public static void parquet-mr_f7844_0(String name, Command command)
{    HelpFormatter format = new HelpFormatter();    PrintWriter err = new PrintWriter(Main.err, true);    Options options = command.getOptions();    showUsage(format, err, "parquet-" + name, command);}
public static void parquet-mr_f7845_0()
{    HelpFormatter format = new HelpFormatter();    PrintWriter err = new PrintWriter(Main.err, true);    Map<String, Command> all = Registry.allCommands();    boolean first = true;    for (Map.Entry<String, Command> entry : all.entrySet()) {        String name = entry.getKey();        Command command = entry.getValue();        if (!first)            err.println();        first = false;        showUsage(format, err, "parquet-tools " + name, command);    }}
public static void parquet-mr_f7846_0(String message, boolean usage)
{    die(message, usage, null, null);}
public static void parquet-mr_f7847_0(Throwable th, boolean usage)
{    die(th, usage, null, null);}
public static void parquet-mr_f7848_0(String message, boolean usage, String name, Command command)
{    if (message != null) {        Main.err.println(message);        Main.err.println();    }    if (usage) {        if (name == null && command == null) {            showUsage();        } else {            showUsage(name, command);        }    }    System.exit(1);}
public static void parquet-mr_f7849_0(Throwable th, boolean usage, String name, Command command)
{    die(th.toString(), usage, name, command);}
public static void parquet-mr_f7850_0(String[] args)
{    Main.out = System.out;    Main.err = System.err;    PrintStream VoidStream = new PrintStream(new OutputStream() {        @Override        public void write(int b) throws IOException {        }        @Override        public void write(byte[] b) throws IOException {        }        @Override        public void write(byte[] b, int off, int len) throws IOException {        }        @Override        public void flush() throws IOException {        }        @Override        public void close() throws IOException {        }    });    System.setOut(VoidStream);    System.setErr(VoidStream);    if (args.length == 0) {        die("No command specified", true, null, null);    }    String name = args[0];    if ("-h".equals(name) || "--help".equals(name)) {        showUsage();        System.exit(0);    }    Command command = Registry.getCommandByName(name);    if (command == null) {        die("Unknown command: " + name, true, null, null);    }    boolean debug = false;    try {        String[] cargs = Arrays.copyOfRange(args, 1, args.length);        Options opts = mergeOptions(OPTIONS, command.getOptions());        boolean extra = command.supportsExtraArgs();        CommandLineParser parser = new PosixParser();        CommandLine cmd = parser.parse(opts == null ? new Options() : opts, cargs, extra);        if (cmd.hasOption('h')) {            showUsage(name, command);            System.exit(0);        }        if (cmd.hasOption("no-color")) {            System.setProperty("DISABLE_COLORS", "true");        }        debug = cmd.hasOption("debug");        command.execute(cmd);    } catch (ParseException ex) {        if (debug)            ex.printStackTrace(Main.err);        die("Invalid arguments: " + ex.getMessage(), true, name, command);    } catch (Throwable th) {        if (debug)            th.printStackTrace(Main.err);        die(th, false, name, command);    }}
public void parquet-mr_f7851_0(int b) throws IOException
{}
public void parquet-mr_f7852_0(byte[] b) throws IOException
{}
public void parquet-mr_f7853_0(byte[] b, int off, int len) throws IOException
{}
public void parquet-mr_f7854_0() throws IOException
{}
public void parquet-mr_f7855_0() throws IOException
{}
protected Object parquet-mr_f7856_0()
{    Object[] result = new Object[values.size()];    for (int i = 0; i < values.size(); i++) {        result[i] = toJsonValue(values.get(i).getValue());    }    return result;}
public void parquet-mr_f7857_0()
{    record = new SimpleListRecord();}
protected Object parquet-mr_f7858_0()
{    Map<String, Object> result = Maps.newLinkedHashMap();    for (NameValue value : values) {        String key = null;        Object val = null;        for (NameValue kv : ((SimpleRecord) value.getValue()).values) {            String kvName = kv.getName();            Object kvValue = kv.getValue();            if (kvName.equals("key")) {                key = keyToString(kvValue);            } else if (kvName.equals("value")) {                val = toJsonValue(kvValue);            }        }        result.put(key, val);    }    return result;}
 String parquet-mr_f7859_0(Object kvValue)
{    if (kvValue == null) {        return "null";    }    Class<?> type = kvValue.getClass();    if (type.isArray()) {        if (type.getComponentType() == boolean.class) {            return Arrays.toString((boolean[]) kvValue);        } else if (type.getComponentType() == byte.class) {            return new BinaryNode((byte[]) kvValue).asText();        } else if (type.getComponentType() == char.class) {            return Arrays.toString((char[]) kvValue);        } else if (type.getComponentType() == double.class) {            return Arrays.toString((double[]) kvValue);        } else if (type.getComponentType() == float.class) {            return Arrays.toString((float[]) kvValue);        } else if (type.getComponentType() == int.class) {            return Arrays.toString((int[]) kvValue);        } else if (type.getComponentType() == long.class) {            return Arrays.toString((long[]) kvValue);        } else if (type.getComponentType() == short.class) {            return Arrays.toString((short[]) kvValue);        } else {            return Arrays.toString((Object[]) kvValue);        }    } else {        return String.valueOf(kvValue);    }}
public void parquet-mr_f7860_0()
{    record = new SimpleMapRecord();}
public RecordMaterializer<SimpleRecord> parquet-mr_f7861_0(Configuration conf, Map<String, String> metaData, MessageType schema, ReadContext context)
{    return new SimpleRecordMaterializer(schema);}
public ReadContext parquet-mr_f7862_0(InitContext context)
{    return new ReadContext(context.getFileSchema());}
public void parquet-mr_f7863_0(String name, Object value)
{    values.add(new NameValue(name, value));}
public List<NameValue> parquet-mr_f7864_0()
{    return Collections.unmodifiableList(values);}
public String parquet-mr_f7865_0()
{    return values.toString();}
public void parquet-mr_f7866_0()
{    prettyPrint(new PrintWriter(System.out, true));}
public void parquet-mr_f7867_0(PrintWriter out)
{    prettyPrint(out, 0);}
public void parquet-mr_f7868_0(PrintWriter out, int depth)
{    for (NameValue value : values) {        out.print(Strings.repeat(".", depth));        out.print(value.getName());        Object val = value.getValue();        if (val == null) {            out.print(" = ");            out.print("<null>");        } else if (byte[].class == val.getClass()) {            out.print(" = ");            out.print(new BinaryNode((byte[]) val).asText());        } else if (short[].class == val.getClass()) {            out.print(" = ");            out.print(Arrays.toString((short[]) val));        } else if (int[].class == val.getClass()) {            out.print(" = ");            out.print(Arrays.toString((int[]) val));        } else if (long[].class == val.getClass()) {            out.print(" = ");            out.print(Arrays.toString((long[]) val));        } else if (float[].class == val.getClass()) {            out.print(" = ");            out.print(Arrays.toString((float[]) val));        } else if (double[].class == val.getClass()) {            out.print(" = ");            out.print(Arrays.toString((double[]) val));        } else if (boolean[].class == val.getClass()) {            out.print(" = ");            out.print(Arrays.toString((boolean[]) val));        } else if (val.getClass().isArray()) {            out.print(" = ");            out.print(Arrays.deepToString((Object[]) val));        } else if (SimpleRecord.class.isAssignableFrom(val.getClass())) {            out.println(":");            ((SimpleRecord) val).prettyPrint(out, depth + 1);            continue;        } else {            out.print(" = ");            out.print(String.valueOf(val));        }        out.println();    }}
public void parquet-mr_f7869_0(PrintWriter out) throws IOException
{    ObjectMapper mapper = new ObjectMapper();    out.write(mapper.writeValueAsString(this.toJsonObject()));}
protected Object parquet-mr_f7870_0()
{    Map<String, Object> result = Maps.newLinkedHashMap();    for (NameValue value : values) {        result.put(value.getName(), toJsonValue(value.getValue()));    }    return result;}
protected static Object parquet-mr_f7871_0(Object val)
{    if (SimpleRecord.class.isAssignableFrom(val.getClass())) {        return ((SimpleRecord) val).toJsonObject();    } else if (byte[].class == val.getClass()) {        return new BinaryNode((byte[]) val);    } else {        return val;    }}
public String parquet-mr_f7872_0()
{    return name + ": " + value;}
public String parquet-mr_f7873_0()
{    return name;}
public Object parquet-mr_f7874_0()
{    return value;}
private Converter parquet-mr_f7875_0(Type field)
{    LogicalTypeAnnotation ltype = field.getLogicalTypeAnnotation();    if (field.isPrimitive()) {        if (ltype != null) {            return ltype.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<Converter>() {                @Override                public Optional<Converter> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType) {                    return of(new StringConverter(field.getName()));                }                @Override                public Optional<Converter> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {                    int scale = decimalLogicalType.getScale();                    return of(new DecimalConverter(field.getName(), scale));                }            }).orElse(new SimplePrimitiveConverter(field.getName()));        }        return new SimplePrimitiveConverter(field.getName());    }    GroupType groupType = field.asGroupType();    if (ltype != null) {        return ltype.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<Converter>() {            @Override            public Optional<Converter> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType) {                return of(new SimpleMapRecordConverter(groupType, field.getName(), SimpleRecordConverter.this));            }            @Override            public Optional<Converter> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType) {                return of(new SimpleListRecordConverter(groupType, field.getName(), SimpleRecordConverter.this));            }        }).orElse(new SimpleRecordConverter(groupType, field.getName(), this));    }    return new SimpleRecordConverter(groupType, field.getName(), this);}
public Optional<Converter> parquet-mr_f7876_0(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    return of(new StringConverter(field.getName()));}
public Optional<Converter> parquet-mr_f7877_0(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    int scale = decimalLogicalType.getScale();    return of(new DecimalConverter(field.getName(), scale));}
public Optional<Converter> parquet-mr_f7878_0(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    return of(new SimpleMapRecordConverter(groupType, field.getName(), SimpleRecordConverter.this));}
public Optional<Converter> parquet-mr_f7879_0(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    return of(new SimpleListRecordConverter(groupType, field.getName(), SimpleRecordConverter.this));}
public Converter parquet-mr_f7880_0(int fieldIndex)
{    return converters[fieldIndex];}
 SimpleRecord parquet-mr_f7881_0()
{    return record;}
public void parquet-mr_f7882_0()
{    record = new SimpleRecord();}
public void parquet-mr_f7883_0()
{    if (parent != null) {        parent.getCurrentRecord().add(name, record);    }}
public void parquet-mr_f7884_0(Binary value)
{    record.add(name, value.getBytes());}
public void parquet-mr_f7885_0(boolean value)
{    record.add(name, value);}
public void parquet-mr_f7886_0(double value)
{    record.add(name, value);}
public void parquet-mr_f7887_0(float value)
{    record.add(name, value);}
public void parquet-mr_f7888_0(int value)
{    record.add(name, value);}
public void parquet-mr_f7889_0(long value)
{    record.add(name, value);}
public void parquet-mr_f7890_0(Binary value)
{    record.add(name, value.toStringUsingUTF8());}
public void parquet-mr_f7891_0(Binary value)
{    record.add(name, new BigDecimal(new BigInteger(value.getBytes()), scale));}
public void parquet-mr_f7892_0(int value)
{    record.add(name, BigDecimal.valueOf(value).movePointLeft(scale));}
public void parquet-mr_f7893_0(long value)
{    record.add(name, BigDecimal.valueOf(value).movePointLeft(scale));}
public SimpleRecord parquet-mr_f7894_0()
{    return root.getCurrentRecord();}
public GroupConverter parquet-mr_f7895_0()
{    return root;}
public static void parquet-mr_f7896_0(PrettyPrintWriter out, ParquetMetadata meta)
{    showDetails(out, meta.getFileMetaData());    long i = 1;    for (BlockMetaData bmeta : meta.getBlocks()) {        out.println();        showDetails(out, bmeta, i++);    }}
public static void parquet-mr_f7897_0(PrettyPrintWriter out, FileMetaData meta)
{    out.format("creator: %s%n", meta.getCreatedBy());    Map<String, String> extra = meta.getKeyValueMetaData();    if (extra != null) {        for (Map.Entry<String, String> entry : meta.getKeyValueMetaData().entrySet()) {            out.print("extra: ");            out.incrementTabLevel();            out.format("%s = %s%n", entry.getKey(), entry.getValue());            out.decrementTabLevel();        }    }    out.println();    out.format("file schema: %s%n", meta.getSchema().getName());    out.rule('-');    showDetails(out, meta.getSchema());}
public static void parquet-mr_f7898_0(PrettyPrintWriter out, BlockMetaData meta)
{    showDetails(out, meta, null);}
private static void parquet-mr_f7899_0(PrettyPrintWriter out, BlockMetaData meta, Long num)
{    long rows = meta.getRowCount();    long tbs = meta.getTotalByteSize();    long offset = meta.getStartingPos();    out.format("row group%s: RC:%d TS:%d OFFSET:%d%n", (num == null ? "" : " " + num), rows, tbs, offset);    out.rule('-');    showDetails(out, meta.getColumns());}
public static void parquet-mr_f7900_0(PrettyPrintWriter out, List<ColumnChunkMetaData> ccmeta)
{    Map<String, Object> chunks = new LinkedHashMap<String, Object>();    for (ColumnChunkMetaData cmeta : ccmeta) {        String[] path = cmeta.getPath().toArray();        Map<String, Object> current = chunks;        for (int i = 0; i < path.length - 1; ++i) {            String next = path[i];            if (!current.containsKey(next)) {                current.put(next, new LinkedHashMap<String, Object>());            }            current = (Map<String, Object>) current.get(next);        }        current.put(path[path.length - 1], cmeta);    }    showColumnChunkDetails(out, chunks, 0);}
private static void parquet-mr_f7901_0(PrettyPrintWriter out, Map<String, Object> current, int depth)
{    for (Map.Entry<String, Object> entry : current.entrySet()) {        String name = Strings.repeat(".", depth) + entry.getKey();        Object value = entry.getValue();        if (value instanceof Map) {            out.println(name + ": ");            showColumnChunkDetails(out, (Map<String, Object>) value, depth + 1);        } else {            out.print(name + ": ");            showDetails(out, (ColumnChunkMetaData) value, false);        }    }}
public static void parquet-mr_f7902_0(PrettyPrintWriter out, ColumnChunkMetaData meta)
{    showDetails(out, meta, true);}
private static void parquet-mr_f7903_0(PrettyPrintWriter out, ColumnChunkMetaData meta, boolean name)
{    long doff = meta.getDictionaryPageOffset();    long foff = meta.getFirstDataPageOffset();    long tsize = meta.getTotalSize();    long usize = meta.getTotalUncompressedSize();    long count = meta.getValueCount();    double ratio = usize / (double) tsize;    String encodings = Joiner.on(',').skipNulls().join(meta.getEncodings());    if (name) {        String path = Joiner.on('.').skipNulls().join(meta.getPath());        out.format("%s: ", path);    }    out.format(" %s", meta.getType());    out.format(" %s", meta.getCodec());    out.format(" DO:%d", doff);    out.format(" FPO:%d", foff);    out.format(" SZ:%d/%d/%.2f", tsize, usize, ratio);    out.format(" VC:%d", count);    if (!encodings.isEmpty())        out.format(" ENC:%s", encodings);    Statistics<?> stats = meta.getStatistics();    if (stats != null) {        out.format(" ST:[%s]", stats);    } else {        out.format(" ST:[none]");    }    out.println();}
public static void parquet-mr_f7904_0(PrettyPrintWriter out, ColumnDescriptor desc)
{    String path = Joiner.on(".").skipNulls().join(desc.getPath());    PrimitiveTypeName type = desc.getType();    int defl = desc.getMaxDefinitionLevel();    int repl = desc.getMaxRepetitionLevel();    out.format("column desc: %s T:%s R:%d D:%d%n", path, type, repl, defl);}
public static void parquet-mr_f7905_0(PrettyPrintWriter out, MessageType type)
{    List<String> cpath = new ArrayList<String>();    for (Type ftype : type.getFields()) {        showDetails(out, ftype, 0, type, cpath);    }}
public static void parquet-mr_f7906_0(PrettyPrintWriter out, GroupType type)
{    showDetails(out, type, 0, null, null);}
public static void parquet-mr_f7907_0(PrettyPrintWriter out, PrimitiveType type)
{    showDetails(out, type, 0, null, null);}
public static void parquet-mr_f7908_0(PrettyPrintWriter out, Type type)
{    showDetails(out, type, 0, null, null);}
private static void parquet-mr_f7909_0(PrettyPrintWriter out, GroupType type, int depth, MessageType container, List<String> cpath)
{    String name = Strings.repeat(".", depth) + type.getName();    Repetition rep = type.getRepetition();    int fcount = type.getFieldCount();    out.format("%s: %s F:%d%n", name, rep, fcount);    cpath.add(type.getName());    for (Type ftype : type.getFields()) {        showDetails(out, ftype, depth + 1, container, cpath);    }    cpath.remove(cpath.size() - 1);}
private static void parquet-mr_f7910_0(PrettyPrintWriter out, PrimitiveType type, int depth, MessageType container, List<String> cpath)
{    String name = Strings.repeat(".", depth) + type.getName();    OriginalType otype = type.getOriginalType();    Repetition rep = type.getRepetition();    PrimitiveTypeName ptype = type.getPrimitiveTypeName();    out.format("%s: %s %s", name, rep, ptype);    if (otype != null)        out.format(" O:%s", otype);    if (container != null) {        cpath.add(type.getName());        String[] paths = cpath.toArray(new String[cpath.size()]);        cpath.remove(cpath.size() - 1);        ColumnDescriptor desc = container.getColumnDescription(paths);        int defl = desc.getMaxDefinitionLevel();        int repl = desc.getMaxRepetitionLevel();        out.format(" R:%d D:%d", repl, defl);    }    out.println();}
private static void parquet-mr_f7911_0(PrettyPrintWriter out, Type type, int depth, MessageType container, List<String> cpath)
{    if (type instanceof GroupType) {        showDetails(out, type.asGroupType(), depth, container, cpath);        return;    } else if (type instanceof PrimitiveType) {        showDetails(out, type.asPrimitiveType(), depth, container, cpath);        return;    }}
public void parquet-mr_f7912_0(int level)
{    this.tabLevel = level;    this.tabs = Strings.repeat(" ", tabWidth * level);    if (flushOnTab)        flushColumns();}
public void parquet-mr_f7913_0()
{    setTabLevel(tabLevel + 1);}
public void parquet-mr_f7914_0()
{    if (tabLevel == 0) {        return;    }    setTabLevel(tabLevel - 1);}
private int parquet-mr_f7915_0()
{    int max = 0;    for (Line line : buffer) {        int num = line.countCharacter(columnSeparator);        if (num > max) {            max = num;        }    }    return max > maxColumns ? maxColumns : max;}
private int[] parquet-mr_f7916_0()
{    int columns = determineNumColumns();    if (columns == 0) {        return null;    }    int[] widths = new int[columns];    for (Line line : buffer) {        for (int last = 0, idx = 0; last < line.length() && idx < columns; ++idx) {            int pos = line.indexOf(columnSeparator, last);            if (pos < 0)                break;            int wid = pos - last + 1 + columnPadding;            if (wid > widths[idx]) {                widths[idx] = wid;            }            last = line.firstNonWhiteSpace(idx + 1);        }    }    return widths;}
private Line parquet-mr_f7917_0(int[] widths, Line line) throws IOException
{    int last = 0;    for (int i = 0; i < widths.length; ++i) {        int width = widths[i];        int idx = line.indexOf(columnSeparator, last);        if (idx < 0)            break;        if ((idx + 1) <= width) {            line.spaceOut(width - (idx + 1), idx + 1);        }        last = line.firstNonWhiteSpace(idx + 1);    }    return line;}
public void parquet-mr_f7918_0()
{    flushColumns(false);}
private void parquet-mr_f7919_0(boolean preserveLast)
{    int size = buffer.size();    int[] widths = null;    if (autoColumn) {        widths = determineColumnWidths();    }    StringBuilder builder = new StringBuilder();    try {        for (int i = 0; i < size - 1; ++i) {            Line line = buffer.get(i);            if (widths != null) {                line = toColumns(widths, line);            }            fixupLine(line);            builder.setLength(0);            line.toString(builder);            super.out.append(builder.toString());            super.out.append(LINE_SEP);        }        if (!preserveLast) {            Line line = buffer.get(size - 1);            if (widths != null) {                line = toColumns(widths, line);            }            fixupLine(line);            builder.setLength(0);            line.toString(builder);            super.out.append(builder.toString());        }        super.out.flush();    } catch (IOException ex) {    }    Line addback = null;    if (preserveLast) {        addback = buffer.get(size - 1);    }    buffer.clear();    if (addback != null)        buffer.add(addback);    else        buffer.add(new Line());}
private void parquet-mr_f7920_0()
{    flushIfNeeded(false);}
private void parquet-mr_f7921_0(boolean preserveLast)
{    if (!autoColumn || buffer.size() > maxBufferedLines) {        flushColumns(preserveLast);    }}
private void parquet-mr_f7922_0(String s)
{    int size = buffer.size();    Line value = buffer.get(size - 1);    if (value.isEmpty()) {        value.append(tabs());    }    value.append(span(s));}
private void parquet-mr_f7923_0(Line line)
{    if (autoCrop) {        line.trimTo(consoleWidth, appendToLongLine);    }}
private void parquet-mr_f7924_0(String s, boolean mayHaveNewlines)
{    if (s == null) {        appendToCurrent("null");        return;    }    if (s.isEmpty()) {        return;    }    if (LINE_SEP.equals(s)) {        buffer.add(new Line());        flushIfNeeded();        return;    }    if (whiteSpaceHandler != null) {        boolean endswith = s.endsWith(LINE_SEP);        switch(whiteSpaceHandler) {            case ELIMINATE_NEWLINES:                s = s.replaceAll("\\r\\n|\\r|\\n", " ");                break;            case COLLAPSE_WHITESPACE:                s = s.replaceAll("\\s+", " ");                break;        }        mayHaveNewlines = endswith;        if (endswith)            s = s + LINE_SEP;    }    if (!mayHaveNewlines) {        appendToCurrent(s);        return;    }    String[] lines = s.split("\\r?\\n", -1);    appendToCurrent(lines[0]);    for (int i = 1; i < lines.length; ++i) {        String value = lines[i];        if (value.isEmpty()) {            buffer.add(new Line());        } else {            Line line = new Line();            line.append(tabs());            line.append(span(value, true));            buffer.add(line);        }    }    resetColor();    flushIfNeeded(true);}
public void parquet-mr_f7925_0(String s)
{    print(s, true);}
public void parquet-mr_f7926_0()
{    print(LINE_SEP, true);    flushIfNeeded();}
public void parquet-mr_f7927_0(String x)
{    print(x);    println();}
public void parquet-mr_f7928_0(boolean b)
{    print(String.valueOf(b), false);}
public void parquet-mr_f7929_0(char c)
{    print(String.valueOf(c), false);}
public void parquet-mr_f7930_0(int i)
{    print(String.valueOf(i), false);}
public void parquet-mr_f7931_0(long l)
{    print(String.valueOf(l), false);}
public void parquet-mr_f7932_0(float f)
{    print(String.valueOf(f), false);}
public void parquet-mr_f7933_0(double d)
{    print(String.valueOf(d), false);}
public void parquet-mr_f7934_0(char[] s)
{    print(String.valueOf(s), true);}
public void parquet-mr_f7935_0(Object obj)
{    print(String.valueOf(obj), true);}
public PrintWriter parquet-mr_f7936_0(String format, Object... args)
{    return printf(formatter.locale(), format, args);}
public PrintWriter parquet-mr_f7937_0(Locale l, String format, Object... args)
{    formatter.format(l, format, args);    String results = formatString.toString();    formatString.setLength(0);    print(results);    flushIfNeeded();    return this;}
public PrintWriter parquet-mr_f7938_0(String format, Object... args)
{    return printf(format, args);}
public PrintWriter parquet-mr_f7939_0(Locale l, String format, Object... args)
{    return printf(l, format, args);}
public PrintWriter parquet-mr_f7940_0(char c)
{    print(c);    return this;}
public PrintWriter parquet-mr_f7941_0(CharSequence csq)
{    if (csq == null) {        print("null");        return this;    }    return append(csq, 0, csq.length());}
public PrintWriter parquet-mr_f7942_0(CharSequence csq, int start, int end)
{    if (csq == null) {        print("null");        return this;    }    print(csq.subSequence(start, end).toString());    return this;}
public void parquet-mr_f7943_0(boolean x)
{    print(x);    println();}
public void parquet-mr_f7944_0(char x)
{    print(x);    println();}
public void parquet-mr_f7945_0(int x)
{    print(x);    println();}
public void parquet-mr_f7946_0(long x)
{    print(x);    println();}
public void parquet-mr_f7947_0(float x)
{    print(x);    println();}
public void parquet-mr_f7948_0(double x)
{    print(x);    println();}
public void parquet-mr_f7949_0(char[] x)
{    print(x);    println();}
public void parquet-mr_f7950_0(Object x)
{    print(x);    println();}
public void parquet-mr_f7951_0(char c)
{    if (tabs.length() >= consoleWidth)        return;    int width = consoleWidth;    if (width == Integer.MAX_VALUE) {        width = 100;    }    println(Strings.repeat(String.valueOf(c), width - tabs.length()));}
public PrettyPrintWriter parquet-mr_f7952_0(boolean predicate)
{    if (!predicate && acceptColorModification) {        resetColor();    } else {        acceptColorModification = false;    }    return this;}
public PrettyPrintWriter parquet-mr_f7953_0()
{    acceptColorModification = false;    return this;}
public PrettyPrintWriter parquet-mr_f7954_0()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_BLACK;    return this;}
public PrettyPrintWriter parquet-mr_f7955_0()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_RED;    return this;}
public PrettyPrintWriter parquet-mr_f7956_0()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_GREEN;    return this;}
public PrettyPrintWriter parquet-mr_f7957_0()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_YELLOW;    return this;}
public PrettyPrintWriter parquet-mr_f7958_0()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_BLUE;    return this;}
public PrettyPrintWriter parquet-mr_f7959_0()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_MAGENTA;    return this;}
public PrettyPrintWriter parquet-mr_f7960_0()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_CYAN;    return this;}
public PrettyPrintWriter parquet-mr_f7961_0()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_WHITE;    return this;}
public PrettyPrintWriter parquet-mr_f7962_0()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_BLACK;    return this;}
public PrettyPrintWriter parquet-mr_f7963_0()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_RED;    return this;}
public PrettyPrintWriter parquet-mr_f7964_0()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_GREEN;    return this;}
public PrettyPrintWriter parquet-mr_f7965_0()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_YELLOW;    return this;}
public PrettyPrintWriter parquet-mr_f7966_0()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_BLUE;    return this;}
public PrettyPrintWriter parquet-mr_f7967_0()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_MAGENTA;    return this;}
public PrettyPrintWriter parquet-mr_f7968_0()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_CYAN;    return this;}
public PrettyPrintWriter parquet-mr_f7969_0()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_WHITE;    return this;}
public PrettyPrintWriter parquet-mr_f7970_0()
{    if (!acceptColorModification)        return this;    colorMode = MODE_BOLD;    return this;}
public PrettyPrintWriter parquet-mr_f7971_0()
{    if (!acceptColorModification)        return this;    colorMode = MODE_BLINK;    return this;}
public PrettyPrintWriter parquet-mr_f7972_0()
{    if (!acceptColorModification)        return this;    colorMode = MODE_CONCEALED;    return this;}
public PrettyPrintWriter parquet-mr_f7973_0()
{    if (!acceptColorModification)        return this;    colorMode = MODE_OFF;    return this;}
public PrettyPrintWriter parquet-mr_f7974_0()
{    if (!acceptColorModification)        return this;    colorMode = MODE_UNDER;    return this;}
public PrettyPrintWriter parquet-mr_f7975_0()
{    if (!acceptColorModification)        return this;    colorMode = MODE_REVERSE;    return this;}
public static Builder parquet-mr_f7976_0()
{    return new Builder(Main.out).withAutoFlush();}
public static Builder parquet-mr_f7977_0()
{    return new Builder(Main.err).withAutoFlush();}
public static Builder parquet-mr_f7978_0(OutputStream out)
{    return new Builder(out);}
public Builder parquet-mr_f7979_0()
{    this.autoFlush = true;    return this;}
public Builder parquet-mr_f7980_0()
{    return withAutoCrop(DEFAULT_WIDTH);}
public Builder parquet-mr_f7981_0(int consoleWidth)
{    return withAutoCrop(consoleWidth, DEFAULT_APPEND);}
public Builder parquet-mr_f7982_0(int consoleWidth, String appendToLong)
{    return withAutoCrop(consoleWidth, mkspan(appendToLong));}
public Builder parquet-mr_f7983_0(int consoleWidth, Span appendToLong)
{    this.consoleWidth = consoleWidth;    this.appendToLongLine = appendToLong;    this.autoCrop = true;    return this;}
public Builder parquet-mr_f7984_0(int tabWidth)
{    this.tabWidth = tabWidth;    return this;}
public Builder parquet-mr_f7985_0()
{    return withAutoColumn(DEFAULT_COLUMN_SEP);}
public Builder parquet-mr_f7986_0(char columnSeparator)
{    return withAutoColumn(columnSeparator, DEFAULT_MAX_COLUMNS);}
public Builder parquet-mr_f7987_0(char columnSeparator, int maxColumns)
{    this.autoColumn = true;    this.columnSeparator = columnSeparator;    this.maxColumns = maxColumns;    return this;}
public Builder parquet-mr_f7988_0(int columnPadding)
{    this.columnPadding = columnPadding;    return this;}
public Builder parquet-mr_f7989_0(WhiteSpaceHandler whiteSpaceHandler)
{    this.whiteSpaceHandler = whiteSpaceHandler;    return this;}
public Builder parquet-mr_f7990_0(long maxBufferedLines)
{    this.maxBufferedLines = maxBufferedLines;    return this;}
public Builder parquet-mr_f7991_0()
{    this.flushOnTab = true;    return this;}
public PrettyPrintWriter parquet-mr_f7992_0()
{    return new PrettyPrintWriter(out, autoFlush, autoColumn, autoCrop, appendToLongLine, consoleWidth, tabWidth, columnSeparator, maxColumns, columnPadding, maxBufferedLines, flushOnTab, whiteSpaceHandler);}
private Span parquet-mr_f7993_0()
{    return new Span(tabs);}
private Span parquet-mr_f7994_0(String span)
{    return span(span, false);}
private void parquet-mr_f7995_0()
{    acceptColorModification = true;    colorMode = null;    colorForeground = null;    colorBackground = null;}
public static Span parquet-mr_f7996_0(String span)
{    return new Span(span);}
public static Span parquet-mr_f7997_0(String span, String color)
{    return mkspan(span, null, color, null);}
public static Span parquet-mr_f7998_0(String span, String colorMode, String colorForeground, String colorBackground)
{    if (DEFAULT_COLORS > 0 && (colorMode != null || colorForeground != null || colorBackground != null)) {        String color = "\u001B[" + Joiner.on(';').skipNulls().join(colorMode, colorForeground, colorBackground) + "m";        return new Span(span, color);    } else {        return mkspan(span);    }}
private Span parquet-mr_f7999_0(String span, boolean keepColor)
{    Span result;    if (DEFAULT_COLORS > 0 && (colorMode != null || colorForeground != null || colorBackground != null)) {        result = mkspan(span, colorMode, colorForeground, colorBackground);    } else {        result = mkspan(span);    }    if (!keepColor) {        resetColor();    }    return result;}
public void parquet-mr_f8000_0(Span span)
{    length += span.length();    if (spans.isEmpty()) {        spans.add(span);        return;    }    Span last = spans.get(spans.size() - 1);    if (last.canAppend(span)) {        last.append(span);    } else {        spans.add(span);    }}
public boolean parquet-mr_f8001_0()
{    return length == 0;}
public int parquet-mr_f8002_0()
{    return length;}
public int parquet-mr_f8003_0(char ch, int start)
{    int offset = 0;    for (Span span : spans) {        if (start > span.length()) {            start -= span.length();            continue;        }        int idx = span.indexOf(ch, start);        if (idx >= 0)            return offset + idx;        offset += span.length() - start;        start = 0;    }    return -1;}
public void parquet-mr_f8004_0(int width, int start)
{    for (Span span : spans) {        if (start > span.length()) {            start -= span.length();            continue;        }        span.spaceOut(width, start);        return;    }}
public int parquet-mr_f8005_0(int start)
{    return start;}
public int parquet-mr_f8006_0(char ch)
{    int result = 0;    for (Span span : spans) {        result += span.countCharacter(ch);    }    return result;}
public void parquet-mr_f8007_0(int width, Span appendToLongLine)
{    int i = 0;    int remaining = width;    for (i = 0; i < spans.size(); ++i) {        Span next = spans.get(i);        if (next.length() > remaining) {            ++i;            next.trimTo(remaining, appendToLongLine);            break;        }        remaining -= next.length();    }    for (; i < spans.size(); ++i) {        spans.remove(i);    }}
public void parquet-mr_f8008_0(StringBuilder builder)
{    for (Span span : spans) {        span.toString(builder);    }}
public int parquet-mr_f8009_0()
{    return span.length();}
public boolean parquet-mr_f8010_0()
{    return span.isEmpty();}
public int parquet-mr_f8011_0(char ch, int start)
{    return span.indexOf(ch, start);}
public void parquet-mr_f8012_0(int width, int start)
{    int removeTo = start;    while (removeTo < span.length() && Character.isWhitespace(span.charAt(removeTo))) {        removeTo++;    }    span = span.substring(0, start) + Strings.repeat(" ", width) + span.substring(removeTo);}
public int parquet-mr_f8013_0(char ch)
{    int result = 0;    for (int i = 0; i < span.length(); ++i) {        if (span.charAt(i) == ch) {            result++;        }    }    return result;}
public void parquet-mr_f8014_0(int width, Span appendToLongLine)
{    if (appendToLongLine != null && !appendToLongLine.isEmpty()) {        int shortten = appendToLongLine.length();        if (shortten > width)            shortten = width;        span = span.substring(0, width - shortten) + appendToLongLine;    } else {        span = span.substring(0, width + 1);    }}
public String parquet-mr_f8015_0()
{    StringBuilder builder = new StringBuilder();    toString(builder);    return builder.toString();}
public void parquet-mr_f8016_0(StringBuilder builder)
{    if (color != null)        builder.append(color);    builder.append(span);    if (color != null)        builder.append(RESET);}
public void parquet-mr_f8017_0(Span other)
{    span = span + other.span;}
public boolean parquet-mr_f8018_0(Span other)
{    if (color == null && other == null)        return true;    if (color == null && other != null)        return false;    return color.equals(other);}
private List<T> parquet-mr_f8019_0(T... objects)
{    return Arrays.asList(objects);}
private Map.Entry<String, T> parquet-mr_f8020_0(final String key, final T value)
{    return new Map.Entry<String, T>() {        @Override        public String getKey() {            return key;        }        @Override        public T getValue() {            return value;        }        @Override        public T setValue(T value) {            throw new UnsupportedOperationException();        }    };}
public String parquet-mr_f8021_0()
{    return key;}
public T parquet-mr_f8022_0()
{    return value;}
public T parquet-mr_f8023_0(T value)
{    throw new UnsupportedOperationException();}
private Map<String, ?> parquet-mr_f8024_0(Map.Entry<String, ?>... entries) throws IOException
{    Map<String, Object> entriesAsMap = new LinkedHashMap<String, Object>();    for (Map.Entry<String, ?> entry : entries) {        entriesAsMap.put(entry.getKey(), entry.getValue());    }    return entriesAsMap;}
private SimpleRecord.NameValue parquet-mr_f8025_0(String name, Object value)
{    return new SimpleRecord.NameValue(name, value);}
private String parquet-mr_f8026_0(Object object) throws IOException
{    ObjectMapper mapper = new ObjectMapper();    return mapper.writeValueAsString(object);}
public void parquet-mr_f8027_0() throws Exception
{    SimpleRecord simple = new SimpleRecord();    MessageType schema = new MessageType("schema", new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.BINARY, "reqd"), new PrimitiveType(Type.Repetition.OPTIONAL, PrimitiveType.PrimitiveTypeName.DOUBLE, "opt"), new PrimitiveType(Type.Repetition.REPEATED, PrimitiveType.PrimitiveTypeName.INT32, "odd"), new PrimitiveType(Type.Repetition.REPEATED, PrimitiveType.PrimitiveTypeName.INT64, "even"));    simple.values.add(kv("reqd", "a required value"));    simple.values.add(kv("opt", 1.2345));    simple.values.add(kv("odd", 1));    simple.values.add(kv("odd", 3));    simple.values.add(kv("odd", 5));    simple.values.add(kv("odd", 7));    simple.values.add(kv("odd", 9));    simple.values.add(kv("even", 2));    simple.values.add(kv("even", 4));    simple.values.add(kv("even", 6));    simple.values.add(kv("even", 8));    simple.values.add(kv("even", 10));    String expected = asJsonString(obj(entry("reqd", "a required value"), entry("opt", 1.2345), entry("odd", array(1, 3, 5, 7, 9)), entry("even", array(2, 4, 6, 8, 10))));    String actual = JsonRecordFormatter.fromSchema(schema).formatRecord(simple);    assertEquals(expected, actual);}
public void parquet-mr_f8028_0() throws Exception
{    SimpleRecord simple = new SimpleRecord();    MessageType schema = new MessageType("schema", new PrimitiveType(Type.Repetition.REPEATED, PrimitiveType.PrimitiveTypeName.BINARY, "flat-string"), new GroupType(Type.Repetition.OPTIONAL, "subgroup", new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.INT32, "flat-int"), new PrimitiveType(Type.Repetition.REPEATED, PrimitiveType.PrimitiveTypeName.BINARY, "string-list")));    SimpleRecord subgroup = new SimpleRecord();    subgroup.values.add(kv("flat-int", 12345));    subgroup.values.add(kv("string-list", "two"));    subgroup.values.add(kv("string-list", "four"));    subgroup.values.add(kv("string-list", "six"));    subgroup.values.add(kv("string-list", "eight"));    subgroup.values.add(kv("string-list", "ten"));    simple.values.add(kv("flat-string", "one"));    simple.values.add(kv("flat-string", "two"));    simple.values.add(kv("flat-string", "three"));    simple.values.add(kv("flat-string", "four"));    simple.values.add(kv("flat-string", "five"));    simple.values.add(kv("subgroup", subgroup));    String actual = JsonRecordFormatter.fromSchema(schema).formatRecord(simple);    String expected = asJsonString(obj(entry("flat-string", array("one", "two", "three", "four", "five")), entry("subgroup", obj(entry("flat-int", 12345), entry("string-list", array("two", "four", "six", "eight", "ten"))))));    assertEquals(expected, actual);}
public void parquet-mr_f8029_0() throws Exception
{    SimpleRecord simple = new SimpleRecord();    MessageType schema = new MessageType("schema", new GroupType(Type.Repetition.REPEATED, "repeat-group", new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.INT64, "flat-int"), new PrimitiveType(Type.Repetition.REPEATED, PrimitiveType.PrimitiveTypeName.DOUBLE, "repeat-double")));    SimpleRecord repeatGroup = new SimpleRecord();    repeatGroup.values.add(kv("flat-int", 76543));    repeatGroup.values.add(kv("repeat-double", 1.2345));    repeatGroup.values.add(kv("repeat-double", 5.6789));    repeatGroup.values.add(kv("repeat-double", 10.11121314));    repeatGroup.values.add(kv("repeat-double", 0.4321));    repeatGroup.values.add(kv("repeat-double", 7.6543));    simple.values.add(kv("repeat-group", repeatGroup));    repeatGroup = new SimpleRecord();    repeatGroup.values.add(kv("flat-int", 12345));    repeatGroup.values.add(kv("repeat-double", 1.1));    repeatGroup.values.add(kv("repeat-double", 1.2));    repeatGroup.values.add(kv("repeat-double", 1.3));    repeatGroup.values.add(kv("repeat-double", 1.4));    repeatGroup.values.add(kv("repeat-double", 1.5));    simple.values.add(kv("repeat-group", repeatGroup));    repeatGroup = new SimpleRecord();    repeatGroup.values.add(kv("flat-int", 10293));    repeatGroup.values.add(kv("repeat-double", 9.5));    repeatGroup.values.add(kv("repeat-double", 9.4));    repeatGroup.values.add(kv("repeat-double", 9.3));    repeatGroup.values.add(kv("repeat-double", 9.2));    repeatGroup.values.add(kv("repeat-double", 9.1));    simple.values.add(kv("repeat-group", repeatGroup));    String actual = JsonRecordFormatter.fromSchema(schema).formatRecord(simple);    String expected = asJsonString(obj(entry("repeat-group", array(obj(entry("flat-int", 76543), entry("repeat-double", array(1.2345, 5.6789, 10.11121314, 0.4321, 7.6543))), obj(entry("flat-int", 12345), entry("repeat-double", array(1.1, 1.2, 1.3, 1.4, 1.5))), obj(entry("flat-int", 10293), entry("repeat-double", array(9.5, 9.4, 9.3, 9.2, 9.1)))))));    assertEquals(expected, actual);}
public String parquet-mr_f8030_0()
{    return "TestRecord {" + x + "," + y + "}";}
public void parquet-mr_f8031_0()
{    SimpleMapRecord r = new SimpleMapRecord();    Assert.assertEquals("null", r.keyToString(null));    Assert.assertEquals("[true, false, true]", r.keyToString(new boolean[] { true, false, true }));    Assert.assertEquals("[a, z]", r.keyToString(new char[] { 'a', 'z' }));    Assert.assertEquals("[1.0, 3.0]", r.keyToString(new double[] { 1.0, 3.0 }));    Assert.assertEquals("[2.0, 4.0]", r.keyToString(new float[] { 2.0f, 4.0f }));    Assert.assertEquals("[100, 999]", r.keyToString(new int[] { 100, 999 }));    Assert.assertEquals("[23, 37]", r.keyToString(new long[] { 23l, 37l }));    Assert.assertEquals("[-1, -2]", r.keyToString(new short[] { (short) -1, (short) -2 }));    Assert.assertEquals("dGVzdA==", r.keyToString("test".getBytes()));    Assert.assertEquals("TestRecord {222,333}", r.keyToString(new TestRecord(222, 333)));}
public String parquet-mr_f8032_0()
{    return "TestRecord {" + x + "," + y + "}";}
public void parquet-mr_f8033_0()
{    SimpleMapRecord r = new SimpleMapRecord();    Assert.assertEquals("null", r.keyToString(null));    Assert.assertEquals("true", r.keyToString(true));    Assert.assertEquals("a", r.keyToString('a'));    Assert.assertEquals("3.0", r.keyToString(3.0));    Assert.assertEquals("4.0", r.keyToString(4.0f));    Assert.assertEquals("100", r.keyToString(100));    Assert.assertEquals("37", r.keyToString(37l));    Assert.assertEquals("-1", r.keyToString((short) -1));    Assert.assertEquals("test", r.keyToString("test"));    Assert.assertEquals("123.123", r.keyToString(new BigDecimal("123.123")));}
public void parquet-mr_f8034_0() throws IOException
{    try (ParquetReader<SimpleRecord> reader = ParquetReader.builder(new SimpleReadSupport(), new Path(testFile().getAbsolutePath())).build()) {        for (SimpleRecord record = reader.read(); record != null; record = reader.read()) {            for (SimpleRecord.NameValue value : record.getValues()) {                switch(value.getName()) {                    case INT32_FIELD:                        Assert.assertEquals(32, value.getValue());                        break;                    case INT64_FIELD:                        Assert.assertEquals(64L, value.getValue());                        break;                    case FLOAT_FIELD:                        Assert.assertEquals(1.0f, value.getValue());                        break;                    case DOUBLE_FIELD:                        Assert.assertEquals(2.0d, value.getValue());                        break;                    case BINARY_FIELD:                        Assert.assertArrayEquals("foobar".getBytes(), (byte[]) value.getValue());                        break;                    case FIXED_LEN_BYTE_ARRAY_FIELD:                        Assert.assertArrayEquals(new byte[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 }, (byte[]) value.getValue());                        break;                }            }        }    }}
public void parquet-mr_f8035_0() throws IOException
{    createTestParquetFile();}
private static MessageType parquet-mr_f8036_0()
{    return new MessageType("schema", new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.INT32, INT32_FIELD), new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.INT64, INT64_FIELD), new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.FLOAT, FLOAT_FIELD), new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.DOUBLE, DOUBLE_FIELD), new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.BINARY, BINARY_FIELD), new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, 12, FIXED_LEN_BYTE_ARRAY_FIELD));}
private void parquet-mr_f8037_0() throws IOException
{    Path fsPath = new Path(testFile().getPath());    Configuration conf = new Configuration();    MessageType schema = createSchema();    SimpleGroupFactory fact = new SimpleGroupFactory(schema);    GroupWriteSupport.setSchema(schema, conf);    try (ParquetWriter<Group> writer = new ParquetWriter<>(fsPath, new GroupWriteSupport(), CompressionCodecName.UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetProperties.WriterVersion.PARQUET_2_0, conf)) {        writer.write(fact.newGroup().append(INT32_FIELD, 32).append(INT64_FIELD, 64L).append(FLOAT_FIELD, 1.0f).append(DOUBLE_FIELD, 2.0d).append(BINARY_FIELD, Binary.fromString("foobar")).append(FIXED_LEN_BYTE_ARRAY_FIELD, Binary.fromConstantByteArray(new byte[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 })));    }}
private File parquet-mr_f8038_0()
{    return new File(this.tempFolder.getRoot(), getClass().getSimpleName() + ".parquet");}
