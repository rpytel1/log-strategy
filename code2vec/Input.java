public String beam_f0_0(PipelineOptions options)
{    return options.getJobName().replace('-', '_');}
public String beam_f1_0(PipelineOptions options)
{    return "projects/" + options.as(GcpOptions.class).getProject() + "/subscriptions/" + options.getJobName();}
public String beam_f2_0(PipelineOptions options)
{    return "projects/" + options.as(GcpOptions.class).getProject() + "/topics/" + options.getJobName();}
public void beam_f3_0() throws IOException
{    Sleeper sleeper = Sleeper.DEFAULT;    BackOff backOff = FluentBackoff.DEFAULT.withMaxRetries(3).withInitialBackoff(Duration.millis(200)).backoff();    Throwable lastException = null;    try {        do {            try {                setupPubsub();                setupBigQueryTable();                return;            } catch (GoogleJsonResponseException e) {                lastException = e;            }        } while (BackOffUtils.next(sleeper, backOff));    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        }    throw new RuntimeException(lastException);}
public void beam_f4_0() throws IOException
{    ExamplePubsubTopicAndSubscriptionOptions pubsubOptions = options.as(ExamplePubsubTopicAndSubscriptionOptions.class);    if (!pubsubOptions.getPubsubTopic().isEmpty()) {        pendingMessages.add("**********************Set Up Pubsub************************");        setupPubsubTopic(pubsubOptions.getPubsubTopic());        pendingMessages.add("The Pub/Sub topic has been set up for this example: " + pubsubOptions.getPubsubTopic());        if (!pubsubOptions.getPubsubSubscription().isEmpty()) {            setupPubsubSubscription(pubsubOptions.getPubsubTopic(), pubsubOptions.getPubsubSubscription());            pendingMessages.add("The Pub/Sub subscription has been set up for this example: " + pubsubOptions.getPubsubSubscription());        }    }}
public void beam_f5_0() throws IOException
{    ExampleBigQueryTableOptions bigQueryTableOptions = options.as(ExampleBigQueryTableOptions.class);    if (bigQueryTableOptions.getBigQueryDataset() != null && bigQueryTableOptions.getBigQueryTable() != null && bigQueryTableOptions.getBigQuerySchema() != null) {        pendingMessages.add("******************Set Up Big Query Table*******************");        setupBigQueryTable(bigQueryTableOptions.getProject(), bigQueryTableOptions.getBigQueryDataset(), bigQueryTableOptions.getBigQueryTable(), bigQueryTableOptions.getBigQuerySchema());        pendingMessages.add("The BigQuery table has been set up for this example: " + bigQueryTableOptions.getProject() + ":" + bigQueryTableOptions.getBigQueryDataset() + "." + bigQueryTableOptions.getBigQueryTable());    }}
private void beam_f6_0()
{    pendingMessages.add("*************************Tear Down*************************");    ExamplePubsubTopicAndSubscriptionOptions pubsubOptions = options.as(ExamplePubsubTopicAndSubscriptionOptions.class);    if (!pubsubOptions.getPubsubTopic().isEmpty()) {        try {            deletePubsubTopic(pubsubOptions.getPubsubTopic());            pendingMessages.add("The Pub/Sub topic has been deleted: " + pubsubOptions.getPubsubTopic());        } catch (IOException e) {            pendingMessages.add("Failed to delete the Pub/Sub topic : " + pubsubOptions.getPubsubTopic());        }        if (!pubsubOptions.getPubsubSubscription().isEmpty()) {            try {                deletePubsubSubscription(pubsubOptions.getPubsubSubscription());                pendingMessages.add("The Pub/Sub subscription has been deleted: " + pubsubOptions.getPubsubSubscription());            } catch (IOException e) {                pendingMessages.add("Failed to delete the Pub/Sub subscription : " + pubsubOptions.getPubsubSubscription());            }        }    }    ExampleBigQueryTableOptions bigQueryTableOptions = options.as(ExampleBigQueryTableOptions.class);    if (bigQueryTableOptions.getBigQueryDataset() != null && bigQueryTableOptions.getBigQueryTable() != null && bigQueryTableOptions.getBigQuerySchema() != null) {        pendingMessages.add("The BigQuery table might contain the example's output, " + "and it is not deleted automatically: " + bigQueryTableOptions.getProject() + ":" + bigQueryTableOptions.getBigQueryDataset() + "." + bigQueryTableOptions.getBigQueryTable());        pendingMessages.add("Please go to the Developers Console to delete it manually." + " Otherwise, you may be charged for its usage.");    }}
private static Bigquery.Builder beam_f7_0(BigQueryOptions options)
{    return new Bigquery.Builder(Transport.getTransport(), Transport.getJsonFactory(), chainHttpRequestInitializer(options.getGcpCredential(),     new RetryHttpRequestInitializer(ImmutableList.of(404)))).setApplicationName(options.getAppName()).setGoogleClientRequestInitializer(options.getGoogleApiTrace());}
private static Pubsub.Builder beam_f8_0(PubsubOptions options)
{    return new Pubsub.Builder(Transport.getTransport(), Transport.getJsonFactory(), chainHttpRequestInitializer(options.getGcpCredential(),     new RetryHttpRequestInitializer(ImmutableList.of(404)))).setRootUrl(options.getPubsubRootUrl()).setApplicationName(options.getAppName()).setGoogleClientRequestInitializer(options.getGoogleApiTrace());}
private static HttpRequestInitializer beam_f9_0(Credentials credential, HttpRequestInitializer httpRequestInitializer)
{    if (credential == null) {        return new ChainingHttpRequestInitializer(new NullCredentialInitializer(), httpRequestInitializer);    } else {        return new ChainingHttpRequestInitializer(new HttpCredentialsAdapter(credential), httpRequestInitializer);    }}
private void beam_f10_0(String projectId, String datasetId, String tableId, TableSchema schema) throws IOException
{    if (bigQueryClient == null) {        bigQueryClient = newBigQueryClient(options.as(BigQueryOptions.class)).build();    }    Datasets datasetService = bigQueryClient.datasets();    if (executeNullIfNotFound(datasetService.get(projectId, datasetId)) == null) {        Dataset newDataset = new Dataset().setDatasetReference(new DatasetReference().setProjectId(projectId).setDatasetId(datasetId));        datasetService.insert(projectId, newDataset).execute();    }    Tables tableService = bigQueryClient.tables();    Table table = executeNullIfNotFound(tableService.get(projectId, datasetId, tableId));    if (table == null) {        Table newTable = new Table().setSchema(schema).setTableReference(new TableReference().setProjectId(projectId).setDatasetId(datasetId).setTableId(tableId));        tableService.insert(projectId, datasetId, newTable).execute();    } else if (!table.getSchema().equals(schema)) {        throw new RuntimeException("Table exists and schemas do not match, expecting: " + schema.toPrettyString() + ", actual: " + table.getSchema().toPrettyString());    }}
private void beam_f11_0(String topic) throws IOException
{    if (pubsubClient == null) {        pubsubClient = newPubsubClient(options.as(PubsubOptions.class)).build();    }    if (executeNullIfNotFound(pubsubClient.projects().topics().get(topic)) == null) {        pubsubClient.projects().topics().create(topic, new Topic().setName(topic)).execute();    }}
private void beam_f12_0(String topic, String subscription) throws IOException
{    if (pubsubClient == null) {        pubsubClient = newPubsubClient(options.as(PubsubOptions.class)).build();    }    if (executeNullIfNotFound(pubsubClient.projects().subscriptions().get(subscription)) == null) {        Subscription subInfo = new Subscription().setAckDeadlineSeconds(60).setTopic(topic);        pubsubClient.projects().subscriptions().create(subscription, subInfo).execute();    }}
private void beam_f13_0(String topic) throws IOException
{    if (pubsubClient == null) {        pubsubClient = newPubsubClient(options.as(PubsubOptions.class)).build();    }    if (executeNullIfNotFound(pubsubClient.projects().topics().get(topic)) != null) {        pubsubClient.projects().topics().delete(topic).execute();    }}
private void beam_f14_0(String subscription) throws IOException
{    if (pubsubClient == null) {        pubsubClient = newPubsubClient(options.as(PubsubOptions.class)).build();    }    if (executeNullIfNotFound(pubsubClient.projects().subscriptions().get(subscription)) != null) {        pubsubClient.projects().subscriptions().delete(subscription).execute();    }}
public void beam_f15_0(PipelineResult result)
{    pipelinesToCancel.add(result);    if (!options.as(ExampleOptions.class).getKeepJobsRunning()) {        addShutdownHook(pipelinesToCancel);    }    try {        result.waitUntilFinish();    } catch (UnsupportedOperationException e) {                        tearDown();        printPendingMessages();    } catch (Exception e) {        throw new RuntimeException("Failed to wait the pipeline until finish: " + result);    }}
private void beam_f16_0(final Collection<PipelineResult> pipelineResults)
{    Runtime.getRuntime().addShutdownHook(new Thread(() -> {        tearDown();        printPendingMessages();        for (PipelineResult pipelineResult : pipelineResults) {            try {                pipelineResult.cancel();            } catch (IOException e) {                System.out.println("Failed to cancel the job.");                System.out.println(e.getMessage());            }        }        for (PipelineResult pipelineResult : pipelineResults) {            boolean cancellationVerified = false;            for (int retryAttempts = 6; retryAttempts > 0; retryAttempts--) {                if (pipelineResult.getState().isTerminal()) {                    cancellationVerified = true;                    break;                } else {                    System.out.println("The example pipeline is still running. Verifying the cancellation.");                }                Uninterruptibles.sleepUninterruptibly(10, TimeUnit.SECONDS);            }            if (!cancellationVerified) {                System.out.println("Failed to verify the cancellation for job: " + pipelineResult);            }        }    }));}
private void beam_f17_0()
{    System.out.println();    System.out.println("***********************************************************");    System.out.println("***********************************************************");    for (String message : pendingMessages) {        System.out.println(message);    }    System.out.println("***********************************************************");    System.out.println("***********************************************************");}
private static T beam_f18_0(AbstractGoogleClientRequest<T> request) throws IOException
{    try {        return request.execute();    } catch (GoogleJsonResponseException e) {        if (e.getStatusCode() == SC_NOT_FOUND) {            return null;        } else {            throw e;        }    }}
public PDone beam_f19_0(PCollection<String> input)
{    ResourceId resource = FileBasedSink.convertToFileResourceIfPossible(filenamePrefix);    TextIO.Write write = TextIO.write().to(new PerWindowFiles(resource)).withTempDirectory(resource.getCurrentDirectory()).withWindowedWrites();    if (numShards != null) {        write = write.withNumShards(numShards);    }    return input.apply(write);}
public String beam_f20_0(IntervalWindow window)
{    String prefix = baseFilename.isDirectory() ? "" : firstNonNull(baseFilename.getFilename(), "");    return String.format("%s-%s-%s", prefix, FORMATTER.print(window.start()), FORMATTER.print(window.end()));}
public ResourceId beam_f21_0(int shardNumber, int numShards, BoundedWindow window, PaneInfo paneInfo, OutputFileHints outputFileHints)
{    IntervalWindow intervalWindow = (IntervalWindow) window;    String filename = String.format("%s-%s-of-%s%s", filenamePrefixForWindow(intervalWindow), shardNumber, numShards, outputFileHints.getSuggestedFilenameSuffix());    return baseFilename.getCurrentDirectory().resolve(filename, StandardResolveOptions.RESOLVE_FILE);}
public ResourceId beam_f22_0(int shardNumber, int numShards, OutputFileHints outputFileHints)
{    throw new UnsupportedOperationException("Unsupported.");}
public static ComputeTopCompletions beam_f23_0(int candidatesPerPrefix, boolean recursive)
{    return new ComputeTopCompletions(candidatesPerPrefix, recursive);}
public PCollection<KV<String, List<CompletionCandidate>>> beam_f24_0(PCollection<String> input)
{    PCollection<CompletionCandidate> candidates = input.apply(Count.perElement()).apply("CreateCompletionCandidates", ParDo.of(new DoFn<KV<String, Long>, CompletionCandidate>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(new CompletionCandidate(c.element().getKey(), c.element().getValue()));        }    }));        if (recursive) {        return candidates.apply(new ComputeTopRecursive(candidatesPerPrefix, 1)).apply(Flatten.pCollections());    } else {        return candidates.apply(new ComputeTopFlat(candidatesPerPrefix, 1));    }}
public void beam_f25_0(ProcessContext c)
{    c.output(new CompletionCandidate(c.element().getKey(), c.element().getValue()));}
public PCollection<KV<String, List<CompletionCandidate>>> beam_f26_0(PCollection<CompletionCandidate> input)
{    return input.apply(ParDo.of(new AllPrefixes(minPrefix))).apply(Top.<String, CompletionCandidate>largestPerKey(candidatesPerPrefix).withHotKeyFanout(new HotKeyFanout()));}
public Integer beam_f27_0(String input)
{    return (int) Math.pow(4, 5 - input.length());}
public int beam_f28_0(KV<String, List<CompletionCandidate>> elem, int numPartitions)
{    return elem.getKey().length() > minPrefix ? 0 : 1;}
public void beam_f29_0(ProcessContext c)
{    for (CompletionCandidate cc : c.element().getValue()) {        c.output(cc);    }}
public PCollectionList<KV<String, List<CompletionCandidate>>> beam_f30_0(PCollection<CompletionCandidate> input)
{    if (minPrefix > 10) {                return input.apply(new ComputeTopFlat(candidatesPerPrefix, minPrefix)).apply(Partition.of(2, new KeySizePartitionFn()));    } else {                                PCollectionList<KV<String, List<CompletionCandidate>>> larger = input.apply(new ComputeTopRecursive(candidatesPerPrefix, minPrefix + 1));                PCollection<KV<String, List<CompletionCandidate>>> small = PCollectionList.of(larger.get(1).apply(ParDo.of(new FlattenTops()))).and(input.apply(Filter.by(c -> c.getValue().length() == minPrefix))).apply("FlattenSmall", Flatten.pCollections()).apply(ParDo.of(new AllPrefixes(minPrefix, minPrefix))).apply(Top.largestPerKey(candidatesPerPrefix));        PCollection<KV<String, List<CompletionCandidate>>> flattenLarger = larger.apply("FlattenLarge", Flatten.pCollections());        return PCollectionList.of(flattenLarger).and(small);    }}
public void beam_f31_0(ProcessContext c)
{    String word = c.element().value;    for (int i = minPrefix; i <= Math.min(word.length(), maxPrefix); i++) {        c.output(KV.of(word.substring(0, i), c.element()));    }}
public long beam_f32_0()
{    return count;}
public String beam_f33_0()
{    return value;}
public int beam_f34_0(CompletionCandidate o)
{    if (this.count < o.count) {        return -1;    } else if (this.count == o.count) {        return this.value.compareTo(o.value);    } else {        return 1;    }}
public boolean beam_f35_0(Object other)
{    if (other instanceof CompletionCandidate) {        CompletionCandidate that = (CompletionCandidate) other;        return this.count == that.count && this.value.equals(that.value);    } else {        return false;    }}
public int beam_f36_0()
{    return Long.valueOf(count).hashCode() ^ value.hashCode();}
public String beam_f37_0()
{    return "CompletionCandidate[" + value + ", " + count + "]";}
public void beam_f38_0(ProcessContext c)
{    Matcher m = Pattern.compile("#\\S+").matcher(c.element());    while (m.find()) {        c.output(m.group().substring(1));    }}
public void beam_f39_0(ProcessContext c)
{    List<TableRow> completions = new ArrayList<>();    for (CompletionCandidate cc : c.element().getValue()) {        completions.add(new TableRow().set("count", cc.getCount()).set("tag", cc.getValue()));    }    TableRow row = new TableRow().set("prefix", c.element().getKey()).set("tags", completions);    c.output(row);}
 static TableSchema beam_f40_0()
{    List<TableFieldSchema> tagFields = new ArrayList<>();    tagFields.add(new TableFieldSchema().setName("count").setType("INTEGER"));    tagFields.add(new TableFieldSchema().setName("tag").setType("STRING"));    List<TableFieldSchema> fields = new ArrayList<>();    fields.add(new TableFieldSchema().setName("prefix").setType("STRING"));    fields.add(new TableFieldSchema().setName("tags").setType("RECORD").setMode("REPEATED").setFields(tagFields));    return new TableSchema().setFields(fields);}
public void beam_f41_0(ProcessContext c)
{    Entity.Builder entityBuilder = Entity.newBuilder();    Key key = makeKey(makeKey(kind, ancestorKey).build(), kind, c.element().getKey()).build();    entityBuilder.setKey(key);    List<Value> candidates = new ArrayList<>();    Map<String, Value> properties = new HashMap<>();    for (CompletionCandidate tag : c.element().getValue()) {        Entity.Builder tagEntity = Entity.newBuilder();        properties.put("tag", makeValue(tag.value).build());        properties.put("count", makeValue(tag.count).build());        candidates.add(makeValue(tagEntity).build());    }    properties.put("candidates", makeValue(candidates).build());    entityBuilder.putAllProperties(properties);    c.output(entityBuilder.build());}
public static void beam_f42_0(Options options) throws IOException
{    options.setBigQuerySchema(FormatForBigquery.getSchema());    ExampleUtils exampleUtils = new ExampleUtils(options);            WindowFn<Object, ?> windowFn;    if (options.isStreaming()) {        checkArgument(!options.getOutputToDatastore(), "DatastoreIO is not supported in streaming.");        windowFn = SlidingWindows.of(Duration.standardMinutes(30)).every(Duration.standardSeconds(5));    } else {        windowFn = new GlobalWindows();    }        Pipeline p = Pipeline.create(options);    PCollection<KV<String, List<CompletionCandidate>>> toWrite = p.apply(TextIO.read().from(options.getInputFile())).apply(ParDo.of(new ExtractHashtags())).apply(Window.into(windowFn)).apply(ComputeTopCompletions.top(10, options.getRecursive()));    if (options.getOutputToDatastore()) {        toWrite.apply("FormatForDatastore", ParDo.of(new FormatForDatastore(options.getKind(), options.getDatastoreAncestorKey()))).apply(DatastoreIO.v1().write().withProjectId(MoreObjects.firstNonNull(options.getOutputProject(), options.getProject())));    }    if (options.getOutputToBigQuery()) {        exampleUtils.setupBigQueryTable();        TableReference tableRef = new TableReference();        tableRef.setProjectId(options.getProject());        tableRef.setDatasetId(options.getBigQueryDataset());        tableRef.setTableId(options.getBigQueryTable());        toWrite.apply(ParDo.of(new FormatForBigquery())).apply(BigQueryIO.writeTableRows().to(tableRef).withSchema(FormatForBigquery.getSchema()).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(options.isStreaming() ? BigQueryIO.Write.WriteDisposition.WRITE_APPEND : BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));    }    if (options.getOutputToChecksum()) {        PCollection<Long> checksum = toWrite.apply(ParDo.of(new DoFn<KV<String, List<CompletionCandidate>>, Long>() {            @ProcessElement            public void process(ProcessContext c) {                KV<String, List<CompletionCandidate>> elm = c.element();                Long listHash = c.element().getValue().stream().mapToLong(cc -> cc.hashCode()).sum();                c.output(Long.valueOf(elm.getKey().hashCode()) + listHash);            }        })).apply(Sum.longsGlobally());        PAssert.that(checksum).containsInAnyOrder(options.getExpectedChecksum());    }        PipelineResult result = p.run();        exampleUtils.waitToFinish(result);}
public void beam_f43_0(ProcessContext c)
{    KV<String, List<CompletionCandidate>> elm = c.element();    Long listHash = c.element().getValue().stream().mapToLong(cc -> cc.hashCode()).sum();    c.output(Long.valueOf(elm.getKey().hashCode()) + listHash);}
public static void beam_f44_0(String[] args) throws IOException
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    runAutocompletePipeline(options);}
public PCollection<KV<String, Integer>> beam_f45_1(PCollection<KV<String, Integer>> userScores)
{        PCollection<KV<String, Integer>> sumScores = userScores.apply("UserSum", Sum.integersPerKey());        final PCollectionView<Double> globalMeanScore = sumScores.apply(Values.create()).apply(Mean.<Integer>globally().asSingletonView());        PCollection<KV<String, Integer>> filtered = sumScores.apply("ProcessAndFilter", ParDo.of(new DoFn<KV<String, Integer>, KV<String, Integer>>() {        private final Counter numSpammerUsers = Metrics.counter("main", "SpammerUsers");        @ProcessElement        public void processElement(ProcessContext c) {            Integer score = c.element().getValue();            Double gmc = c.sideInput(globalMeanScore);            if (score > (gmc * SCORE_WEIGHT)) {                                numSpammerUsers.inc();                c.output(c.element());            }        }    }).withSideInputs(globalMeanScore));    return filtered;}
public void beam_f46_1(ProcessContext c)
{    Integer score = c.element().getValue();    Double gmc = c.sideInput(globalMeanScore);    if (score > (gmc * SCORE_WEIGHT)) {                numSpammerUsers.inc();        c.output(c.element());    }}
public void beam_f47_0(ProcessContext c, BoundedWindow window)
{    IntervalWindow w = (IntervalWindow) window;    int duration = new Duration(w.start(), w.end()).toPeriod().toStandardMinutes().getMinutes();    c.output(duration);}
protected static Map<String, WriteWindowedToBigQuery.FieldInfo<KV<String, Integer>>> beam_f48_0()
{    Map<String, WriteWindowedToBigQuery.FieldInfo<KV<String, Integer>>> tableConfigure = new HashMap<>();    tableConfigure.put("team", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> c.element().getKey()));    tableConfigure.put("total_score", new WriteWindowedToBigQuery.FieldInfo<>("INTEGER", (c, w) -> c.element().getValue()));    tableConfigure.put("window_start", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> {        IntervalWindow window = (IntervalWindow) w;        return GameConstants.DATE_TIME_FORMATTER.print(window.start());    }));    tableConfigure.put("processing_time", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> GameConstants.DATE_TIME_FORMATTER.print(Instant.now())));    return tableConfigure;}
protected static Map<String, WriteWindowedToBigQuery.FieldInfo<Double>> beam_f49_0()
{    Map<String, WriteWindowedToBigQuery.FieldInfo<Double>> tableConfigure = new HashMap<>();    tableConfigure.put("window_start", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> {        IntervalWindow window = (IntervalWindow) w;        return GameConstants.DATE_TIME_FORMATTER.print(window.start());    }));    tableConfigure.put("mean_duration", new WriteWindowedToBigQuery.FieldInfo<>("FLOAT", (c, w) -> c.element()));    return tableConfigure;}
public static void beam_f50_0(String[] args) throws Exception
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);        options.setStreaming(true);    ExampleUtils exampleUtils = new ExampleUtils(options);    Pipeline pipeline = Pipeline.create(options);        PCollection<GameActionInfo> rawEvents = pipeline.apply(PubsubIO.readStrings().withTimestampAttribute(GameConstants.TIMESTAMP_ATTRIBUTE).fromTopic(options.getTopic())).apply("ParseGameEvent", ParDo.of(new ParseEventFn()));        PCollection<KV<String, Integer>> userEvents = rawEvents.apply("ExtractUserScore", MapElements.into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers())).via((GameActionInfo gInfo) -> KV.of(gInfo.getUser(), gInfo.getScore())));            final PCollectionView<Map<String, Integer>> spammersView = userEvents.apply("FixedWindowsUser", Window.into(FixedWindows.of(Duration.standardMinutes(options.getFixedWindowDuration())))).apply("CalculateSpammyUsers", new CalculateSpammyUsers()).apply("CreateSpammersView", View.asMap());                        rawEvents.apply("WindowIntoFixedWindows", Window.into(FixedWindows.of(Duration.standardMinutes(options.getFixedWindowDuration())))).apply("FilterOutSpammers", ParDo.of(new DoFn<GameActionInfo, GameActionInfo>() {        @ProcessElement        public void processElement(ProcessContext c) {                        if (c.sideInput(spammersView).get(c.element().getUser().trim()) == null) {                c.output(c.element());            }        }    }).withSideInputs(spammersView)).apply("ExtractTeamScore", new ExtractAndSumScore("team")).apply("WriteTeamSums", new WriteWindowedToBigQuery<>(options.as(GcpOptions.class).getProject(), options.getDataset(), options.getGameStatsTablePrefix() + "_team", configureWindowedWrite()));                        userEvents.apply("WindowIntoSessions", Window.<KV<String, Integer>>into(Sessions.withGapDuration(Duration.standardMinutes(options.getSessionGap()))).withTimestampCombiner(TimestampCombiner.END_OF_WINDOW)).apply(Combine.perKey(x -> 0)).apply("UserSessionActivity", ParDo.of(new UserSessionInfoFn())).apply("WindowToExtractSessionMean", Window.into(FixedWindows.of(Duration.standardMinutes(options.getUserActivityWindowDuration())))).apply(Mean.<Integer>globally().withoutDefaults()).apply("WriteAvgSessionLength", new WriteWindowedToBigQuery<>(options.as(GcpOptions.class).getProject(), options.getDataset(), options.getGameStatsTablePrefix() + "_sessions", configureSessionWindowWrite()));                PipelineResult result = pipeline.run();    exampleUtils.waitToFinish(result);}
public void beam_f51_0(ProcessContext c)
{        if (c.sideInput(spammersView).get(c.element().getUser().trim()) == null) {        c.output(c.element());    }}
protected static Map<String, WriteToText.FieldFn<KV<String, Integer>>> beam_f52_0()
{    Map<String, WriteToText.FieldFn<KV<String, Integer>>> config = new HashMap<>();    config.put("team", (c, w) -> c.element().getKey());    config.put("total_score", (c, w) -> c.element().getValue());    config.put("window_start", (c, w) -> {        IntervalWindow window = (IntervalWindow) w;        return GameConstants.DATE_TIME_FORMATTER.print(window.start());    });    return config;}
public static void beam_f53_0(String[] args) throws Exception
{        Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    Pipeline pipeline = Pipeline.create(options);    final Instant stopMinTimestamp = new Instant(minFmt.parseMillis(options.getStopMin()));    final Instant startMinTimestamp = new Instant(minFmt.parseMillis(options.getStartMin()));        pipeline.apply(TextIO.read().from(options.getInput())).apply("ParseGameEvent", ParDo.of(new ParseEventFn())).apply("FilterStartTime", Filter.by((GameActionInfo gInfo) -> gInfo.getTimestamp() > startMinTimestamp.getMillis())).apply("FilterEndTime", Filter.by((GameActionInfo gInfo) -> gInfo.getTimestamp() < stopMinTimestamp.getMillis())).apply("AddEventTimestamps", WithTimestamps.of((GameActionInfo i) -> new Instant(i.getTimestamp()))).apply("FixedWindowsTeam", Window.into(FixedWindows.of(Duration.standardMinutes(options.getWindowDuration())))).apply("ExtractTeamScore", new ExtractAndSumScore("team")).apply("WriteTeamScoreSums", new WriteToText<>(options.getOutput(), configureOutput(), true));    pipeline.run().waitUntilFinish();}
 String beam_f54_0()
{    return teamName;}
 String beam_f55_0()
{    return robot;}
 long beam_f56_0()
{    return startTimeInMillis;}
 long beam_f57_0()
{    return startTimeInMillis + (expirationPeriod * 60L * 1000L);}
 String beam_f58_0()
{    int userNum = random.nextInt(numMembers);    return "user" + userNum + "_" + teamName;}
 int beam_f59_0()
{    return numMembers;}
public String beam_f60_0()
{    return "(" + teamName + ", num members: " + numMembers() + ", starting at: " + startTimeInMillis + ", expires in: " + expirationPeriod + ", robot: " + robot + ")";}
private static String beam_f61_0(ArrayList<String> list)
{    int index = random.nextInt(list.size());    return list.get(index);}
private static TeamInfo beam_f62_0(ArrayList<TeamInfo> list)
{    int index = random.nextInt(list.size());    TeamInfo team = list.get(index);        long currTime = System.currentTimeMillis();    if ((team.getEndTimeInMillis() < currTime) || team.numMembers() == 0) {        System.out.println("\nteam " + team + " is too old; replacing.");        System.out.println("start time: " + team.getStartTimeInMillis() + ", end time: " + team.getEndTimeInMillis() + ", current time:" + currTime);        removeTeam(index);                return addLiveTeam();    } else {        return team;    }}
private static synchronized TeamInfo beam_f63_0()
{    String teamName = randomElement(COLORS) + randomElement(ANIMALS);    String robot = null;        if (random.nextInt(ROBOT_PROBABILITY) == 0) {        robot = "Robot-" + random.nextInt(NUM_ROBOTS);    }        TeamInfo newTeam = new TeamInfo(teamName, System.currentTimeMillis(), robot);    liveTeams.add(newTeam);    System.out.println("[+" + newTeam + "]");    return newTeam;}
private static synchronized void beam_f64_0(int teamIndex)
{    TeamInfo removedTeam = liveTeams.remove(teamIndex);    System.out.println("[-" + removedTeam + "]");}
private static String beam_f65_0(Long currTime, int delayInMillis)
{    TeamInfo team = randomTeam(liveTeams);    String teamName = team.getTeamName();    String user;    final int parseErrorRate = 900000;    String robot = team.getRobot();        if (robot != null) {                if (random.nextInt(team.numMembers() / 2) == 0) {            user = robot;        } else {            user = team.getRandomUser();        }    } else {                user = team.getRandomUser();    }    String event = user + "," + teamName + "," + random.nextInt(MAX_SCORE);        if (random.nextInt(parseErrorRate) == 0) {        System.out.println("Introducing a parse error.");        event = "THIS LINE REPRESENTS CORRUPT DATA AND WILL CAUSE A PARSE ERROR";    }    return addTimeInfoToEvent(event, currTime, delayInMillis);}
private static String beam_f66_0(String message, Long currTime, int delayInMillis)
{    String eventTimeString = Long.toString((currTime - delayInMillis) / 1000 * 1000);        String dateString = GameConstants.DATE_TIME_FORMATTER.print(currTime);    message = message + "," + eventTimeString + "," + dateString;    return message;}
public static void beam_f67_0(int numMessages, int delayInMillis) throws IOException
{    List<PubsubMessage> pubsubMessages = new ArrayList<>();    for (int i = 0; i < Math.max(1, numMessages); i++) {        Long currTime = System.currentTimeMillis();        String message = generateEvent(currTime, delayInMillis);        PubsubMessage pubsubMessage = new PubsubMessage().encodeData(message.getBytes("UTF-8"));        pubsubMessage.setAttributes(ImmutableMap.of(GameConstants.TIMESTAMP_ATTRIBUTE, Long.toString((currTime - delayInMillis) / 1000 * 1000)));        if (delayInMillis != 0) {            System.out.println(pubsubMessage.getAttributes());            System.out.println("late data for: " + message);        }        pubsubMessages.add(pubsubMessage);    }    PublishRequest publishRequest = new PublishRequest();    publishRequest.setMessages(pubsubMessages);    pubsub.projects().topics().publish(topic, publishRequest).execute();}
public static void beam_f68_0(String fileName, int numMessages, int delayInMillis) throws IOException
{    PrintWriter out = new PrintWriter(new OutputStreamWriter(new BufferedOutputStream(new FileOutputStream(fileName, true)), "UTF-8"));    try {        for (int i = 0; i < Math.max(1, numMessages); i++) {            Long currTime = System.currentTimeMillis();            String message = generateEvent(currTime, delayInMillis);            out.println(message);        }    } catch (Exception e) {        System.err.print("Error in writing generated events to file");        e.printStackTrace();    } finally {        out.flush();        out.close();    }}
public static void beam_f69_0(String[] args) throws IOException, InterruptedException
{    if (args.length < 3) {        System.out.println("Usage: Injector project-name (topic-name|none) (filename|none)");        System.exit(1);    }    boolean writeToFile = false;    boolean writeToPubsub = true;    project = args[0];    String topicName = args[1];    String fileName = args[2];        if ("none".equalsIgnoreCase(topicName)) {        writeToFile = true;        writeToPubsub = false;    }    if (writeToPubsub) {                pubsub = InjectorUtils.getClient();                topic = InjectorUtils.getFullyQualifiedTopicName(project, topicName);        InjectorUtils.createTopic(pubsub, topic);        System.out.println("Injecting to topic: " + topic);    } else {        if ("none".equalsIgnoreCase(fileName)) {            System.out.println("Filename not specified.");            System.exit(1);        }        System.out.println("Writing to file: " + fileName);    }    System.out.println("Starting Injector");        while (liveTeams.size() < NUM_LIVE_TEAMS) {        addLiveTeam();    }        for (int i = 0; true; i++) {        if (Thread.activeCount() > 10) {            System.err.println("I'm falling behind!");        }                final int numMessages;        final int delayInMillis;        if (i % LATE_DATA_RATE == 0) {                        delayInMillis = BASE_DELAY_IN_MILLIS + random.nextInt(FUZZY_DELAY_IN_MILLIS);            numMessages = 1;            System.out.println("DELAY(" + delayInMillis + ", " + numMessages + ")");        } else {            System.out.print(".");            delayInMillis = 0;            numMessages = MIN_QPS + random.nextInt(QPS_RANGE);        }        if (writeToFile) {                        publishDataToFile(fileName, numMessages, delayInMillis);        } else {                                    new Thread(() -> {                try {                    publishData(numMessages, delayInMillis);                } catch (IOException e) {                    System.err.println(e);                }            }).start();        }                Thread.sleep(THREAD_SLEEP_MS);    }}
public static Pubsub beam_f70_0(final HttpTransport httpTransport, final JsonFactory jsonFactory) throws IOException
{    checkNotNull(httpTransport);    checkNotNull(jsonFactory);    GoogleCredential credential = GoogleCredential.getApplicationDefault(httpTransport, jsonFactory);    if (credential.createScopedRequired()) {        credential = credential.createScoped(PubsubScopes.all());    }    if (credential.getClientAuthentication() != null) {        System.out.println("\n***Warning! You are not using service account credentials to " + "authenticate.\nYou need to use service account credentials for this example," + "\nsince user-level credentials do not have enough pubsub quota,\nand so you will run " + "out of PubSub quota very quickly.\nSee " + "https://developers.google.com/identity/protocols/application-default-credentials.");        System.exit(1);    }    HttpRequestInitializer initializer = new RetryHttpInitializerWrapper(credential);    return new Pubsub.Builder(httpTransport, jsonFactory, initializer).setApplicationName(APP_NAME).build();}
public static Pubsub beam_f71_0() throws IOException
{    return getClient(Utils.getDefaultTransport(), Utils.getDefaultJsonFactory());}
public static String beam_f72_0(final String project, final String topic)
{    return String.format("projects/%s/topics/%s", project, topic);}
public static void beam_f73_0(Pubsub client, String fullTopicName) throws IOException
{    System.out.println("fullTopicName " + fullTopicName);    try {        client.projects().topics().get(fullTopicName).execute();    } catch (GoogleJsonResponseException e) {        if (e.getStatusCode() == HttpStatusCodes.STATUS_CODE_NOT_FOUND) {            Topic topic = client.projects().topics().create(fullTopicName, new Topic()).execute();            System.out.printf("Topic %s was created.%n", topic.getName());        }    }}
public final void beam_f74_1(final HttpRequest request)
{        request.setReadTimeout(2 * ONEMINITUES);    final HttpUnsuccessfulResponseHandler backoffHandler = new HttpBackOffUnsuccessfulResponseHandler(new ExponentialBackOff()).setSleeper(sleeper);    request.setInterceptor(wrappedCredential);    request.setUnsuccessfulResponseHandler((request1, response, supportsRetry) -> {        if (wrappedCredential.handleResponse(request1, response, supportsRetry)) {                        return true;        } else if (backoffHandler.handleResponse(request1, response, supportsRetry)) {                                    return true;        } else {            return false;        }    });    request.setIOExceptionHandler(new HttpBackOffIOExceptionHandler(new ExponentialBackOff()).setSleeper(sleeper));}
protected static Map<String, WriteWindowedToBigQuery.FieldInfo<KV<String, Integer>>> beam_f75_0()
{    Map<String, WriteWindowedToBigQuery.FieldInfo<KV<String, Integer>>> tableConfigure = new HashMap<>();    tableConfigure.put("team", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> c.element().getKey()));    tableConfigure.put("total_score", new WriteWindowedToBigQuery.FieldInfo<>("INTEGER", (c, w) -> c.element().getValue()));    tableConfigure.put("window_start", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> {        IntervalWindow window = (IntervalWindow) w;        return GameConstants.DATE_TIME_FORMATTER.print(window.start());    }));    tableConfigure.put("processing_time", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> GameConstants.DATE_TIME_FORMATTER.print(Instant.now())));    tableConfigure.put("timing", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> c.pane().getTiming().toString()));    return tableConfigure;}
protected static Map<String, WriteToBigQuery.FieldInfo<KV<String, Integer>>> beam_f76_0()
{    Map<String, WriteToBigQuery.FieldInfo<KV<String, Integer>>> tableConfigure = new HashMap<>();    tableConfigure.put("user", new WriteToBigQuery.FieldInfo<>("STRING", (c, w) -> c.element().getKey()));    tableConfigure.put("total_score", new WriteToBigQuery.FieldInfo<>("INTEGER", (c, w) -> c.element().getValue()));    return tableConfigure;}
protected static Map<String, WriteToBigQuery.FieldInfo<KV<String, Integer>>> beam_f77_0()
{    Map<String, WriteToBigQuery.FieldInfo<KV<String, Integer>>> tableConfigure = configureBigQueryWrite();    tableConfigure.put("processing_time", new WriteToBigQuery.FieldInfo<>("STRING", (c, w) -> GameConstants.DATE_TIME_FORMATTER.print(Instant.now())));    return tableConfigure;}
public static void beam_f78_0(String[] args) throws Exception
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);        options.setStreaming(true);    ExampleUtils exampleUtils = new ExampleUtils(options);    Pipeline pipeline = Pipeline.create(options);            PCollection<GameActionInfo> gameEvents = pipeline.apply(PubsubIO.readStrings().withTimestampAttribute(GameConstants.TIMESTAMP_ATTRIBUTE).fromTopic(options.getTopic())).apply("ParseGameEvent", ParDo.of(new ParseEventFn()));    gameEvents.apply("CalculateTeamScores", new CalculateTeamScores(Duration.standardMinutes(options.getTeamWindowDuration()), Duration.standardMinutes(options.getAllowedLateness()))).apply("WriteTeamScoreSums", new WriteWindowedToBigQuery<>(options.as(GcpOptions.class).getProject(), options.getDataset(), options.getLeaderBoardTableName() + "_team", configureWindowedTableWrite()));    gameEvents.apply("CalculateUserScores", new CalculateUserScores(Duration.standardMinutes(options.getAllowedLateness()))).apply("WriteUserScoreSums", new WriteToBigQuery<>(options.as(GcpOptions.class).getProject(), options.getDataset(), options.getLeaderBoardTableName() + "_user", configureGlobalWindowBigQueryWrite()));            PipelineResult result = pipeline.run();    exampleUtils.waitToFinish(result);}
public PCollection<KV<String, Integer>> beam_f79_0(PCollection<GameActionInfo> infos)
{    return infos.apply("LeaderboardTeamFixedWindows", Window.<GameActionInfo>into(FixedWindows.of(teamWindowDuration)).triggering(AfterWatermark.pastEndOfWindow().withEarlyFirings(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(FIVE_MINUTES)).withLateFirings(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(TEN_MINUTES))).withAllowedLateness(allowedLateness).accumulatingFiredPanes()).apply("ExtractTeamScore", new ExtractAndSumScore("team"));}
public PCollection<KV<String, Integer>> beam_f80_0(PCollection<GameActionInfo> input)
{    return input.apply("LeaderboardUserGlobalWindow", Window.<GameActionInfo>into(new GlobalWindows()).triggering(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(TEN_MINUTES))).accumulatingFiredPanes().withAllowedLateness(allowedLateness)).apply("ExtractUserScore", new ExtractAndSumScore("user"));}
private static Map<String, FieldInfo<KV<String, Integer>>> beam_f81_0()
{    Map<String, WriteWindowedToBigQuery.FieldInfo<KV<String, Integer>>> tableConfigure = new HashMap<>();    tableConfigure.put("team", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> c.element().getKey()));    tableConfigure.put("total_score", new WriteWindowedToBigQuery.FieldInfo<>("INTEGER", (c, w) -> c.element().getValue()));    tableConfigure.put("processing_time", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> GameConstants.DATE_TIME_FORMATTER.print(Instant.now())));    return tableConfigure;}
public static void beam_f82_0(String[] args) throws Exception
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);        options.setStreaming(true);    ExampleUtils exampleUtils = new ExampleUtils(options);    Pipeline pipeline = Pipeline.create(options);    pipeline.apply(PubsubIO.readStrings().withTimestampAttribute(GameConstants.TIMESTAMP_ATTRIBUTE).fromTopic(options.getTopic())).apply("ParseGameEvent", ParDo.of(new ParseEventFn())).apply("MapTeamAsKey", MapElements.into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptor.of(GameActionInfo.class))).via((GameActionInfo gInfo) -> KV.of(gInfo.team, gInfo))).apply("UpdateTeamScore", ParDo.of(new UpdateTeamScoreFn(options.getThresholdScore()))).apply("WriteTeamLeaders", new WriteWindowedToBigQuery<>(options.as(GcpOptions.class).getProject(), options.getDataset(), options.getLeaderBoardTableName() + "_team_leader", configureCompleteWindowedTableWrite()));            PipelineResult result = pipeline.run();    exampleUtils.waitToFinish(result);}
public String beam_f84_0()
{    return this.user;}
public String beam_f85_0()
{    return this.team;}
public Integer beam_f86_0()
{    return this.score;}
public Long beam_f87_0()
{    return this.timestamp;}
public String beam_f88_0(String keyname)
{    if ("team".equals(keyname)) {        return this.team;    } else {                return this.user;    }}
public boolean beam_f89_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || o.getClass() != this.getClass()) {        return false;    }    GameActionInfo gameActionInfo = (GameActionInfo) o;    if (!this.getUser().equals(gameActionInfo.getUser())) {        return false;    }    if (!this.getTeam().equals(gameActionInfo.getTeam())) {        return false;    }    if (!this.getScore().equals(gameActionInfo.getScore())) {        return false;    }    return this.getTimestamp().equals(gameActionInfo.getTimestamp());}
public int beam_f90_0()
{    return Objects.hash(user, team, score, timestamp);}
public void beam_f91_1(ProcessContext c)
{    String[] components = c.element().split(",", -1);    try {        String user = components[0].trim();        String team = components[1].trim();        Integer score = Integer.parseInt(components[2].trim());        Long timestamp = Long.parseLong(components[3].trim());        GameActionInfo gInfo = new GameActionInfo(user, team, score, timestamp);        c.output(gInfo);    } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {        numParseErrors.inc();            }}
public PCollection<KV<String, Integer>> beam_f92_0(PCollection<GameActionInfo> gameInfo)
{    return gameInfo.apply(MapElements.into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers())).via((GameActionInfo gInfo) -> KV.of(gInfo.getKey(field), gInfo.getScore()))).apply(Sum.integersPerKey());}
protected static Map<String, WriteToText.FieldFn<KV<String, Integer>>> beam_f93_0()
{    Map<String, WriteToText.FieldFn<KV<String, Integer>>> config = new HashMap<>();    config.put("user", (c, w) -> c.element().getKey());    config.put("total_score", (c, w) -> c.element().getValue());    return config;}
public static void beam_f94_0(String[] args) throws Exception
{        Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    Pipeline pipeline = Pipeline.create(options);        pipeline.apply(TextIO.read().from(options.getInput())).apply("ParseGameEvent", ParDo.of(new ParseEventFn())).apply("ExtractUserScore", new ExtractAndSumScore("user")).apply("WriteUserScoreSums", new WriteToText<>(options.getOutput(), configureOutput(), false));        pipeline.run().waitUntilFinish();}
 String beam_f95_0()
{    return this.fieldType;}
 FieldFn<InputT> beam_f96_0()
{    return this.fieldFn;}
public void beam_f97_0(ProcessContext c, BoundedWindow window)
{    TableRow row = new TableRow();    for (Map.Entry<String, FieldInfo<InputT>> entry : fieldInfo.entrySet()) {        String key = entry.getKey();        FieldInfo<InputT> fcnInfo = entry.getValue();        FieldFn<InputT> fcn = fcnInfo.getFieldFn();        row.set(key, fcn.apply(c, window));    }    c.output(row);}
protected TableSchema beam_f98_0()
{    List<TableFieldSchema> fields = new ArrayList<>();    for (Map.Entry<String, FieldInfo<InputT>> entry : fieldInfo.entrySet()) {        String key = entry.getKey();        FieldInfo<InputT> fcnInfo = entry.getValue();        String bqType = fcnInfo.getFieldType();        fields.add(new TableFieldSchema().setName(key).setType(bqType));    }    return new TableSchema().setFields(fields);}
public PDone beam_f99_0(PCollection<InputT> teamAndScore)
{    teamAndScore.apply("ConvertToRow", ParDo.of(new BuildRowFn())).apply(BigQueryIO.writeTableRows().to(getTable(projectId, datasetId, tableName)).withSchema(getSchema()).withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(WriteDisposition.WRITE_APPEND));    return PDone.in(teamAndScore.getPipeline());}
 static TableReference beam_f100_0(String projectId, String datasetId, String tableName)
{    TableReference table = new TableReference();    table.setDatasetId(datasetId);    table.setProjectId(projectId);    table.setTableId(tableName);    return table;}
public void beam_f101_0(ProcessContext c, BoundedWindow window)
{    List<String> fields = new ArrayList<>();    for (Map.Entry<String, FieldFn<InputT>> entry : fieldFn.entrySet()) {        String key = entry.getKey();        FieldFn<InputT> fcn = entry.getValue();        fields.add(key + ": " + fcn.apply(c, window));    }    String result = fields.stream().collect(Collectors.joining(", "));    c.output(result);}
public PDone beam_f102_0(PCollection<String> input)
{        checkArgument(input.getWindowingStrategy().getWindowFn().windowCoder() == IntervalWindow.getCoder());    ResourceId resource = FileBasedSink.convertToFileResourceIfPossible(filenamePrefix);    return input.apply(TextIO.write().to(new PerWindowFiles(resource)).withTempDirectory(resource.getCurrentDirectory()).withWindowedWrites().withNumShards(3));}
public String beam_f103_0(IntervalWindow window)
{    String filePrefix = prefix.isDirectory() ? "" : prefix.getFilename();    return String.format("%s-%s-%s", filePrefix, formatter.print(window.start()), formatter.print(window.end()));}
public ResourceId beam_f104_0(int shardNumber, int numShards, BoundedWindow window, PaneInfo paneInfo, OutputFileHints outputFileHints)
{    IntervalWindow intervalWindow = (IntervalWindow) window;    String filename = String.format("%s-%s-of-%s%s", filenamePrefixForWindow(intervalWindow), shardNumber, numShards, outputFileHints.getSuggestedFilenameSuffix());    return prefix.getCurrentDirectory().resolve(filename, StandardResolveOptions.RESOLVE_FILE);}
public ResourceId beam_f105_0(int shardNumber, int numShards, OutputFileHints outputFileHints)
{    throw new UnsupportedOperationException("Unsupported.");}
public PDone beam_f106_0(PCollection<InputT> teamAndScore)
{    if (windowed) {        teamAndScore.apply("ConvertToRow", ParDo.of(new BuildRowFn())).apply(new WriteToText.WriteOneFilePerWindow(filenamePrefix));    } else {        teamAndScore.apply("ConvertToRow", ParDo.of(new BuildRowFn())).apply(TextIO.write().to(filenamePrefix));    }    return PDone.in(teamAndScore.getPipeline());}
public void beam_f107_0(ProcessContext c, BoundedWindow window)
{    TableRow row = new TableRow();    for (Map.Entry<String, FieldInfo<T>> entry : fieldInfo.entrySet()) {        String key = entry.getKey();        FieldInfo<T> fcnInfo = entry.getValue();        row.set(key, fcnInfo.getFieldFn().apply(c, window));    }    c.output(row);}
public PDone beam_f108_0(PCollection<T> teamAndScore)
{    teamAndScore.apply("ConvertToRow", ParDo.of(new BuildRowFn())).apply(BigQueryIO.writeTableRows().to(getTable(projectId, datasetId, tableName)).withSchema(getSchema()).withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(WriteDisposition.WRITE_APPEND));    return PDone.in(teamAndScore.getPipeline());}
public void beam_f109_0(ProcessContext c)
{    String[] words = c.element().split(ExampleUtils.TOKENIZER_PATTERN, -1);    for (String word : words) {        if (!word.isEmpty()) {            c.output(word);        }    }}
public void beam_f110_0(ProcessContext c)
{    c.output(c.element().toUpperCase());}
public void beam_f111_0(ProcessContext c)
{    c.output(new TableRow().set("string_field", c.element()));}
 static TableSchema beam_f112_0()
{    return new TableSchema().setFields(Collections.singletonList(new TableFieldSchema().setName("string_field").setType("STRING")));}
public static void beam_f113_0(String[] args) throws IOException
{    StreamingWordExtractOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(StreamingWordExtractOptions.class);    options.setStreaming(true);    options.setBigQuerySchema(StringToRowConverter.getSchema());    ExampleUtils exampleUtils = new ExampleUtils(options);    exampleUtils.setup();    Pipeline pipeline = Pipeline.create(options);    String tableSpec = new StringBuilder().append(options.getProject()).append(":").append(options.getBigQueryDataset()).append(".").append(options.getBigQueryTable()).toString();    pipeline.apply("ReadLines", TextIO.read().from(options.getInputFile())).apply(ParDo.of(new ExtractWords())).apply(ParDo.of(new Uppercase())).apply(ParDo.of(new StringToRowConverter())).apply(BigQueryIO.writeTableRows().to(tableSpec).withSchema(StringToRowConverter.getSchema()));    PipelineResult result = pipeline.run();        exampleUtils.waitToFinish(result);}
public static Set<URI> beam_f114_0(Options options) throws URISyntaxException, IOException
{    URI baseUri = new URI(options.getInput());        URI absoluteUri;    if (baseUri.getScheme() != null) {        absoluteUri = baseUri;    } else {        absoluteUri = new URI("file", baseUri.getAuthority(), baseUri.getPath(), baseUri.getQuery(), baseUri.getFragment());    }    Set<URI> uris = new HashSet<>();    if ("file".equals(absoluteUri.getScheme())) {        File directory = new File(absoluteUri);        for (String entry : Optional.fromNullable(directory.list()).or(new String[] {})) {            File path = new File(directory, entry);            uris.add(path.toURI());        }    } else if ("gs".equals(absoluteUri.getScheme())) {        GcsUtil gcsUtil = options.as(GcsOptions.class).getGcsUtil();        URI gcsUriGlob = new URI(absoluteUri.getScheme(), absoluteUri.getAuthority(), absoluteUri.getPath() + "*", absoluteUri.getQuery(), absoluteUri.getFragment());        for (GcsPath entry : gcsUtil.expand(GcsPath.fromUri(gcsUriGlob))) {            uris.add(entry.toUri());        }    }    return uris;}
public PCollection<KV<URI, String>> beam_f115_0(PBegin input)
{    Pipeline pipeline = input.getPipeline();            PCollectionList<KV<URI, String>> urisToLines = PCollectionList.empty(pipeline);        for (final URI uri : uris) {        String uriString;        if ("file".equals(uri.getScheme())) {            uriString = new File(uri).getPath();        } else {            uriString = uri.toString();        }        PCollection<KV<URI, String>> oneUriToLines = pipeline.apply("TextIO.Read(" + uriString + ")", TextIO.read().from(uriString)).apply("WithKeys(" + uriString + ")", WithKeys.of(uri)).setCoder(KvCoder.of(StringDelegateCoder.of(URI.class), StringUtf8Coder.of()));        urisToLines = urisToLines.and(oneUriToLines);    }    return urisToLines.apply(Flatten.pCollections());}
public PCollection<KV<String, KV<URI, Double>>> beam_f116_1(PCollection<KV<URI, String>> uriToContent)
{                final PCollectionView<Long> totalDocuments = uriToContent.apply("GetURIs", Keys.create()).apply("DistinctDocs", Distinct.create()).apply(Count.globally()).apply(View.asSingleton());            PCollection<KV<URI, String>> uriToWords = uriToContent.apply("SplitWords", ParDo.of(new DoFn<KV<URI, String>, KV<URI, String>>() {        @ProcessElement        public void processElement(ProcessContext c) {            URI uri = c.element().getKey();            String line = c.element().getValue();            for (String word : line.split("\\W+", -1)) {                                if ("love".equalsIgnoreCase(word)) {                                    }                if (!word.isEmpty()) {                    c.output(KV.of(uri, word.toLowerCase()));                }            }        }    }));            PCollection<KV<String, Long>> wordToDocCount = uriToWords.apply("DistinctWords", Distinct.create()).apply(Values.create()).apply("CountDocs", Count.perElement());            PCollection<KV<URI, Long>> uriToWordTotal = uriToWords.apply("GetURIs2", Keys.create()).apply("CountWords", Count.perElement());                PCollection<KV<KV<URI, String>, Long>> uriAndWordToCount = uriToWords.apply("CountWordDocPairs", Count.perElement());                    PCollection<KV<URI, KV<String, Long>>> uriToWordAndCount = uriAndWordToCount.apply("ShiftKeys", ParDo.of(new DoFn<KV<KV<URI, String>, Long>, KV<URI, KV<String, Long>>>() {        @ProcessElement        public void processElement(ProcessContext c) {            URI uri = c.element().getKey().getKey();            String word = c.element().getKey().getValue();            Long occurrences = c.element().getValue();            c.output(KV.of(uri, KV.of(word, occurrences)));        }    }));                            final TupleTag<Long> wordTotalsTag = new TupleTag<>();    final TupleTag<KV<String, Long>> wordCountsTag = new TupleTag<>();    KeyedPCollectionTuple<URI> coGbkInput = KeyedPCollectionTuple.of(wordTotalsTag, uriToWordTotal).and(wordCountsTag, uriToWordAndCount);                                    PCollection<KV<URI, CoGbkResult>> uriToWordAndCountAndTotal = coGbkInput.apply("CoGroupByUri", CoGroupByKey.create());                    PCollection<KV<String, KV<URI, Double>>> wordToUriAndTf = uriToWordAndCountAndTotal.apply("ComputeTermFrequencies", ParDo.of(new DoFn<KV<URI, CoGbkResult>, KV<String, KV<URI, Double>>>() {        @ProcessElement        public void processElement(ProcessContext c) {            URI uri = c.element().getKey();            Long wordTotal = c.element().getValue().getOnly(wordTotalsTag);            for (KV<String, Long> wordAndCount : c.element().getValue().getAll(wordCountsTag)) {                String word = wordAndCount.getKey();                Long wordCount = wordAndCount.getValue();                Double termFrequency = wordCount.doubleValue() / wordTotal.doubleValue();                c.output(KV.of(word, KV.of(uri, termFrequency)));            }        }    }));                            PCollection<KV<String, Double>> wordToDf = wordToDocCount.apply("ComputeDocFrequencies", ParDo.of(new DoFn<KV<String, Long>, KV<String, Double>>() {        @ProcessElement        public void processElement(ProcessContext c) {            String word = c.element().getKey();            Long documentCount = c.element().getValue();            Long documentTotal = c.sideInput(totalDocuments);            Double documentFrequency = documentCount.doubleValue() / documentTotal.doubleValue();            c.output(KV.of(word, documentFrequency));        }    }).withSideInputs(totalDocuments));            final TupleTag<KV<URI, Double>> tfTag = new TupleTag<>();    final TupleTag<Double> dfTag = new TupleTag<>();    PCollection<KV<String, CoGbkResult>> wordToUriAndTfAndDf = KeyedPCollectionTuple.of(tfTag, wordToUriAndTf).and(dfTag, wordToDf).apply(CoGroupByKey.create());                        PCollection<KV<String, KV<URI, Double>>> wordToUriAndTfIdf = wordToUriAndTfAndDf.apply("ComputeTfIdf", ParDo.of(new DoFn<KV<String, CoGbkResult>, KV<String, KV<URI, Double>>>() {        @ProcessElement        public void processElement(ProcessContext c) {            String word = c.element().getKey();            Double df = c.element().getValue().getOnly(dfTag);            for (KV<URI, Double> uriAndTf : c.element().getValue().getAll(tfTag)) {                URI uri = uriAndTf.getKey();                Double tf = uriAndTf.getValue();                Double tfIdf = tf * Math.log(1 / df);                c.output(KV.of(word, KV.of(uri, tfIdf)));            }        }    }));    return wordToUriAndTfIdf;}
public void beam_f117_1(ProcessContext c)
{    URI uri = c.element().getKey();    String line = c.element().getValue();    for (String word : line.split("\\W+", -1)) {                if ("love".equalsIgnoreCase(word)) {                    }        if (!word.isEmpty()) {            c.output(KV.of(uri, word.toLowerCase()));        }    }}
public void beam_f118_0(ProcessContext c)
{    URI uri = c.element().getKey().getKey();    String word = c.element().getKey().getValue();    Long occurrences = c.element().getValue();    c.output(KV.of(uri, KV.of(word, occurrences)));}
public void beam_f119_0(ProcessContext c)
{    URI uri = c.element().getKey();    Long wordTotal = c.element().getValue().getOnly(wordTotalsTag);    for (KV<String, Long> wordAndCount : c.element().getValue().getAll(wordCountsTag)) {        String word = wordAndCount.getKey();        Long wordCount = wordAndCount.getValue();        Double termFrequency = wordCount.doubleValue() / wordTotal.doubleValue();        c.output(KV.of(word, KV.of(uri, termFrequency)));    }}
public void beam_f120_0(ProcessContext c)
{    String word = c.element().getKey();    Long documentCount = c.element().getValue();    Long documentTotal = c.sideInput(totalDocuments);    Double documentFrequency = documentCount.doubleValue() / documentTotal.doubleValue();    c.output(KV.of(word, documentFrequency));}
public void beam_f121_0(ProcessContext c)
{    String word = c.element().getKey();    Double df = c.element().getValue().getOnly(dfTag);    for (KV<URI, Double> uriAndTf : c.element().getValue().getAll(tfTag)) {        URI uri = uriAndTf.getKey();        Double tf = uriAndTf.getValue();        Double tfIdf = tf * Math.log(1 / df);        c.output(KV.of(word, KV.of(uri, tfIdf)));    }}
public PDone beam_f122_0(PCollection<KV<String, KV<URI, Double>>> wordToUriAndTfIdf)
{    return wordToUriAndTfIdf.apply("Format", ParDo.of(new DoFn<KV<String, KV<URI, Double>>, String>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(String.format("%s,\t%s,\t%f", c.element().getKey(), c.element().getValue().getKey(), c.element().getValue().getValue()));        }    })).apply(TextIO.write().to(output).withSuffix(".csv"));}
public void beam_f123_0(ProcessContext c)
{    c.output(String.format("%s,\t%s,\t%f", c.element().getKey(), c.element().getValue().getKey(), c.element().getValue().getValue()));}
 static void beam_f124_0(Options options) throws Exception
{    Pipeline pipeline = Pipeline.create(options);    pipeline.getCoderRegistry().registerCoderForClass(URI.class, StringDelegateCoder.of(URI.class));    pipeline.apply(new ReadDocuments(listInputDocuments(options))).apply(new ComputeTfIdf()).apply(new WriteTfIdf(options.getOutput()));    pipeline.run().waitUntilFinish();}
public static void beam_f125_0(String[] args) throws Exception
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    runTfIdf(options);}
public void beam_f126_0(ProcessContext c)
{    TableRow row = c.element();    int timestamp;        try {        timestamp = ((BigDecimal) row.get("timestamp")).intValue();    } catch (ClassCastException e) {        timestamp = ((Integer) row.get("timestamp")).intValue();    }    String userName = (String) row.get("contributor_username");    if (userName != null) {                c.outputWithTimestamp(userName, new Instant(timestamp * 1000L));    }}
public PCollection<KV<String, Long>> beam_f127_0(PCollection<String> actions)
{    return actions.apply(Window.into(Sessions.withGapDuration(Duration.standardHours(1)))).apply(Count.perElement());}
public PCollection<List<KV<String, Long>>> beam_f128_0(PCollection<KV<String, Long>> sessions)
{    SerializableComparator<KV<String, Long>> comparator = (o1, o2) -> ComparisonChain.start().compare(o1.getValue(), o2.getValue()).compare(o1.getKey(), o2.getKey()).result();    return sessions.apply(Window.into(CalendarWindows.months(1))).apply(Top.of(1, comparator).withoutDefaults());}
public void beam_f129_0(ProcessContext c, BoundedWindow window)
{    c.output(KV.of(c.element().getKey() + " : " + window, c.element().getValue()));}
public void beam_f130_0(ProcessContext c, BoundedWindow window)
{    for (KV<String, Long> item : c.element()) {        String session = item.getKey();        long count = item.getValue();        c.output(session + " : " + count + " : " + ((IntervalWindow) window).start());    }}
public TableRow beam_f131_0(String input)
{    try {        return Transport.getJsonFactory().fromString(input, TableRow.class);    } catch (IOException e) {        throw new RuntimeException("Failed parsing table row json", e);    }}
public PCollection<String> beam_f132_0(PCollection<TableRow> input)
{    return input.apply(ParDo.of(new ExtractUserAndTimestamp())).apply("SampleUsers", ParDo.of(new DoFn<String, String>() {        @ProcessElement        public void processElement(ProcessContext c) {            if (Math.abs((long) c.element().hashCode()) <= Integer.MAX_VALUE * samplingThreshold) {                c.output(c.element());            }        }    })).apply(new ComputeSessions()).apply("SessionsToStrings", ParDo.of(new SessionsToStringsDoFn())).apply(new TopPerMonth()).apply("FormatOutput", ParDo.of(new FormatOutputDoFn()));}
public void beam_f133_0(ProcessContext c)
{    if (Math.abs((long) c.element().hashCode()) <= Integer.MAX_VALUE * samplingThreshold) {        c.output(c.element());    }}
public static void beam_f134_0(Options options)
{    Pipeline p = Pipeline.create(options);    double samplingThreshold = 0.1;    p.apply(TextIO.read().from(options.getWikiInput())).apply(MapElements.via(new ParseTableRowJson())).apply(new ComputeTopSessions(samplingThreshold)).apply("Write", TextIO.write().to(options.getOutput()));    p.run().waitUntilFinish();}
public static void beam_f135_0(String[] args)
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    run(options);}
public String beam_f136_0()
{    return this.stationId;}
public String beam_f137_0()
{    return this.lane;}
public String beam_f138_0()
{    return this.direction;}
public String beam_f139_0()
{    return this.freeway;}
public String beam_f140_0()
{    return this.recordedTimestamp;}
public Integer beam_f141_0()
{    return this.laneFlow;}
public Double beam_f142_0()
{    return this.laneAO;}
public Double beam_f143_0()
{    return this.laneAS;}
public Integer beam_f144_0()
{    return this.totalFlow;}
public void beam_f145_0(DoFn<String, String>.ProcessContext c) throws Exception
{    String[] items = c.element().split(",", -1);    if (items.length > 0) {        try {            String timestamp = items[0];            c.outputWithTimestamp(c.element(), new Instant(dateTimeFormat.parseMillis(timestamp)));        } catch (IllegalArgumentException e) {                }    }}
public void beam_f146_0(ProcessContext c)
{    String[] items = c.element().split(",", -1);    if (items.length < 48) {                return;    }        String timestamp = items[0];    String stationId = items[1];    String freeway = items[2];    String direction = items[3];    Integer totalFlow = tryIntParse(items[7]);    for (int i = 1; i <= 8; ++i) {        Integer laneFlow = tryIntParse(items[6 + 5 * i]);        Double laneAvgOccupancy = tryDoubleParse(items[7 + 5 * i]);        Double laneAvgSpeed = tryDoubleParse(items[8 + 5 * i]);        if (laneFlow == null || laneAvgOccupancy == null || laneAvgSpeed == null) {            return;        }        LaneInfo laneInfo = new LaneInfo(stationId, "lane" + i, direction, freeway, timestamp, laneFlow, laneAvgOccupancy, laneAvgSpeed, totalFlow);        c.output(KV.of(stationId, laneInfo));    }}
public LaneInfo beam_f147_0(Iterable<LaneInfo> input)
{    Integer max = 0;    LaneInfo maxInfo = new LaneInfo();    for (LaneInfo item : input) {        Integer flow = item.getLaneFlow();        if (flow != null && (flow >= max)) {            max = flow;            maxInfo = item;        }    }    return maxInfo;}
public void beam_f148_0(ProcessContext c)
{    LaneInfo laneInfo = c.element().getValue();    TableRow row = new TableRow().set("station_id", c.element().getKey()).set("direction", laneInfo.getDirection()).set("freeway", laneInfo.getFreeway()).set("lane_max_flow", laneInfo.getLaneFlow()).set("lane", laneInfo.getLane()).set("avg_occ", laneInfo.getLaneAO()).set("avg_speed", laneInfo.getLaneAS()).set("total_flow", laneInfo.getTotalFlow()).set("recorded_timestamp", laneInfo.getRecordedTimestamp()).set("window_timestamp", c.timestamp().toString());    c.output(row);}
 static TableSchema beam_f149_0()
{    List<TableFieldSchema> fields = new ArrayList<>();    fields.add(new TableFieldSchema().setName("station_id").setType("STRING"));    fields.add(new TableFieldSchema().setName("direction").setType("STRING"));    fields.add(new TableFieldSchema().setName("freeway").setType("STRING"));    fields.add(new TableFieldSchema().setName("lane_max_flow").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("lane").setType("STRING"));    fields.add(new TableFieldSchema().setName("avg_occ").setType("FLOAT"));    fields.add(new TableFieldSchema().setName("avg_speed").setType("FLOAT"));    fields.add(new TableFieldSchema().setName("total_flow").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("window_timestamp").setType("TIMESTAMP"));    fields.add(new TableFieldSchema().setName("recorded_timestamp").setType("STRING"));    TableSchema schema = new TableSchema().setFields(fields);    return schema;}
public PCollection<TableRow> beam_f150_0(PCollection<KV<String, LaneInfo>> flowInfo)
{        PCollection<KV<String, LaneInfo>> flowMaxes = flowInfo.apply(Combine.perKey(new MaxFlow()));        PCollection<TableRow> results = flowMaxes.apply(ParDo.of(new FormatMaxesFn()));    return results;}
public PCollection<String> beam_f151_0(PBegin begin)
{    return begin.apply(TextIO.read().from(inputFile)).apply(ParDo.of(new ExtractTimestamps()));}
public static void beam_f152_0(TrafficMaxLaneFlowOptions options) throws IOException
{        ExampleUtils exampleUtils = new ExampleUtils(options);    exampleUtils.setup();    Pipeline pipeline = Pipeline.create(options);    TableReference tableRef = new TableReference();    tableRef.setProjectId(options.getProject());    tableRef.setDatasetId(options.getBigQueryDataset());    tableRef.setTableId(options.getBigQueryTable());    pipeline.apply("ReadLines", new ReadFileAndExtractTimestamps(options.getInputFile())).apply(ParDo.of(new ExtractFlowInfoFn())).apply(Window.into(SlidingWindows.of(Duration.standardMinutes(options.getWindowDuration())).every(Duration.standardMinutes(options.getWindowSlideEvery())))).apply(new MaxLaneFlow()).apply(BigQueryIO.writeTableRows().to(tableRef).withSchema(FormatMaxesFn.getSchema()));        PipelineResult result = pipeline.run();        exampleUtils.waitToFinish(result);}
public static void beam_f153_0(String[] args) throws IOException
{    TrafficMaxLaneFlowOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(TrafficMaxLaneFlowOptions.class);    options.setBigQuerySchema(FormatMaxesFn.getSchema());    runTrafficMaxLaneFlow(options);}
private static Integer beam_f154_0(String number)
{    try {        return Integer.parseInt(number);    } catch (NumberFormatException e) {        return null;    }}
private static Double beam_f155_0(String number)
{    try {        return Double.parseDouble(number);    } catch (NumberFormatException e) {        return null;    }}
public String beam_f156_0()
{    return this.stationId;}
public Double beam_f157_0()
{    return this.avgSpeed;}
public int beam_f158_0(StationSpeed other)
{    return Long.compare(this.timestamp, other.timestamp);}
public boolean beam_f159_0(Object object)
{    if (object == null) {        return false;    }    if (object.getClass() != getClass()) {        return false;    }    StationSpeed otherStationSpeed = (StationSpeed) object;    return Objects.equals(this.timestamp, otherStationSpeed.timestamp);}
public int beam_f160_0()
{    return this.timestamp.hashCode();}
public String beam_f161_0()
{    return this.route;}
public Double beam_f162_0()
{    return this.avgSpeed;}
public Boolean beam_f163_0()
{    return this.slowdownEvent;}
public void beam_f164_0(DoFn<String, String>.ProcessContext c) throws Exception
{    String[] items = c.element().split(",");    String timestamp = tryParseTimestamp(items);    if (timestamp != null) {        try {            c.outputWithTimestamp(c.element(), new Instant(dateTimeFormat.parseMillis(timestamp)));        } catch (IllegalArgumentException e) {                }    }}
public void beam_f165_0(ProcessContext c)
{    String[] items = c.element().split(",");    String stationType = tryParseStationType(items);        if ("ML".equals(stationType)) {        Double avgSpeed = tryParseAvgSpeed(items);        String stationId = tryParseStationId(items);                if (avgSpeed != null && stationId != null && sdStations.containsKey(stationId)) {            StationSpeed stationSpeed = new StationSpeed(stationId, avgSpeed, c.timestamp().getMillis());                        KV<String, StationSpeed> outputValue = KV.of(sdStations.get(stationId), stationSpeed);            c.output(outputValue);        }    }}
public void beam_f166_0(ProcessContext c) throws IOException
{    String route = c.element().getKey();    double speedSum = 0.0;    int speedCount = 0;    int speedups = 0;    int slowdowns = 0;    List<StationSpeed> infoList = Lists.newArrayList(c.element().getValue());        Collections.sort(infoList);    Map<String, Double> prevSpeeds = new HashMap<>();        for (StationSpeed item : infoList) {        Double speed = item.getAvgSpeed();        if (speed != null) {            speedSum += speed;            speedCount++;            Double lastSpeed = prevSpeeds.get(item.getStationId());            if (lastSpeed != null) {                if (lastSpeed < speed) {                    speedups += 1;                } else {                    slowdowns += 1;                }            }            prevSpeeds.put(item.getStationId(), speed);        }    }    if (speedCount == 0) {                return;    }    double speedAvg = speedSum / speedCount;    boolean slowdownEvent = slowdowns >= 2 * speedups;    RouteInfo routeInfo = new RouteInfo(route, speedAvg, slowdownEvent);    c.output(KV.of(route, routeInfo));}
public void beam_f167_0(ProcessContext c)
{    RouteInfo routeInfo = c.element().getValue();    TableRow row = new TableRow().set("avg_speed", routeInfo.getAvgSpeed()).set("slowdown_event", routeInfo.getSlowdownEvent()).set("route", c.element().getKey()).set("window_timestamp", c.timestamp().toString());    c.output(row);}
 static TableSchema beam_f168_0()
{    List<TableFieldSchema> fields = new ArrayList<>();    fields.add(new TableFieldSchema().setName("route").setType("STRING"));    fields.add(new TableFieldSchema().setName("avg_speed").setType("FLOAT"));    fields.add(new TableFieldSchema().setName("slowdown_event").setType("BOOLEAN"));    fields.add(new TableFieldSchema().setName("window_timestamp").setType("TIMESTAMP"));    return new TableSchema().setFields(fields);}
public PCollection<TableRow> beam_f169_0(PCollection<KV<String, StationSpeed>> stationSpeed)
{            PCollection<KV<String, Iterable<StationSpeed>>> timeGroup = stationSpeed.apply(GroupByKey.create());        PCollection<KV<String, RouteInfo>> stats = timeGroup.apply(ParDo.of(new GatherStats()));        PCollection<TableRow> results = stats.apply(ParDo.of(new FormatStatsFn()));    return results;}
public PCollection<String> beam_f170_0(PBegin begin)
{    return begin.apply(TextIO.read().from(inputFile)).apply(ParDo.of(new ExtractTimestamps()));}
public static void beam_f171_0(TrafficRoutesOptions options) throws IOException
{        ExampleUtils exampleUtils = new ExampleUtils(options);    exampleUtils.setup();    Pipeline pipeline = Pipeline.create(options);    TableReference tableRef = new TableReference();    tableRef.setProjectId(options.getProject());    tableRef.setDatasetId(options.getBigQueryDataset());    tableRef.setTableId(options.getBigQueryTable());    pipeline.apply("ReadLines", new ReadFileAndExtractTimestamps(options.getInputFile())).apply(ParDo.of(new ExtractStationSpeedFn())).apply(Window.into(SlidingWindows.of(Duration.standardMinutes(options.getWindowDuration())).every(Duration.standardMinutes(options.getWindowSlideEvery())))).apply(new TrackSpeed()).apply(BigQueryIO.writeTableRows().to(tableRef).withSchema(FormatStatsFn.getSchema()));        PipelineResult result = pipeline.run();        exampleUtils.waitToFinish(result);}
public static void beam_f172_0(String[] args) throws IOException
{    TrafficRoutesOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(TrafficRoutesOptions.class);    options.setBigQuerySchema(FormatStatsFn.getSchema());    runTrafficRoutes(options);}
private static Double beam_f173_0(String[] inputItems)
{    try {        return Double.parseDouble(tryParseString(inputItems, 9));    } catch (NumberFormatException | NullPointerException e) {        return null;    }}
private static String beam_f174_0(String[] inputItems)
{    return tryParseString(inputItems, 4);}
private static String beam_f175_0(String[] inputItems)
{    return tryParseString(inputItems, 1);}
private static String beam_f176_0(String[] inputItems)
{    return tryParseString(inputItems, 0);}
private static String beam_f177_0(String[] inputItems, int index)
{    return inputItems.length > index ? inputItems[index] : null;}
private static Map<String, String> beam_f178_0()
{    Map<String, String> stations = new LinkedHashMap<>();        stations.put("1108413", "SDRoute1");        stations.put("1108699", "SDRoute2");    stations.put("1108702", "SDRoute2");    return stations;}
public void beam_f179_0(ProcessContext c)
{    TableRow row = c.element();    if ((Boolean) row.get("tornado")) {        c.output(Integer.parseInt((String) row.get("month")));    }}
public void beam_f180_0(ProcessContext c)
{    TableRow row = new TableRow().set("month", c.element().getKey()).set("tornado_count", c.element().getValue());    c.output(row);}
public PCollection<TableRow> beam_f181_0(PCollection<TableRow> rows)
{        PCollection<Integer> tornadoes = rows.apply(ParDo.of(new ExtractTornadoesFn()));        PCollection<KV<Integer, Long>> tornadoCounts = tornadoes.apply(Count.perElement());        PCollection<TableRow> results = tornadoCounts.apply(ParDo.of(new FormatCountsFn()));    return results;}
 static void beam_f182_0(Options options)
{    Pipeline p = Pipeline.create(options);        List<TableFieldSchema> fields = new ArrayList<>();    fields.add(new TableFieldSchema().setName("month").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("tornado_count").setType("INTEGER"));    TableSchema schema = new TableSchema().setFields(fields);    PCollection<TableRow> rowsFromBigQuery;    switch(options.getReadMethod()) {        case DIRECT_READ:            rowsFromBigQuery = p.apply(BigQueryIO.readTableRows().from(options.getInput()).withMethod(Method.DIRECT_READ).withSelectedFields(Lists.newArrayList("month", "tornado")));            break;        default:            rowsFromBigQuery = p.apply(BigQueryIO.readTableRows().from(options.getInput()).withMethod(options.getReadMethod()));            break;    }    rowsFromBigQuery.apply(new CountTornadoes()).apply(BigQueryIO.writeTableRows().to(options.getOutput()).withSchema(schema).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));    p.run().waitUntilFinish();}
public static void beam_f183_0(String[] args)
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    runBigQueryTornadoes(options);}
public void beam_f184_0(ProcessContext c)
{    TableRow row = c.element();    String playName = (String) row.get("corpus");    String word = (String) row.get("word");    if (word.length() >= MIN_WORD_LENGTH) {        c.output(KV.of(word, playName));    } else {                        smallerWords.inc();    }}
public void beam_f185_0(ProcessContext c)
{    TableRow row = new TableRow().set("word", c.element().getKey()).set("all_plays", c.element().getValue());    c.output(row);}
public PCollection<TableRow> beam_f186_0(PCollection<TableRow> rows)
{        PCollection<KV<String, String>> words = rows.apply(ParDo.of(new ExtractLargeWordsFn()));        PCollection<KV<String, String>> wordAllPlays = words.apply(Combine.perKey(new ConcatWords()));        PCollection<TableRow> results = wordAllPlays.apply(ParDo.of(new FormatShakespeareOutputFn()));    return results;}
public String beam_f187_0(Iterable<String> input)
{    StringBuilder all = new StringBuilder();    for (String item : input) {        if (!item.isEmpty()) {            if (all.length() == 0) {                all.append(item);            } else {                all.append(",");                all.append(item);            }        }    }    return all.toString();}
public static void beam_f188_0(String[] args) throws Exception
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    Pipeline p = Pipeline.create(options);        List<TableFieldSchema> fields = new ArrayList<>();    fields.add(new TableFieldSchema().setName("word").setType("STRING"));    fields.add(new TableFieldSchema().setName("all_plays").setType("STRING"));    TableSchema schema = new TableSchema().setFields(fields);    p.apply(BigQueryIO.readTableRows().from(options.getInput())).apply(new PlaysForWord()).apply(BigQueryIO.writeTableRows().to(options.getOutput()).withSchema(schema).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));    p.run().waitUntilFinish();}
public String beam_f189_0(PipelineOptions options)
{    if (options.getTempLocation() != null) {        return GcsPath.fromUri(options.getTempLocation()).resolve("deduped.txt").toString();    } else {        throw new IllegalArgumentException("Must specify --output or --tempLocation");    }}
public static void beam_f190_0(String[] args) throws Exception
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    Pipeline p = Pipeline.create(options);    p.apply("ReadLines", TextIO.read().from(options.getInput())).apply(Distinct.create()).apply("DedupedShakespeare", TextIO.write().to(options.getOutput()));    p.run().waitUntilFinish();}
public void beam_f191_0(ProcessContext c)
{    TableRow row = c.element();        Integer year = Integer.parseInt((String) row.get("year"));    Integer month = Integer.parseInt((String) row.get("month"));    Integer day = Integer.parseInt((String) row.get("day"));    Double meanTemp = Double.parseDouble(row.get("mean_temp").toString());        TableRow outRow = new TableRow().set("year", year).set("month", month).set("day", day).set("mean_temp", meanTemp);    c.output(outRow);}
public void beam_f192_0(ProcessContext c)
{    TableRow row = c.element();    Integer month;    month = (Integer) row.get("month");    if (month.equals(this.monthFilter)) {        c.output(row);    }}
public void beam_f193_0(ProcessContext c)
{    TableRow row = c.element();    Double meanTemp = Double.parseDouble(row.get("mean_temp").toString());    c.output(meanTemp);}
public PCollection<TableRow> beam_f194_0(PCollection<TableRow> rows)
{        PCollection<Double> meanTemps = rows.apply(ParDo.of(new ExtractTempFn()));            final PCollectionView<Double> globalMeanTemp = meanTemps.apply(Mean.globally()).apply(View.asSingleton());        PCollection<TableRow> monthFilteredRows = rows.apply(ParDo.of(new FilterSingleMonthDataFn(monthFilter)));                    PCollection<TableRow> filteredRows = monthFilteredRows.apply("ParseAndFilter", ParDo.of(new DoFn<TableRow, TableRow>() {        @ProcessElement        public void processElement(ProcessContext c) {            Double meanTemp = Double.parseDouble(c.element().get("mean_temp").toString());            Double gTemp = c.sideInput(globalMeanTemp);            if (meanTemp < gTemp) {                c.output(c.element());            }        }    }).withSideInputs(globalMeanTemp));    return filteredRows;}
public void beam_f195_0(ProcessContext c)
{    Double meanTemp = Double.parseDouble(c.element().get("mean_temp").toString());    Double gTemp = c.sideInput(globalMeanTemp);    if (meanTemp < gTemp) {        c.output(c.element());    }}
private static TableSchema beam_f196_0()
{    List<TableFieldSchema> fields = new ArrayList<>();    fields.add(new TableFieldSchema().setName("year").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("month").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("day").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("mean_temp").setType("FLOAT"));    return new TableSchema().setFields(fields);}
public static void beam_f197_0(String[] args) throws Exception
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    Pipeline p = Pipeline.create(options);    TableSchema schema = buildWeatherSchemaProjection();    p.apply(BigQueryIO.readTableRows().from(options.getInput())).apply(ParDo.of(new ProjectionFn())).apply(new BelowGlobalMean(options.getMonthFilter())).apply(BigQueryIO.writeTableRows().to(options.getOutput()).withSchema(schema).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));    p.run().waitUntilFinish();}
 static PCollection<String> beam_f198_0(PCollection<TableRow> eventsTable, PCollection<TableRow> countryCodes) throws Exception
{    final TupleTag<String> eventInfoTag = new TupleTag<>();    final TupleTag<String> countryInfoTag = new TupleTag<>();            PCollection<KV<String, String>> eventInfo = eventsTable.apply(ParDo.of(new ExtractEventDataFn()));    PCollection<KV<String, String>> countryInfo = countryCodes.apply(ParDo.of(new ExtractCountryInfoFn()));        PCollection<KV<String, CoGbkResult>> kvpCollection = KeyedPCollectionTuple.of(eventInfoTag, eventInfo).and(countryInfoTag, countryInfo).apply(CoGroupByKey.create());            PCollection<KV<String, String>> finalResultCollection = kvpCollection.apply("Process", ParDo.of(new DoFn<KV<String, CoGbkResult>, KV<String, String>>() {        @ProcessElement        public void processElement(ProcessContext c) {            KV<String, CoGbkResult> e = c.element();            String countryCode = e.getKey();            String countryName = "none";            countryName = e.getValue().getOnly(countryInfoTag);            for (String eventInfo : c.element().getValue().getAll(eventInfoTag)) {                                c.output(KV.of(countryCode, "Country name: " + countryName + ", Event info: " + eventInfo));            }        }    }));        PCollection<String> formattedResults = finalResultCollection.apply("Format", ParDo.of(new DoFn<KV<String, String>, String>() {        @ProcessElement        public void processElement(ProcessContext c) {            String outputstring = "Country code: " + c.element().getKey() + ", " + c.element().getValue();            c.output(outputstring);        }    }));    return formattedResults;}
public void beam_f199_0(ProcessContext c)
{    KV<String, CoGbkResult> e = c.element();    String countryCode = e.getKey();    String countryName = "none";    countryName = e.getValue().getOnly(countryInfoTag);    for (String eventInfo : c.element().getValue().getAll(eventInfoTag)) {                c.output(KV.of(countryCode, "Country name: " + countryName + ", Event info: " + eventInfo));    }}
public void beam_f200_0(ProcessContext c)
{    String outputstring = "Country code: " + c.element().getKey() + ", " + c.element().getValue();    c.output(outputstring);}
public void beam_f201_0(ProcessContext c)
{    TableRow row = c.element();    String countryCode = (String) row.get("ActionGeo_CountryCode");    String sqlDate = (String) row.get("SQLDATE");    String actor1Name = (String) row.get("Actor1Name");    String sourceUrl = (String) row.get("SOURCEURL");    String eventInfo = "Date: " + sqlDate + ", Actor1: " + actor1Name + ", url: " + sourceUrl;    c.output(KV.of(countryCode, eventInfo));}
public void beam_f202_0(ProcessContext c)
{    TableRow row = c.element();    String countryCode = (String) row.get("FIPSCC");    String countryName = (String) row.get("HumanName");    c.output(KV.of(countryCode, countryName));}
public static void beam_f203_0(String[] args) throws Exception
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    Pipeline p = Pipeline.create(options);            PCollection<TableRow> eventsTable = p.apply(BigQueryIO.readTableRows().from(GDELT_EVENTS_TABLE));    PCollection<TableRow> countryCodes = p.apply(BigQueryIO.readTableRows().from(COUNTRY_CODES));    PCollection<String> formattedResults = joinEvents(eventsTable, countryCodes);    formattedResults.apply(TextIO.write().to(options.getOutput()));    p.run().waitUntilFinish();}
public void beam_f204_0(ProcessContext c)
{    TableRow row = c.element();    Integer month = Integer.parseInt((String) row.get("month"));    Double meanTemp = Double.parseDouble(row.get("mean_temp").toString());    c.output(KV.of(month, meanTemp));}
public void beam_f205_0(ProcessContext c)
{    TableRow row = new TableRow().set("month", c.element().getKey()).set("max_mean_temp", c.element().getValue());    c.output(row);}
public PCollection<TableRow> beam_f206_0(PCollection<TableRow> rows)
{        PCollection<KV<Integer, Double>> temps = rows.apply(ParDo.of(new ExtractTempFn()));        PCollection<KV<Integer, Double>> tempMaxes = temps.apply(Max.doublesPerKey());        PCollection<TableRow> results = tempMaxes.apply(ParDo.of(new FormatMaxesFn()));    return results;}
public static void beam_f207_0(String[] args) throws Exception
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    Pipeline p = Pipeline.create(options);        List<TableFieldSchema> fields = new ArrayList<>();    fields.add(new TableFieldSchema().setName("month").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("max_mean_temp").setType("FLOAT"));    TableSchema schema = new TableSchema().setFields(fields);    p.apply(BigQueryIO.readTableRows().from(options.getInput())).apply(new MaxMeanTemp()).apply(BigQueryIO.writeTableRows().to(options.getOutput()).withSchema(schema).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));    p.run().waitUntilFinish();}
public PCollectionList<TableRow> beam_f208_0(PCollection<KV<String, Integer>> flowInfo)
{                                                                PCollection<TableRow> defaultTriggerResults = flowInfo.apply("Default", Window.<    KV<String, Integer>>into(FixedWindows.of(Duration.standardMinutes(windowDuration))).triggering(Repeatedly.forever(AfterWatermark.pastEndOfWindow())).withAllowedLateness(Duration.ZERO).discardingFiredPanes()).apply(new TotalFlow("default"));                                                                PCollection<TableRow> withAllowedLatenessResults = flowInfo.apply("WithLateData", Window.<KV<String, Integer>>into(FixedWindows.of(Duration.standardMinutes(windowDuration))).triggering(Repeatedly.forever(AfterWatermark.pastEndOfWindow())).discardingFiredPanes().withAllowedLateness(ONE_DAY)).apply(new TotalFlow("withAllowedLateness"));                                                            PCollection<TableRow> speculativeResults = flowInfo.apply("Speculative", Window.<KV<String, Integer>>into(FixedWindows.of(Duration.standardMinutes(windowDuration))).triggering(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(ONE_MINUTE))).accumulatingFiredPanes().withAllowedLateness(ONE_DAY)).apply(new TotalFlow("speculative"));                                                                            PCollection<TableRow> sequentialResults = flowInfo.apply("Sequential", Window.<KV<String, Integer>>into(FixedWindows.of(Duration.standardMinutes(windowDuration))).triggering(AfterEach.inOrder(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(ONE_MINUTE)).orFinally(AfterWatermark.pastEndOfWindow()), Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(FIVE_MINUTES)))).accumulatingFiredPanes().withAllowedLateness(ONE_DAY)).apply(new TotalFlow("sequential"));        PCollectionList<TableRow> resultsList = PCollectionList.of(defaultTriggerResults).and(withAllowedLatenessResults).and(speculativeResults).and(sequentialResults);    return resultsList;}
public PCollection<TableRow> beam_f209_0(PCollection<KV<String, Integer>> flowInfo)
{    PCollection<KV<String, Iterable<Integer>>> flowPerFreeway = flowInfo.apply(GroupByKey.create());    PCollection<KV<String, String>> results = flowPerFreeway.apply(ParDo.of(new DoFn<KV<String, Iterable<Integer>>, KV<String, String>>() {        @ProcessElement        public void processElement(ProcessContext c) throws Exception {            Iterable<Integer> flows = c.element().getValue();            Integer sum = 0;            Long numberOfRecords = 0L;            for (Integer value : flows) {                sum += value;                numberOfRecords++;            }            c.output(KV.of(c.element().getKey(), sum + "," + numberOfRecords));        }    }));    PCollection<TableRow> output = results.apply(ParDo.of(new FormatTotalFlow(triggerType)));    return output;}
public void beam_f210_0(ProcessContext c) throws Exception
{    Iterable<Integer> flows = c.element().getValue();    Integer sum = 0;    Long numberOfRecords = 0L;    for (Integer value : flows) {        sum += value;        numberOfRecords++;    }    c.output(KV.of(c.element().getKey(), sum + "," + numberOfRecords));}
public void beam_f211_0(ProcessContext c, BoundedWindow window) throws Exception
{    String[] values = c.element().getValue().split(",", -1);    TableRow row = new TableRow().set("trigger_type", triggerType).set("freeway", c.element().getKey()).set("total_flow", Integer.parseInt(values[0])).set("number_of_records", Long.parseLong(values[1])).set("window", window.toString()).set("isFirst", c.pane().isFirst()).set("isLast", c.pane().isLast()).set("timing", c.pane().getTiming().toString()).set("event_time", c.timestamp().toString()).set("processing_time", Instant.now().toString());    c.output(row);}
public void beam_f212_0(ProcessContext c) throws Exception
{    String[] laneInfo = c.element().split(",", -1);    if ("timestamp".equals(laneInfo[0])) {                return;    }    if (laneInfo.length < VALID_NUM_FIELDS) {                return;    }    String freeway = laneInfo[2];    Integer totalFlow = tryIntegerParse(laneInfo[7]);        if (totalFlow == null || totalFlow <= 0) {        return;    }    c.output(KV.of(freeway, totalFlow));}
public static void beam_f213_0(String[] args) throws Exception
{    TrafficFlowOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(TrafficFlowOptions.class);    options.setStreaming(true);    options.setBigQuerySchema(getSchema());    ExampleUtils exampleUtils = new ExampleUtils(options);    exampleUtils.setup();    Pipeline pipeline = Pipeline.create(options);    TableReference tableRef = getTableReference(options.getProject(), options.getBigQueryDataset(), options.getBigQueryTable());    PCollectionList<TableRow> resultList = pipeline.apply("ReadMyFile", TextIO.read().from(options.getInput())).apply("InsertRandomDelays", ParDo.of(new InsertDelays())).apply(ParDo.of(new ExtractFlowInfo())).apply(new CalculateTotalFlow(options.getWindowDuration()));    for (int i = 0; i < resultList.size(); i++) {        resultList.get(i).apply(BigQueryIO.writeTableRows().to(tableRef).withSchema(getSchema()));    }    PipelineResult result = pipeline.run();        exampleUtils.waitToFinish(result);}
public void beam_f214_0(ProcessContext c) throws Exception
{    Instant timestamp = Instant.now();    Random random = new Random();    if (random.nextDouble() < THRESHOLD) {        int range = MAX_DELAY - MIN_DELAY;        int delayInMinutes = random.nextInt(range) + MIN_DELAY;        long delayInMillis = TimeUnit.MINUTES.toMillis(delayInMinutes);        timestamp = new Instant(timestamp.getMillis() - delayInMillis);    }    c.outputWithTimestamp(c.element(), timestamp);}
private static TableReference beam_f215_0(String project, String dataset, String table)
{    TableReference tableRef = new TableReference();    tableRef.setProjectId(project);    tableRef.setDatasetId(dataset);    tableRef.setTableId(table);    return tableRef;}
private static TableSchema beam_f216_0()
{    List<TableFieldSchema> fields = new ArrayList<>();    fields.add(new TableFieldSchema().setName("trigger_type").setType("STRING"));    fields.add(new TableFieldSchema().setName("freeway").setType("STRING"));    fields.add(new TableFieldSchema().setName("total_flow").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("number_of_records").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("window").setType("STRING"));    fields.add(new TableFieldSchema().setName("isFirst").setType("BOOLEAN"));    fields.add(new TableFieldSchema().setName("isLast").setType("BOOLEAN"));    fields.add(new TableFieldSchema().setName("timing").setType("STRING"));    fields.add(new TableFieldSchema().setName("event_time").setType("TIMESTAMP"));    fields.add(new TableFieldSchema().setName("processing_time").setType("TIMESTAMP"));    return new TableSchema().setFields(fields);}
private static Integer beam_f217_0(String number)
{    try {        return Integer.parseInt(number);    } catch (NumberFormatException e) {        return null;    }}
public void beam_f218_1(ProcessContext c)
{    if (filter.matcher(c.element().getKey()).matches()) {                                matchedWords.inc();        c.output(c.element());    } else {                                LOG.trace("Did not match: " + c.element().getKey());        unmatchedWords.inc();    }}
 static void beam_f219_0(WordCountOptions options)
{    Pipeline p = Pipeline.create(options);    PCollection<KV<String, Long>> filteredWords = p.apply("ReadLines", TextIO.read().from(options.getInputFile())).apply(new WordCount.CountWords()).apply(ParDo.of(new FilterTextFn(options.getFilterPattern())));    /*     * Concept #3: PAssert is a set of convenient PTransforms in the style of     * Hamcrest's collection matchers that can be used when writing Pipeline level tests     * to validate the contents of PCollections. PAssert is best used in unit tests     * with small data sets but is demonstrated here as a teaching tool.     *     * <p>Below we verify that the set of filtered words matches our expected counts. Note     * that PAssert does not provide any output and that successful completion of the     * Pipeline implies that the expectations were met. Learn more at     * https://beam.apache.org/documentation/pipelines/test-your-pipeline/ on how to test     * your Pipeline and see {@link DebuggingWordCountTest} for an example unit test.     */    List<KV<String, Long>> expectedResults = Arrays.asList(KV.of("Flourish", 3L), KV.of("stomach", 1L));    PAssert.that(filteredWords).containsInAnyOrder(expectedResults);    p.run().waitUntilFinish();}
public static void beam_f220_0(String[] args)
{    WordCountOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(WordCountOptions.class);    runDebuggingWordCount(options);}
public static void beam_f221_0(String[] args)
{                    PipelineOptions options = PipelineOptionsFactory.create();                                                                Pipeline p = Pipeline.create(options);                    p.apply(TextIO.read().from("gs://apache-beam-samples/shakespeare/*")).apply(FlatMapElements.into(TypeDescriptors.strings()).via((String word) -> Arrays.asList(word.split("[^\\p{L}]+")))).apply(Filter.by((String word) -> !word.isEmpty())).apply(Count.perElement()).apply(MapElements.into(TypeDescriptors.strings()).via((KV<String, Long> wordCount) -> wordCount.getKey() + ": " + wordCount.getValue())).apply(TextIO.write().to("wordcounts"));    p.run().waitUntilFinish();}
public static void beam_f222_0(Pipeline p)
{    modelBigQueryIO(p, "", "", "");}
public static void beam_f223_0(Pipeline p, String writeProject, String writeDataset, String writeTable)
{    {                String tableSpec = "clouddataflow-readonly:samples.weather_stations";        }    {                String tableSpec = "samples.weather_stations";        }    {                TableReference tableSpec = new TableReference().setProjectId("clouddataflow-readonly").setDatasetId("samples").setTableId("weather_stations");        }    {                TableRow row = new TableRow();        row.set("string", "abc");        byte[] rawbytes = { (byte) 0xab, (byte) 0xac };        row.set("bytes", new String(Base64.getEncoder().encodeToString(rawbytes)));        row.set("integer", 5);        row.set("float", 0.5);        row.set("numeric", 5);        row.set("boolean", true);        row.set("timestamp", "2018-12-31 12:44:31.744957 UTC");        row.set("date", "2018-12-31");        row.set("time", "12:44:31");        row.set("datetime", "2019-06-11T14:44:31");        row.set("geography", "POINT(30 10)");        }    {        String tableSpec = "clouddataflow-readonly:samples.weather_stations";                PCollection<Double> maxTemperatures = p.apply(BigQueryIO.readTableRows().from(tableSpec)).apply(MapElements.into(TypeDescriptors.doubles()).via((TableRow row) -> (Double) row.get("max_temperature")));        }    {        String tableSpec = "clouddataflow-readonly:samples.weather_stations";                PCollection<Double> maxTemperatures = p.apply(BigQueryIO.read((SchemaAndRecord elem) -> (Double) elem.getRecord().get("max_temperature")).from(tableSpec).withCoder(DoubleCoder.of()));        }    {                PCollection<Double> maxTemperatures = p.apply(BigQueryIO.read((SchemaAndRecord elem) -> (Double) elem.getRecord().get("max_temperature")).fromQuery("SELECT max_temperature FROM [clouddataflow-readonly:samples.weather_stations]").withCoder(DoubleCoder.of()));        }    {                PCollection<Double> maxTemperatures = p.apply(BigQueryIO.read((SchemaAndRecord elem) -> (Double) elem.getRecord().get("max_temperature")).fromQuery("SELECT max_temperature FROM `clouddataflow-readonly.samples.weather_stations`").usingStandardSql().withCoder(DoubleCoder.of()));        }        String tableSchemaJson = "" + "{" + "  \"fields\": [" + "    {" + "      \"name\": \"source\"," + "      \"type\": \"STRING\"," + "      \"mode\": \"NULLABLE\"" + "    }," + "    {" + "      \"name\": \"quote\"," + "      \"type\": \"STRING\"," + "      \"mode\": \"REQUIRED\"" + "    }" + "  ]" + "}";        {        String tableSpec = "clouddataflow-readonly:samples.weather_stations";        if (!writeProject.isEmpty() && !writeDataset.isEmpty() && !writeTable.isEmpty()) {            tableSpec = writeProject + ":" + writeDataset + "." + writeTable;        }                TableSchema tableSchema = new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("source").setType("STRING").setMode("NULLABLE"), new TableFieldSchema().setName("quote").setType("STRING").setMode("REQUIRED")));                        /*      @DefaultCoder(AvroCoder.class)      static class Quote {        final String source;        final String quote;        public Quote() {          this.source = "";          this.quote = "";        }        public Quote(String source, String quote) {          this.source = source;          this.quote = quote;        }      }      */        PCollection<Quote> quotes = p.apply(Create.of(new Quote("Mahatma Gandhi", "My life is my message."), new Quote("Yoda", "Do, or do not. There is no 'try'.")));                        quotes.apply(MapElements.into(TypeDescriptor.of(TableRow.class)).via((Quote elem) -> new TableRow().set("source", elem.source).set("quote", elem.quote))).apply(BigQueryIO.writeTableRows().to(tableSpec).withSchema(tableSchema).withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(WriteDisposition.WRITE_TRUNCATE));                        quotes.apply(BigQueryIO.<Quote>write().to(tableSpec).withSchema(tableSchema).withFormatFunction((Quote elem) -> new TableRow().set("source", elem.source).set("quote", elem.quote)).withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(WriteDisposition.WRITE_TRUNCATE));                        quotes.apply(BigQueryIO.<Quote>write().to(tableSpec).withJsonSchema(tableSchemaJson).withFormatFunction((Quote elem) -> new TableRow().set("source", elem.source).set("quote", elem.quote)).withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(WriteDisposition.WRITE_TRUNCATE));        }    {                /*      @DefaultCoder(AvroCoder.class)      static class WeatherData {        final long year;        final long month;        final long day;        final double maxTemp;        public WeatherData() {          this.year = 0;          this.month = 0;          this.day = 0;          this.maxTemp = 0.0f;        }        public WeatherData(long year, long month, long day, double maxTemp) {          this.year = year;          this.month = month;          this.day = day;          this.maxTemp = maxTemp;        }      }      */        PCollection<WeatherData> weatherData = p.apply(BigQueryIO.read((SchemaAndRecord elem) -> {            GenericRecord record = elem.getRecord();            return new WeatherData((Long) record.get("year"), (Long) record.get("month"), (Long) record.get("day"), (Double) record.get("max_temperature"));        }).fromQuery("SELECT year, month, day, max_temperature " + "FROM [clouddataflow-readonly:samples.weather_stations] " + "WHERE year BETWEEN 2007 AND 2009").withCoder(AvroCoder.of(WeatherData.class)));                weatherData.apply(BigQueryIO.<WeatherData>write().to(new DynamicDestinations<WeatherData, Long>() {            @Override            public Long getDestination(ValueInSingleWindow<WeatherData> elem) {                return elem.getValue().year;            }            @Override            public TableDestination getTable(Long destination) {                return new TableDestination(new TableReference().setProjectId(writeProject).setDatasetId(writeDataset).setTableId(writeTable + "_" + destination), "Table for year " + destination);            }            @Override            public TableSchema getSchema(Long destination) {                return new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("year").setType("INTEGER").setMode("REQUIRED"), new TableFieldSchema().setName("month").setType("INTEGER").setMode("REQUIRED"), new TableFieldSchema().setName("day").setType("INTEGER").setMode("REQUIRED"), new TableFieldSchema().setName("maxTemp").setType("FLOAT").setMode("NULLABLE")));            }        }).withFormatFunction((WeatherData elem) -> new TableRow().set("year", elem.year).set("month", elem.month).set("day", elem.day).set("maxTemp", elem.maxTemp)).withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(WriteDisposition.WRITE_TRUNCATE));                String tableSpec = "clouddataflow-readonly:samples.weather_stations";        if (!writeProject.isEmpty() && !writeDataset.isEmpty() && !writeTable.isEmpty()) {            tableSpec = writeProject + ":" + writeDataset + "." + writeTable + "_partitioning";        }        TableSchema tableSchema = new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("year").setType("INTEGER").setMode("REQUIRED"), new TableFieldSchema().setName("month").setType("INTEGER").setMode("REQUIRED"), new TableFieldSchema().setName("day").setType("INTEGER").setMode("REQUIRED"), new TableFieldSchema().setName("maxTemp").setType("FLOAT").setMode("NULLABLE")));                weatherData.apply(BigQueryIO.<WeatherData>write().to(tableSpec + "_partitioning").withSchema(tableSchema).withFormatFunction((WeatherData elem) -> new TableRow().set("year", elem.year).set("month", elem.month).set("day", elem.day).set("maxTemp", elem.maxTemp)).withTimePartitioning(new TimePartitioning().setType("DAY")).withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(WriteDisposition.WRITE_TRUNCATE));        }}
public Long beam_f224_0(ValueInSingleWindow<WeatherData> elem)
{    return elem.getValue().year;}